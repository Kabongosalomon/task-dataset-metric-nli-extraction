<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
							<email>pengsida@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
							<email>liuyuan@cad.zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
							<email>huangqx@cs.utexas.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
							<email>bao@cad.zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
							<email>xzhou@cad.zju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the challenge of 6DoF pose estimation from a single RGB image under severe occlusion or truncation. Many recent works have shown that a two-stage approach, which first detects keypoints and then solves a Perspective-n-Point (PnP) problem for pose estimation, achieves remarkable performance. However, most of these methods only localize a set of sparse keypoints by regressing their image coordinates or heatmaps, which are sensitive to occlusion and truncation. Instead, we introduce a Pixel-wise Voting Network (PVNet) to regress pixel-wise unit vectors pointing to the keypoints and use these vectors to vote for keypoint locations using RANSAC. This creates a flexible representation for localizing occluded or truncated keypoints. Another important feature of this representation is that it provides uncertainties of keypoint locations that can be further leveraged by the PnP solver. Experiments show that the proposed approach outperforms the state of the art on the LINEMOD, Occlusion LINEMOD and YCB-Video datasets by a large margin, while being efficient for real-time pose estimation. We further create a Truncation LINEMOD dataset to validate the robustness of our approach against truncation. The code will be avaliable at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object pose estimation aims to detect objects and estimate their orientations and translations relative to a canonical frame <ref type="bibr" target="#b40">[42]</ref>. Accurate pose estimations are essential for a variety of applications such as augmented reality, autonomous driving and robotic manipulation. For instance, fast and robust pose estimation is crucial in Amazon Picking Challenge <ref type="bibr" target="#b7">[8]</ref>, where a robot needs to pick objects from * Authors contributed equally  <ref type="figure">Figure 1</ref>. The 6D pose estimation problem is formulated as a Perspective-n-Point (PnP) problem in this paper, which requires correspondences between 2D and 3D keypoints, as illustrated in (d) and (e). We predict unit vectors pointing to keypoints for each pixel, as shown in (b), and localize 2D keypoints in a RANSACbased voting scheme, as shown in (c). The proposed method is robust to occlusion (g) and truncation (h), where the green bounding boxes represent the ground truth poses and the blue bounding boxes represent our predictions. a warehouse shelf. This paper focuses on the specific setting of recovering the 6DoF pose of an object, i.e., rotation and translation in 3D, from a single RGB image of the object. This problem is quite challenging from many perspectives, including object detection under severe occlusions, variations in lighting and appearance, and cluttered background objects. Traditional methods <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b16">18]</ref> have shown that pose estimation can be achieved by establishing the correspondences between an object image and the object model. They rely on hand-crafted features, which are not robust to image variations and background clutters. Deep learning based methods <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b5">6]</ref> train end-to-end neural networks that take an image as input and output its corresponding pose. However, generalization remains as an issue, as it is unclear that such end-to-end methods learn sufficient feature representations for pose estimation.</p><p>Some recent methods <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b37">39]</ref> use CNNs to first regress 2D keypoints and then compute 6D pose parameters using the Perspective-n-Point (PnP) algorithm. In other words, the detected keypoints serve as an intermediate representation for pose estimation. Such two-stage approaches achieve state-of-the-art performance, thanks to robust detection of keypoints. However, these methods have difficulty in tackling occluded and truncated objects, since part of their keypoints are unseen. Although CNNs may predict these unseen keypoints by memorizing similar patterns, generalization remains difficult. We argue that addressing occlusion and truncation requires dense predictions, namely pixel-wise or patch-wise estimates for the final output or intermediate representations. To this end, we propose a novel framework for 6D pose estimation using a Pixel-wise Voting Network (PVNet). The basic idea is illustrated in <ref type="figure">Figure 1</ref>. Instead of directly regressing image coordinates of keypoints, PVNet predicts unit vectors that represent directions from each pixel of the object towards the keypoints. These directions then vote for the keypoint locations based on RANSAC <ref type="bibr" target="#b11">[12]</ref>. This voting scheme is motivated from a property of rigid objects that once we see some local parts, we are able to infer the relative directions to other parts.</p><p>Our approach essentially creates a vector-field representation for keypoint localization. In contrast to coordinate or heatmap based representations, learning such a representation enforces the network to focus on local features of objects and spatial relations between object parts. As a result, the location of an invisible part can be inferred from the visible parts. In addition, this vector-field representation is able to represent object keypoints that are even outside the input image. All these advantages make it an ideal representation for occluded or truncated objects. Xiang et al. <ref type="bibr" target="#b41">[43]</ref> proposed a similar idea to detect objects and here we use it to localize keypoints.</p><p>Another advantage of the proposed approach is that the dense outputs provide rich information for the PnP solver to deal with inaccurate keypoint predictions. Specifically, RANSAC-based voting prunes outlier predictions and also gives a spatial probability distribution for each keypoint. Such uncertainties of keypoint locations give the PnP solver more freedom to identify consistent correspondences for predicting the final pose. Experiments show that the uncertainty-driven PnP algorithm improves the accuracy of pose estimation.</p><p>We evaluate our approach on LINEMOD <ref type="bibr" target="#b16">[18]</ref>, Occlusion LINEMOD <ref type="bibr" target="#b2">[3]</ref> and YCB-Video <ref type="bibr" target="#b41">[43]</ref> datasets, which are widely-used benchmark datasets for 6D pose estimation. Across all datasets, PVNet exhibits state-of-the-art performances. We also demonstrate the capability of our approach to handle truncated objects on a new dataset called Truncation LINEMOD which is created by randomly cropping images of LINEMOD. Furthermore, our approach is efficient, which runs 25 fps on a GTX 1080ti GPU, to be used for real-time pose estimation.</p><p>In summary, this work has the following contributions:</p><p>• We propose a novel framework for 6D pose estimation using a pixel-wise voting network (PVNet), which learns a vector-field representation for robust 2D keypoint localization and naturally deals with occlusion and truncation.</p><p>• We propose to utilize an uncertainty-driven PnP algorithm to account for uncertainties in 2D keypoint localizations, based on the dense predictions from PVNet.</p><p>• We demonstrate significant performance improvements of our approach compared to the state of the art on benchmark datasets (ADD: 86.3% vs. 79% and 40.8% vs. 30.4% on LINEMOD and OCCLUSION, respectively). We also create a new dataset for evaluation on truncated objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Holistic methods. Given an image, some methods aim to estimate the 3D location and orientation of the object in a single shot. Traditional methods mainly rely on template matching techniques <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b44">46]</ref>, which are sensitive to cluttered environments and appearance changes. Recently, CNNs have shown significant robustness to environment variations. As a pioneer, PoseNet <ref type="bibr" target="#b20">[22]</ref> introduces a CNN architecture to directly regress a 6D camera pose from a single RGB image, a task similar to object pose estimation. However, directly localizing objects in 3D is difficult due to a lack of depth information and the large search space. To overcome this problem, PoseCNN <ref type="bibr" target="#b41">[43]</ref> localizes objects in the 2D image and predicts their depths to obtain the 3D location. However, directly estimating the 3D rotation is also difficult, since the non-linearity of the rotation space makes CNNs less generalizable. To avoid this problem, <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b36">38]</ref> discretize the rotation space and cast the 3D rotation estimation into a classification task. Such discretization produces a coarse result and a post-refinement is essential to get an accurate 6DoF pose.</p><p>Keypoint-based methods. Instead of directly obtaining the pose from an image, keypoint-based methods adopt a two-stage pipeline: they first predict 2D keypoints of the object and then compute the pose through 2D-3D correspondences with a PnP algorithm. 2D keypoint detection is relatively easier than 3D localization and rotation estimation. For objects of rich textures, traditional methods <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b1">2]</ref> detect local keypoints robustly, so the object pose is estimated both efficiently and accurately, even under cluttered scenes and severe occlusions. However, traditional methods  have difficulty in handling texture-less objects and processing low-resolution images <ref type="bibr" target="#b21">[23]</ref>. To solve this problem, recent works define a set of semantic keypoints and use CNNs as keypoint detectors. <ref type="bibr" target="#b31">[33]</ref> uses segmentation to identify image regions that contain objects and regresses keypoints from the detected image regions. <ref type="bibr" target="#b37">[39]</ref> employs the YOLO architecture <ref type="bibr" target="#b32">[34]</ref> to estimate the object keypoints. Their networks make predictions based on a low-resolution feature map. When global distractions occur, such as occlusions, the feature map is interfered <ref type="bibr" target="#b28">[30]</ref> and the pose estimation accuracy drops. Motivated by the success of 2D human pose estimation <ref type="bibr" target="#b27">[29]</ref>, another category of methods <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b28">30]</ref> outputs pixel-wise heatmaps of keypoints to address the issue of occlusion. However, since heatmaps are fix-sized, these methods have difficulty in handling truncated objects, whose keypoints may be outside the input image. In contrast, our method makes pixel-wise predictions for 2D keypoints using a more flexible representation, i.e., vector field. The keypoint locations are determined by voting from the directions, which are suitable for truncated objects.</p><p>Dense methods. In these methods, every pixel or patch produces a prediction for the desired output, and then casts a vote for the final result in a generalized Hough voting scheme <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr">14]</ref>. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">28]</ref> use a random forest to predict 3D object coordinates for each pixel and produce 2D-3D correspondence hypotheses using geometric constraints. To utilize the powerful CNNs, <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b8">9]</ref> densely sample image patches and use networks to extract features for the latter voting. However, these methods require RGB-D data. In the presence of RGB data alone, <ref type="bibr" target="#b3">[4]</ref> uses an auto-context regression framework <ref type="bibr" target="#b38">[40]</ref> to produce pixel-wise distributions of 3D object coordinates. Compared with sparse keypoints, object coordinates provide dense 2D-3D correspondences for pose estimation, which is more robust to occlusion. But regressing object coordinates is more difficult than keypoint detection due to the larger output space. Our approach makes dense predictions for keypoint localization. It can be regarded as a hyprid of keypoint-based and dense methods, which combines advantages of both methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed approach</head><p>In this paper, we propose a novel framework for 6DoF object pose estimation. Given an image, the task of pose estimation is to detect objects and estimate their orientations and translations in 3D. Specifically, 6D pose is represented by a rigid transformation (R; t) from the object coordinate system to the camera coordinate system, where R represents the 3D rotation and t represents the 3D translation.</p><p>Inspired by recent methods <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b37">39]</ref>, we estimate the object pose using a two-stage pipeline: we first detect 2D object keypoints using CNNs and then compute 6D pose parameters using the PnP algorithm. Our innovation is in a new representation for 2D object keypoints as well as a modified PnP algorithm for pose estimation. Specifically, our method uses a Pixel-wise Voting Network (PVNet) to detect 2D keypoints in a RANSAC-like fashion, which robustly handles occluded and truncated objects. The RANSAC-based voting also gives a spatial probability distribution of each keypoint, allowing us to estimate the 6D pose with an uncertainty-driven PnP. <ref type="figure" target="#fig_2">Figure 2</ref> overviews the proposed pipeline for keypoint localization. Given an RGB image, PVNet predicts pixelwise object labels and unit vectors that represent the direction from every pixel to every keypoint. Given the directions to a certain object keypoint from all pixels belonging to that object, we generate hypotheses of 2D locations for that keypoint as well as the confidence scores through RANSAC-based voting. Based on these hypotheses, we estimate the mean and covariance of the spatial probability distribution for each keypoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Voting-based keypoint localization</head><p>In contrast to directly regressing keypoint locations from an image patch <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b37">39]</ref>, the task of predicting pixel-wise directions enforces the network to focus more on local features of objects and alleviates the influence of cluttered background. Another advantage of this approach is the ability to represent keypoints that are occluded or outside the image. Even if a keypoint is invisible, it can be correctly located according to the directions estimated from other visible parts of the object.</p><p>More specifically, PVNet performs two tasks: semantic segmentation and vector-field prediction. For a pixel p, PVNet outputs the semantic label that associates it with a specific object and the unit vector v k (p) that represents the direction from the pixel p to a 2D keypoint x k of the object.</p><formula xml:id="formula_0">The vector v k (p) is defined as v k (p) = x k − p x k − p 2 .<label>(1)</label></formula><p>Given semantic labels and unit vectors, we generate keypoint hypotheses in a RANSAC-based voting scheme. First, we find the pixels of the target object using semantic labels. Then, we randomly choose two pixels and take the intersection of their vectors as a hypothesis h k,i for the keypoint x k . This step is repeated N times to generate a set of hypotheses {h k,i |i = 1, 2, ..., N } that represent possible keypoint locations. Finally, all pixels of the object vote for these hypotheses. Specifically, the voting score w k,i of a hypothesis h k,i is defined as </p><formula xml:id="formula_1">w k,i = p∈O I (h k,i − p) T h k,i − p 2 v k (p) ≥ θ ,<label>(2)</label></formula><p>where I represents the indicator function, θ is a threshold (0.99 in all experiments), and p ∈ O means that the pixel p belongs to the object O. Intuitively, a higher voting score means that a hypothesis is more confident as it coincides with more predicted directions. The resulting hypotheses characterize the spatial probability distribution of a keypoint in the image. <ref type="figure" target="#fig_2">Figure 2</ref>(e) shows an example. Finally, the mean µ k and the covariance Σ k for a keypoint x k are estimated by:</p><formula xml:id="formula_2">µ k = N i=1 w k,i h k,i N i=1 w k,i ,<label>(3)</label></formula><formula xml:id="formula_3">Σ k = N i=1 w k,i (h k,i − µ k )(h k,i − µ k ) T N i=1 w k,i ,<label>(4)</label></formula><p>which are used latter for uncertainty-driven PnP described in Section 3.2.</p><p>Keypoint selection. The keypoints need to be defined based on the 3D object model. Many recent methods <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b28">30]</ref> use the eight corners of the 3D bounding box of the object as the keypoints. An example is shown in <ref type="figure" target="#fig_3">Figure 3(a)</ref>. These bounding box corners are far away from the object pixels in the image. The longer distance to the object pixels results in larger localization errors, since the keypoint hypotheses are generated using the vectors that start at the object pixels. <ref type="figure" target="#fig_3">Figure 3(b)</ref> and (c) show the hypotheses of a bounding box corner and a keypoint selected on the object surface, respectively, which are generated by our PVNet. The keypoint on the object surface usually has a much smaller variance in the localization. Therefore, the keypoints should be selected on the object surface in our approach. Meanwhile, these keypoints should spread out on the object to make the PnP algorithm more stable. Considering the two requirements, we select K kepoints using the farthest point sampling (FPS) algorithm. First, we initialize the keypoint set by adding the object center. Then, we repeatedly find a point on the object surface, which is farthest to the current keypoint set, and add it to the set until the size of the set reaches K. The empirical results in Section 5.3 show that this strategy produces better results than using the bounding box corners. We also compare the results using different numbers of keypoints. Considering both accuracy and efficiency, we suggest K = 8 according to the experiment results.</p><p>Multiple instances. Our method can handle multiple instances based on the strategy proposed in <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b29">31]</ref>. For each object class, we generate the hypotheses of the object centers and their voting scores using our proposed voting scheme. Then, we find the modes among the hypotheses and mark these modes as centers of different instances. Finally, the instance masks are obtained by assigning pixels to the nearest instance center they vote for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Uncertainty-driven PnP</head><p>Given 2D keypoint locations for each object, its 6D pose can be computed by solving the PnP problem using an offthe-shelf PnP solver, e.g., the EPnP <ref type="bibr" target="#b22">[24]</ref> used in many previous methods <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b31">33]</ref>. However, most of them ignore the fact that different keypoints may have different confidences and uncertainty patterns, which should be considered when solving the PnP problem.</p><p>As introduced in Section 3.1, our voting-based method estimates a spatial probability distribution for each keypoint. Given the estimated mean µ k and covariance matrix Σ k for k = 1, · · · , K, we compute the 6D pose (R, t) by minimizing the Mahalanobis distance:</p><formula xml:id="formula_4">minimize R,t K k=1 (x k − µ k ) T Σ −1 k (x k − µ k ), x k = π(RX k + t),<label>(5)</label></formula><p>where X k is the 3D coordinate of the keypoint,x k is the 2D projection of X k , and π is the perspective projection function. The parameters R and t are initialized by EPnP <ref type="bibr" target="#b22">[24]</ref> based on four keypoints, whose covariance matrices have the smallest traces. Then, we solve (5) using the Levenberg-Marquardt algorithm. In <ref type="bibr" target="#b10">[11]</ref>, the authors also consider the feature uncertainties by minimizing the approximated Sampson errors. In our method, we directly minimize the reprojection errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>Assuming there are C classes of objects and K keypoints for each class, PVNet takes as input the H × W × 3 image, processes it with a fully convolutional architecture, and outputs the H ×W ×(K ×2×C) tensor representing unit vectors and H ×W ×(C +1) tensor representing class probabilities. We use a pretrained ResNet-18 <ref type="bibr" target="#b14">[16]</ref> as the backbone network, and we make three revisions on it. First, when the feature map of the network has the size H/8 × W/8, we do not downsample the feature map anymore by discarding the subsequent pooling layers. Second, to keep the receptive fields unchanged, the subsequent convolutions are replaced with suitable dilated convolutions <ref type="bibr" target="#b43">[45]</ref>. Third, the fully connected layers in the original ResNet-18 are replaced with convolution layers. Then, we repeatedly perform skip connection, convolution and upsampling on the feature map, until its size reaches H × W , as shown in <ref type="figure" target="#fig_2">Figure 2</ref>(b). By applying a 1 × 1 convolution on the final feature map, we obtain the unit vectors and class probabilities.</p><p>We implement hypothesis generation, pixel-wise voting and density estimation using CUDA. The EPnP <ref type="bibr" target="#b22">[24]</ref> used to initialize the pose is implemented in OpenCV <ref type="bibr" target="#b4">[5]</ref>. To obtain the final pose, we use the iterative solver Ceres <ref type="bibr" target="#b0">[1]</ref> to minimize the Mahalanobis distance <ref type="bibr" target="#b4">(5)</ref>. For symmetric objects, there are ambiguities of keypoint locations. To eliminate the ambiguities, we rotate the symmetric object to a canonical pose during training, as suggested by <ref type="bibr" target="#b31">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training strategy</head><p>We use the smooth 1 loss proposed in <ref type="bibr" target="#b12">[13]</ref> for learning unit vectors. The corresponding loss function is defined as</p><formula xml:id="formula_5">(w) = K k=1 p∈O 1 (∆v k (p; w)| x ) + 1 (∆v k (p; w)| y ), ∆v k (p; w) =ṽ k (p; w) − v k (p),<label>(6)</label></formula><p>where w represents the parameters of PVNet,ṽ k is the predicted vector, v k is the ground truth unit vector, and ∆v k | x and ∆v k | y represent the two elements of ∆v k , respectively. For training semantic labels, a softmax cross-entropy loss is adopted. Note that during testing, we do not need the predicted vectors to be unit because the subsequent processing uses only the directions of the vectors.</p><p>To prevent overfitting, we add synthetic images to the training set. For each object, we render 10000 images whose viewpoints are uniformly sampled. We further synthesize another 10000 images using the "Cut and Paste" strategy proposed in <ref type="bibr" target="#b9">[10]</ref>. The background of each synthetic image is randomly sampled from SUN397 <ref type="bibr" target="#b42">[44]</ref>. We also apply online data augmentation including random cropping, resizing, rotation and color jittering during training. We set the initial learning rate as 0.001 and halve it every 20 epochs. All models are trained for 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>LINEMOD <ref type="bibr" target="#b16">[18]</ref> is a standard benchmark for 6D object pose estimation. This dataset exhibits many challenges for pose estimation: cluttered scenes, texture-less objects, and lighting condition variations.</p><p>Occlusion LINEMOD <ref type="bibr" target="#b2">[3]</ref> was created by additionally annotating a subset of the LINEMOD images. Each image contains multiple annotated objects, and these objects are heavily occluded, which poses a great challenge for pose estimation.</p><p>Truncation LINEMOD To fully evaluate our method on truncated objects, we create this dataset by randomly cropping images in the LINEMOD dataset. After cropping, only 40% to 60% of the area of the target object remain in the image. Some examples are shown in <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>Note that, in our experiments, the Occlusion LINEMOD and Truncation LINEMOD are used for testing only. Our model tested on these two datasets is only trained on the LINEMOD dataset. <ref type="bibr" target="#b41">[43]</ref> is a recently proposed dataset. The images are collected from the YCB object set <ref type="bibr" target="#b6">[7]</ref>. This dataset is challenging due to the varying lighting conditions, significant image noise and occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YCB-Video</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evalutation metrics</head><p>We evaluate our method using two common metrics: 2D projection metric <ref type="bibr" target="#b3">[4]</ref> and average 3D distance of model points (ADD) metric <ref type="bibr" target="#b16">[18]</ref>.</p><p>2D Projection metric. This metric computes the mean distance between the projections of 3D model points given the estimated and the ground truth pose. A pose is considered as correct if the distance is less than 5 pixels.</p><p>ADD metric. With the ADD metric <ref type="bibr" target="#b16">[18]</ref>, we transform the model points by the estimated and the ground truth poses, respectively, and compute the mean distance between the two transformed point sets. When the distance is less than 10% of the model's diameter, it is claimed that the estimated pose is correct. For symmetric objects, we use the ADD-S metric <ref type="bibr" target="#b41">[43]</ref>, where the mean distance is computed based on the closest point distance. We denote these two metrics as ADD(-S) and use the one appropriate to the object. When evaluating on the YCB-Video dataset, we compute the ADD(-S) AUC proposed in <ref type="bibr" target="#b41">[43]</ref>. The ADD(-S) AUC is the area under the accuracy-threshold curve, which is obtained by varying the distance threshold in evaluation.  <ref type="table">Table 1</ref>. Ablation studies on different configurations for pose estimation on the Occlusion LINEMOD dataset. These results are accuracies in terms of the ADD(-S) metric, where glue and eggbox are considered as symmetric objects. Tekin <ref type="bibr" target="#b37">[39]</ref> detects the keypoints by regression, while other configurations use the proposed voting-based keypoint localization. BBox 8 shows the result of our method using the keypoints defined in <ref type="bibr" target="#b37">[39]</ref>. FPS K means that we detect K surface keypoints generated by the FPS algorithm. Un means that we use the uncertainty-driven PnP. In configurations without Un, the pose is estimated using the EPnP <ref type="bibr" target="#b22">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation studies</head><p>We conduct ablation studies to compare different keypoint detection methods, keypoint selection schemes, numbers of keypoints and PnP algorithms, on the Occlusion LINEMOD dataset. <ref type="table">Table 1</ref> summarizes the results of ablation studies.</p><p>To compare PVNet with <ref type="bibr" target="#b37">[39]</ref>, we re-implement the same pipeline as <ref type="bibr" target="#b37">[39]</ref> but use PVNet to detect the keypoints which include 8 bounding box corners and the object center. The result is listed in the column "BBox 8" in <ref type="table">Table 1</ref>. The column "Tekin" shows the original result of <ref type="bibr" target="#b37">[39]</ref>, which directly regresses coordinates of keypoints via a CNN. Comparing the two columns demonstrates that pixel-wise voting is more robust to occlusion.</p><p>To analyze the keypoint selection schemes discussed in Section 3.1, we compare the pose estimation results based on different keypoint sets: "BBox 8" that includes 8 bounding box corners plus the center and "FPS 8" that includes 8 surface points selected by the FPS algorithm plus the center. Comparing "BBox 8" with "FPS 8" in <ref type="table">Table 1</ref> shows that the proposed FPS scheme results in better pose estimation.</p><p>When exploring the influence of the keypoint number on pose estimation, we train PVNet to detect 4, 8 and 12 surface keypoints plus the object center, respectively. All the three sets of keypoints are selected by the FPS algorithm as described in Section 3.1. Comparing columns "FPS 4", "FPS 8" and "FPS 12" shows that the accuracy of pose estimation increases with the keypoint number. But the gap between "FPS 8" and "FPS 12" is negligible. Considering efficiency, we use "FPS 8" in all the other experiments.</p><p>To validate the benefit of considering the uncertainties in solving the PnP problem, we replace the EPnP <ref type="bibr" target="#b22">[24]</ref>    <ref type="table">Table 3</ref>. The accuracies of our method and the baseline methods on the LINEMOD dataset in terms of the ADD(-S) metric, where glue and eggbox are considered as symmetric objects.</p><p>shown in the last column "FPS 8 + Un" in <ref type="table">Table 1</ref>, which demonstrate that considering uncertainties of keypoint locations improves the accuracy of pose estimation. The configuration "FPS 8 + Un" is the final configuration for our approach, which is denoted by "OURS" in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with the state-of-the-art methods</head><p>We compare with the state-of-the-art methods which take RGB images as input and output 6D object poses.</p><p>Performance on the LINEMOD dataset. In <ref type="table" target="#tab_2">Table 2</ref>, we compare our method with <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b37">39]</ref> on the LINEMOD dataset in terms of the 2D projection metric. <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b37">39]</ref> detect keypoints by regression, while our method uses the proposed voting-based keypoint localization. BB8 <ref type="bibr" target="#b31">[33]</ref> trains another CNN to refine the predicted pose and the refined results are shown in a separate column. Our method achieves   <ref type="table">Table 5</ref>. The accuracies of our method and the baseline methods on the Occlusion LINEMOD dataset in terms of the ADD(-S) metric, where glue and eggbox are considered as symmetric objects.</p><p>the state-of-the-art performance on all objects without the need of a separate refinement stage. <ref type="table">Table 3</ref> shows the comparison of our methods with <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b37">39]</ref> in terms of the ADD(-S) metric. Note that we compute the ADD-S metric for the eggbox and the glue, which are symmetric, as suggested in <ref type="bibr" target="#b41">[43]</ref>. Comparing to these methods without using refinement, our method outperforms them by a large margin of at least 30.32%. SSD-6D <ref type="bibr" target="#b18">[20]</ref> significantly improves its own performance using edge alignment to refine the estimated pose. Nevertheless, our method still outperforms it by 7.27%.</p><p>Robustness to occlusion. We use the model trained on the LINEMOD dataset for testing on the Occlusion LINEMOD dataset. <ref type="table" target="#tab_4">Table 4</ref> and <ref type="table">Table 5</ref> summarize the comparison with <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b28">30]</ref> on the Occlusion LINEMOD dataset in terms of the 2D projection metric and the ADD(-S) metric, respectively. For both metrics, our method achieves the best performance among all methods. In particular, our method outperforms other methods by a margin of 10.37% in terms of the ADD(-S) metric. Some qualitative results are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. The improved performance demonstrates that the proposed vector-field representation enables PVNet to learn the relationship between parts of the object, so that the occluded keypoints can be robustly recovered by the visible parts.    <ref type="table">Table 6</ref>. Our results on the Truncation LINEMOD dataset in terms of the 2D projection and the ADD(-S) metrics.</p><p>Robustness to truncation. We evaluate our method on the Truncation LINEMOD dataset. Note that, the model used for testing is only trained on the LINEMOD dataset. <ref type="table">Table 6</ref> shows quantitative results in terms of the 2D projection and ADD(-S) metrics. We also test the released model from <ref type="bibr" target="#b37">[39]</ref>, but it does not obtain reasonable results as it is not designed for this case. <ref type="figure" target="#fig_5">Figure 5</ref> shows some qualitative results. Even the objects are partially visible, our method robustly recovers their poses. We show two failure cases in the last column of <ref type="figure" target="#fig_5">Figure 5</ref>, where the visible parts do not provide enough information to infer the poses. This phenomenon is particularly obvious for small objects, such as duck and ape, which have lower accuracies of the pose estimation.  <ref type="table">Table 7</ref>. The accuracies of our method and the baseline methods on the YCB-Video dataset in terms of the 2D projection and the ADD(-S) AUC metrics.</p><p>Performance on the YCB-Video dataset. In <ref type="table">Table 7</ref>, we compare our method with <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b28">30]</ref> on the YCB-Video dataset in terms of the 2D projection and the ADD(-S) AUC metrics. Our method again achieves the state-of-the-art performance and surpasses Oberweger <ref type="bibr" target="#b28">[30]</ref> which is specially designed for dealing with occlusion. The results of PoseCNN were obtained from Oberweger <ref type="bibr" target="#b28">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Running time</head><p>Given a 480 × 640 image, our method runs at 25 fps on a desktop with an Intel i7 3.7GHz CPU and a GTX 1080 Ti GPU, which is efficient for real-time pose estimation. Specifically, our implementation takes 10.9 ms for data loading, 3.3 ms for network forward propagation, 22.8 ms for the RANSAC-based voting scheme, and 3.1 ms for the uncertainty-driven PnP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the keypoint localization: (a) An image of the Occlusion LINEMOD dataset. (b) The architecture of PVNet. (c) Pixel-wise unit vectors pointing to the object keypoints. (d) Semantic labels. (e) Hypotheses of the keypoint locations generated by voting. The hypotheses with higher voting scores are brighter. (f) Probability distributions of the keypoint locations estimated from hypotheses. The mean of a distribution is represented by a red star and the covariance matrix is shown by ellipses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>(a) A 3D object model and its 3D bounding box. (b) Hypotheses produced by PVNet for a bounding box corner. (c) Hypotheses produced by PVNet for a keypoint selected on the object surface. The smaller variance of the surface keypoint shows that it is easier to localize the surface keypoint than the bounding box corner in our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualizations of results on the Occlusion LINEMOD dataset. Green 3D bounding boxes represent the ground truth poses while blue 3D bounding boxes represent our predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>We create a new dataset named Truncation LINEMOD by randomly cropping each image of the LINEMOD dataset. Visualizations of results on the Truncation LINEMOD dataset are shown. Green 3D bounding boxes represent the ground truth poses while blue 3D bounding boxes represent our predictions. The images of the last column are the failure cases, where the visible parts are too ambiguous to provide enough information for the pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>65.04 18.81 63.21 64.87 63.30 cat 0.67 15.00 16.01 17.35 16.68 16.68 duck 1.14 15.95 13.85 26.12 24.89 25.24 driller 7.66 55.60 12.19 62.19 64.17 65.65 eggbox -35.23 36.77 44.96 41.53 50.17 glue 10.08 42.64 24.81 47.32 51.94 49.62 holepuncher 5.45 35.06 15.98 39.50 40.16 39.67 average 6.42 33.88 17.96 39.76 39.92 40.77</figDesc><table><row><cell>methods</cell><cell cols="3">Tekin BBox FPS [39] 8 4</cell><cell>FPS 8</cell><cell>FPS FPS 8 12 + Un</cell></row><row><cell>ape</cell><cell>2.48</cell><cell>6.50</cell><cell cols="3">5.31 17.44 15.1</cell><cell>15.81</cell></row><row><cell>can</cell><cell>17.48</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The accuracies of our method and the baseline methods on the LINEMOD dataset in terms of the 2D projection metric.</figDesc><table><row><cell></cell><cell>w/o refinement</cell><cell cols="2">w/ refinement</cell></row><row><cell>methods</cell><cell cols="3">BB8 SSD-6D Tekin OURS BB8 SSD-6D [33] [20] [39] [33] [20]</cell></row><row><cell>ape</cell><cell cols="2">27.9 0.00 21.62 43.62 40.4</cell><cell>65</cell></row><row><cell cols="3">benchwise 62.0 0.18 81.80 99.90 91.8</cell><cell>80</cell></row><row><cell>cam</cell><cell cols="2">40.1 0.41 36.57 86.86 55.7</cell><cell>78</cell></row><row><cell>can</cell><cell cols="2">48.1 1.35 68.80 95.47 64.1</cell><cell>86</cell></row><row><cell>cat</cell><cell cols="2">45.2 0.51 41.82 79.34 62.6</cell><cell>70</cell></row><row><cell>driller</cell><cell cols="2">58.6 2.58 63.51 96.43 74.4</cell><cell>73</cell></row><row><cell>duck</cell><cell cols="2">32.8 0.00 27.23 52.58 44.30</cell><cell>66</cell></row><row><cell cols="3">eggbox 40.0 8.90 69.58 99.15 57.8</cell><cell>100</cell></row><row><cell>glue</cell><cell cols="2">27.0 0.00 80.02 95.66 41.2</cell><cell>100</cell></row><row><cell cols="3">holepuncher 42.4 0.30 42.63 81.92 67.20</cell><cell>49</cell></row><row><cell>iron</cell><cell cols="2">67.0 8.86 74.97 98.88 84.7</cell><cell>78</cell></row><row><cell>lamp</cell><cell cols="2">39.9 8.20 71.11 99.33 76.5</cell><cell>73</cell></row><row><cell>phone</cell><cell cols="2">35.2 0.18 47.74 92.41 54.0</cell><cell>79</cell></row><row><cell cols="3">average 43.6 2.42 55.95 86.27 62.7</cell><cell>79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The accuracies of our method and the baseline methods on the Occlusion LINEMOD dataset in terms of the 2D projection metric.</figDesc><table><row><cell>methods</cell><cell cols="4">Tekin PoseCNN Oberweger OURS [39] [43] [30]</cell></row><row><cell>ape</cell><cell>2.48</cell><cell>9.6</cell><cell>17.6</cell><cell>15.81</cell></row><row><cell>can</cell><cell>17.48</cell><cell>45.2</cell><cell>53.9</cell><cell>63.30</cell></row><row><cell>cat</cell><cell>0.67</cell><cell>0.93</cell><cell>3.31</cell><cell>16.68</cell></row><row><cell>duck</cell><cell>1.14</cell><cell>19.6</cell><cell>19.2</cell><cell>25.24</cell></row><row><cell>driller</cell><cell>7.66</cell><cell>41.4</cell><cell>62.4</cell><cell>65.65</cell></row><row><cell>eggbox</cell><cell>-</cell><cell>22</cell><cell>25.9</cell><cell>50.17</cell></row><row><cell>glue</cell><cell>10.08</cell><cell>38.5</cell><cell>39.6</cell><cell>49.62</cell></row><row><cell cols="2">holepuncher 5.45</cell><cell>22.1</cell><cell>21.3</cell><cell>39.67</cell></row><row><cell>average</cell><cell>6.42</cell><cell>24.9</cell><cell>30.4</cell><cell>40.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>22.39 42.01 40.91 30.86 31.48</figDesc><table><row><cell>objects</cell><cell>ape</cell><cell>benc-hvise</cell><cell>cam</cell><cell>can</cell><cell>cat</cell><cell cols="2">driller duck</cell></row><row><cell>2D Projection</cell><cell>52.59</cell><cell cols="6">58.19 54.87 57.44 61.66 43.27 54.23</cell></row><row><cell>ADD(-S)</cell><cell>12.78</cell><cell cols="6">42.80 27.73 32.94 25.19 37.04 12.36</cell></row><row><cell>objects</cell><cell cols="2">eggbox glue</cell><cell>holep-uncher</cell><cell>iron</cell><cell cols="2">lamp phone</cell><cell>avg</cell></row><row><cell>2D Projection</cell><cell>87.23</cell><cell cols="6">86.64 53.84 46.53 46.94 51.35 58.06</cell></row><row><cell>ADD(-S)</cell><cell>44.13</cell><cell>38.11</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced a novel framework for 6DoF object pose estimation, which consists of the pixel-wise voting network (PVNet) for keypoint localization and the uncertaintydriven PnP for final pose estimation. We showed that predicting the vector fields followed by RANSAC-based voting for keypoint localization gained a superior performance than direct regression of keypoint coordinates, especially for occluded or truncated objects. We also showed that considering the uncertainties of predicted keypoint locations in solving the PnP problem further improved pose estimation. We reported the state-of-the-art performances on all three widely-used benchmark datasets and demonstrated the robustness of the proposed approach on a new dataset of truncated objects.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mierle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename><surname>Ceres</surname></persName>
		</author>
		<ptr target="http://ceres-solver.org.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opencv</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">When regression meets manifold learning for object recognition and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis and observations from the first amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Causo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Wurman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="188" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recovering 6d object pose and predicting next-bestview in the crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Leveraging feature uncertainty in the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Binefa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Viewpoint-aware object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative mixture-of-templates for viewpoint classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradient response maps for realtime detection of textureless objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>T-PAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="876" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparing images using the hausdorff distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Klanderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Rucklidge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="850" to="863" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Monocular model-based 3d tracking of rigid objects: A survey. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">independent object class detection using 3d feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schertler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Global hypothesis generation for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Derpanis, and K. Daniilidis. 6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rothganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="231" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depthencoded hough voting for joint object detection and shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Auto-context and its application to highlevel vision tasks and 3d brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Single image 3d object detection and pose estimation for grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
