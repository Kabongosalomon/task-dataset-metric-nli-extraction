<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compositional Learning of Image-Text Query for Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Umer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mercateo AG</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwaar</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Labintcev</surname></persName>
							<email>egor.labintcev@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mercateo AG</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kleinsteuber</surname></persName>
							<email>martin.kleinsteuber@mercateo.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Mercateo AG</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compositional Learning of Image-Text Query for Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we investigate the problem of retrieving images from a database based on a multi-modal (imagetext) query. Specifically, the query text prompts some modification in the query image and the task is to retrieve images with the desired modifications. For instance, a user of an E-Commerce platform is interested in buying a dress, which should look similar to her friend's dress, but the dress should be of white color with a ribbon sash. In this case, we would like the algorithm to retrieve some dresses with desired modifications in the query dress. We propose an autoencoder based model, Com-poseAE, to learn the composition of image and text query for retrieving images. We adopt a deep metric learning approach and learn a metric that pushes composition of source image and text query closer to the target images. We also propose a rotational symmetry constraint on the optimization problem. Our approach is able to outperform the state-of-the-art method TIRG [23] on three benchmark datasets, namely: MIT-States, Fashion200k and Fashion IQ. In order to ensure fair comparison, we introduce strong baselines by enhancing TIRG method. To ensure reproducibility of the results, we publish our code here: https://anonymous.4open.science/r/ d1babc3c-0e72-448a-8594-b618bae876dc/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the peculiar features of human perception is multi-modality. We unconsciously attach attributes to objects, which can sometimes uniquely identify them. For instance, when a person says apple it is quite natural that an image of an apple, which may be green or red in color, forms in their mind. In information retrieval, the user seeks information from a retrieval system by sending a query. * indicates equal contribution. Umer contributed the rotation in complex space idea, complex projection module and rotationally symmetric loss. Egor and Umer contributed the idea of fusing different modalities before concatenation. Egor came up with the reconstruction loss idea and contributed by testing hypotheses and running numerous experiments. <ref type="figure">Figure 1</ref>. Potential application scenario of this task in an advanced retrieval system Traditional information retrieval systems allow a unimodal query, i.e., either a text or an image. Advanced information retrieval systems should enable the users in expressing the concept in their mind by allowing a multi-modal query.</p><p>In this work, we consider such an advanced retrieval system, where users can retrieve images from a database based on a multi-modal query. Concretely, we have an image retrieval task where the input query is specified in the form of an image and natural language expressions describing the desired modifications in the query image. Such a retrieval system offers a natural and effective interface <ref type="bibr" target="#b17">[18]</ref>. This task has applications in the domain of E-Commerce search, surveillance systems and internet search. <ref type="figure">Fig. 1</ref> shows a potential application scenario of this task.</p><p>Recently, Vo et al. <ref type="bibr" target="#b22">[23]</ref> have proposed the Text Image Residual Gating (TIRG) method for composing the query image and text for image retrieval. They have achieved state-of-the-art (SOTA) results on this task. However, their approach does not perform well for real-world application scenarios, i.e. with long and detailed texts (see <ref type="bibr">Sec. 4.4)</ref>. We think the reason is that their approach is too focused on changing the image space and does not give the query text its due importance. The gating connection takes elementwise product of query image features with image-text representation after passing it through two fully connected layers. In short, TIRG assigns huge importance to query image features by putting it directly in the final composed representation. Similar to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>, they employ LSTM for extracting features from the query text. This works fine for simple queries but fails for more realistic queries.</p><p>In this paper, we attempt to overcome these limitations by proposing ComposeAE, an autoencoder based approach for composing the modalities in the multi-modal query. We employ a pre-trained BERT model <ref type="bibr" target="#b0">[1]</ref> for extracting text features, instead of LSTM. We hypothesize that by jointly conditioning on both left and right context, BERT is able to give better representation for the complex queries. Similar to TIRG <ref type="bibr" target="#b22">[23]</ref>, we use a pre-trained ResNet-17 model for extracting image features. The extracted image and text features have different statistical properties as they are extracted from independent uni-modal models. We argue that it will not be beneficial to fuse them by passing through a few fully connected layers, as typically done in image-text joint embeddings <ref type="bibr" target="#b23">[24]</ref>.</p><p>We adopt a novel approach and map these features to a complex space. We propose that the target image representation is an element-wise rotation of the representation of the source image in this complex space. The information about the degree of rotation is specified by the text features. We learn the composition of these complex vectors and their mapping to the target image space by adopting a deep metric learning (DML) approach. In this formulation, text features take a central role in defining the relationship between query image and target image. This also implies that the search space for learning the composition features is restricted. From a DML point of view, this restriction proves to be quite vital in learning a good similarity metric.</p><p>We also propose an explicit rotational symmetry constraint on the optimization problem based on our novel formulation of composing the image and text features. Specifically, we require that multiplication of the target image features with the complex conjugate of the query text features should yield a representation similar to the query image features. We explore the effectiveness of this constraint in our experiments (see Sec. 4.5).</p><p>We validate the effectiveness of our approach on three datasets: MIT-States, Fashion200k and Fashion IQ. In Sec. 4, we show empirically that ComposeAE is able to learn a better composition of image and text queries and outperforms SOTA method. In DML, it has been recently shown that improvements in reported results are exagger-ated and performance comparisons are done unfairly <ref type="bibr" target="#b10">[11]</ref>.</p><p>In our experiments, we took special care to ensure fair comparison. For instance, we introduce several variants of TIRG. Some of them show huge improvements over the original TIRG. We also conduct several ablation studies to quantify the contribution of different modules in the improvement of the ComposeAE performance.</p><p>Our main contributions are summarized below:</p><p>• We propose a ComposeAE model to learn the composed representation of image and text query.</p><p>• We adopt a novel approach and argue that the source image and the target image lie in a common complex space. They are rotations of each other and the degree of rotation is encoded via query text features.</p><p>• We propose a rotational symmetry constraint on the optimization problem.</p><p>• ComposeAE outperforms the SOTA method TIRG by a huge margin, i.e., 30.12% on Fashion200k and 11.13% on MIT-States on the Recall@10 metric.</p><p>• We enhance SOTA method TIRG <ref type="bibr" target="#b22">[23]</ref> to ensure fair comparison and identify its limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep metric learning (DML) has become a popular technique for solving retrieval problems. DML aims to learn a metric such that the distances between samples of the same class are smaller than the distances between the samples of different classes. The task where DML has been employed extensively is the cross-modal retrieval, i.e. retrieving images based on text query and getting captions from the database based on the image query <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>In the domain of Visual Question Answering (VQA), many methods have been proposed to fuse the text and image inputs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref>. We review below a few closely related methods. Relationship <ref type="bibr" target="#b18">[19]</ref> is a method based on relational reasoning. Image features are extracted from CNN and text features from LSTM to create a set of relationship features. These features are then passed through a MLP and after averaging them the composed representation is obtained. FiLM <ref type="bibr" target="#b16">[17]</ref> method tries to "influence" the source image by applying an affine transformation to the output of a hidden layer in the network. In order to perform complex operations, this linear transformation needs to be applied to several hidden layers. Another prominent method is parameter hashing <ref type="bibr" target="#b15">[16]</ref> where one of the fully-connected layers in a CNN acts as the dynamic parameter layer.</p><p>In this work, we focus on the image retrieval problem based on the image and text query. This task has been studied recently by Vo et al. <ref type="bibr" target="#b22">[23]</ref>. They propose a gated feature connection in order to keep the composed representa-tion of query image and text in the same space as that of the target image. They also incorporate a residual connection which learns the similarity between concatenation of image-text features and the target image features. Another simple but effective approach is Show and Tell <ref type="bibr" target="#b21">[22]</ref>. They train a LSTM to predict the next word in the sequence after it has seen the image and previous words. The final state of this LSTM is considered the composed representation. Han et al. <ref type="bibr" target="#b5">[6]</ref> presents an interesting approach to learn spatiallyaware attributes from product description and then use them to retrieve products from the database. But their text query is limited to a predefined set of attributes. Nagarajan et al. <ref type="bibr" target="#b11">[12]</ref> proposed an embedding approach, "Attribute as Operator", where text query is embedded as a transformation matrix. The image features are then transformed with this matrix to get the composed representation.</p><p>This task is also closely related with interactive image retrieval task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref> and attribute-based product retrieval task <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref>. These approaches have their limitations such as that the query texts are limited to a fixed set of relative attributes <ref type="bibr" target="#b26">[27]</ref>, require multiple rounds of natural language queries as input <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref> or that query texts can be only one word i.e. an attribute <ref type="bibr" target="#b5">[6]</ref>. In contrast, the input query text in our approach is not limited to a fixed set of attributes and does not require multiple interactions with the user. Different from our work, the focus of these methods is on modeling the interaction between user and the agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Let X = {x 1 , x 2 , · · · , x n } denote the set of query images, T = {t 1 , t 2 , · · · , t n } denote the set of query texts and Y = {y 1 , y 2 , · · · , y n } denote the set of target images. Let ψ(·) denote the pre-trained image model, which takes an image as input and returns image features in a ddimensional space. Let κ(·, ·) denote the similarity kernel, which we implement as a dot product between its inputs. The task is to learn a composed representation of the imagetext query, denoted by g(x, t; Θ), by maximising</p><formula xml:id="formula_0">max Θ κ(g(x, t; Θ), ψ(y)),<label>(1)</label></formula><p>where Θ denotes all the network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motivation for Complex Projection</head><p>In deep learning, researchers aim to formulate the learning problem in such a way that the solution space is restricted in a meaningful way. This helps in learning better and robust representations. The objective function (Equation 1) maximizes the similarity between the output of the composition function of the image-text query and the target image features. Thus, it is intuitive to model the query image, query text and target image lying in some common space. One drawback of TIRG is that it does not emphasize the importance of text features in defining the relationship between the query image and the target image.</p><p>Based on these insights of the learning problem, we restrict the compositional learning of query image and text features in such a way that: (i) query and target image features lie in the same space, (ii) text features encode the transition from query image to target image in this space and (iii) transition is symmetric, i.e. some function of the text features must encode the reverse transition from target image to query image.</p><p>In order to incorporate these characteristics in the composed representation, we propose that the query image and target image are rotations (transitions) of each other in a complex space. The rotation is determined by the text features. This enables incorporating the desired text information about the image in the common complex space. The reason for choosing the complex space is that some function of text features required for the transition to be symmetric can easily be defined as the complex conjugate of the text features in the complex space (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>Choosing such projection also enables us to define a constraint on the optimization problem, referred to as rotational symmetry constraint (see <ref type="bibr">Equations 12,</ref><ref type="bibr">13 and 14)</ref>. We will empirically verify the effectiveness of this constraint in learning better composed representations. We will also explore the effect on performance if we fuse image and text information in the real space. Refer to Sec. 4.5.</p><p>An advantage of modelling the reverse transition in this way is that we do not require captions of query image. This is quite useful in practice, since a user-friendly retrieval system will not ask the user to describe the query image for it. In the public datasets, query image captions are not always available, e.g. for Fashion IQ dataset. In addition to that, it also forces the model to learn a good "internal" representation of the text features in the complex space.</p><p>Interestingly, such restrictions on the learning problem serve as implicit regularization. e.g., the text features only influence angles of the composed representation. This is in line with recent developments in deep learning theory <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref>. Neyshabur et al. <ref type="bibr" target="#b14">[15]</ref> showed that imposing simple but global constraints on the parameter space of deep networks is an effective way of analyzing learning theoretic properties and may aid in decreasing the generalization error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>Now we describe ComposeAE, an autoencoder based approach for composing the modalities in the multi-modal query. <ref type="figure" target="#fig_1">Figure 3</ref> presents the overview of the ComposeAE architecture.</p><p>For the image query, we extract the image feature vector living in a d-dimensional space, using the image model ψ(·) (e.g. ResNet-17), which we denote as:</p><formula xml:id="formula_1">ψ(x) = z ∈ R d .<label>(2)</label></formula><p>Similarly, for the text query t, we extract the text feature vector living in an h-dimensional space, using the BERT model <ref type="bibr" target="#b0">[1]</ref>, β(·) as:</p><formula xml:id="formula_2">β(t) = q ∈ R h .<label>(3)</label></formula><p>Since the image features z and text features q are extracted from independent uni-modal models; they have different statistical properties and follow complex distributions. Typically in image-text joint embeddings <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, these features are combined using fully connected layers or gating mechanisms.</p><p>In contrast to this we propose that the source image and target image are rotations of each other in some complex space, say, C k . Specifically, the target image representation is an element-wise rotation of the representation of the source image in this complex space. The information of how much rotation is needed to get from source to target image is encoded via the query text features. During training, we learn the appropriate mapping functions which give us the composition of z and q in C k . We learn the angles from the text features q, specifying element-wise rotation of source image features.</p><p>More precisely, we learn a mapping γ : R k → {D ∈ R k×k | D is diagonal} and obtain the coordinate-wise complex rotations via</p><formula xml:id="formula_3">δ = E{jγ(q)},</formula><p>where E denotes the matrix exponential function and j is square root of −1. The mapping γ is implemented as a multilayer perceptron (MLP) i.e. two fully-connected layers with non-linear activation.</p><p>Next, we learn a mapping function, η : R d → C k , which maps image features z to the complex space. η is also implemented as a MLP. The composed representation denoted by φ ∈ C k can be written as:</p><formula xml:id="formula_4">φ = δ η(z)<label>(4)</label></formula><p>The optimization problem defined in Eq. 1 aims to maximize the similarity between the composed features and the target image features extracted from the image model. Thus, we need to learn a mapping function, ρ : C k → R d , from the complex space C k back to the d-dimensional real space where extracted target image features exist. ρ is implemented as MLP.</p><p>In order to better capture the underlying cross-modal similarity structure in the data, we learn another mapping, denoted as ρ conv . It is implemented as two fully connected layers followed by a single convolutional layer. This enables learning effective local interactions among different features. In addition to φ, ρ conv also takes raw features z and q as input. ρ conv plays a really important role for queries where the query text asks for a modification that is spatially localized. e.g., a user wants a t-shirt with a different logo on the front (see second row in <ref type="figure" target="#fig_2">Fig. 4</ref>).</p><p>Let f (z, q) denote the overall composition function which learns how to effectively compose extracted image and text features for target image retrieval. The final representation, ϑ ∈ R d , of the composed image-text features can be written as follows:</p><formula xml:id="formula_5">ϑ = f (z, q) = a ρ(φ) + b ρ conv (φ, z, q),<label>(5)</label></formula><p>where a and b are learnable parameters.</p><p>In autoencoder terminology, the encoder has learnt the composed representation of image and text query, ϑ. Next, we learn to reconstruct the extracted image z and text features q from ϑ. Separate decoders are learned for each modality, i.e., image decoder and text decoder denoted by d img and d txt respectively. The reason for using the decoders and reconstruction losses is two-fold: first, it acts as regularizer on the learnt composed representation and secondly, it forces the composition function to retain relevant text and image information in the final representation. Empirically, we have seen that these losses reduce the variation in the performance and aid in preventing overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Objective</head><p>We adopt a deep metric learning (DML) approach to train ComposeAE. Our training objective is to learn a similarity metric, κ(·, ·) : R d × R d → R, between composed image-text query features ϑ and extracted target image features ψ(y). The composition function f (z, q) should learn to map semantically similar points from the data manifold in R d ×R h onto metrically close points in R d . Analogously, f (·, ·) should push the composed representation away from non-similar images in R d .</p><p>For sample i from the training mini-batch of size N , let ϑ i denote the composition feature, ψ(y i ) denote the target image features and ψ(ỹ i ) denote the randomly selected negative image from the mini-batch. We follow TIRG <ref type="bibr" target="#b22">[23]</ref> in choosing the base loss for the datasets.</p><p>So, for MIT-States dataset, we employ triplet loss with soft margin as a base loss. It is given by:</p><formula xml:id="formula_6">L ST = 1 M N N i=1 M m=1 log 1+exp{κ(ϑ i ,ψ(ỹ i,m )) − κ(ϑ i ,ψ(y i ))} ,<label>(6)</label></formula><p>where M denotes the number of triplets for each training sample i. In our experiments, we choose the same value as mentioned in the TIRG code, i.e. 3. For Fashion200k and Fashion IQ datasets, the base loss is the softmax loss with similarity kernels, denoted as L SM AX . For each training sample i, we normalize the similarity between the composed query-image features (ϑ i ) and target image features by dividing it with the sum of similarities between ϑ i and all the target images in the batch. This is equivalent to the classification based loss in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>.</p><formula xml:id="formula_7">L SM AX = 1 N N i=1 − log exp{κ(ϑ i , ψ(y i ))} N j=1 exp{κ(ϑ i , ψ(y j ))} ,<label>(7)</label></formula><p>In addition to the base loss, we also incorporate two reconstruction losses in our training objective. They act as regularizers on the learning of the composed representation. The image reconstruction loss is given by:</p><formula xml:id="formula_8">L RI = 1 N N i=1 z i −ẑ i 2 2 ,<label>(8)</label></formula><formula xml:id="formula_9">whereẑ i = d img (ϑ i ).</formula><p>Similarly, the text reconstruction loss is given by:</p><formula xml:id="formula_10">L RT = 1 N N i=1 q i −q i 2 2 ,<label>(9)</label></formula><p>whereq i = d txt (ϑ i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Rotational Symmetry Loss</head><p>As discussed in subsection 3.2, based on our novel formulation of learning the composition function, we can include a rotational symmetry loss in our training objective. Specifically, we require that the composition of the target image features with the complex conjugate of the text features should be similar to the query image features. In concrete terms, first we obtain the complex conjugate of the text features projected in the complex space. It is given by:</p><formula xml:id="formula_11">δ * = E{−jγ(q)}.<label>(10)</label></formula><p>Letφ denote the composition of δ * with the target image features ψ(y) in the complex space. Concretely:</p><formula xml:id="formula_12">φ = δ * η(ψ(y))<label>(11)</label></formula><p>Finally, we compute the composed representation, denoted by ϑ * , in the following way:</p><formula xml:id="formula_13">ϑ * = f (ψ(y), q) = a ρ(φ) + b ρ conv (φ, ψ(y), q) (12)</formula><p>The rotational symmetry constraint translates to maximizing this similarity kernel: κ(ϑ * , z). We incorporate this constraint in our training objective by employing softmax loss or soft-triplet loss depending on the dataset. Since for Fashion datasets, the base loss is L SM AX , we calculate the rotational symmetry loss, L SM AX SY M , as follows:</p><formula xml:id="formula_14">L SM AX SY M = 1 N N i=1 − log exp{κ(ϑ * i , z i )} N j=1 exp{κ(ϑ * i , z j )} ,<label>(13)</label></formula><p>Analogously, the resulting loss function, L ST SY M , for MIT-States is given by:</p><formula xml:id="formula_15">L ST SY M = 1 M N N i=1 M m=1 log 1+exp{κ(ϑ * i ,z i,m ) − κ(ϑ * i ,z i )} ,<label>(14)</label></formula><p>The total loss is computed by the weighted sum of above mentioned losses. It is given by:</p><formula xml:id="formula_16">L T = L BASE + λ SY M L BASE SY M + λ RI L RI + λ RT L RT ,<label>(15)</label></formula><p>where BASE ∈ {SM AX, ST } depending on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We evaluate our approach on three real-world datasets, namely: MIT-States <ref type="bibr" target="#b6">[7]</ref>, Fashion200k <ref type="bibr" target="#b5">[6]</ref> and Fashion IQ <ref type="bibr" target="#b4">[5]</ref>. For evaluation, we follow the same protocols as other recent works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>. We use recall at rank k, denoted as R@k, as our evaluation metric. We repeat each experiment 5 times in order to estimate the mean and the standard deviation in the performance of the models.</p><p>To ensure fair comparison, we keep the same hyperparameters as TIRG <ref type="bibr" target="#b22">[23]</ref> and use the same optimizer (SGD with momentum). Similar to TIRG, we use ResNet-17 for image feature extraction to get 512-dimensional feature vector. In contrast to TIRG, we use pretrained BERT <ref type="bibr" target="#b0">[1]</ref> for encoding text query. Concretely, we employ BERT-as-service <ref type="bibr" target="#b24">[25]</ref> and use Uncased BERT-Base which outputs a 768-dimensional feature vector for a text query. Further implementation details can be found in the code: https://anonymous.4open.science/r/ d1babc3c-0e72-448a-8594-b618bae876dc/. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIT-States Fashion200k Fashion IQ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We compare the results of ComposeAE with several methods, namely: Show and Tell, Parameter Hashing, Attribute as Operator, Relationship, FiLM and TIRG. We explained them briefly in Sec. 2.</p><p>In order to identify the limitations of TIRG and to ensure fair comparison with our method, we introduce three variants of TIRG. First, we employ the BERT model as a text model instead of LSTM, which will be referred to as TIRG with BERT. Secondly, we keep the LSTM but text query contains full target captions. We refer to it as TIRG with Complete Text Query. Thirdly, we combine these two variants and get TIRG with BERT and Complete Text Query. The reason for complete text query baselines is that the original TIRG approach generates text query by finding one word difference in the source and target image captions. It disregards all other words in the target captions.</p><p>While such formulation of queries may be effective on some datasets, but the restriction on the specific form (or length) of text query largely constrain the information that a user can convey to benefit the retrieval process. Thus, such an approach of generating text query has limited applications in real life scenarios, where a user usually describes the modification text with multiple words. This argument is also supported by several recent studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref>. In our experiments, Fashion IQ dataset contains queries asked by humans in natural language, with an average length of 13.5 words. (see <ref type="table" target="#tab_1">Table 4</ref>.1). Due to this reason, we can not get results of original TIRG on this dataset.  gories). The input image (say "unripe tomato") is sampled and the text query asks to change the state to ripe. The algorithm is considered successful if it retrieves the correct target image ("ripe tomato") from the pool of all test images. Note that the description (image caption) itself is not available to the algorithm. Fashion200k <ref type="bibr" target="#b5">[6]</ref> consists of ∼200k images of 5 different fashion categories, namely: pants, skirts, dresses, tops and jackets. Each image has a human annotated caption, e.g. "blue knee length skirt". Fashion IQ <ref type="bibr" target="#b4">[5]</ref> is a challenging dataset consisting of 77684 images belonging to three categories: dresses, top-tees and shirts. In contrast to two other datasets, Fashion IQ has two human written annotations for each target image. We report the performance on the validation set as the test set labels are not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion of Results</head><p>Tables 2, 3 and 4 summarize the results of the performance comparison. In the following, we discuss these results to gain some important insights into the problem.</p><p>First, we note that our proposed method ComposeAE outperforms other methods by a significant margin. On Fashion200k, the performance improvement of Com-poseAE over the original TIRG and its enhanced variants is most significant. Specifically, in terms of R@10 metric, the performance improvement over the second best method is 6.96% and 30.12% over the original TIRG method . Similarly on R@10, for MIT-States, ComposeAE outperforms the second best method by 2.35% and by 11.13% over the original TIRG method. For the Fashion IQ dataset , Com-poseAE has 2.61% and 3.82% better performance than the second best method in terms of R@10 and R@100 respectively.</p><p>Second, we observe that the performance of the methods on MIT-States and Fashion200k datasets is in a similar range as compared to the range on the Fashion IQ. For in-    <ref type="table" target="#tab_1">Table 4</ref>.1). That is even though the algorithm retrieves semantically similar images but they will not be considered correct by the recall metric. For instance, for the first query in <ref type="figure" target="#fig_2">Fig.4</ref>, we can see that the second, third and fourth image are semantically similar and modify the image as described by the query text. But if the third image which is the labelled target image did not appear in top-5, then R@5 would have been zero for this query. Third, for MIT-States and Fashion200k datasets, we observe that the TIRG variant which replaces LSTM with BERT as a text model results in slight degradation of the performance. On the other hand, the performance of the TIRG variant which uses complete text (caption) query is quite better than the original TIRG. However, for the Fashion IQ dataset which represents a real-world application scenario, the performance of TIRG with complete text query is significantly worse. Concretely, TIRG with complete text query performs 253% worse than ComposeAE on R@10. The reason for this huge variation is that the average length of complete text query for MIT-States and Fash- ion200k datasets is 2 and 3.5 respectively. Whereas average length of complete text query for Fashion IQ is 12.4. It is because TIRG uses the LSTM model and the composition is done in a way which underestimates the importance of the text query. This shows that TIRG approach does not perform well when the query text description is more realistic and complex.</p><p>Fourth, one of the baselines (TIRG with BERT and Complete Text Query) that we introduced shows significant improvement over the original TIRG. Specifically, in terms of R@10, the performance gain over original TIRG is 8.58% and 21.65% on MIT-States and Fashion200k respectively. This method is also the second best performing method on all datasets. We think that with more detailed text query, BERT is able to give better representation of the query and this in turn helps in the improvement of the performance.</p><p>Qualitative Results: <ref type="figure" target="#fig_2">Fig.4</ref> presents some qualitative retrieval results for Fashion IQ. For the first query, we see that all images are in "blue print" as requested by text query. The second request in the text query was that the dress should be "short sleeves", four out of top-5 images fulfill this requirement. For the second query, we can observe that all retrieved images share the same semantics and are visually similar to the target images. Qualitative results for other two datasets are given in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>We have conducted various ablation studies, in order to gain insight into which parts of our approach helps in the high performance of ComposeAE. <ref type="table" target="#tab_6">Table 5</ref> presents the quantitative results of these studies. Impact of L SY M : on the performance can be seen on Row 2. For Fashion200k and Fashion IQ datasets, the decrease in performance is quite significant: 7.17% and 12.38% respectively. While for MIT-States, the impact of incorporating L SY M is not that significant. It may be because the text query is quite simple in the MIT-states case, i.e. 2 words. This needs further investigation. Efficacy of Mapping to Complex Space: ComposeAE has a complex projection module, see <ref type="figure" target="#fig_1">Fig. 3</ref>. We removed this module to quantify its effect on the performance. Row 3 shows that there is a drop in performance for all three datasets. This strengthens our hypothesis that it is better to map the extracted image and text features into a common complex space than simple concatenation in real space. Convolutional versus Fully-Connected Mapping: Com-poseAE has two modules for mapping the features from complex space to target image space, i.e., one with only fully-connected layers ρ(·) and the second with an additional convolutional layer ρ conv (·). Rows 4 and 5 show that the performance is quite similar for fashion datasets. While for MIT-States, ComposeAE without ρ conv (·) performs much better. Overall, it can be observed that for all three datasets both modules contribute in improving the performance of ComposeAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose ComposeAE to compose the representation of source image rotated with the modification text in a complex space. This composed representation is mapped to the target image space and a similarity metric is learned. Based on our novel formulation of the problem, we introduce a rotational symmetry loss in our training objective. Our experiments on three datasets show that ComposeAE consistently outperforms SOTA method on this task. We enhance SOTA method TIRG <ref type="bibr" target="#b22">[23]</ref> to ensure fair comparison and identify its limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Important Notes on Fashion IQ Dataset</head><p>In Fashion IQ dataset, ∼ 49% annotations describe the target image directly. While ∼ 32% annotations compares target and source images, e.g. "is red with a cat logo on front" and the second annotation is, "is more pop culture and adolescent". The dataset consists of three nonoverlapping subsets, namely "dress", "top-tee" and "shirt". We join the two annotations with the text " and it" to get a description similar to a normal sentence a user might ask on an E-Com platform. Now the complete text query is: "is red with a cat logo on front and it is more pop culture and adolescent". Furthermore, we combine the train sets of all three categories to form a bigger training set and train a single model on it. Analogously, we also combine the validation sets to form a single validation set.</p><p>A challenge was conducted in ICCV 2019 on Fashion IQ dataset 1 . The website also has some technical reports submitted by the best performing teams. The numbers reported in these reports are quite high, even for TIRG approach. We investigated the reasons and reached the conclusion that these technical reports have have quite different settings. It is not possible for us to compare our results with them in a fair manner. The reasons and differences are delineated briefly as:</p><p>• They treat Fashion IQ as three independent datasets and train one model for each category ("dress", "toptee" and "shirt"). This results in better performance for each category.</p><p>• They do pre-training on external datasets like Fashiongen, Fashion200k etc. It is well-known that such transfer learning (via pre-training) inevitably increases the performance of any model.</p><p>• They employ product attributes as side information in their models. In our experiments, we do not consider in such side information and rely solely on the image and text query.</p><p>• They employ higher capacity models such as ResNet101, ResNet-152 etc. In original TIRG and in all our experiments, we use ResNet17 as image model.</p><p>• Since these reports developed models specifically for the competition, they have incorporated several hacks, like ensembeling, data augmentation techniques etc.</p><p>• Unfortunately, none of the technical reports have published their code. Thus, we are not able to assess the performance of their model in our experiment setting.</p><p>In short, it is neither possible for us to reproduce their results nor are we able to fairly compare the performance of their models in a common experiment setting. For the first query, we see that two "burnt bush images are retrieved. We can observe that other retrieved images share the same semantics and are visually similar to the target images. In second and third row, we note that same objects in different states can look drastically different. This highlights the importance of incorporating the text information in the composed representation.</p><p>Some qualitative retrieval results for Fashion200k dataset are presented in <ref type="figure">Fig. 6</ref>. In these results, we observe that the model is able to capture the style and color information quite well. In the first row, we see similar sleeveless dresses with sequin. Similarly, in other two queries, the model successfully images from the same product category, i.e. jacket and skirts. Moreover, the retrieved images seem to follow the desired modifications expressed in the query text remarkably well.</p><p>It is pertinent to highlight that the captions under the images are the ground truth. They are not available to the model as additional input during training or inference. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Conceptual Diagram of Rotation of the Images in Complex Space. Blue and Red Circle represent the query and the target image respectively. δ represents the rotation in the complex space, learned from the query text features. δ * represents the complex conjugate of the rotation in the complex space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>ComposeAE Architecture: Image retrieval using text and image query. Dotted lines indicate connections needed for calculating rotational symmetry loss (seeEquations 12, 13 and 14). Here 1 refers to LBASE, 2 refers to L BASE SY M , 3 refers to LRI and 4 refers to LRT .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative Results: Retrieval examples from FashionIQ Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1 https://sites.google.com/view/lingir/fashion-iq B. Qualitative Results Fig. 5 presents some qualitative retrieval examples for MIT-States dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Qualitative Results: Retrieval examples from MIT-States Dataset Qualitative Results: Retrieval examples from Fashion200k Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>1 summarizes the statistics of the datasets. The train-test split of the datasets is the same for all the methods. MIT-States<ref type="bibr" target="#b6">[7]</ref> dataset consists of ∼60k diverse realworld images where each image is described by an adjective (state) and a noun (categories). For instance, "sliced potato", "ripe tomato" etc. There are 245 nouns in the dataset and 49 of them are reserved for testing, i.e. there is no overlap between training and testing queries in terms of nouns (categories). This split ensures that the algorithm is able to learn the composition on the unseen nouns (cate-±0.1 31.0 ±0.5 42.0 ±0.8 Att. as Operator 8.8 ±0.1 27.3 ±0.3 39.1 ±0.3 Relationship 12.3 ±0.5 31.9 ±0.7 42.9 ±0.9 FiLM 10.1 ±0.3 27.7 ±0.7 38.3 ±0.7 TIRG 12.2 ±0.4 31.9 ±0.3 43.1 ±0.3 TIRG with BERT 12.3 ±0.6 31.8 ±0.3 42.6 ±0.8 ±1.9 28.7 ±2.5 34.1 ±2.9 TIRG with BERT and Complete Text Query 13.3 ±0.6 34.5 ±1.0 46.8 ±1.1 ComposeAE 13.9 ±0.5 35.3 ±0.8 47.9 ±0.7</figDesc><table><row><cell>Method</cell><cell>R@1</cell><cell>R@5</cell><cell>R@10</cell></row><row><cell cols="2">Show and Tell 11.9 TIRG with</cell><cell></cell><cell></cell></row><row><cell>Complete Text Query</cell><cell>7.9</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Model performance comparison on MIT-States. The best number is in bold and the second best is underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>±1.1 40.2 ±1.7 61.8 ±0.9 Param Hashing 12.2 ±1.1 40.0 ±1.1 61.7 ±0.8 Relationship 13.0 ±0.6 40.5 ±0.7 62.4 ±0.6 FiLM 12.9 ±0.7 39.5 ±2.1 61.9 ±1.9 TIRG 14.1 ±0.6 42.5 ±0.7 63.8 ±0.8 TIRG with BERT 14.2 ±1.0 41.9 ±1.0 63.3 ±0.9 TIRG with Complete Text Query 18.1 ±1.9 52.4 ±2.7 73.1 ±2.1 TIRG with BERT and Complete Text Query 19.9 ±1.0 51.7 ±1.5 71.8 ±1.3 ComposeAE 22.8 ±0.8 55.3 ±0.6 73.4 ±0.4</figDesc><table><row><cell>Method</cell><cell>R@1</cell><cell>R@10</cell><cell>R@50</cell></row><row><cell>Han et al. [6]</cell><cell>6.3</cell><cell>19.9</cell><cell>38.3</cell></row><row><cell>Show and Tell</cell><cell>12.3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Model performance comparison on Fashion200k. The best number is in bold and the second best is underlined. ±0.6 9.18 ±0.9 9.45 ±0.<ref type="bibr" target="#b7">8</ref> TIRG with BERT and Complete Text Query 11.5 ±0.<ref type="bibr" target="#b7">8</ref> 28.8 ±1.<ref type="bibr" target="#b4">5</ref> 28.8 ±1.<ref type="bibr" target="#b5">6</ref> ComposeAE 11.8 ±0.9 29.4 ±1.1 29.9 ±1.3</figDesc><table><row><cell>Method</cell><cell>R@10</cell><cell>R@50</cell><cell>R@100</cell></row><row><cell>TIRG with</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Complete Text Query 3.34</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Model performance comparison on Fashion IQ. The best number is in bold and the second best is underlined.</figDesc><table><row><cell>stance, in terms of R@10, the performance of TIRG with</cell></row><row><cell>BERT and Complete Text Query is 46.8 and 51.8 on MIT-</cell></row><row><cell>States and Fashion200k datasets while it is 11.5 for Fashion</cell></row><row><cell>IQ. The reasons which make Fashion IQ the most challeng-</cell></row><row><cell>ing among the three datasets are: (i) the text query is quite</cell></row><row><cell>complex and detailed and (ii) there is only one target im-</cell></row><row><cell>age per query (See</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="3">Fashion200k MIT-States Fashion IQ</cell></row><row><cell>ComposeAE</cell><cell>55.3</cell><cell>47.9</cell><cell>11.8</cell></row><row><cell>-without L SY M</cell><cell>51.6</cell><cell>47.6</cell><cell>10.5</cell></row><row><cell>-Concat in real space</cell><cell>48.4</cell><cell>46.2</cell><cell>09.8</cell></row><row><cell>-without ρ conv</cell><cell>52.8</cell><cell>47.1</cell><cell>10.7</cell></row><row><cell>-without ρ</cell><cell>52.2</cell><cell>45.2</cell><cell>11.1</cell></row></table><note>. Retrieval performance (R@10) of ablation studies.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Alan Schelten, Rayyan A. Khan and Till Brychcy for insightful discussions which helped in improving the quality of this work. This work has been supported by the Bavarian Ministry of Economic Affairs, Regional Development and Energy through the WoWNet project IUK-1902-003// IUK625/002.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dialog-based interactive image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12794</idno>
		<title level="m">The fashion iq dataset: Retrieving images by combining side information and relative natural language feedback</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic spatially-aware fashion concept discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Phoenix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discovering states and transformations in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICCV</title>
		<meeting>the IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attributes as operators: factorizing unseen attribute-object compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="169" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5947" to="5956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">In search of the real inductive bias: On the role of implicit regularization in deep learning. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6614</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Norm-based capacity control in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
		<idno>PMLR. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th Conference on Learning Theory</title>
		<editor>Peter Grnwald, Elad Hazan, and Satyen Kale</editor>
		<meeting>The 28th Conference on Learning Theory<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3" to="06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards building large scale multimodal domain-aware conversation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Drill-down: Interactive retrieval of complex scenes using natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuwen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Cascante-Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Composing text and image for image retrieval -an empirical odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">bert-as-service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://github.com/hanxiao/bert-as-service" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Memory-augmented attribute manipulation networks for interactive fashion search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dual-path convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
