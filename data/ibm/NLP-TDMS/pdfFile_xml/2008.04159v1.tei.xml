<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.XX, XXX.XXXX 1 Depth Quality Aware Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglizhao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qingdao University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qingdao University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qingdao University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.XX, XXX.XXXX 1 Depth Quality Aware Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-RGB-D Salient Object Detection, Weakly Su- pervised Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The existing fusion based RGB-D salient object detection methods usually adopt the bi-stream structure to strike the fusion trade-off between RGB and depth (D). The D quality usually varies from scene to scene, while the SOTA bi-stream approaches are depth quality unaware, which easily result in substantial difficulties in achieving complementary fusion status between RGB and D, leading to poor fusion results in facing of low-quality D. Thus, this paper attempts to integrate a novel depth quality aware subnet into the classic bi-stream structure, aiming to assess the depth quality before conducting the selective RGB-D fusion. Compared with the SOTA bi-stream methods, the major highlight of our method is its ability to lessen the importance of those low-quality, no-contribution, or even negative-contribution D regions during the RGB-D fusion, achieving a much improved complementary status between RGB and D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION AND MOTIVATION</head><p>The conventional image salient object detection aims to fast locate the most eye attractors, and this topic has received intensive research attentions in recent decades <ref type="bibr" target="#b0">[1]</ref>. As a lightweight pre-processing tool, the down-stream applications of image salient object detection usually include video saliency <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, quality assessment <ref type="bibr" target="#b6">[7]</ref>, video tracking <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, video background extraction <ref type="bibr" target="#b9">[10]</ref> and so on.</p><p>Different to the RGB salient object detection methods which conduct their saliency predictions using RGB information solely <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we focus on the RGB-D salient object detection, which is more challenge than the RGB salient object detection due to the newly available D channel, and we abbreviate "depth" as "D" for simplicity. In general, the key rationale of the saliency clue computations in RGB-D images is quite similar to RGB salient object detection methods, in which the RGB-D saliency clues can be easily measured by conducing the contrast computation over the RGB channels and the D channel independently. Thus, as a subsequent stage after obtaining the RGB-D saliency clues, how to strike an optimal complementary trade-off between RGB and D is the main challenge of the RGB-D salient object detection.</p><p>In general, the fusion based SOTA (state-of-the-art) RGB-D salient object detection methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> usually follow the bi-stream structure, in which their two substreams compute the RGB saliency clues and the D saliency clues respectively, and these two saliency clues will be latterly combined as the final RGB-D saliency. After entering the deep learning era, the RGB-D salient object detection methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="figure">Fig. 1</ref>: The method pipeline of the proposed method. The major highlight of our method is the newly devised DCA (Depth Contribution Assessment) subnet, which conducts the explicit feature-level RGB-D fusion before performing the selective deep RGB-D fusion, see the blue arrow (more details can be found in <ref type="figure" target="#fig_0">Fig. 3</ref>). <ref type="bibr" target="#b17">[18]</ref> widely adopt the pre-trained semantical deep models (e.g., VGG, ResNet) to compute high discriminative deep features automatically for their RGB and D saliency streams, and these deep features will latterly be fed into the fusion layers (e.g., full-connected layers or full-convolutional layers) to achieve the selective deep fusion between RGB and D, see the top section in <ref type="figure">Fig. 1</ref>. However, such bi-stream fusion schemes easily reach to their performance bottle-necks due to the following reason: the depth quality usually varies from scene to scene, while the SOTA bi-stream approaches (e.g., <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>) are depth quality unaware, which easily result in substantial difficulties in achieving complementary fusion status between RGB and D, leading to poor fusion results in facing of low-quality D.</p><p>On the one hand, the D channel is not always capable of benefiting its RGB counterpart; on the contrary, it is merely able to benefit its RGB counterpart occasionally and partially, which suggests us to conduct a biased initial fusion before performing selective deep fusion. On the other hand, though the widely adopted fusion schemes (e.g., full connected/convolutional layers) are capable of biasing their fusion balances in some extent, yet such biasing degrees are far away from achieving an optimal RGB-D complementary status, arXiv:2008.04159v1 [cs.CV] 7 Aug 2020 <ref type="figure">Fig. 2</ref>: The depth quality demonstrations, in which the left part illustrates those images with high-quality depth, while the right part shows those images with low-quality depth; in general, it is difficult to obtain high performance D Saliency in facing of images with low-quality depth. because these schemes are unaware of the depth quality during their supervised model training. For example, the training loss of the D stream still exists when facing a training image with extremely low-quality D (e.g., the 3rd row of the right part in <ref type="figure">Fig. 2</ref>), which easily leads to the learning ambiguity or learning over-fitting, preventing the fusion layers to completely bias to the RGB stream.</p><p>To handle the above mentioned problems, in this paper, we integrate a novel DCA (Depth Contribution Assessment) subnet into the bi-stream structure, aiming to conduct depth quality assessment before performing the selective fusion, and we have demonstrated its method overview in the bottom part of <ref type="figure">Fig. 1</ref>. Our DCA subnet is inspired by the following two common attributes of those D regions which are capable of benefiting their RGB counterparts during the RGB-D saliency fusion: 1) only those high-quality D regions (e.g., the right part of <ref type="figure">Fig. 2</ref>) are potentially able to benefit the RGB stream; 2) among of these high-quality D regions, only a small part of it, which have exhibited different saliency predictions to the RGB saliency stream, are the most valuable D regions during the RGB-D saliency fusion.</p><p>Therefore, our method attempts to weakly label those D regions which simultaneously meet the above 2 aspects as the pseudo-GT for the DCA subnet training. Once the DCA subnet has been trained, we use its predictions to guide the explicit feature-level fusion (i.e., M1-M4 in <ref type="figure" target="#fig_0">Fig. 3</ref>) before conducting the selective RGB-D deep fusion. Compared with the conventional selective deep fusion, our novel method is depth quality aware, which is much better at achieving an optimal complementary trade-off between RGB and D. In summary, the major contributions of this paper can be summarized as follows:</p><p>• We have raised one crucial factor which determines the RGB-D salient object detection performance, i.e., it will be able to achieve a much improved fusion if we lessen the importance of those low-quality, low-contribution, or even negative-contribution D regions beforehand; • We have proposed a novel weakly supervised DCA (Depth Contribution Assessment) subnet, aiming to pre-dict which D regions may potentially be able to benefit the RGB stream; • We have devised a novel selective fusion network to make full use of the DCA subnet, achieving a much improved complementary fusion status between RGB and D; • We have conducted massive quantitative evaluations (i.e., comparisons to 12 most recent SOTA methods over 5 datasets) to validate the effectiveness and show the performance superiority of our method. • Both the source code and data are available online at https://github.com/qdu1995/DQSD, which will be able to benefit the RGB-D salient object detection field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. The RGB Salient Object Detection Methods</head><p>The RGB image salient object detection methods use RGB information solely to derive image saliency. The conventional RGB salient object detection methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> were developed by using the handcrafted features (e.g., the most representative background prior <ref type="bibr" target="#b22">[23]</ref>) to conduct the multi-scale contrast/uniqueness computations for the low-level saliency clues, e.g., the classic regional contrast computation <ref type="bibr" target="#b23">[24]</ref>.</p><p>With the rapid development of deep learning techniques, the SOTA (state-of-the-art) RGB salient object detection deep models <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> have outperformed the conventional handcrafted methods in both accuracy and efficiency. Recently, the research trends of the deep learning based RGB salient object detection mainly include: the multi-scale deep feature integration by using the side-layer output of the low-resolution layers <ref type="bibr" target="#b27">[28]</ref>, the stage-wise saliency recurrent <ref type="bibr" target="#b28">[29]</ref>, the multiscale spatial/channel attentions <ref type="bibr" target="#b29">[30]</ref>, and the object boundary enforcement <ref type="bibr" target="#b30">[31]</ref>. Since the RGB salient object detection is not the main foci of this paper, we shall not cover this topic any further. Also, it is worthy mentioning that the pre-trained RGB saliency deep models can be directly applied as the RGB saliency subnet in the bi-stream network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The RGB-D Salient Object Detection Methods</head><p>The RGB-D salient object detection methods can be roughly divided into 2 groups according to their main research focuss: 1) how to reveal saliency clue in D channel; 2) how to fuse RGB saliency with D saliency to achieve an improved RGB-D saliency. The first group methods mainly follow the conventional handcrafted manner to design novel feature space for an improved D saliency, and their key rationale are quite similar to that of the RGB salient object detection methods. The most representative method is the work proposed by Feng et al. in <ref type="bibr" target="#b31">[32]</ref>, which devised a novel depth angular direction based metric to suppress those non-salient areas with high RGB contrasts.</p><p>After entering the deep learning era, any off-the-shelf RGB saliency deep models will be able to predict D saliency if we fine-tune it using the D channel instead. Thus, we shall focus on the RGB-D fusion schemes here. Qu et al. <ref type="bibr" target="#b32">[33]</ref> have adopted the conventional handcrafted manners to compute the low-level saliency clues over both RGB and D channels within the superpixel-wise manner, e.g., the local/global contrast computation and the boundary contrast computation. Then, these low-level saliency clues will be fed into CNN (Convolutional Neural Network) to compute high discriminative deep features (with 4096 dimensions), which will be latterly input into full connected layers to regress towards the RGB-D saliency ground truth, aiming to achieve the selective deep fusion between RGB and D. Similarly, R.Shigematsu et al. <ref type="bibr" target="#b16">[17]</ref> measured D saliency using the newly designed background enclosure distribution, and this novel saliency clue was latterly integrated with multiple top-down and bottom-up saliency clues via CNN framework. Though much improvements have been made by using such "hybrid" none end-to-end methods, it has two major limitations: 1) its fusion performance heavily dependents on the handcrafted low-level saliency clues; 2) its superpixel-wise detection (i.e., the full connected layers) is time consuming in general.</p><p>To alleviate the above mentioned limitations, Zhu et al. <ref type="bibr" target="#b33">[34]</ref> have adopted the full convolutional network based bi-stream structure. As the main stream, its RGB saliency is similar to the SOTA RGB saliency deep models, which follows the classic UNet encoder-decoder structure for a fast RGB saliency computation. Its major highlight is that the side-layers in the D stream are aligned with the RGB saliency decoder layers, aiming to achieve the feature-level selective RGB-D saliency fusion. Similarly, Han et al. <ref type="bibr" target="#b17">[18]</ref> have followed the bi-stream structure, whose D saliency was computed by using the taskrelevant initialization with deep supervision in the hidden layers. To achieve selective deep fusion, the work <ref type="bibr" target="#b17">[18]</ref> has adopted a combination layer to connect its RGB saliency stream to its D saliency stream. However, due to the lack of shallower semantic information, the RGB-D saliency map predicted by <ref type="bibr" target="#b17">[18]</ref> may occasionally exist massive false-alarm detections.</p><p>To increase the information exchange between RGB and D channels, Chen et al. <ref type="bibr" target="#b13">[14]</ref> have devised a residual function to measure the complementary degree between RGB and D, and then it resorted the level-wise supervision to achieve crossmodal RGB-D fusion. Further, Chen et al. <ref type="bibr" target="#b34">[35]</ref> have devised a three-stream network, i.e., the conventional RGB-D bistream with a novel bottom-up cross-model stream. The major highlights of this bottom-up cross-modal stream comprise twofold: 1) this novel stream is able to learn high discriminative deep features; 2) this novel stream is able to complement both RGB and D streams when these two conflicting with each other. Wang et al. <ref type="bibr" target="#b14">[15]</ref> have followed the bi-stream network structure to predict RGB saliency and D saliency respectively. The major highlight of this work is that it has resorted an additional subnet to weakly learn a switch-map, which is formulated by measuring the difference between its RGB saliency and saliency GT, and this switch-map will explicitly guide the RGB-D saliency fusion. The behind rationale of <ref type="bibr" target="#b14">[15]</ref> is based on the assumption that those image regions with incorrect RGB saliency may potentially get fixed by using the D channel. However, its pseudo-GT, which will latterly be used to learn a deep model to predict the RGB-D switchmap, is problematically formulated, because it is almost an impossible task for the deep network to predict which RGB pattern may fail to produce correct saliency detection, resulting in an over-fitted switch-map eventually.</p><p>Most recently, Zhao et al. <ref type="bibr" target="#b35">[36]</ref> have attempted to enhance the depth quality by using the newly designed contrast prior, which transferred the contrast information from the RGB channels to the D channel, aiming to enlarge the depth differences between salient objects and their non-salient surroundings nearby. In fact, the key motivation of our work is partially similar to <ref type="bibr" target="#b35">[36]</ref>, i.e., the <ref type="bibr" target="#b35">[36]</ref> has noted that the RGB-D fusion performance may get degenerated in facing of those lowquality D, thus Zhao et al. has adopted the RGB channels to improve the depth quality from the contrast perspective; in our work, we also attempt to alleviate the side-effect of those low-quality depth information, yet we solve this problem by evaluating the depth quality explicitly before RGB-D fusion, archiving a much improved RGB-D fusion performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD A. Method Overview</head><p>We have demonstrated the overall network architecture in <ref type="figure" target="#fig_0">Fig. 3</ref>. Our method mainly consists of 4 components, including 1) the RGB saliency stream, 2) the D saliency stream, 3) the DCA (Depth Contribution Assessment, Sec. III-C) subnet and 4) the MSF (Multi-scale Fusion, Sec. III-E) subnet.</p><p>The RGB and D saliency streams adopt the classic UNet <ref type="bibr" target="#b36">[37]</ref> encoder-decoder network architecture, which recursively makes full use of the multi-scale deep features between in its encoder and decoder layers (Sec. III-B). Then, based on the stage-wise saliency predictions from the RGB and D saliency streams (i.e., the green arrows in <ref type="figure" target="#fig_0">Fig. 3</ref>), we formulate the pseudo-GT to weakly train the newly designed DCA subnet (Sec. III-C), which aims to indicate which regions in the D channel are potentially able to benefit their RGB counterparts for the salient object detection task. Once the DCA subnet has been trained, we use its prediction (ω) to guide an initial feature-level fusion, which explicitly combines the side-outputs of the RGB saliency stream with the sideoutputs of the D saliency stream as the fused RGB-D deep features, i.e., the M1-M4 in <ref type="figure" target="#fig_0">Fig. 3</ref>. Moreover, we have devised a novel network to conduct multi-scale selective deep fusion for the previously fused deep features (Sec. III-E), in which we recursively convolve M1-M4 with spatial attentions to compute high-quality RGB-D salient object detection results (see the blue arrows in <ref type="figure" target="#fig_0">Fig. 3</ref>). Specifically, we have connected the RGB encoder layers with the D encoder layers (see the gray arrows in <ref type="figure" target="#fig_0">Fig. 3</ref>), aiming to ensure a high-quality RGB saliency before performing the subsequent RGB-D fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Subnet Preliminaries</head><p>In our method, the RGB saliency stream, the D saliency stream and the DCA (Depth Contribution Assessment) subnet are all following the conventional full convolutional network architectures, which respectively take the RGB channel, the D channel and the hybrid RGB+D as input, aiming to regress their individual given input to their learning objectives respectively. To obtain the discriminative semantical deep features for these subnets, we choose the off-the-shelf VGG19 <ref type="bibr" target="#b37">[38]</ref> as the backbone encoder and any other feature backbone (e.g., ResNet <ref type="bibr" target="#b38">[39]</ref> and Res2Net <ref type="bibr" target="#b39">[40]</ref>) is also OK but it may produce different performances <ref type="table" target="#tab_0">(Table VI)</ref>, see the yellow regions in <ref type="figure" target="#fig_0">Fig. 3</ref>. Meanwhile, the decoder layers (the blue regions) take the deepest feature block of the encoder layers as input, which recursively conduct sequential up-samplings (U ) and convolutions (Conv) to ensure an identical output resolution to the input image. Thus, the overall data flow of the DCA subnet can be represented as Eq. 1.</p><formula xml:id="formula_0">ω = ...Conv U Conv V GG(RGB + D) ...,<label>(1)</label></formula><p>where ω ∈ R 224×224 denotes the MVDR (Most Valuable Depth Regions) which are predicted by the DCA subnet; U is the up-sampling operation and Conv is the convolutional operation; RGBD ∈ R 224×224×4 denotes the {RGB + D} input; V GG is the pre-trained VGG19; we use the symbol "..." to denote those Conv and U operations which are omitted here for simplicity.</p><p>To enhance the tiny details of the detected saliency map, the decoder layers in the RGB and D saliency subnets recursively convolve the deep features between consecutive deep layers, which iteratively make full use of the multi-scale deep features. We represent the detailed dataflow of the RGB saliency subnet as Eq. 2.</p><formula xml:id="formula_1">E2 = Conv C D, U (E1) , F = Conv Conv(E2) ,<label>(2)</label></formula><p>where C and U denote the concatenation operation and the up-sampling operation respectively; D, E1, E2 and F can be found in <ref type="figure" target="#fig_0">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Depth Contribution Assessment (DCA) Subnet</head><p>As we have mentioned before, the DCA subnet aims to predict which D regions may potentially be able to benefit the RGB saliency subnet for the salient object detection task, and we abbreviate such D regions as "MVDR (Most Valuable Depth Regions)". To train the DCA subnet, we should prepare its training instances (i.e., RGB-D images) with well labelled MVDR as the learning objectives.</p><p>In fact, there exists one remarkable common attribute of those MVDR, i.e., the D saliency (DSal ∈ R 224×224 ) of those MVDR should outperform the corresponding RGB saliency (RGBSal ∈ R 224×224 ). Since the RGBSal, the DSal, and the human annotated binary saliency GT are simultaneously available for the current training stage, we formulate the pseudo-GT (pGT = {P + B}) of the DCA training set via Eq. 3 and Eq. 4, attempting to weakly train the DCA subnet for the prediction of those MVDR in the given RGB-D image. <ref type="figure">Fig. 4</ref>: The qualitative demonstrations of the ω (the 6th column) predicted by DCA subnet, in which the 5th column and the 7th column show the performance variation between these two different fusion schemes, i.e., without using the ω (5th column) and using ω (7th column).</p><formula xml:id="formula_2">P = pos(DSal − RGBSal) GT larger DSal in the salient regions ,<label>(3)</label></formula><formula xml:id="formula_3">B = pos(RGBSal − DSal) (1 − GT ) smaller DSal in the non−salient regions ,<label>(4)</label></formula><p>where GT ∈ {0, 1} denotes the human well annotated binary saliency ground truth; denotes the element-wise Hadamard product; pos is a function which assigns those negative elements to zero. Actually, for those regions inside the salient regions indicated by GT , Eq. 3 aims to locate those regions with larger DSal than RGBSal, which means that such D regions can better highlight the salient regions than their RGB counterparts. Meanwhile, for those regions inside the non-salient regions indicated by GT , Eq. 4 aims to locate those regions with smaller DSal than RGBSal, which means that such D regions can well suppress those non-salient surroundings.</p><p>So, we formulate the pGT = {P +B}, and its key rationales can be summarized into the following two aspects: 1) in the case of those D regions with their GT = 1, it is quite intuitive to assign these D regions as MVDR if their DSal &gt; RGBSal; 2) on the other hand, as for those D regions with their GT = 0, we should also assign these D regions as MVDR if DSal &lt; RGBSal.</p><p>Then, we directly use the above weakly formulated pseudo-GTs as the training objective for our DCA subnet, and we represent its learning loss as Eq. 5.</p><formula xml:id="formula_4">Loss = L(ω, pGT ), ω = DeCoder EnCoder(RGBD) ,<label>(5)</label></formula><p>where L denotes the cross-entropy loss; ω ∈ [0, 1] 224×224 is the MVDR predicted by the DCA subnet, and we have demonstrated its qualitative results in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Why the MVDR (Most Valuable Depth Regions, ω) Can Be Learned?</head><p>Generally speaking, the key rationale of the deep learning techniques is to memorize all of its given training instances, aiming to generalize toward the given learning objective when facing unseen data. Thus, a "reasonable" learning objective is extremely important for deep networks to output their desired results, e.g., we shouldn't anticipate to train a deep network for an impossible mission, such as predicting lottery winning numbers.</p><p>In our case, the learning objective for the DCA (Depth Contribution Assessment) subnet (Sec. III-C) is to learn how to predict/locate those D regions which are potentially able to benefit their RGB counterparts for the salient object detection task. So, following the conventional hand-crafted thinking model, we may approximately achieve the aforementioned learning objective by conducting the following two sequential tasks: 1) because only those high-quality D regions may potentially be able to benefit their RGB counterparts, we coarsely locate those high-quality D regions first; 2) then, based on those high-quality D regions determined by the task 1), we preserve those D regions which are able to provide more accurate saliency clues than their RGB counterparts.</p><p>Theoretically, it is quite intuitive to fulfill the task 1) via the following principle: those high-quality D regions must be nearby those image pixels which exhibit strong consistency in their gradient values between the RGB channel and the D channel. As for the task 2), because both RGBSal and DSal are simultaneously available, we can easily locate the MVDR by simply filtering those D regions which have similar RGBSal and DSal.</p><p>Thus far, we have provided a feasible hand-crafted solution to approximately full fill the learning objective of the DCA subnet, showing its technical soundness. Also, it is worthy mentioning that the learned DCA subnet is certain to outperform the aforementioned hand-crafted manner in both accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multi-scale Fusion Subnet</head><p>By using the DCA subnet, we can easily obtain the MVDR (Most Valuable Depth Regions, ω) in the given testing RGB-D image. Thus, we may simply achieve an explicit RGB-D saliency fusion (RGBDSal) by using Eq. 6.</p><formula xml:id="formula_5">RGBDSal = ω DSal + (1 − ω) RGBSal. (6)</formula><p>In fact, the above hand-crafted RGBDSal is slightly worse than the conventional bi-stream selective fusion, and its fusion performance is heavily dependent on either the DSal or the RGBSal. To further improve, we use the ω to guide the feature-level selective deep fusion between the RGB saliency subnet and the D saliency subnet.</p><p>To be specific, for each one of the last four decoder feature blocks, we convolve it as the side-output with fixed channel number (64) while leaving its resolution unchanged. For example, as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, the first side-output of the RGB saliency subnet (RGB s 1 ) can be formulated by using 64 {1 × 1} kernels over the feature block F . Thus, we formulate the feature-level fusion as Eq. 7.</p><formula xml:id="formula_6">M i = ω D s i + (1 − ω) RGB s i , i ∈ [1, 4],<label>(7)</label></formula><p>where M i denote the i-th fused RGB-D deep features. To achieve an optimal complementary fusion status between the multi-scale deep features, i.e., the M i <ref type="figure" target="#fig_0">(Fig. 3)</ref>, we compute the final RGB-D saliency (F Sal) via recursive iterations, which consist of the following sequential steps:</p><formula xml:id="formula_7">1. temp ← A Conv C U (M 1 ), M 2 , 2. temp ← A Conv C U (temp), M 3 , 3. temp ← A Conv C U (temp), M 4 , 4. F Sal ← Conv(temp),<label>(8)</label></formula><p>where U and C respectively denote the up-sampling operation and feature concatenation operation; A denotes the spatial attention operation as Eq. 9, in which h denotes a 1 × 1 convolution with 1 output channel; temp is an auxiliary container.</p><p>temp ← 1 + Sigmoid h(temp) × temp.</p><p>The behind rationale of the above recursive iterations (Eq. 8) is to make full use of both the salient object localization information provided by the low-resolution layers, and the tiny saliency details provided by the high-resolution layers. Meanwhile, the spatial attention operations aim to highlight those most valuable saliency clues between different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Network Training</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, our network consists of 4 components, including: 1) the RGB saliency subnet (top); 2) the D saliency subnet (bottom); 3) the DCA (Depth Contribution Assessment) subnet (middle); 4) the MSF (Multi-scale Fusion) subnet which is above the DCA subnet.</p><p>Sine the pseudo-GTs (Sec. III-C) are indispensable for the DCA subnet training, we first train the RGB and D saliency subnets respectively (see the stage 1 in <ref type="figure" target="#fig_1">Fig. 5</ref>). Next, we weakly train the DCA subnet by using the pseudo-GTs formulated via Eq. 3 and Eq. 4, see the stage 2 in <ref type="figure" target="#fig_1">Fig. 5</ref>. Once the DCA subnet has been trained, we use its prediction ω to guide the feature-level deep fusion, obtaining the multiscale RGB-D deep features (i.e., M1, M2, M3 and M4 in <ref type="figure" target="#fig_1">Fig. 5</ref>). Because the performance of such feature-level fusion is positively related to both its fusion inputs (i.e., RGBSal and DSal), we attempt to further improve the performance of the RGB saliency subnet by connecting its encoder layers with the multi-scale deep features of the encoding layers in the D saliency subnet, and then we fine-tune this novel RGB saliency subnet as the Stage-3 in <ref type="figure" target="#fig_1">Fig. 5</ref>. Finally, we jointly fine-tune the entire network over the entire training set, including the MSF subnet as well (i.e., the stage 4 in <ref type="figure" target="#fig_1">Fig. 5)</ref>.</p><p>Specifically, the pseudo-GTs for the DCA subnet training may be ill-formulated if we use the entire training set to train the RGB and D saliency subnets. To avoid such case, we equally divide the training set into 2 parts, one for the stage-wise training of the RGB and D saliency subnets, and another facilitate the formulation of the pseudo-GTs for the DCA subnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We have evaluated our approach on 5 publicly available datasets: NJUDS <ref type="bibr" target="#b40">[41]</ref>, NLPR <ref type="bibr" target="#b41">[42]</ref>, STEREO <ref type="bibr" target="#b42">[43]</ref>, DES <ref type="bibr" target="#b43">[44]</ref>, LFSD <ref type="bibr" target="#b44">[45]</ref>. The NJUDS <ref type="bibr" target="#b40">[41]</ref> dataset contains 1985 RGB-D images with well annotated binary saliency ground truth. The NLPR <ref type="bibr" target="#b41">[42]</ref> dataset contains 1000 RGB-D images from either the indoor or outdoor scenes. The STEREO <ref type="bibr" target="#b42">[43]</ref> dataset includes 1000 images with low-quality D, DES <ref type="bibr" target="#b43">[44]</ref> dataset has 135 indoor pictures taken by Kinect, and LFSD <ref type="bibr" target="#b44">[45]</ref> dataset has 100 RGB-D images.</p><p>In order to make a fair comparison with the SOTA methods, we follow the same training/testing data split scheme as <ref type="bibr" target="#b35">[36]</ref>, in which we divide the NJUDS dataset (1985 images) into 2 parts, i.e., 1400 for training and the rest for testing; we also divide the NLPR dataset (1K images) into 2 parts, 650 for training and the rest for testing; all the STEREO, DES and LFSD datasets mentioned above will be used for testing. Thus, our training set totally consists of 2050 images.</p><p>In addition, our RGB saliency subnet was pre-trained using two large-scale RGB datasets, i.e., the MSRA10K and DUTS datasets with a total of 20,553 RGB images; our D saliency subnet was pre-trained using the widely-used 2050 RGB-D images. All training and testing images are resized to 224 × 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Our experiments were performed on a workstation with a GTX1080Ti GPU (with 11G memory) and 64G RAM. Our network was implemented in Python 3.6 with TensorFlow1.4. We set the batch size to 4 and adopt the ADAM optimizer to update and calculate network parameters. All subnets in our method adopt the pre-trained VGG19 as the backbone net. It takes about 7 hours (15 epochs) to pre-train our RGB saliency subnet over the above mentioned RGB training set (20K). Then, it takes another 1 hour to fine-tune the D saliency subnet over the entire RGB-D training set (2K) using the D channel only (15 epochs). Then, the DCA (Depth Contribution Assessment) subnet training takes almost 3 hours. Finally, it takes about 1.6 hours (12 epochs with learning rate from 0.0001 to 0.00001) to jointly fine-tune the entire network. Our method totally needs almost 0.08s for a single RGB-D image during the testing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>We adopt the standard metrics to conduct the quantitative evaluations, including S-measure <ref type="bibr" target="#b45">[46]</ref>, E-measure <ref type="bibr" target="#b46">[47]</ref>, Fmeasure and MAE. The S-measure <ref type="bibr" target="#b45">[46]</ref> is a new structural similarity measure, which is defined as: Eq. 10.</p><formula xml:id="formula_9">S = α × S o + (1 − α) × S r ,<label>(10)</label></formula><p>where we set α = 0.5 to balance the region-aware (So) and object-aware (Sr) structural similarity. The F-measure metric takes both precision and recall into account simultaneously, and its definition is shown as:</p><formula xml:id="formula_10">F = β 2 + 1 × Precision × Recall β 2 × Precision + Recall ,<label>(11)</label></formula><p>were we set β 2 = 0.3 as suggested in <ref type="bibr" target="#b47">[48]</ref>.</p><p>The MAE <ref type="bibr" target="#b28">[29]</ref> definition is shown as:</p><formula xml:id="formula_11">MAE = 1 T T j=1 |S j − GT j |.<label>(12)</label></formula><p>where S and GT represent the saliency map and the saliency ground truth respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Component Evaluation</head><p>Overall component evaluation. To validate the effectiveness of each major component in our method, here, we have conducted an extensive component evaluation, including RGBSal, DSal, DCA (Depth Contribution Assessment) subnet (Sec. III-C), and the MSF (Multi-scale Fusion) subnet (Sec. III-E). The detailed quantitative results over the adopted datasets can be found in <ref type="table" target="#tab_0">Table I</ref>, in which the "RGBSal" denotes the results of the RGB saliency subnet and the "DSal" denotes the results of the D saliency subnet. Next, we have tested the conventional selective RGB-D fusion (denoted by the "RGBDSal" in <ref type="table" target="#tab_0">Table I)</ref> as <ref type="bibr" target="#b33">[34]</ref>, which effectively improves the overall detection performance significantly as expected. Then, we further use the ω (predicted by the DCA subnet) to guide the feature-level fusion before conducting the selective deep fusion (i.e., Eq. 7), which outperforms the "RGBDSal" significantly, showing the effectiveness of our DCA subnet.</p><p>Specifically, the total network parameter size of the proposed "triple-stream" network is about 120M, while the conventional "bi-stream" RGBDSal network is about 88M. To make our component evaluation more convincing, we have newly tested a new version "RGBDSal", in which we have increased the channel number of M 1−4 from 64 to 256, and thus the parameter size is increased from the original 88M to the current 120M, making this new network has almost the same parameter size as the proposed "+ω". We use "RGBDSal + " to denote the revised new version, whose performance has decreased slightly. The main reason causing this result may be explained from the following aspect: as we all know, we may not always achieve a better performance by simply increasing network capacity, and, in sharp contrast, a heavy network design with more parameters may lead to      difficulty in network training and even degenerate the overall performance. Finally, we further conduct the feature-level fusion with multi-scale selective fusion network ("+MSF") as mentioned in Sec. III-E, and it has achieved the best performance in all tested datasets, indicating the effectiveness of our novel multiscale selective fusion network.</p><formula xml:id="formula_12">Dataset NJDUS STEREO DES NLPR LFSD Metric Sm ↑ meanF ↑ MAE ↓ Sm ↑ meanF ↑ MAE ↓ Sm ↑ meanF ↑ MAE ↓ Sm ↑ meanF ↑ MAE ↓ Sm ↑ meanF ↑ MAE ↓ ADD 0.</formula><formula xml:id="formula_13">Dataset NJDUS STEREO DES NLPR LFSD Metric Sm ↑ meanF ↑ MAE ↓ Sm ↑ meanF ↑ MAE ↓ Sm ↑ meanF ↑ MAE ↓ Sm ↑ meanF ↑ MAE ↓ Sm ↑ meanF ↑ MAE ↓</formula><p>Ablation study on the DCA subnet. We have further conducted several experiments to show the effectiveness of our DCA (Depth Contribution Assessment) subnet (Sec. III-C), and the quantitative results can be found in <ref type="table" target="#tab_0">Table II</ref>, in which "ADD" and "CON " respectively denote conducting the feature-level RGB-D fusion by using the addition operation and the 1 × 1 convolutional operation, and the performance of these two schemes are limited due to their nature of depth quality-unaware; "P " and "B" respectively denote the results of training DCA subnet using either the P (Eq. 3) or B (Eq. 4) based pseudo-GT only, which have demonstrated slight performance improvement, but not much. In fact, as for the salient object detection in RGB-D image, the main contribution of the D channel are two aspects: 1) the D channel helps the RGB channel to highlight the salient objects (i.e., Eq. 3); 2) more importantly, the D channel can effectively suppress those non-salient nearby surroundings (Eq. 4). So, our DCA subnet may fail to measure the contribution of the D channel if we solely use either P or B during its training period. We have demonstrated the final results, i.e., "P + B" in the bottom row of <ref type="table" target="#tab_0">Table II</ref>, which shows the effectiveness of our DCA subnet.</p><p>Ablation study on the MSF subnet. We have further conducted several experiments to show the effectiveness of our MSF (Multi-scale Fusion) subnet (Sec. III-E), and the quantitative results can be found in <ref type="table" target="#tab_0">Table III</ref>. Once the DCA subnet has been trained, we can directly use its prediction (ω) to guide an explicit "SimpleFusion" between the stage-wise RGBSal and the stage-wise DSal as the Eq. 6. Since such fusion scheme has only used the stage-wise RGBSal and DSal, it is difficult for such fusion to achieve the full complementary status between the RGB subnet and the D subnet (i.e., the multi-scale information), and thus this scheme has exhibited the worst performance in <ref type="table" target="#tab_0">Table III (SimpleF usion)</ref>. Then, we have tested four different deep fusion schemes, in which these schemes are all based on the RGB-D deep features guided by the ω via Eq. 7. The "ω(RGB+D)" and the "ω(RGBD+D)" denote two schemes which use the ω to obtain four fused side-outputs of its decoder layers first, and then these sideoutputs will be simultaneously convolved as the final saliency output. The major difference between the ω(RGB + D) and the ω(RGBD + D) is that the ω(RGB + D) do not use any encoder short-connections between its RGB subnet and D subnet, while the ω(RGBD + D) has connected its RGB subnet with D subnet as the gray lines in the left part of <ref type="figure" target="#fig_0">Fig. 3</ref>. The quantitative results (ω(RGBD+D)&gt;ω(RGB +D)) have suggested that the complementary fusion between RGB and D can be benefited from both the following 2 aspects: 1) the classic RGB-D selective deep fusion at the encoder stage (to obtain RGBD saliency); 2) our ω guided selective deep fusion at the decoder layer (to complement the RGBD saliency with the D saliency). Moreover, we have tested the performance of our MSF subnet, in which we feed the aforementioned four fused side-outputs into the MSF subnet to compute the final saliency map. Similarly, we have tested two different schemes, i.e., "M SF (RGB+D)" and "M SF (RGBD+D)", which respectively correlate to the ω(RGB + D) and the ω(RGBD + D), showing the effectiveness of using RGBD saliency to replace RGB saliency during fusion. Meanwhile, such results have also demonstrated the advantages of our MSF network than the conventional selective fusion scheme. Specially, we have listed the detailed component evaluation network architecture in <ref type="figure" target="#fig_3">Fig. 7</ref>. Also, we have demonstrated the qualitative demonstrations of each key component in <ref type="figure" target="#fig_2">Fig. 6</ref>.</p><p>Effectiveness of B (Eq. 4). Compared with the "RGBDSal" that adopts the full automatic fusion scheme, the quantitative/qualitative results of "+ω" are simply obtained by performing weighted summation between RGBSal and DSala typical hand-crafted manner, which has overlooked the "multi-scale" complementary nature between RGBSal and DSal, leading to limited performance frequently. Nevertheless, because of its special advantage-depth quality aware, the "+ω" can still outperform the RGBDSal in terms of both detection completeness and accuracy.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 6</ref>, though the "+ω" can improve the overall performance, it may occasionally produce more falsealarm detections in non-salient regions due to the following two reasons: 1) Since the "+ω" has followed a hand-crafted methodology, the fusion procedure cannot take full advantage of the newly available depth contribution assessment map, i.e., the "ω". 2) Moreover, the "ω" itself still exists incorrect predictions (from both P and B), producing false-alarm detections easily in non-salient regions. Thus, though P may bring more complete SOD, it easily introduces more false-alarm detections in non-salient regions, even in the case that B can compress these false-alarm detections in some extent. In a word, the demonstrations in <ref type="figure" target="#fig_2">Fig. 6</ref> are the results determined by both P and B jointly, and it is quite reasonable to illustrate more false-alarm detections occasionally.</p><p>Also, the "B" component is designed to predict those most valuable depth regions in non-salient regions, in which the value of depth channel in non-salient regions is just to suppress those false-alarm detections. Therefore, the effectiveness of B towards suppressing non-salient regions only relies in the case whether those regions with strong feature responses in B must correlate to low feature responses in DSal. To support this claim, we have provided an additional quantitative result towards the relationship between P, B and P+B in terms of suppressing non-salient image regions.</p><p>As shown in <ref type="table" target="#tab_0">Table IV</ref>, ω 1 and ω 2 are measured by Eq. 13 and Eq. 14 respectively.</p><formula xml:id="formula_14">ω 1 = ||(1 − GT ) (ω − 0.8) + || 0 ||(ω − 0.8) + || 0 ,<label>(13)</label></formula><formula xml:id="formula_15">ω 2 = ||(1 − GT ) (0.1 − DSal) + (ω − 0.8) + || 0 ||(1 − GT ) (ω − 0.8) + || 0 ,<label>(14)</label></formula><p>where (·) + set all negative elements into zero; || · || 0 denotes the L 0 -norm; 0.1 and 0.8 are two predefined hard-thresholds. The behind rationale of "ω 1 " is to measure the percentage of high-quality D regions predicted by the DCA in the nonsalient regions. The "ω 2 " further measures the percentage of regions in ω 1 which can really compress non-salient regions.</p><p>From <ref type="table" target="#tab_0">Table IV</ref>, we can easily notice the following three aspects: 1) Compared with the component P, the component B tends to compress non-salient regions in general.</p><p>2) As expected, the combined P+B is better than P but worse than B in compressing the non-salient regions. These two aspects can well explain why the "+ω" (i.e., the P+B) may occasionally produce more false-alarm detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with the SOTA Methods</head><p>We have compared our method with 12 most recent SOTA (3 top-level RGB methods and 9 RGB-D methods) methods over the aforementioned 5 datasets: 1) the compared RGB methods include CPD19 <ref type="bibr" target="#b50">[51]</ref>, EGNet19 <ref type="bibr" target="#b30">[31]</ref> and BASNet19 <ref type="bibr" target="#b51">[52]</ref>; 2) the compared RGB-D methods include MDSF17 <ref type="bibr" target="#b52">[53]</ref>, DF17 <ref type="bibr" target="#b32">[33]</ref>, PDNet18 <ref type="bibr" target="#b33">[34]</ref>, CTMF18 <ref type="bibr" target="#b17">[18]</ref>, PCF18 <ref type="bibr" target="#b13">[14]</ref>, AFNet19 <ref type="bibr" target="#b14">[15]</ref>, MMCI19 <ref type="bibr" target="#b53">[54]</ref>, TANet19 <ref type="bibr" target="#b34">[35]</ref>, and CPFP19 <ref type="bibr" target="#b35">[36]</ref>.</p><p>We have demonstrated the detailed quantitative comparison results in <ref type="table" target="#tab_6">Table V</ref>, and the qualitative comparisons can be found in <ref type="figure" target="#fig_4">Fig. 8</ref>. As shown in <ref type="table" target="#tab_6">Table V</ref>, our method has achieved the best performance in all the tested datasets except the STEREO dataset. Since the depth maps of STEREO are  frequently with extremely low-quality, which cannot separate the salient objects from their non-salient surroundings nearby in most case. Thus, the performance gain came from the DSal subbranch, which meant to complement the RGBSal for a better overall performance, may get vanished and lead to inferior performance than the conventional color saliency models such as BASNet19. Moreover, in sharp contrast to the conventional SOTA RGB-D salient object detection methods, our method is depth quality aware, which is able to bias the fusion balance to its RGB subnet in the STEREO dataset, avoiding the performance degeneration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Different Feature Backbones</head><p>We have newly tested our method by using other two feature extractors, i.e., ResNet <ref type="bibr" target="#b38">[39]</ref> and Res2Net <ref type="bibr" target="#b39">[40]</ref>. As shown in <ref type="table" target="#tab_0">Table VI</ref>, we have listed 7 (6 Ours + 1 CPFP, where we use the CPFP as the reference) quantitative results after using different feature backbones and different training strategies, including {VGG, VGG*}, {ResNet50, ResNet50*}, and {Res2Net, Res2Net*}, where "*" denotes that the model is initially pre-trained using additional 20K RGB images.</p><p>We have noticed that the pre-training process using 20K RGB images will improve the performance of the VGG based model significantly, while such improvements become marginal towards the ResNet and Res2Net based models.</p><p>For a fair comparison, we choose the "Ours(ResNet)" to represent the overall performance of the proposed method, which is trained without using any additional RGB images. Compared with the vanilla CPFP19, our method outperforms it in all tested datasets. Specifically, our method outperforms the CPFP19 in the DES dataset significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Limitations</head><p>Compared with the conventional selective bi-stream fusion methods (e.g., PDNet), the key innovation of the proposed method is to devise a novel depth quality aware venue to eliminate side-effects from the low-quality depth information by biasing more towards the DSal during the fusion process. Thus, the overall performance of our method is heavily dependent on its subbranches, i.e., the RGBSal subbranch and the DSal subbranch. As a result, our method cannot perform well when both RGB and D streams could not well detect the salient objects, which is clearly a common problem of the bi-stream based SOD methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper has proposed a novel RGB-D salient object detection methods. We have followed the widely used bistream structure as the baseline network, however, the bistream structure has one major limitation: it is unaware of the depth quality, which may limit its performance in facing of images with low-quality D. To handle this limitation, we have adopted the weakly supervised learning scheme to train a novel subnet named DCA (Depth Contribution Assessment). This novel DCA subnet is able to guide an explicit featurelevel RGB-D fusion before conducting the selective deep fusion, making the conventional RGB-D bi-stream structure to become depth quality aware. Moreover, we have introduced a novel selective deep fusion scheme to take full advantage of the DCA based multi-scale complementary information between the RGB subnet and the D subnet, achieving a much improved RGB-D salient object detection. Finally, we have conducted massive quantitative evaluation to validate the effectiveness of our method. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>The overall network architecture of our proposed method, and we have listed the network details in the bottom-right; we have demonstrated the highlight of this paper in the middle, i.e., the DCA (Depth Contribution Assessment) subnet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>The demonstrations of our stage-wise training scheme (Sec. III-F).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>The qualitative demonstrations of several important component mentioned in our ablation experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>The detailed component evaluation network configurations, in which the "Final" denotes the final saliency detection results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative comparisons with the SOTA methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>The overall component evaluation. ↑ denotes larger is better, and ↓ denotes smaller is better.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>NJDUS</cell><cell></cell><cell></cell><cell>STEREO</cell><cell></cell><cell></cell><cell>DES</cell><cell></cell><cell></cell><cell>NLPR</cell><cell></cell><cell></cell><cell>LFSD</cell><cell></cell></row><row><cell>Metric</cell><cell>Sm ↑</cell><cell>meanF ↑</cell><cell>MAE ↓</cell><cell>Sm ↑</cell><cell>meanF ↑</cell><cell>MAE ↓</cell><cell>Sm ↑</cell><cell>meanF ↑</cell><cell>MAE ↓</cell><cell>Sm ↑</cell><cell>meanF ↑</cell><cell>MAE ↓</cell><cell>Sm ↑</cell><cell>meanF ↑</cell><cell>MAE ↓</cell></row><row><cell>RGBSal</cell><cell>0.856</cell><cell>0.801</cell><cell>0.080</cell><cell>0.883</cell><cell>0.825</cell><cell>0.063</cell><cell>0.874</cell><cell>0.792</cell><cell>0.047</cell><cell>0.896</cell><cell>0.819</cell><cell>0.042</cell><cell>0.808</cell><cell>0.756</cell><cell>0.120</cell></row><row><cell>DSal</cell><cell>0.827</cell><cell>0.777</cell><cell>0.091</cell><cell>0.769</cell><cell>0.683</cell><cell>0.116</cell><cell>0.905</cell><cell>0.835</cell><cell>0.040</cell><cell>0.834</cell><cell>0.739</cell><cell>0.063</cell><cell>0.757</cell><cell>0.702</cell><cell>0.140</cell></row><row><cell>RGBDSal</cell><cell>0.885</cell><cell>0.850</cell><cell>0.056</cell><cell>0.883</cell><cell>0.838</cell><cell>0.056</cell><cell>0.920</cell><cell>0.870</cell><cell>0.028</cell><cell>0.908</cell><cell>0.853</cell><cell>0.033</cell><cell>0.827</cell><cell>0.796</cell><cell>0.102</cell></row><row><cell>RGBDSal +</cell><cell>0.874</cell><cell>0.840</cell><cell>0.061</cell><cell>0.870</cell><cell>0.823</cell><cell>0.060</cell><cell>0.911</cell><cell>0.873</cell><cell>0.027</cell><cell>0.901</cell><cell>0.851</cell><cell>0.033</cell><cell>0.789</cell><cell>0.76</cell><cell>0.110</cell></row><row><cell>+ω</cell><cell>0.891</cell><cell>0.855</cell><cell>0.056</cell><cell>0.889</cell><cell>0.838</cell><cell>0.055</cell><cell>0.931</cell><cell>0.885</cell><cell>0.026</cell><cell>0.917</cell><cell>0.866</cell><cell>0.031</cell><cell>0.848</cell><cell>0.813</cell><cell>0.091</cell></row><row><cell>+MSF</cell><cell>0.897</cell><cell>0.873</cell><cell>0.052</cell><cell>0.892</cell><cell>0.854</cell><cell>0.051</cell><cell>0.935</cell><cell>0.901</cell><cell>0.021</cell><cell>0.916</cell><cell>0.864</cell><cell>0.029</cell><cell>0.851</cell><cell>0.826</cell><cell>0.085</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Ablation study on the DCA subnet. ↑ denotes larger is better, and ↓ denotes smaller is better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Ablation study on the MSF subnet. ↑ denotes larger is better, and ↓ denotes smaller is better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Quantitative evidence towards the effectiveness of the component B (Eq. 4).</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>NJDUS</cell><cell></cell><cell></cell><cell>STEREO</cell><cell></cell><cell></cell><cell>DES</cell><cell></cell><cell></cell><cell>NLPR</cell><cell></cell><cell></cell><cell>LFSD</cell><cell></cell></row><row><cell>w</cell><cell>P</cell><cell>B</cell><cell>P+B</cell><cell>P</cell><cell>B</cell><cell>P+B</cell><cell>P</cell><cell>B</cell><cell>P+B</cell><cell>P</cell><cell>B</cell><cell>P+B</cell><cell>P</cell><cell>B</cell><cell>P+B</cell></row><row><cell>ω1</cell><cell>29.9%</cell><cell>62.2%</cell><cell>45.9%</cell><cell>39.5%</cell><cell>66.5%</cell><cell>51.8%</cell><cell>29.9%</cell><cell>67.7%</cell><cell>50.0%</cell><cell>39.3%</cell><cell>69.7%</cell><cell>57.2%</cell><cell>30.1%</cell><cell>63.9%</cell><cell>45.1%</cell></row><row><cell>ω2</cell><cell>49.9%</cell><cell>74.5%</cell><cell>66.8%</cell><cell>30.9%</cell><cell>63.9%</cell><cell>52.3%</cell><cell>40.1%</cell><cell>71.7%</cell><cell>61.0%</cell><cell>50.2%</cell><cell>74.0%</cell><cell>65.5%</cell><cell>41.9%</cell><cell>71.3%</cell><cell>59.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Quantitative comparison results including S-measure, meanF and MAE on 5 public datasets. ↑ denotes larger is better, and ↓ denotes smaller is better. The top results are highlighted in bold font. NLR: NLPR<ref type="bibr" target="#b41">[42]</ref>; NJU: NJUDS<ref type="bibr" target="#b40">[41]</ref>; O: MSRA10K [49] + DUTS TR<ref type="bibr" target="#b49">[50]</ref>.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Training Details</cell><cell></cell><cell>NJDUS</cell><cell></cell><cell></cell><cell>STEREO</cell><cell></cell><cell></cell><cell>DES</cell><cell></cell><cell></cell><cell>NLPR</cell><cell></cell><cell></cell><cell>LFSD</cell><cell></cell></row><row><cell>Metric</cell><cell>Image Num.</cell><cell>Dataset</cell><cell>Sm ↑</cell><cell>meanF ↑</cell><cell>MAE ↓</cell><cell>Sm ↑</cell><cell>meanF ↑</cell><cell>MAE ↓</cell><cell>Sm ↑</cell><cell cols="2">meanF ↑ MAE ↓</cell><cell>Sm ↑</cell><cell cols="2">meanF ↑ MAE ↓</cell><cell>Sm ↑</cell><cell cols="2">meanF ↑ MAE ↓</cell></row><row><cell>CPD19 [51]</cell><cell>0.70K+1.5K</cell><cell>NLR+NJU</cell><cell>0.872</cell><cell>0.847</cell><cell>0.059</cell><cell>0.888</cell><cell>0.856</cell><cell>0.050</cell><cell>0.877</cell><cell>0.849</cell><cell>0.038</cell><cell>0.901</cell><cell>0.860</cell><cell>0.034</cell><cell>0.797</cell><cell>0.771</cell><cell>0.112</cell></row><row><cell>EGNet19 [31]</cell><cell>0.70K+1.5K</cell><cell>NLR+NJU</cell><cell>0.818</cell><cell>0.784</cell><cell>0.088</cell><cell>0.826</cell><cell>0.786</cell><cell>0.079</cell><cell>0.811</cell><cell>0.777</cell><cell>0.058</cell><cell>0.850</cell><cell>0.796</cell><cell>0.051</cell><cell>0.816</cell><cell>0.790</cell><cell>0.103</cell></row><row><cell>BASNet19 [52]</cell><cell>0.70K+1.5K</cell><cell>NLR+NJU</cell><cell>0.826</cell><cell>0.797</cell><cell>0.082</cell><cell>0.857</cell><cell>0.823</cell><cell>0.063</cell><cell>0.826</cell><cell>0.772</cell><cell>0.052</cell><cell>0.889</cell><cell>0.841</cell><cell>0.037</cell><cell>0.748</cell><cell>0.717</cell><cell>0.132</cell></row><row><cell>MDSF17 [53]</cell><cell>0.50K+0.5K</cell><cell>NLR+NJU</cell><cell>0.748</cell><cell>0.628</cell><cell>0.157</cell><cell>0.728</cell><cell>0.527</cell><cell>0.176</cell><cell>0.741</cell><cell>0.523</cell><cell>0.122</cell><cell>0.805</cell><cell>0.649</cell><cell>0.095</cell><cell>0.700</cell><cell>0.521</cell><cell>0.190</cell></row><row><cell>DF17 [33]</cell><cell>0.75K+1.0K</cell><cell>NLR+NJU</cell><cell>0.763</cell><cell>0.650</cell><cell>0.141</cell><cell>0.757</cell><cell>0.617</cell><cell>0.141</cell><cell>0.752</cell><cell>0.604</cell><cell>0.093</cell><cell>0.802</cell><cell>0.664</cell><cell>0.085</cell><cell>0.791</cell><cell>0.679</cell><cell>0.138</cell></row><row><cell>CTMF18 [18]</cell><cell>0.65K+1.4K</cell><cell>NLR+NJU</cell><cell>0.849</cell><cell>0.779</cell><cell>0.085</cell><cell>0.848</cell><cell>0.758</cell><cell>0.086</cell><cell>0.863</cell><cell>0.756</cell><cell>0.055</cell><cell>0.860</cell><cell>0.740</cell><cell>0.056</cell><cell>0.796</cell><cell>0.756</cell><cell>0.119</cell></row><row><cell>PCF18 [14]</cell><cell>0.70K+1.5K</cell><cell>NLR+NJU</cell><cell>0.877</cell><cell>0.840</cell><cell>0.059</cell><cell>0.875</cell><cell>0.818</cell><cell>0.064</cell><cell>0.842</cell><cell>0.765</cell><cell>0.049</cell><cell>0.874</cell><cell>0.802</cell><cell>0.044</cell><cell>0.794</cell><cell>0.761</cell><cell>0.112</cell></row><row><cell>PDNet18 [34]</cell><cell cols="2">0.70K+1.5K+21K NLR+NJU+O</cell><cell>0.877</cell><cell>0.814</cell><cell>0.071</cell><cell>0.830</cell><cell>0.730</cell><cell>0.092</cell><cell>0.887</cell><cell>0.795</cell><cell>0.045</cell><cell>0.887</cell><cell>0.802</cell><cell>0.050</cell><cell>0.847</cell><cell>0.779</cell><cell>0.107</cell></row><row><cell>AFNet19 [15]</cell><cell>0.70K+1.5K</cell><cell>NLR+NJU</cell><cell>0.772</cell><cell>0.764</cell><cell>0.100</cell><cell>0.825</cell><cell>0.806</cell><cell>0.075</cell><cell>0.770</cell><cell>0.713</cell><cell>0.068</cell><cell>0.799</cell><cell>0.755</cell><cell>0.058</cell><cell>0.738</cell><cell>0.735</cell><cell>0.133</cell></row><row><cell>MMCI19 [54]</cell><cell>0.70K+1.5K</cell><cell>NLR+NJU</cell><cell>0.858</cell><cell>0.793</cell><cell>0.079</cell><cell>0.873</cell><cell>0.813</cell><cell>0.068</cell><cell>0.848</cell><cell>0.735</cell><cell>0.065</cell><cell>0.856</cell><cell>0.737</cell><cell>0.059</cell><cell>0.787</cell><cell>0.722</cell><cell>0.132</cell></row><row><cell>TANet19 [35]</cell><cell>0.70K+1.5K</cell><cell>NLR+NJU</cell><cell>0.878</cell><cell>0.841</cell><cell>0.060</cell><cell>0.871</cell><cell>0.828</cell><cell>0.060</cell><cell>0.858</cell><cell>0.790</cell><cell>0.046</cell><cell>0.886</cell><cell>0.819</cell><cell>0.041</cell><cell>0.801</cell><cell>0.771</cell><cell>0.111</cell></row><row><cell>CPFP19 [36]</cell><cell>0.70K+1.5K</cell><cell>NLR+NJU</cell><cell>0.878</cell><cell>0.850</cell><cell>0.053</cell><cell>0.879</cell><cell>0.841</cell><cell>0.051</cell><cell>0.872</cell><cell>0.824</cell><cell>0.038</cell><cell>0.888</cell><cell>0.840</cell><cell>0.036</cell><cell>0.828</cell><cell>0.811</cell><cell>0.088</cell></row><row><cell>Ours(ResNet)</cell><cell>0.70K+1.5K</cell><cell>NLR+NJU</cell><cell>0.889</cell><cell>0.863</cell><cell>0.051</cell><cell>0.880</cell><cell>0.846</cell><cell>0.049</cell><cell>0.912</cell><cell>0.884</cell><cell>0.025</cell><cell>0.903</cell><cell>0.866</cell><cell>0.032</cell><cell>0.831</cell><cell>0.810</cell><cell>0.086</cell></row><row><cell>Ours(VGG)</cell><cell cols="2">0.70K+1.5K+21K NLR+NJU+O</cell><cell>0.897</cell><cell>0.873</cell><cell>0.052</cell><cell>0.892</cell><cell>0.854</cell><cell>0.051</cell><cell>0.935</cell><cell>0.901</cell><cell>0.021</cell><cell>0.916</cell><cell>0.864</cell><cell>0.029</cell><cell>0.851</cell><cell>0.826</cell><cell>0.085</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Quantitative results using different feature backbones. The top three are highlighted in red, blue and green. "*" denotes that the model is initially pre-trained using additional 20K RGB images.</figDesc><table><row><cell></cell><cell></cell><cell>NJUDS</cell><cell></cell><cell></cell><cell>STEREO</cell><cell></cell><cell></cell><cell>DES</cell><cell></cell><cell></cell><cell>NLPR</cell><cell></cell><cell></cell><cell>LFSD</cell><cell></cell></row><row><cell>Method</cell><cell>Sm ↑</cell><cell>meanF ↑</cell><cell>MAE ↓</cell><cell>Sm ↑</cell><cell>meanF ↑</cell><cell>MAE ↓</cell><cell>Sm ↑</cell><cell cols="2">meanF ↑ MAE ↓</cell><cell>Sm ↑</cell><cell cols="2">meanF ↑ MAE ↓</cell><cell>Sm ↑</cell><cell cols="2">meanF ↑ MAE ↓</cell></row><row><cell>CPFP19</cell><cell>0.878</cell><cell>0.850</cell><cell>0.053</cell><cell>0.879</cell><cell>0.841</cell><cell>0.051</cell><cell>0.872</cell><cell>0.824</cell><cell>0.038</cell><cell>0.888</cell><cell>0.840</cell><cell>0.036</cell><cell>0.828</cell><cell>0.811</cell><cell>0.088</cell></row><row><cell>Ours(VGG)</cell><cell>0.854</cell><cell>0.814</cell><cell>0.074</cell><cell>0.855</cell><cell>0.808</cell><cell>0.068</cell><cell>0.915</cell><cell>0.884</cell><cell>0.027</cell><cell>0.875</cell><cell>0.817</cell><cell>0.046</cell><cell>0.796</cell><cell>0.768</cell><cell>0.112</cell></row><row><cell>Ours(Res2Net)</cell><cell>0.883</cell><cell>0.857</cell><cell>0.054</cell><cell>0.877</cell><cell>0.840</cell><cell>0.053</cell><cell>0.917</cell><cell>0.886</cell><cell>0.025</cell><cell>0.901</cell><cell>0.860</cell><cell>0.034</cell><cell>0.827</cell><cell>0.808</cell><cell>0.095</cell></row><row><cell>Ours*(ResNet)</cell><cell>0.886</cell><cell>0.865</cell><cell>0.057</cell><cell>0.891</cell><cell>0.860</cell><cell>0.047</cell><cell>0.913</cell><cell>0.888</cell><cell>0.025</cell><cell>0.910</cell><cell>0.868</cell><cell>0.030</cell><cell>0.829</cell><cell>0.810</cell><cell>0.093</cell></row><row><cell>Ours*(Res2Net)</cell><cell>0.883</cell><cell>0.864</cell><cell>0.055</cell><cell>0.891</cell><cell>0.863</cell><cell>0.046</cell><cell>0.933</cell><cell>0.910</cell><cell>0.020</cell><cell>0.912</cell><cell>0.874</cell><cell>0.030</cell><cell>0.824</cell><cell>0.806</cell><cell>0.094</cell></row><row><cell>Ours(ResNet)</cell><cell>0.889</cell><cell>0.863</cell><cell>0.051</cell><cell>0.880</cell><cell>0.846</cell><cell>0.049</cell><cell>0.912</cell><cell>0.884</cell><cell>0.025</cell><cell>0.903</cell><cell>0.866</cell><cell>0.032</cell><cell>0.831</cell><cell>0.810</cell><cell>0.086</cell></row><row><cell>Ours*(VGG)</cell><cell>0.897</cell><cell>0.873</cell><cell>0.052</cell><cell>0.892</cell><cell>0.854</cell><cell>0.051</cell><cell>0.935</cell><cell>0.901</cell><cell>0.021</cell><cell>0.916</cell><cell>0.864</cell><cell>0.029</cell><cell>0.851</cell><cell>0.826</cell><cell>0.085</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments. This research is supported in part by</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>IEEE Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved robust video saliency detection based on long-term spatial-temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Process. (TIP)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1090" to="1100" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video saliency detection via spatial-temporal fusion and low-rank coherency diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Process. (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3156" to="3170" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate and robust video saliency detection via selfpaced diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia (TMM), p. early access</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bi-level feature learning for video saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3324" to="3336" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The application of visual saliency models in objective image quality assessment: A statistical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Netw. Learn. Syst. (TNNLS)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1266" to="1278" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time and robust object tracking in video via low-rank coherency analysis in feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. (PR)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2885" to="2905" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust salient motion detection in non-stationary videos via novel integrated strategies of spatio-temporal coherency clues and low-rank analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. (PR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="410" to="432" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Res-pca: A scalable approach to recovering low-rank matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Salient object detection via multiple instance joint re-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure-sensitive saliency detection via multilevel rank analysis in intrinsic feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Process. (TIP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2303" to="2316" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting global priors for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive fusion for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="55" to="277" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cnns-based rgb-d saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning rgb-d salient object detection using background enclosure, depth contrast, and topdown features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shigematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. W. (ICCVW</title>
		<meeting>IEEE Int. Conf. Comput. Vis. W. (ICCVW</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2749" to="2757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cnns-based rgb-d saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Cybern. (TCYB)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Salient object detection via structured matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="818" to="832" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Co-saliency detection for rgbd images based on multi-constraint feature matching and cross label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Process. (TIP)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="568" to="579" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semisupervised learning based on a novel iterative optimization model for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Netw. Learn. Syst. (TNNLS)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="225" to="241" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>IEEE Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contrast-oriented deep neural networks for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Netw. Learn. Syst. (TNNLS)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6038" to="6051" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Motion guided attention for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7274" to="7283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Motion guided attention for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Local background enclosure for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Process. (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pdnet: Prior-model guided depth-enhanced network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Multimedia and Expo (ICME)</title>
		<meeting>IEEE Int. Conf. on Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Process. (TIP)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<publisher>MIC-CAI</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection using anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>IEEE Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Internet Multimedia Computing and Service</title>
		<meeting>International Conference on Internet Multimedia Computing and Service</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2806" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Enhancedalignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Process. (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multiscale multi-path and cross-modal interactions for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
