<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformers without Tears: Improving the Normalization of Self-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toan</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
							<email>tnguye28@nd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Salazar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame †</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Amazon AWS AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformers without Tears: Improving the Normalization of Self-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We evaluate three simple, normalizationcentric changes to improve Transformer training. First, we show that pre-norm residual connections (PRENORM) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose 2 normalization with a single scale parameter (SCALENORM) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FIXNORM). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the highresource setting (WMT '14 English-German), SCALENORM and FIXNORM remain competitive but PRENORM degrades performance. Preprocessing scripts and code are released at https://github.com/tnq177/ transformers_without_tears.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> has become the dominant architecture for neural machine translation (NMT) due to its train-time parallelism and strong downstream performance. Various modifications have been proposed to improve the efficiency of its multi-head attention and feedforward sublayers <ref type="bibr" target="#b6">(Guo et al., 2019;</ref><ref type="bibr" target="#b33">Sukhbaatar et al., 2019)</ref>. Our work focuses on layer normalization (LAYERNORM) , which we show has an outsized role in the * Equal contribution. † Work done during an internship at Amazon AWS AI.</p><p>convergence and performance of the Transformer in two ways:</p><p>Placement of normalization.</p><p>The original Transformer uses post-norm residual units (POSTNORM), where layer normalization occurs after the sublayer and residual addition. However, <ref type="bibr" target="#b3">Chen et al. (2018)</ref> found that pre-norm residual units (PRENORM), where layer normalization occurs immediately before the sublayer, were instrumental to their model's performance. <ref type="bibr" target="#b38">Wang et al. (2019)</ref> compare the two, showing that PRENORM makes backpropagation more efficient over depth and training Transformers with deep, 30-layer encoders.</p><p>Our work demonstrates additional consequences in the base (≤6-layer encoder) Transformer regime. We show that PRENORM enables warmup-free, validation-based training with large learning rates even for small batches, in contrast to past work on scaling NMT <ref type="bibr" target="#b21">(Ott et al., 2018)</ref>. We also partly reclaim POSTNORM's stability via smaller initializations, although PRENORM is less sensitive to this magnitude and can improve performance. However, despite PRENORM's recent adoption in many NMT frameworks, we find it degrades base Transformer performance on WMT '14 English-German.</p><p>Choice of normalization. <ref type="bibr" target="#b29">Santurkar et al. (2018)</ref> show that batch normalization's effectiveness is not from reducing internal covariate shift, but from smoothing the loss landscape. They achieve similar or better performance with non-variance-based normalizations in image classification. Hence, we propose replacing LAYERNORM with the simpler scaled 2 normalization (SCALENORM), which normalizes activation vectors to a single learned length g. This is both inspired by and synergistic with jointly fixing the word embedding lengths (FIXNORM) <ref type="bibr" target="#b19">(Nguyen and Chiang, 2018)</ref>. These changes improve the training speed and lowresource performance of the Transformer without affecting high-resource performance.</p><p>On five low-resource pairs from the TED Talks <ref type="bibr" target="#b27">(Qi et al., 2018)</ref> and IWSLT '15 <ref type="bibr" target="#b2">(Cettolo et al., 2015)</ref> corpora, we first train state-of-theart Transformer models (+4.0 BLEU on average over the best published NMT bitext-only numbers). We then apply PRENORM, FIXNORM, and SCALENORM for an average total improvement of +1.1 BLEU, where each addition contributes at least +0.3 BLEU (Section 3), and attain a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We validate our intuitions in Section 4 by showing sharper performance curves (i.e., improvements occur at earlier epochs) and more consistent gradient norms. We also examine the per-sublayer g's learned by SCALENORM, which suggest future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Identity mappings for transformers</head><p>Residual connections <ref type="bibr" target="#b7">(He et al., 2016a)</ref> were first introduced to facilitate the training of deep convolutional networks, where the output of the -th layer F is summed with its input:</p><formula xml:id="formula_0">x +1 = x + F (x ).</formula><p>(1)</p><p>The identity term x is crucial to greatly extending the depth of such networks <ref type="bibr" target="#b8">(He et al., 2016b)</ref>. If one were to scale x by a scalar λ , then the contribution of x to the final layer F L is ( L−1 i= λ i )x . For deep networks with dozens or even hundreds of layers L, the term L−1 i= λ i becomes very large if λ i &gt; 1 or very small if λ i &lt; 1, for enough i. When backpropagating from the last layer L back to , these multiplicative terms can cause exploding or vanishing gradients, respectively. Therefore they fix λ i = 1, keeping the total residual path an identity map.</p><p>The original Transformer applies LAYER-NORM after the sublayer and residual addition (POSTNORM):</p><formula xml:id="formula_1">x +1 = LAYERNORM(x + F (x )). (2)</formula><p>We conjecture this has caused past convergence failures <ref type="bibr" target="#b25">(Popel and Bojar, 2018;</ref><ref type="bibr" target="#b32">Shazeer and Stern, 2018)</ref>, with LAYERNORMs in the residual path acting similarly to λ i = 1; furthermore, warmup was needed to let LAYERNORM safely adjust scale during early parts of training. Inspired by <ref type="bibr" target="#b8">He et al. (2016b)</ref>, we apply LAYERNORM immediately before each sublayer (PRENORM):</p><formula xml:id="formula_2">x +1 = x + F (LAYERNORM(x )). (3)</formula><p>This is cited as a stabilizer for Transformer training <ref type="bibr" target="#b3">(Chen et al., 2018;</ref><ref type="bibr" target="#b38">Wang et al., 2019)</ref> and is already implemented in popular toolkits <ref type="bibr" target="#b20">Ott et al., 2019;</ref><ref type="bibr" target="#b9">Hieber et al., 2018)</ref>, though not necessarily used by their default recipes. <ref type="bibr" target="#b38">Wang et al. (2019)</ref> make a similar argument to motivate the success of PRENORM in training very deep Transformers. Note that one must append an additional normalization after both encoder and decoder so their outputs are appropriately scaled. We compare POSTNORM and PRENORM throughout Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Weight initialization</head><p>Xavier normal initialization <ref type="bibr" target="#b5">(Glorot and Bengio, 2010)</ref> initializes a layer's weights W ∈ R d +1 ×d (d is the hidden dimension) with samples from a centered normal distribution with layer-dependent variance:</p><formula xml:id="formula_3">(W ) i,j ∼ N 0, 2 d + d +1 .<label>(4)</label></formula><p>Our experiments with this default initializer find that POSTNORM sometimes fails to converge, especially in our low-resource setting, even with a large number of warmup steps. One explanation is that Xavier normal yields initial weights that are too large. In implementations of the Transformer, one scales the word embeddings by a large value (e.g., √ d ≈ 22.6 for d = 512), giving vectors with an expected square norm of d. LAYERNORM's unit scale at initialization preserves this same effect. Since feedforward layers already have their weights initialized to a smaller standard deviation, i.e., 2 d+4d , we propose reducing the attention layers' initializations from 2 d+d to 2 d+4d as well (SMALLINIT), as a corresponding mitigation. We evaluate the effect of this on POSTNORM vs. PRENORM in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scaled 2 normalization and FIXNORM</head><p>LAYERNORM is inspired by batch normalization <ref type="bibr" target="#b10">(Ioffe and Szegedy, 2015)</ref>, both of which aim to reduce internal covariate shift by fixing the mean and variance of activation distributions. Both have been applied to self-attention <ref type="bibr" target="#b36">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b13">Kool et al., 2019)</ref>. However, <ref type="bibr" target="#b29">Santurkar et al. (2018)</ref> show that batch normalization's success has little to do with covariate shift, but comes instead from smoothing the loss landscape. For example, they divide by the pre-centered p norm instead of the variance and achieve similar or better results in image classification.</p><p>Hence, we propose replacing LAYERNORM with scaled 2 normalization:</p><formula xml:id="formula_4">SCALENORM(x; g) = g x ||x|| .<label>(5)</label></formula><p>This can be viewed as projecting d-dimensional vectors onto a (d − 1)-dimensional hypersphere with learned radius g. This expresses the inductive bias that each sublayer's activations has an ideal "global scale," a notion we empirically validate in Section 4.2. SCALENORM replaces the 2d scale and shift parameters of LAYERNORM with a single learned scalar, improving computational and parameter efficiency while potentially regularizing the loss landscape. This bias has an explicit interpretation at the final layer: large inner products sharpen the output distribution, causing frequent words to disproportionately dominate rare words. This led <ref type="bibr" target="#b19">Nguyen and Chiang (2018)</ref> to introduce FIXNORM(w) = g w ||w|| with fixed g at the last linear layer, to maximize the angular difference of output representations and aid rare word translation. By making g learnable, we can apply SCALENORM and FIXNORM jointly, which means applying the following at the final linear layer:</p><formula xml:id="formula_5">(SCALENORM+FIXNORM)(x, w; g) = g w · x ||w||||x|| .<label>(6)</label></formula><p>Note that this combination at the last layer is equivalent to cosine normalization <ref type="bibr" target="#b16">(Luo et al., 2018)</ref> with a learned scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learning rates</head><p>Despite using an adaptive optimizer, Adam (Kingma and , Transformer training uses a learning rate (LR) schedule with a linear warmup and an inverse square root decay (INVSQRTDECAY):</p><formula xml:id="formula_6">LR(n) = λ √ d min 1 √ n , n n 1.5 warmup ,<label>(7)</label></formula><p>where d is the hidden dimension of the selfattention layers, and λ, n warmup are hyperparameters that determine the highest learning rate achieved and the number of steps to reach it, respectively. These two hyperparameters have been the subject of much empirical study <ref type="bibr" target="#b25">(Popel and Bojar, 2018;</ref><ref type="bibr" target="#b21">Ott et al., 2018)</ref>. In light of our modifications however, we revisit various aspects of this schedule:</p><p>Warmup-free training. We conjectured that warmup is primarily needed when using POST-NORM to gradually learn LAYERNORM parameters without gradient explosion/vanishing (Section 2.1). Hence, we evaluate both PRENORM and POSTNORM without warmup in Section 3.3.</p><p>Large learning rates. To speed up training, one often explores using larger learning rates. In the context of Transformer, <ref type="bibr" target="#b21">Ott et al. (2018)</ref> and <ref type="bibr" target="#b0">Aharoni et al. (2019)</ref> take λ ∈ {2, 3} instead of the conventional λ = 1. <ref type="bibr" target="#b21">Ott et al. (2018)</ref> showed that one can scale up Adam's learning rate to 10 −3 with an extremely large batch (400k tokens). However, the improved convergence provided by our modifications could enable higher learning rates with much small batch sizes (4k tokens), as examined in Section 3.3.</p><p>Validation-based decay. For similar reasons, one might wish to adopt a classic validation-based decay, i.e., training at a high learning rate for as long as tenable, decaying rapidly when development scores flatline. This has inspired usage of fixed decay schemes upon convergence with IN-VSQRTDECAY <ref type="bibr" target="#b4">(Dong et al., 2018;</ref><ref type="bibr" target="#b28">Salazar et al., 2019)</ref>. We revisit VALDECAY under our modifications, where we still perform a linear warmup but then multiply by a scale α decay &lt; 1 when performance on a development set does not improve over patience evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and results</head><p>We train Transformer models for a diverse set of five low-resource translation pairs from the TED Talks <ref type="bibr" target="#b27">(Qi et al., 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Large vs. small initialization</head><p>To see the impact of weight initialization, we run training on the en→vi dataset using warmup steps of 4k, 8k, 16k (  The second row shows that taking a smaller standard deviation on the attention weights (SMALLINIT) restores convergence to POST-NORM. Though the 2/5 ≈ 0.63 adjustment used here seems marginal, operations like residual connections and the products between queries and keys can compound differences in scale. Though both models now achieve similar performance, we note that PRENORM works in all setups, suggesting greater stability during training. For all remaining experiments, we use POSTNORM and PRENORM with SMALLINIT. We find this choice does not affect the performance of PRENORM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scaled 2 normalization and FIXNORM</head><p>To compare SCALENORM and LAYERNORM, we take 8k warmup steps for all further experiments. Since we tie the target input word embedding and the last linear layer's weight (Appendix A), FIXNORM is implemented by applying 2 normalization to the word embedding, with each component initialized uniformly in [−0.01, 0.01]. For non-FIXNORM models, word embeddings are initialized with mean 0 and standard deviation 1/d so they sum to unit variance. All g's in SCALENORM are initialized to √ d. <ref type="table" target="#tab_4">Table 3</ref> shows our results along with some published baselines. First, note that our Transformer baselines with POSTNORM + LAYERNORM (1) are very strong non-multilingual NMT models on these pairs. They outperform the best published numbers, which are all Transformer models in the past year, by an average margin of +4.0 BLEU. Then, we see that PRENORM (2) achieves comparable or slightly better results than POSTNORM on all tasks. FIXNORM <ref type="formula">(3)</ref> gives an additional gain, especially on ar→en (p &lt; 0.01).</p><p>Finally, we replace LAYERNORM with SCALENORM (4). SCALENORM significantly improves on LAYERNORM for two very lowresource pairs, gl→en and sk→en. On the other tasks, it performs comparably to LAYERNORM. Upon aggregating all changes, our final model with SCALENORM and FIXNORM improves over our strong baseline with POSTNORM on all tasks by an average of +1.1 BLEU (p &lt; 0.01), with each change contributing an average of at least +0.3 BLEU. In Section 4.2 and Appendix B, we further examine where the performance gains of SCALENORM come from.</p><p>Moreover, SCALENORM is also faster than LAYERNORM. Recall that for each vector of size d, LAYERNORM needs to compute mean, standard deviation, scaling, and shifting, which costs O(7d) operations. For SCALENORM, we only need O(3d) operations to perform normalization and global scaling. This does not account for further gains due to reduction in parameters. In our implementation, training with SCALENORM is around 5% faster than with LAYERNORM, similar to the speedups on NMT observed by <ref type="bibr" target="#b40">Zhang and Sennrich (2019)</ref>   <ref type="formula">(2018)</ref>; <ref type="bibr" target="#b0">Aharoni et al. (2019)</ref>. †, ‡ and * indicate significant improvement of (3) over (2), (4) over <ref type="formula">(3)</ref>, and (4) over (1), respectively; p &lt; 0.01 via bootstrap resampling <ref type="bibr" target="#b12">(Koehn, 2004</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning rates</head><p>We compare the original learning rate schedule in equation 7 (INVSQRTDECAY) with validationbased decay (VALDECAY), possibly with no warmup (NOWARMUP). We use λ = 1, n warmup = 8k for INVSQRTDECAY and VALDE-CAY. For NOWARMUP, we instead use a learning rate of 3 · 10 −4 for all datasets. For both VALDE-CAY and NOWARMUP, we take α decay = 0.8 and patience = 3. For experiments with high learning rate, we use either VALDECAY or INVSQRT-DECAY with λ = 2 (giving a peak learning rate of ≈ 10 −3 ). All experiments use PRENORM + FIXNORM + SCALENORM.</p><p>In <ref type="table" target="#tab_6">Table 4</ref>, we see that NOWARMUP performs comparably to INVSQRTDECAY and VALDECAY except on gl→en. We believe that in general, one can do without warmup, though it remains useful in the lowest resource settings. In our 2×LR experiments, we can still attain a maximum learning rate of 10 −3 without disproportionately overfitting to small datasets like gl→en.</p><p>One might hypothesize that VALDECAY converges more quickly to better minima than IN-VSQRTDECAY by staying at high learning rates for longer. However, both schedulers achieve similar results with or without doubling the learning rate. This may be due to the tail-end behavior of VALDECAY methods, which can involve multiplicative decays in rapid succession. Finally, our 2×LR experiments, while not yielding better performance, show that PRENORM allows us to train the Transformer with a very high learning rate despite small batches (4k tokens).</p><p>Since PRENORM can train without warmup, we wonder if POSTNORM can do the same. We run experiments on en→vi with NOWARMUP, varying the number of encoder/decoder layers. As seen in <ref type="table" target="#tab_8">Table 5</ref>, POSTNORM often fails without warmup even with 5 or 6 layers. Even at 4 layers, one achieves a subpar result compared to PRENORM. This reaffirms Section 3.1 in showing that PRENORM is more stable than POSTNORM under different settings.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">High-resource setting</head><p>Since all preceding experiments were in lowresource settings, we examine if our claims hold in a high-resource setting. We train the Transformer base model on WMT '14 English-German using FAIRSEQ and report tokenized BLEU scores on newstest2014. Implementation of our methods in FAIRSEQ can be found in Appendix C.</p><p>In <ref type="table" target="#tab_9">Table 6</ref>, SCALENORM and FIXNORM achieve equal or better results than LAYERNORM. Since SCALENORM is also faster, we recom-mend using both as drop-in replacements for LAYERNORM in all settings. Surprisingly, in this task POSTNORM works notably better than PRENORM; one observes similar behavior in <ref type="bibr" target="#b38">Wang et al. (2019)</ref>. We speculate this is related to identity residual networks acting like shallow ensembles <ref type="bibr" target="#b37">(Veit et al., 2016)</ref> and thus undermining the learning of the longest path; further study is required.   <ref type="figure" target="#fig_1">Figure 1</ref> shows that PRENORM not only learns faster than POSTNORM, but also outperforms it throughout training. Adding FIXNORM also gives faster learning at first, but only achieves close performance to that with PRENORM and no FIXNORM. However, once paired with SCALENORM, we attain a better BLEU score at the end. Because of the slow warmup period, SCALENORM with warmup learns slower than SCALENORM without warmup initially; however, they all converge at about the same rate. To visualize how PRENORM helps backpropa-gation, we plot the global gradient norms from our runs in <ref type="figure" target="#fig_2">Figure 2</ref>. POSTNORM produces noisy gradients with many sharp spikes, even towards the end of training. On the other hand, PRENORM has fewer noisy gradients with smaller sizes, even without warmup. LAYERNORM has lower global norms than SCALENORM + FIXNORM but it has more gradient components corresponding to normalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance curves</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Activation scaling and the role of g</head><p>One motivation for SCALENORM was that it expressed a good inductive bias for the global scaling of activations, independent of distributional stability (Section 2.3). In contrast, a contemporaneous work <ref type="bibr" target="#b40">(Zhang and Sennrich, 2019)</ref> proposes root mean square layer normalization (RMSNORM), which still follows layer normalization's motivation but reduces overhead by forgoing additive adjustments, using only a scaling g i per activation a i . Despite their differing motives, tying the g i of RMSNORM and dividing by √ d retrieves SCALENORM.</p><p>Hence we can frame our comparisons in terms of number of learnable parameters. We rerun our PRENORM experiments with RMSNORM. We also consider fixing g = √ d for SCALENORM, where only FIXNORM has learnable g. <ref type="table" target="#tab_11">Table 7</ref> shows that SCALENORM always performs comparably or better than RMSNORM. Surprisingly, the fixed-g model performs comparably to the one with learnable g. However, at higher learning rates (VALDECAY with and without 2×LR), fixed-g models perform much worse on ar→en, en→he and en→vi. We conjecture that learning g is re-   quired to accommodate layer gradients.</p><p>In <ref type="figure">Figure 3</ref>, we plot the learned g values for pairs with 100k+ examples. For all but the decoder-encoder sublayers, we observe a positive correlation between depth and g, giving credence to SCALENORM's inductive bias of global scaling. This trend is clearest in the decoder, where g linearly scales up to the output layer, perhaps in tandem with the discriminativeness of the hidden representations <ref type="bibr" target="#b14">(Liang et al., 2018)</ref>. We also note a negative correlation between the number of training examples and the magnitude of g for attention sublayers, which may reflect overfitting.</p><p>Finally, to affirm our intuition for interpreting g, we plot g values with and without label smoothing <ref type="figure">(Figure 4)</ref>. We see a difference in later layers of the decoder; there, removing label smoothing re-sults in lower g values except at the output layer, where g increases sharply. This corresponds to the known overconfidence of translation models' logits, on which label smoothing has a downscaling effect <ref type="bibr" target="#b17">(Müller et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we presented three simple, normalization-centric changes to the Transformer model, with a focus on NMT. First, we show that while POSTNORM performs better for highresource NMT in the original base Transformer regime, PRENORM is both more stable and more competent in low-resource settings. Second, we propose replacing LAYERNORM with SCALENORM, a fast and effective scaled 2 normalization technique which requires only a sin-gle learned parameter. Finally, we reaffirm the effectiveness of fixing the word embedding norm (FIXNORM). Altogether, PRENORM + FIXNORM + SCALENORM significantly improves NMT on low-resource pairs, with the latter two performing comparably in the high-resource setting, but faster.</p><p>In the future, we would like to investigate the relationship between POSTNORM and PRENORM when using other optimizers such as RADAM , which has been shown to improve Transformer training without warmup. We are also interested in seeing if FIXNORM or SCALENORM at the final linear layer remains effective when paired with an initialization method such as FIXUP , which enables the training of deep neural networks without normalization. One could also explore using other p norms <ref type="bibr" target="#b29">(Santurkar et al., 2018)</ref>.</p><p>we also do word dropout <ref type="bibr" target="#b30">(Sennrich et al., 2016a)</ref> with probability 0.1. However, instead of zeroing the word embeddings, we randomly replace tokens with UNK. For all experiments, we use label smoothing of 0.1 <ref type="bibr" target="#b34">(Szegedy et al., 2016;</ref><ref type="bibr" target="#b24">Pereyra et al., 2017)</ref>. The source and target's input and output embeddings are shared <ref type="bibr" target="#b26">(Press and Wolf, 2017)</ref>, but we mask out words that are not in the target's vocabulary at the final output layer before softmax, by setting their logits to −∞.</p><p>Training. We use a batch size of 4096 and optimize using Adam (Kingma and  with the default parameters β 1 = 0.9, β 2 = 0.999, = 10 −8 . Gradients are clipped when global norm exceeds 1.0 <ref type="bibr" target="#b23">(Pascanu et al., 2013</ref>). An epoch is a predefined number of iterations for each pair. We stop training when a maximum number of epochs has been met or the learning rate becomes too small (10 −6 ). We also do early stopping when the development BLEU has not improved for 20 evaluations. For gl→en, this number is 50. When doing validation-based decay, we use α decay = 0.8 and patience = 3. For complete data and model statistics, please refer to <ref type="table" target="#tab_0">Table 1</ref>. The best checkpoint is selected based on the development BLEU score during training.</p><p>Evaluation. We report tokenized BLEU <ref type="bibr" target="#b22">(Papineni et al., 2002)</ref> with multi-bleu.perl to be comparable with previous works. We also measure statistical significance using bootstrap resampling <ref type="bibr" target="#b12">(Koehn, 2004)</ref>. For WMT '14 English-German, note that one needs to put compounds in ATAT format 2 before calculating BLEU score to be comparable with previous works.  We ask if improvements from SCALENORM on our low-resource tasks are due to improved regularization (a smaller generalization gap) or improved overall performance. We record smoothed train and test perplexities of our PRENORM models in <ref type="table" target="#tab_13">Table 8</ref>. We see suggestive results but no conclusive trends. For ar→en, gl→en, and sk→en, train and test drop slightly, with test more so than train. For en→vi, train perplexity increases and test perplexity decreases an equivalent amount. For en→he, our smallest change between SCALENORM and LAYERNORM, train perplexity negligibly increased and test perplexity remains the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Listings</head><p>See the following page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCALENORM.</head><p>class ScaleNorm(nn.Module):</p><p>"""ScaleNorm""" def __init__(self, scale, eps=1e-5): super(ScaleNorm, self).__init__() self.scale = Parameter(torch.tensor(scale)) self.eps = eps def forward(self, x): norm = self.scale / torch.norm(x, dim=-1, keepdim=True).clamp(min=self.eps) return x * norm FAIRSEQ. We follow FAIRSEQ's tutorial 3 and train a POSTNORM Transformer base model using the following configuration: For SCALENORM, we replace all LAYERNORMs in fairseq/models/transformer.py and fairseq/modules/transformer layer.py with SCALENORM (implemented above). For FIXNORM, we change the word embedding initialization to uniform with range [−0.01, 0.01] and normalize with torch.nn.functional.normalize.</p><p>We note that FAIRSEQ uses Xavier uniform initialization, which is big compared to our SMALLINIT (Section 3.1). We conjecture that FAIRSEQ training remains stable thanks to its large batch size, which gives more stable gradients.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Development BLEU on en→vi with POST-NORM or PRENORM, and with LAYERNORM or SCALENORM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The global norm of gradients when using POSTNORM or PRENORM, and with LAYERNORM, SCALENORM and FIXNORM. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Learned g values for PRENORM + SCALENORM + FIXNORM models, versus depth. Left: Attention sublayers (decoder-encoder denotes decoder sublayers attending on the encoder). Right: Feedforward sublayers and the final linear layer. Learned g values for our PRENORM + SCALENORM + FIXNORM en→vi model (with and without label smoothing), versus depth. Left and Right are the same as in Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>simply include the flags:--encoder-normalize-before --decoder-normalize-before</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>and theIWSLT '15 (Cettolo et al., 2015)  corpora. Details are summarized inTable 1. For more information motivating our choice of pairs and for exact training details, refer to Appendix A.# egs. # src. + tgt. toks. # iters./epoch max. epoch # enc./dec. layers # heads/layer dropout # BPE Data and model properties for low-resource NMT. en→vi is from IWSLT 2015; the rest are from the TED Talks corpus.</figDesc><table><row><cell>gl→en</cell><cell>10k</cell><cell>0.37M</cell><cell>100</cell><cell>1000</cell><cell>4</cell><cell>4</cell><cell>0.4</cell><cell>3k</cell></row><row><cell>sk→en</cell><cell>61k</cell><cell>2.32M</cell><cell>600</cell><cell>200</cell><cell>6</cell><cell>8</cell><cell>0.3</cell><cell>8k</cell></row><row><cell>en→vi</cell><cell>133k</cell><cell>5.99M</cell><cell>1500</cell><cell>200</cell><cell>6</cell><cell>8</cell><cell>0.3</cell><cell>8k</cell></row><row><cell>en→he</cell><cell>212k</cell><cell>7.88M</cell><cell>2000</cell><cell>200</cell><cell>6</cell><cell>8</cell><cell>0.3</cell><cell>8k</cell></row><row><cell>ar→en</cell><cell>214k</cell><cell>8.09M</cell><cell>2000</cell><cell>200</cell><cell>6</cell><cell>8</cell><cell>0.3</cell><cell>8k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">). With default initializa-</cell></row><row><cell cols="4">tion, POSTNORM fails to converge on this dataset</cell></row><row><cell cols="4">even with a long warmup of 16k steps, only reach-</cell></row><row><cell>ing 5.76 BLEU.</cell><cell></cell><cell></cell></row><row><cell>Xavier normal</cell><cell></cell><cell># warmup steps 4k 8k</cell><cell>16k</cell></row><row><cell>Baseline</cell><cell>POSTNORM PRENORM</cell><cell cols="2">fail 28.52 28.73 28.32 fail 5.76</cell></row><row><cell>SMALLINIT</cell><cell cols="3">POSTNORM 28.17 28.20 28.62 PRENORM 28.26 28.44 28.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Development BLEU on en→vi using Xavier normal initialization (baseline versus SMALLINIT).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>'s RMSNORM (which can be viewed as SCALENORM with per-unit scales; see Section 4.2). FIXNORM + SCALENORM (4) 20.91 ‡ * 30.25 ‡ * 32.79</figDesc><table><row><cell></cell><cell cols="6">gl→en sk→en en→vi en→he ar→en average ∆</cell></row><row><cell>POSTNORM + LAYERNORM (published)</cell><cell>16.2</cell><cell>24.0</cell><cell>29.09</cell><cell>23.66</cell><cell>27.84</cell><cell>-4.05</cell></row><row><cell>POSTNORM + LAYERNORM (1)</cell><cell>18.47</cell><cell>29.37</cell><cell>31.94</cell><cell>27.85</cell><cell>33.39</cell><cell>+0.00</cell></row><row><cell>PRENORM + LAYERNORM (2)</cell><cell>19.09</cell><cell>29.45</cell><cell>31.92</cell><cell>28.13</cell><cell>33.79</cell><cell>+0.27</cell></row><row><cell>PRENORM + FIXNORM + LAYERNORM (3)</cell><cell>19.38</cell><cell>29.50</cell><cell>32.45</cell><cell>28.39</cell><cell>34.35  †</cell><cell>+0.61</cell></row><row><cell>PRENORM +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* 28.44* 34.15* +1.10</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test BLEU using POSTNORM or PRENORM and different normalization techniques. Published values are from Wang et al. (2018); Neubig and Hu</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>).</figDesc><table><row><cell></cell><cell cols="5">gl→en sk→en en→vi en→he ar→en</cell></row><row><cell>NOWARMUP</cell><cell>18.00</cell><cell>28.92</cell><cell>28.91</cell><cell>30.33</cell><cell>35.40</cell></row><row><cell>INVSQRTDECAY</cell><cell>22.18</cell><cell>29.08</cell><cell>28.84</cell><cell>30.30</cell><cell>35.33</cell></row><row><cell>VALDECAY</cell><cell>21.45</cell><cell>29.46</cell><cell>28.67</cell><cell>30.69</cell><cell>35.46</cell></row><row><cell>INVSQRTDECAY + 2×LR</cell><cell>21.92</cell><cell>29.03</cell><cell>28.76</cell><cell>30.50</cell><cell>35.33</cell></row><row><cell>VALDECAY + 2×LR</cell><cell>21.63</cell><cell>29.49</cell><cell>28.46</cell><cell>30.13</cell><cell>34.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Development BLEU for PRENORM + FIXNORM + SCALENORM, trained with different learning rate schedulers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Development BLEU on en→vi using NOWARMUP, as number of encoder/decoder layers increases.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>BLEU scores from WMT '14 English-to- German. Published value is from Vaswani et al. (2017).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Test BLEU of 2 -based normalization techniques with different numbers of learned g: O(Ld) vs. O(L) vs. O(1).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>17.950 16.719 17.100 en→he 15.562 14.950 15.906 15.080 ar→en 14.372 13.450 14.165 13.290</figDesc><table><row><cell></cell><cell cols="2">LAYERNORM</cell><cell cols="2">SCALENORM</cell></row><row><cell></cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell></row><row><cell cols="5">gl→en 11.792 54.300 10.151 45.770</cell></row><row><cell cols="5">sk→en 14.078 20.460 14.004 19.080</cell></row><row><cell>en→vi</cell><cell>15.961</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Label-smoothed train/test perplexities when using LAYERNORM and SCALENORM.</figDesc><table><row><cell>2 https://github.com/tensorflow/</cell></row><row><cell>tensor2tensor/blob/master/tensor2tensor/</cell></row><row><cell>utils/get_ende_bleu.sh</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/glample/fastBPE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/pytorch/fairseq/blob/master/examples/scaling_nmt/README.md</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank David Chiang and Katrin Kirchhoff for their support of this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training details</head><p>Data and preprocessing. The pairs are English (en) to Hebrew (he), Vietnamese (vi), and Galician (gl), Slovak (sk), Arabic (ar) to English (en). Because the data is already preprocessed, we only apply BPE <ref type="bibr" target="#b31">(Sennrich et al., 2016b)</ref> with fastBPE 1 . Depending on the data size, we use different numbers of BPE operations.</p><p>We wanted to compare with the latest lowresource works of <ref type="bibr" target="#b18">(Neubig and Hu, 2018;</ref><ref type="bibr" target="#b0">Aharoni et al., 2019)</ref> on the TED Talks corpus <ref type="bibr" target="#b27">(Qi et al., 2018)</ref>. In particular, <ref type="bibr" target="#b0">Aharoni et al. (2019)</ref> identified 4 very low-resource pairs (&lt;70k); we took the two (gl→en, sk→en) that were not extremely low (≤6k). They then identified 4 low-resource pairs with 100k-300k examples; we took the top two (ar→en, en→he). To introduce a second Englishsource pair and to showcase on a well-understood task, we used the en→vi pair from IWSLT '15 with an in-between number of examples (133k). In this way, we have examples of different resource levels, language families, writing directions, and English-source versus -target.</p><p>Model configuration. We set the hidden dimension of the feedforward sublayer to 2048 and the rest to 512, matching <ref type="bibr" target="#b36">Vaswani et al. (2017)</ref>. We use the same dropout rate for output of sublayers, ReLU, and attention weights. Additionally,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massively Multilingual Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1388</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3874" to="3884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Layer Normalization. CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The IWSLT 2015 Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roldano</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1008</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech-Transformer: A No-Recurrence Sequenceto-Sequence Model for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8462506</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1315" to="1325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Sockeye neural machine translation toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMTA</title>
		<meeting><address><addrLine>Artem Sokolov, Ann Clifton, and Matt Post</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13398-014-0173-7.2</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<idno type="DOI">10.1145/2063576.2063688</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attention, Learn to Solve Routing Problems! In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning noise-invariant representations for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="DOI">10.1109/SLT.2018.8639575</idno>
	</analytic>
	<monogr>
		<title level="m">SLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1908.03265</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">When Does Label Smoothing Help? In NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rapid Adaptation of Neural Machine Translation to New Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1103</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="875" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving Lexical Choice in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1031</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">fairseq: A Fast, Extensible Toolkit for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (Demonstrations)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scaling Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.279181</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training Tips for the Transformer Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondej</forename><surname>Bojar</surname></persName>
		</author>
		<idno type="DOI">10.2478/pralin-2018-0002</idno>
	</analytic>
	<monogr>
		<title level="j">Prague Bull. Math. Linguistics</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="70" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using the Output Embedding to Improve Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Felix</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2084</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="535" />
		</imprint>
	</monogr>
	<note>Sarguna Padmanabhan, and Graham Neubig</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-attention Networks for Connectionist Temporal Classification in Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2019.8682539</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7115" to="7119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Edinburgh Neural Machine Translation Systems for WMT 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4603" to="4611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Augmenting Self-attention with Persistent Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno>abs/1907.01470</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tensor2Tensor for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMTA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention is All you Need. In NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="550" to="558" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning Deep Transformer Models for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1810" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Switchout: an efficient data augmentation algorithm for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="856" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Root Mean Square Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
