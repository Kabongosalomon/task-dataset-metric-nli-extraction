<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edge Contraction Pooling for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Diehl</surname></persName>
							<email>f.diehl@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">fortiss GmbH An-Institut Technische</orgName>
								<orgName type="institution">Universität München Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Edge Contraction Pooling for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Network (GNN) research has concentrated on improving convolutional layers, with little attention paid to developing graph pooling layers. Yet pooling layers can enable GNNs to reason over abstracted groups of nodes instead of single nodes. To close this gap, we propose a graph pooling layer relying on the notion of edge contraction: EdgePool learns a localized and sparse hard pooling transform. We show that EdgePool outperforms alternative pooling methods, can be easily integrated into most GNN models, and improves performance on both node and graph classification.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, a fast-growing field of applying deep learning to graphs has emerged. Many of these works are inspired by generalizing Convolutional Neural Networks (CNNs) to the non-euclidian and sparsely connected data that graphs represent. But while a multitude of different Graph Convolutional Networks (GCNs) have been proposed, the number of proposed pooling layers remains small.</p><p>Yet intelligent pooling on graphs holds significant promise: It might both identify clusters (feature-or structure-based) and reduce computational requirements by reducing the number of nodes. Together, these promise to abstract from flat nodes to hierarchical sets of nodes. They are also a stepping stone towards enabling GNNs to modify graph structures instead of only node features. <ref type="figure">Figure 1</ref>: Edge pooling in action on a graph from PROTEINS. The original graph (left-most) is pooled three times and results in the graph depicted to the right. In each step, nodes that will be merged are surrounded by a dashed line of a random colour. In the next step, the nodes are drawn as their convex hull, filled with the same colour. Notice how the the pooled graph keeps the mostly-linear structure of the original graph. Best viewed on screen.</p><p>We propose a new pooling layer based on edge contractions (EdgePool, see <ref type="figure">Fig. 1</ref>), which aims to correct weaknesses in previously proposed learned pooling layers. We do this by viewing the task not as choosing nodes but as choosing edges and pooling the connected nodes. This immediately and naturally takes the graph structure into account and ensures that we never drop nodes completely.</p><p>The main advantages of our proposed EdgePool layer are:</p><p>• EdgePool performs better than other pooling methods.</p><p>• EdgePool can be integrated in existing graph classification architectures.</p><p>• EdgePool can be used for node classification and improves performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Graph pooling strategies can be divided into two types: We can either use fixed pooling methods, usually based on graph topology, or use learned pooling methods. We concentrate on comparisons with learned pooling methods, since these appear to outperform fixed pooling methods.</p><p>DiffPool <ref type="bibr" target="#b19">Ying et al. (2018)</ref> were the first to propose a learned pooling layer. DiffPool learns to soft-assign each node to a fixed number of clusters based on their features. DiffPool works well, but suffers from three disadvantages: (a) The number of clusters has to be chosen in advance, which might cause performance issues when used on datasets with different graph sizes. (b) Since cluster assignment is based only on node features, nodes are assigned to the same cluster based on their features, ignoring distances. (c) The cluster assignment matrix is dense, and in R n×c , where c is the number of clusters. Since c is usually chosen according to the total number of nodes, the cluster assignment matrix scales quadratically with the number of nodes n. They also need several auxiliary objectives (link prediction, node feature 2 regularization, cluster assignment entropy regularization) to train well. In addition to that, the density makes integration into usually sparse GNNs difficult.</p><p>TopKPool Graph U-Net, introduced by <ref type="bibr" target="#b3">Gao &amp; Ji (2018)</ref>, uses a simple top-k choice of nodes for their gPool layer, learning a node score and dropping all but the top nodes. <ref type="bibr" target="#b1">Cangea et al. (2018)</ref> later applied this to graph classification. While this approach is both sparse and variable in graph size, its node choice is dependent on global state. This introduces two new issues: (a) Adding nodes to a graph can change the pooling result of the whole graph. (b) Whole areas of a graph might see no node chosen, which causes loss of information.</p><p>SAGPool <ref type="bibr" target="#b10">Lee et al. (2019)</ref> introduced Self-Attention Graph Pooling (SAGPool). A variant of TopKPool, SAGPool no longer uses only node features to compute node scores but uses graph convolutions to take neighbouring node features into account. While their method improves TopKPool qualitatively, the disadvantages remain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EdgePool</head><p>For our work, we consider a graph G = (V, E), where each of the v nodes has f features V ∈ R v×f . Edges are represented as directed pairs of nodes without weights or features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Edge contraction</head><p>We base our pooling operation on edge contractions. Contracting the edge e = {v i , v j } introduces the new node v e and new edges such that v e is adjacent to all nodes v i or v j has been adjacent to. v i , v j , and all their edges are deleted from the graph. Since edge contractions are commutative, we can also define an edge set contraction. By constructing the set such that no two edges are incident to the same node, we can simply apply the naive notion of single-edge contraction multiple times.</p><p>Intuitively, we choose a single edge to contract by merging its nodes. This new node is then connected to all nodes the merged nodes had been connected to. We repeat this procedure multiple times, taking care not to include a newly-merged node into it. Resulting, pooled, graph. As can be seen, edge BC was not contracted because node C is incident to the previously contracted CD edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Choosing edges</head><p>Given the preconditions mentioned above, we naively choose edges by computing a score for each edge, then iteratively contracting the highest-scoring edge which does not have a newly-merged node incident.</p><p>In our procedure, we compute raw scores for each node as a simple linear combination of the concatenated node features. For an edge from node i to node j, we compute the raw score r as</p><formula xml:id="formula_0">r(e ij ) = W · (n i n j ) + b,<label>(1)</label></formula><p>where n i and n j are the node features and W and b are learned parameters.</p><p>To compute the final node score s ij for an edge, we employ a local softmax normalization over all edges of a node 1 . We modify the final score such that the mean of the score range lies at 1. Later on, this enables us to include the score in the unpooling procedure without issues due to numerical stability. We also found this to lead to better performance in the graph classification task, which we believe is because of better gradient flow. The final score then becomes:</p><formula xml:id="formula_1">s ij = 0.5 + softmax r * j (r ij ).<label>(2)</label></formula><p>Given the edge scores, we now iteratively contract edges according to the scores, ignoring those which have a newly-merged node incident. An illustration of the process is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Note that this will always pool roughly 50% of the total nodes. Contrary to DiffPool and TopKPool, this ratio cannot be changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computing new node features</head><p>There are many strategies for combining the features of pairs of nodes. In particular, we are not restricted to symmetric functions since the edges chosen have a specific direction. Nonetheless, we found that taking the sum of the node features works well.</p><p>To enable the gradient to flow into the scores, we use gating and multiply the combined node features by the edge score:n</p><formula xml:id="formula_2">ij = s ij (n i + n j )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Computational performance</head><p>Given our procedure above, we immediately see that EdgePool can operate on sparse representations. When doing so, both runtime and memory scales linearly in the number of edges. This particularly avoids the scaling issues of DiffPool's cluster assignment matrix.</p><p>Additionally, EdgePool is locally independent: As long as the node scores of two nodes n i and n j and of their neighbours do not change (by changing nodes within the receptive fields), the choice of edge e ij will not change. Accordingly, EdgePool does not have to be computed for the whole graph at once. If the graph changes, only the pooling local to the changed areas needs to be updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Integrating edge features</head><p>EdgePool can be updated to take edge features f ij of edge e ij into account. To do so, we have to include them in the raw score computation (Eq. <ref type="formula" target="#formula_0">(1)</ref>). The simplest approach is to concatenate them:</p><formula xml:id="formula_3">r(e ij ) = W · (n i n j f ij ) + b.<label>(4)</label></formula><p>Additionally, we will likely have to change the procedure to compute new node features; we propose using a weighted linear combination of both nodes' features, the features of the chosen edge, and the features of the reverse edge if it exists.</p><p>Lastly, we need a procedure to combine the edge features of edges that ended at both merged nodes and will therefore be merged. We believe a simple sum should work well here, too. However, we have not conducted experiments on this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Unpooling EdgePool</head><p>To use pooling in the context of node classification, an unpooling operation is necessary. To do so, each EdgePool layer also emits the mapping of each of the previous graph's nodes to the newly-pooled graph's nodes. When unpooling, we then create an inverse mapping of pooled nodes to unpooled nodes. Since we assign each node to exactly one merged node, this mapping can be chained through many pooling layers. Additionally, we divide the unpooled node features by the corresponding edge score:n i =n j = n ij /s ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We design our experiments to answer three questions:</p><p>Q1: Does EdgePool outperform alternative pooling methods?</p><p>Q2: Can EdgePool be used as a plug-and-play addition for any GNN?</p><p>Q3: Can EdgePool be used for node classification?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">General Setup</head><p>We evaluate our models on multiple graph and node classification datasets, and share most of the training procedures between all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>While there are many graph classification datasets available, most of these are small (in both nodes per graph and total graphs). As an example, the popular ENZYMES dataset contains only 600 graphs, making 10-fold crossvalidation (at a test set size of 60) very difficult.</p><p>We conduct 10-fold cross-validation for all datasets and report mean and standard deviation. We choose all folds at random, eschewing the default planetoid split.</p><p>Graph classification datasets For graph classification, we evaluate on four datasets from the collection by <ref type="bibr" target="#b6">Kersting et al. (2016)</ref>. At 1113 graphs, PROTEINS <ref type="bibr" target="#b0">(Borgwardt et al., 2005)</ref> is the smallest, but has been used extensively as a benchmark dataset. The task is to predict whether a given protein is an enzyme. The two reddit-based datasets <ref type="bibr">(Yanardag &amp; Vishwanathan, 2015)</ref> depict user responses in an online discussion. The task is to predict the subreddit, out of two (REDDIT-BINARY) or eleven (REDDIT-MULTI-12K) choices. Lastly, each COLLAB (ibid) graph models scientific collaborations of one researcher. The task is to classify which of three fields the researcher belongs to. Neither COLLAB nor the two reddit-based datasets have any features.</p><p>Node classification datasets We also evaluate EdgePool on five semi-supervised node classification datasets. CORA <ref type="bibr" target="#b11">(Namata et al., 2012)</ref>, CITESEER, and PUBMED <ref type="bibr" target="#b13">(Sen et al., 2008)</ref> model citation networks. In these, nodes are documents and edges model citations. The goal is to classify the subfield of each of the documents. The PHOTO and COMPUTER datasets <ref type="bibr" target="#b14">(Shchur et al., 2018)</ref> are part of the Amazon co-purchasing graph. Nodes are products and edges model co-purchases between products. The goal is to predict the product category.</p><p>Each of these datasets is a semi-supervised node classification task from bag-of-word features. We use 20 nodes per class as training data and 30 nodes per class as test data. Every other node is unlabelled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>While we use different models, several setup parameters have been chosen identically between all models. Each is trained for a total of 200 epochs using the Adam optimizer <ref type="bibr" target="#b7">(Kingma &amp; Ba, 2014)</ref> with a learning rate of 10 −3 , which is halved every 50 epochs. 128 graphs are batched together at each step by treating them as a single unconnected graph. We use 128 channels except for PROTEINS and the node classification datasets, where we used 64. This setup follows <ref type="bibr" target="#b19">Ying et al. (2018)</ref>.</p><p>All models use both dropout and batch normalization <ref type="bibr" target="#b5">(Ioffe &amp; Szegedy, 2015)</ref>. We found batch normalization to suffer greatly when evaluated using population statistics and instead use mini-batch statistics even during testing.</p><p>We also found using edge score dropout significantly increased EdgePool's performance, and set every edge score to 0 with a chance of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental design</head><p>To answer the questions we have posed, we design three different experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Q1: Does EdgePool outperform alternative pooling approaches?</head><p>To evaluate this, we use the same architecture as used by <ref type="bibr" target="#b19">Ying et al. (2018)</ref> for DiffPool: The model has three SAGEConv blocks <ref type="bibr" target="#b4">(Hamilton et al., 2017)</ref> whose outputs are globally mean-pooled and concatenated. Final classification occurs after two fully-connected layers. The base model does not pool nodes, every other model pools after every block. Note that DiffPool uses a siamese architecture, using separate SAGEConv blocks to compute cluster assignments. We restrict DiffPool to a maximum of 750 nodes per graph and set TopKPool's pool ratio to 0.5 to remain comparable to EdgePool.</p><p>Additionally, we only use the cross-entropy loss to train the model. To ensure a fair comparison, we also do this for DiffPool, which originally used three additional auxiliary losses and tasks to stabilize training and precomputed additional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Q2: Can EdgePool be integrated in existing architectures?</head><p>To evaluate whether EdgePool can be integrated into pre-existing architectures, we follow the model configuration from pytorch-geometric's benchmarks <ref type="bibr" target="#b2">(Fey &amp; Lenssen, 2019)</ref>. Speficially, we use a total of seven convolutional layers, followed by a global pooling layer and two fully-connected layers. If pooling is used, it is added after every second convolutional layer (i.e. there are three pooling layers).</p><p>The convolutional layers we evaluate this on are GCN <ref type="bibr">(Kipf &amp; Welling)</ref>, GIN and GIN0 <ref type="bibr" target="#b16">(Xu et al., 2019)</ref>, and GraphSAGE <ref type="bibr" target="#b4">(Hamilton et al., 2017)</ref> both with and without accumulating intermediate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Q3: Can EdgePool be used for node classification?</head><p>On node classification tasks, we evaluate a simple architecture, varying the convolutional layers. We evaluate GCN, GIN and GIN0, and GAT <ref type="bibr" target="#b15">(Veličković et al., 2017)</ref>. Again, we also evaluate a MLPs layer. As with Q2, we use seven convolutional layers. We pool after the second and fourth and unpool after the fifth and seventh, with shortcuts between the poolings. The concatenated features are then used by a two-layer MLP to predict each node's class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and discussion</head><p>We implemented the models using PyTorch <ref type="bibr" target="#b12">(Paszke et al., 2017)</ref> and in particular the pytorchgeometric library <ref type="bibr" target="#b2">(Fey &amp; Lenssen, 2019)</ref>. Experiments were conducted on several Geforce 1080Ti GPUs in parallel, leveraging Singularity containers <ref type="bibr" target="#b9">(Kurtzer et al., 2017)</ref> for reproducibility. <ref type="table" target="#tab_0">Table 1</ref> shows mean accuracy and standard deviation for graph classification tasks. As can be seen, EdgePool consistently improves performance over the non-pooling models and TopKPool. Discounting PROTEINS due to close performance, it outperforms all other pooling approaches on two tasks, and is only outperformed by DiffPool on one task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EdgePool vs. alternative pooling approaches</head><p>This answer Q1: EdgePool consistently outperforms all pooling methods but DiffPool. While DiffPool might perform better on some graphs, EdgePool scales far better and can be used on large graph sizes. Unfortunately, the performance increases of EdgePool are not consistent over datasets and models. This makes it impossible to make a specific recommendation on situations in which one should or should not include EdgePool in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EdgePool in existing architectures</head><p>However, we can still answer Q2: It is easily possible to integrate EdgePool in existing architectures. Doing so will lead to an estimated improvement of about 2 p.p., but might for some combinations of model and dataset decrease performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EdgePool for node classification</head><p>As <ref type="table" target="#tab_3">Table 3</ref> shows, GNNs using EdgePool can be integrated in node classification architectures and improves performance for 21 of 25 dataset/model combinations.</p><p>In particular, note the increase in performance for the MLP. In several of these tasks, an MLP augmented with EdgePool shows competitive performance to GNN algorithms. For GNN algorithms, EdgePool improves performance by an average of 3.5 p.p., performing worst on PUBMED (no improvement on average) and for GCNs (decrease by 0.1 p.p.). It performs best for GIN and GIN0, at 5.8 p.p. and 6.6 p.p. improvements respectively.</p><p>This answers Q3: EdgePool will, in most cases, improve performance for node classification. The expected improvement is an average of 3.5 p.p..</p><p>(a) EdgePool example. The graph is simplified in each step. In particular, note the two node groups marked by the orange arrows. Locally, these form two unconnected paths which stay unconnected throughout the pooling process.</p><p>(b) In this case, EdgePool fails to pool human-visible structures together. The two nodes marked by the orange arrow are closely connected to their neighbours. In the final pooling configuration, however, they have not been joined with them and remain separate. <ref type="figure">Figure 3</ref>: Edge pooling on graphs from PROTEINS. Visualization is identical to <ref type="figure">Fig. 1</ref>. Best viewed on screen and zoomed in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visual inspection</head><p>Both <ref type="figure">Fig. 1 and Fig. 3</ref> show examples of the pooling resulting from using EdgePool. In particular, they show that EdgePool keeps the linearity of the original protein even after pooling. <ref type="figure">Fig. 3a</ref> shows how unconnected paths (orange arrows) are not pooled, keeping the original graph structure visible even after pooling. However, as <ref type="figure">Fig. 3b</ref> shows, there are situations in which EdgePool causes node poolings which are counter-intuitive to humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed EdgePool, a hard pooling method for Graph Neural Networks, based on edge contraction.</p><p>This pooling is both localized (and therefore independent of non-local graph changes) and sparse (and therefore computationally efficient even on large graphs).</p><p>Except for a single pooling procedure on a single dataset, EdgePool outperforms all previously proposed pooling approaches. We also show that EdgePool can be integrated into a large number of GNN architectures and usually improves performance on both node and graph classification tasks without any adaptions to training or architecture.</p><p>Besides the obvious use of EdgePool in improving existing GNN architectures, we hope it will serve as a stepping stone towards methods that learn how to modify graph structures. We believe this will lead towards methods that no longer operate on nodes but on abstracted groups of nodes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Edge score computation. To reduce visual clutter, we assume both directions of a node to have the same raw score. We also greatly round numbers. (a) The raw scores for each edge.(b) This shows the computed exp (r) for each edge and the sum over all incoming raw scores for each node. (c) Final scores for each edge. The edges chosen to be contracted are marked bold. (d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparing pooling strategies: Accuracy (and standard deviation) on benchmark datasets in percent. Best results are marked bold. [*]<ref type="bibr" target="#b19">Ying et al. (2018)</ref> use several additional techniques and auxiliary losses to stabilize training, and also include additional computed features. We report results without these.</figDesc><table><row><cell></cell><cell>PROTEINS</cell><cell>RDT-B</cell><cell>RDT-12K</cell><cell>COLLAB</cell></row><row><cell>Base Model</cell><cell>71.4 ± 3.2</cell><cell>69.9 ± 3.7</cell><cell>35.1 ± 1.6</cell><cell>65.4 ± 1.5</cell></row><row><cell>DiffPool [*]</cell><cell>72.3 ± 5.8</cell><cell>82.9 ± 3.4</cell><cell cols="2">34.8 ± 1.9 70.1 ± 1.5</cell></row><row><cell>TopKPool</cell><cell>70.6 ± 4.8</cell><cell>68.9 ± 3.2</cell><cell>28.7 ± 1.8</cell><cell>64.6 ± 2.1</cell></row><row><cell>SAGPool</cell><cell>71.8 ± 6.0</cell><cell>84.7 ± 4.4</cell><cell>41.9 ± 3.3</cell><cell>63.9 ± 2.5</cell></row><row><cell>EdgePool</cell><cell cols="3">72.5 ± 3.2 87.3 ± 4.1 45.6 ± 1.8</cell><cell>67.1 ± 2.7</cell></row><row><cell cols="5">results (SAGE nacc). Additionally, we construct a model using node-independent Multi-Layer</cell></row><row><cell cols="5">Perceptrons (MLPs), in which only pooling might lead to communication between nodes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>shows comparative results for different benchmark models with and without EdgePool. On a large majority of GNN/dataset combinations, EdgePool increases performance, by an average of almost 2 percentage points (p.p.). GIN and GIN0 profit the least (mean improvement of 0.3 p.p.), while GraphSAGE profits the most (5.5 p.p.).Interestingly, we can see that EdgePool allows even the MLP model to perform fairly well. This model can only rely on pooling to gain information on the neighbourhood. Nonetheless, it performs competitively on PROTEINS and COLLAB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Integrating EdgePool into existing architectures: Accuracy (in percent) of benchmark models with and without EdgePool. SAGE is short for GraphSAGE; nacc means without accumulating results.</figDesc><table><row><cell>PROTEINS</cell><cell>GCN</cell><cell>GIN</cell><cell>GIN0</cell><cell cols="2">SAGE SAGE nacc</cell><cell>MLP</cell></row><row><cell>No Pooling</cell><cell>71.4 ± 5.0</cell><cell cols="2">70.4 ± 2.7 70.9 ± 73.7</cell><cell cols="2">71.7 ± 3.6 73.0 ± 4.8</cell><cell>71.8 ± 4.2</cell></row><row><cell>EdgePool</cell><cell cols="4">73.1 ± 4.6 72.9 ± 3.6 71.7 ± 3.6 73.5 ± 3.5</cell><cell cols="2">69.9 ± 4.9 73.1 ± 4.6</cell></row><row><cell>RDT-B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Pooling</cell><cell>87.1 ± 2.8</cell><cell>91.9 ± 1.7</cell><cell>92.3 ± 1.5</cell><cell>62.5 ± 4.8</cell><cell>50.3 ± 8.4</cell><cell>51.0 ± 4.3</cell></row><row><cell>EdgePool</cell><cell cols="6">87.8 ± 3.1 92.1 ± 2.2 93.0 ± 1.7 68.0 ± 5.3 64.5 ± 4.6 69.9 ± 2.8</cell></row><row><cell>RDT-12K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">No Pooling 47.6 ± 0.6 49.5 ± 1.1 50.0 ± 1.3</cell><cell>22.9 ± 2.3</cell><cell>24.4 ± 1.4</cell><cell>21.9 ± 1.5</cell></row><row><cell>EdgePool</cell><cell>47.4 ± 2.1</cell><cell>49.3 ± 1.1</cell><cell cols="4">49.6 ± 1.2 36.9 ± 2.1 37.8 ± 2.0 34.6 ± 1.3</cell></row><row><cell>COLLAB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Pooling</cell><cell cols="3">67.0 ± 2.2 74.2 ± 1.8 74.1 ± 1.6</cell><cell>63.6 ± 2.4</cell><cell>64.1 ± 2.1</cell><cell>52.0 ± 2.5</cell></row><row><cell>EdgePool</cell><cell>71.5 ± 2.0</cell><cell>73.0 ± 2.1</cell><cell cols="4">72.2 ± 1.6 64.3 ± 1.9 64.1 ± 2.3 67.8 ± 3.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Using EdgePool for node classification. Accuracy (in percent) of benchmark models with and without EdgePool.</figDesc><table><row><cell>CORA</cell><cell>GCN</cell><cell>GIN</cell><cell>GIN0</cell><cell>GAT</cell><cell>MLP</cell></row><row><cell>No Pooling</cell><cell>71.8 ± 3.4</cell><cell>52.1 ± 4.7</cell><cell>55.9 ± 4.4</cell><cell>68.0 ± 4.5</cell><cell>35.6 ± 2.6</cell></row><row><cell>EdgePool</cell><cell cols="3">72.8 ± 1.9 63.0 ± 5.4 61.3 ± 3.9</cell><cell cols="2">70.3 ± 3.3 58.3 ± 3.6</cell></row><row><cell>CITESEER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Pooling</cell><cell>62.9 ± 2.9</cell><cell>40.9 ± 4.6</cell><cell>41.4 ± 3.8</cell><cell>58.9 ± 2.8</cell><cell>35.5 ± 3.2</cell></row><row><cell>EdgePool</cell><cell cols="3">65.3 ± 2.7 50.6 ± 3.9 49.9 ± 5.7</cell><cell cols="2">61.0 ± 3.4 50.0 ± 3.7</cell></row><row><cell>PUBMED</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Pooling</cell><cell>74.2 ± 1.7</cell><cell>60.8 ± 6.8</cell><cell>61.0 ± 4.4</cell><cell>73.0 ± 2.0</cell><cell>62.4 ± 4.1</cell></row><row><cell>EdgePool</cell><cell cols="3">74.1 ± 2.1 61.0 ± 6.4 61.9 ± 4.9</cell><cell cols="2">72.0 ± 4.7 64.8 ± 3.2</cell></row><row><cell>PHOTO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Pooling</cell><cell>88.4 ± 2.2</cell><cell>69.9 ± 3.2</cell><cell>71.9 ± 4.0</cell><cell>78.5 ± 4.5</cell><cell>59.6 ± 4.9</cell></row><row><cell>EdgePool</cell><cell cols="3">86.5 ± 0.8 77.1 ± 1.8 78.1 ± 1.5</cell><cell cols="2">81.0 ± 4.2 81.4 ± 2.3</cell></row><row><cell>COMPUTER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Pooling</cell><cell>80.0 ± 2.6</cell><cell>53.1 ± 5.5</cell><cell>52.4 ± 3.6</cell><cell>60.6 ± 12.4</cell><cell>43.0 ± 6.7</cell></row><row><cell>EdgePool</cell><cell cols="5">77.9 ± 2.2 58.1 ± 4.8 60.4 ± 4.3 62.5 ± 13.0 69.4 ± 2.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We experimented with a simple tanh gating function, but found softmax normalization to perform better.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bti1007</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards Sparse Hierarchical Graph Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jovanović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2018 Workshop on Relational Representation Learning</title>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graph U-Net</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJePRoAct7" />
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<idno>0717-6163. doi: 10.1007/ s13398-014-0173-7.2</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scientific containers for mobility of compute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Kurtzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sochat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singularity</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0177459</idno>
	</analytic>
	<monogr>
		<title level="j">Plos One</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">177459</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08082</idno>
		<title level="m">Self-Attention Graph Pooling</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Query-driven Active Surveying for Collective Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<idno>2371-9621. doi: 10.1609/ aimag.v29i3.2157</idno>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pitfalls of Graph Neural Network Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2783258.2783417</idno>
		<imprint>
			<date type="published" when="2015-10" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
