<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Relighting Networks for Image Light Source Manipulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution" key="instit1">Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<region>IP</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P K</forename><surname>Lun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Relighting Networks for Image Light Source Manipulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image relighting</term>
					<term>back-projection theory</term>
					<term>deep learn- ing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Manipulating the light source of given images is an interesting task and useful in various applications, including photography and cinematography. Existing methods usually require additional information like the geometric structure of the scene, which may not be available for most images. In this paper, we formulate the single image relighting task and propose a novel Deep Relighting Network (DRN) with three parts: 1) scene reconversion, which aims to reveal the primary scene structure through a deep auto-encoder network, 2) shadow prior estimation, to predict light effect from the new light direction through adversarial learning, and 3) re-renderer, to combine the primary structure with the reconstructed shadow view to form the required estimation under the target light source. Experiments show that the proposed method outperforms other possible methods, both qualitatively and quantitatively. Specifically, the proposed DRN has achieved the best PSNR in the "AIM2020 -Any to one relighting challenge" of the 2020 ECCV conference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image is a popular information carrier in this information era, which is intuitive and easy to understand. The rapid development of display devices stimulates people's demand for high-quality pictures. The visual appearance of the images is highly related to the illumination, which is vital in various applications, like photography and cinematography. Inappropriate illumination usually causes various visual degradation problems, like undesired shadows and distorted colours. However, the light source (like sunlight) is difficult to control, or sometimes unchangeable (for captured images), which increases the difficulty of producing satisfying images. The ways to produce the effect of light source on captured images becomes a hi-tech topic which has attracted considerable attention, because it offers opportunities to retouch the illuminations of the captured images. Some approaches have been proposed that aim to mitigate the degradation caused by improper illuminations. For example, histogram equalization (HE) <ref type="bibr" target="#b40">[38]</ref> rearranges the intensity to obey uniform distribution, which increases the discernment of the low-contrast regions. It balances the illumination of the whole image that manipulates the global light condition. Methods <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b39">37]</ref> in the high-dynamic-range (HDR) field improve the image quality by increasing the dynamic range of the low-contrast regions. The HDR methods can be regarded as a refinement of local contrast but lacks adjustment of the global light. Retinex-based methods <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b38">36]</ref> separate the images as the combination of illumination and reflectance, where the reflectance stores the inherent content of the scene that is unchangeable in different illumination conditions. By refining the illumination, it can improve the visual quality of the images. Low-light image enhancement methods <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b35">33]</ref> amend the visibility of the dark environment that enlighten the whole image. Shadow removal <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b20">19]</ref> is a popular topic in the field of image processing that aims to eliminate the shadow effects caused by the light sources, but cannot simulate the shadows for target light sources. Adjusting the light source provides a flexible and natural way for illumination-based image enhancement. Although considerable research has been devoted to refine the illumination, less effect is being made to study from the view of manipulating the light sources. In other words, changing the illumination by controlling the light source is still in its fancy stage. Literature in relighting field mainly focuses on specific applications, like portrait relighting <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b43">41]</ref>. These methods require prior information (like face landmarks, geometric priors) that cannot be implemented in general scenes.</p><p>Convolutional Neural Network (CNN) recently has attracted notable attention due to its powerful learning capacity. It can digest extensive training data and extract discriminative representations with the support of powerful computational resource. CNN has shown significant advantages in various tasks, like image classification <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b32">31]</ref>, semantic segmentation <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b41">39]</ref>, super-resolution <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b25">24]</ref>, place recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">20]</ref>, etc. CNNs with the deep structure are difficult to train because parameters of the shallow layers are often under gradient vanishing and exploding risks. Residual learning <ref type="bibr" target="#b12">[11]</ref> mitigates the optimizing difficulty by adding a shortcut connection among each processing block. With the assistance of the normalization layers, the gradient can flow from the deep to shallow layers steadily, which dramatically increases the training efficiency of the deep network. The deeper structure usually means more trainable parameters that bring in more powerful learning capacities, which makes it possible to handle more challenging tasks, like single image relighting. The image relighting method in this paper focuses on manipulating the position and color temperature of the light source based on our powerful deep CNN architecture. It not only can adjust the dominant hue, but can also recast the shadows of the given images. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we focus on a specific "any to one" relighting task <ref type="bibr" target="#b6">[5]</ref>, for which the input is under arbitrary light sources (any direction or color temperature, see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>), and the objective is to estimate the image under this specific light source (direction: E, color temperature: 4500K, see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). The proposed method can be generalized for other light-related tasks. Let us highlight the novelty of our proposed approach.</p><p>-Instead of directly mapping the input image to the target light condition, we formulate the relighting task in three parts: scene reconversion, light effects estimation and re-rendering process. -To preserve more information of the down-and up-sampling processes, we insert the back-projection theory to the auto-encoder structure, which benefits the scene reconversion and light-effect estimation. -The light effect is difficult to measure, which increases the training difficult. We use the adversarial learning strategy that is implemented by a new shadow-region discriminator, which gives guidance to the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Back-Projection (BP) theory. BP theory is popular in the field of single-image super-resolution <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b23">22]</ref>. Instead of directly learning the mapping from the input to the target, the BP-based methods iteratively digest the residuals and refine the estimations. It gives more focus on the weakness (i.e., the residuals) that appears at the learning process, which significantly improves the efficiency of the deep CNN architectures. Recent work on low-light image enhancement <ref type="bibr" target="#b35">[33]</ref> extends the BP theory to the light-domain-transfer tasks. It assumes the low-light (LL) and normal-light (NL) images locate at the LL and NL domains separately.</p><p>Firstly, a lightening operator predicts the NL estimation from the LL input. Then, a darkening operator maps the NL estimation back to the LL domain (LL estimation). In the LL domain, the difference (LL residual) between LL input and LL estimation can be found that indicates the weakness of the two transferring operators (lightening and darkening). Afterwards, the LL residual is mapped back to the NL domain (NL residual) through another lightening operator. The NL residual then refines the NL estimation for a better output. Mathematically, the enlightening process can be written as:N</p><formula xml:id="formula_0">= λ 2 L 1 (L) + L 2 (D(L 1 (L)) − λ 1 L)<label>(1)</label></formula><p>where L andN ∈ R H×W×3 denote the LL input image and NL estimation separately. The terms H, W and 3 represent the height, width and RGB channels respectively. The symbols L 1 and L 2 are two lightening operators to enlighten the LL image and LL residual individually. The symbol D is the darkening operator that maps the NL estimation to the LL domain. Two weighting coefficients λ 1 and λ 2 ∈ R are used to balance the residual calculation and final refinement. Adversarial Learning. Transferring an image to a corresponding output image is often formed as a pixel-wised regressing task of which the loss function (like L1-or L2-norm loss) indicates the average error for all pixels. This type of loss functions neglects the cross-correlation among the pixels, which easily distorts the perceptual structure and causes blur outputs. A large number of research works have done on quantitative measures of the perceptual similarity among images, like Structure SIMilarity (SSIM) <ref type="bibr" target="#b37">[35]</ref>, Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b42">[40]</ref>, Gram matrix <ref type="bibr" target="#b8">[7]</ref>, etc. However, the perceptual evaluation basically varies from different visual tasks and is difficult to formulate. The Generative Adversarial Networks (GANs) <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b27">26]</ref> provide a novel solution that embeds the perceptual measurement into the process of adversarial learning. Each GAN consists of a generator and a discriminator. The discriminator aims to find latent perceptual structure inside the target images, which then guides the training of the generator. Subsequently, the generator provides sub-optimal estimations that will work as negative samples for the training process of the discriminator. With the grouped negative and positive (target images) samples, the discriminator conducts a binary classification task, which measures the latent perceptual difference between the two types of samples. The overall training process is shown as: where D and G denote the discriminator and generator separately. The terms X and Y represent the input and target images respectively. In the training process, the generator and discriminator play a two-player minimax game. The discriminator learns to distinguish the estimated images G(X) from the target ones Y. The generator aims to minimize the difference between the estimated G(X) and target images Y. The training process follows the adversarial learning strategies, where the latent distribution inside the target images is increasingly learned and used. Finally, the training will reach a dynamic balance, where the estimations produced by the generator have similar latent perceptual structure as the real target images.</p><formula xml:id="formula_1">min G max D V (D, G) = E Y [logD(Y)] + E X [log(1 − D(G(X)))]<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Approach</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the proposed Deep Relighting Network (DRN) consists of three parts: scene reconversion, shadow prior estimation, and rerenderer. Firstly, the input image is handled in the scene reconversion network (see Section 3.2) to remove the effects of illumination, which extracts inherent structures from the input image. At the same time, another branch (shadow prior estimation, see Section 3.3) focuses on the change of the lighting effect, which recasts shadows according to the target light source. Next, the re-renderer part (see Section 3.4) perceives the lighting effect and re-paints the image with the support of the structure information. Both the scene reconversion and shadow prior estimation networks have a similar deep auto-encoder structure that is an enhanced variation of the "Pix2Pix" network <ref type="bibr" target="#b16">[15]</ref>. The details of the three components are presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Assumption of Relighting</head><p>Any-to-one Single image relighting is a challenging low-level vision task that aims to re-paint the input image X ∈ R H×W×3 (under any light source Φ) with the target light source Ψ. Inspired by the Retinex theory <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b38">36]</ref>, we assume that images can be decomposed into two components, where structure S is the inherent scene information of the image that is unchangeable under different light conditions. Let us define a lighting operation L Φ (·), which provides global illumination and causes the shadow effects for the scene S under the light source Φ. The input image can be written as:</p><formula xml:id="formula_2">X = L Φ (S)<label>(3)</label></formula><p>To re-paint the image X with another light source Ψ, it firstly needs to remove the lighting effect L −1 Φ (·), i.e., reconverting the structure information S from the input image X. Then, with the target light operation L Ψ (·), the image Y with the target light source can be obtained through:</p><formula xml:id="formula_3">Y = L Ψ (L −1 Φ (X))<label>(4)</label></formula><p>The key part of the reconversion process L −1 Φ (·) is to eliminate the shadows, while the lighting operation L Ψ (·) is to paint new shadows for the target light source. However, the geometric information is unavailable in the single image relighting task, which dramatically increases the difficulty of constructing the lighting operation L Ψ (·). Hence, instead of finding the lighting operation L Ψ (·) directly, the proposed method aims to find a transferring operation L (Φ→Ψ) (X) that migrate the light effects (mainly the shadows) from the input to the target, which significantly reduces the difficulty of re-painting the shadows. Finally, a re-rendering process P (·) is used to combine the scene structure and light effects. The whole process can be formulated as:Ŷ</p><formula xml:id="formula_4">= P (L −1 Φ (X), L (Φ→Ψ) (X))<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scene Reconversion</head><p>The objective of the scene reconversion is to extract the inherent structure information from the image so that the lighting effects can be removed. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the network adopts the auto-encoder <ref type="bibr" target="#b24">[23]</ref> structure with a skip connection to transfer the shallow features to the end. Firstly, the input image is down-sampled (acted by the "DBP" in the figure) four times to find the discriminative features (codes) for the scene. The channels are doubled after each down-sampling process to preserve information as much as possible. The features have large receptive fields, which contain much global information that benefits illumination estimation and manipulation. We design a similar auto-encoder structure as the Pix2Pix  method <ref type="bibr" target="#b16">[15]</ref>, where nine residual blocks <ref type="bibr" target="#b12">[11]</ref> ("ResBlocks" in the figure) act to remove the light effects. Next, four blocks up-sample (acted by the "UBP" in the figure) the feature map back to the original size, which is then enriched by the shallow features from the skip connection. The feature map is further aggregated with a feature selection process that is acted by a convolutional layer, which reduces the channels from 64 to 32 (the top-right "Conv."(gray rectangle) as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>). The feature is then sent to the following re-renderer process.</p><p>Back-Projection Block. Instead of solely down-sampling the features with the pooling or stride-convolution process, we adopt the back-projection block that remedies the lost information through residuals. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the Down-sampling Back-Projection (DBP) and Up-sampling Back-Projection (UBP) blocks consist of encoding and decoding operations that map the information between the input and latent spaces. To take the DBP block for example, it firstly maps the input (X) to latent space (Z) through an encoding process (E 1 , acted by a stride convolution layer with filter size of 3 × 3, stride of 2, padding of 1). Then, a decoder (D 2 , acted by a deconvolution layer with filter size of 4 × 4, stride of 2 and padding of 1) maps it back to the input space (X)to calculate the difference (residual, R X = X −X). The residual is encoded (E 2 , acted by a stride convolution layer with filter size of 3 × 3, stride of 2 and padding of 1) to the latent space R Z to remedy the latent code (Ẑ = Z + R Z ).</p><p>Mathematically, the DBP and UBP(similarly, see <ref type="figure" target="#fig_3">Fig. 4</ref>(b)) can be written as:Ẑ</p><formula xml:id="formula_5">= λ 2 E 1 (X) + E 2 (D 2 (E 1 (X)) − λ 1 X) (6) X = λ 2 D 1 (Ẑ) + D 2 (E 2 (D 1 (Ẑ)) − λ 1Ẑ )<label>(7)</label></formula><p>Semi-supervised reconversion. The objective of scene reconversion is to remove the light effect from the input image and construct the inherent structures. However, the ground-truth inherent structure is difficult to define because we have only the observed images. Instead of fully-supervising the network by well-defined ground-truths, it learns to estimate corresponding shadow-free images which might contain redundant information from the inherent structure. Exposure fusion methods <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b26">25]</ref> are widely used to improve the dynamic range of the images captured in uneven light conditions. It takes several images with different exposures, and merges them to an image with better visibility. The Virtual Image Dataset for Illumination Transfer (VIDIT) dataset <ref type="bibr" target="#b13">[12]</ref> contains images from 390 scenes. Each scene is captured 40 times with eight different light directions and five color temperatures. Different light directions cast the shadows at different positions, which makes it possible to build shadow-free images by selecting non-shadow pixels. The same selection strategy <ref type="bibr" target="#b26">[25]</ref> is adopted to build shadow-free images as implemented by the OpenCV package [2]: 1) Pixels that are too dark (underexposure) or too bright (overexposure) are given small weights. 2) Pixels with high saturation (standard deviation of RGB channels) are usually under good illumination that are given large weights. 3) Edges and textures usually contain more information and are considered more important. <ref type="figure" target="#fig_4">Fig. 5</ref> gives an example of exposure fusion. Images in <ref type="figure" target="#fig_4">Fig. 5 (a)</ref> are captured under different light direction and color temperatures. It is obvious that these images contain shadows caused by the point-source light. After using the exposure fusion method (as shown in <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>), one shadow-free image is obtained where the scene structure is obvious. The method is then used at the VIDIT dataset <ref type="bibr" target="#b13">[12]</ref> to generate shadow-free targets for all scenes.</p><p>Adversarial Learning. To train the scene reconversion network, a shadowfree image is formed via a convolutional layer (denoted as "Conv." in the green circle of <ref type="figure" target="#fig_2">Fig. 3)</ref>, which transfers the latency structure back to the image space. However, the shadows cause holes in the input image. To fill the holes with good perceptual consistency, a discriminator is attached to assist the training of the scene reconversion network. We adopt the same discriminator structure as <ref type="bibr" target="#b16">[15]</ref> that stacks four stride-convolution layers which hierarchically extract the global representations. During the training process, the discriminator is assigned to distinguish the estimation (of the scene reconversion network) from the ground-truth shadow-free images. At the beginning, the estimation lacks structure information. The discriminator notices the weakness and makes classification based on it. At the same time, the scene reconversion network is assigned to fake the discriminator, i.e., to equip the estimation with similar structure correlation as the target shadow-free images. Mathematically, the adversarial learning is:</p><formula xml:id="formula_6">L cGAN (G, D) = E (X,Y sf ) [logD(X, Y sf )] + E X [log(1 − D(X, G(X)))]<label>(8)</label></formula><p>where the generator G aims to minimize the loss L cGAN (G, D), i.e., G * = arg min G max D L cGAN (G, D). The discriminator D tries to maximize the loss L cGAN (G, D). The term cGAN indicates it is a conditional GAN structure that the discriminator has the input image X as prior information. Considering the estimated scene structure should be close to the ground-truth shadow-free target Y sf , the conventional L1-norm loss is used to measure the per-pixel error of the estimation. The objective for the scene reconversion network is defined as:</p><formula xml:id="formula_7">G * = λE (X,Y sf ) [||Y sf − G(X)||] + arg min G max D L cGAN (G, D) (9)</formula><p>where the term λ balances the L1-norm and the adversarial losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Shadow Prior Estimation</head><p>Different light sources cause different light effects which produce for example, different shadows and color temperatures. To produce the light effects from the target light source, we design a shadow prior estimation network with the architecture as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. The network adopts a similar structure as the scene reconversion network (as shown in <ref type="figure" target="#fig_2">Fig, 3)</ref>. Specifically, there are three major modifications: 1) This shadow prior estimation network discards the skip connection, because the network gives more focus on the global light effect. The skip connection brings the local features to the output directly, which makes the network lazy to learn the global change.</p><p>2) It has another discriminator that focuses on the shadow regions.</p><p>3) The ground-truth target is the image under the target light source. Mathematically, the objective of the shadow prior estimation network can be described as follows:</p><formula xml:id="formula_8">G * = λE X,Y [||Y − G(X)||] + arg min G max D L cGAN (G, D)<label>(10)</label></formula><p>+arg min</p><formula xml:id="formula_9">G max D shad L cGAN (G, D shad )<label>(11)</label></formula><p>where D shad denotes the shadow-region discriminator (details will be illustrated below), and the term Y denotes the image under the target light source.</p><p>Shadow-region discriminator. The shadow-region discriminator adopts the same structure as <ref type="bibr" target="#b16">[15]</ref> that stacks four stride-convolution layers, which gradually extracts the global feature representations. To focus on the shadow regions, the estimation is firstly rectified to give focus to the low-intensity (dark, usually the shadows) regions through z = min(α, x), where the symbol x denotes the estimated pixel intensity. The term z represents the rectified value that will be inputted to the discriminator. The term α is a pre-defined threshold for the sensitivity of the shadows (empirically, it is set to 0.059 = 15/255). <ref type="figure">Fig. 7</ref>. Structure of the re-renderer. The inputs are two feature maps that come from the scene reconversion and shadow prior networks separately. The rectangles and cubes represent operations and feature maps respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Re-rendering</head><p>After the processing of the scene reconversion and shadow prior estimation networks, the estimated scene structure and light effects will be fused together to produce the relighted output. As shown in <ref type="figure">Fig. 7</ref>, the re-renderer consists of three parts: multi-scale perception, channel-wise recalibration and painting process. Both global and local information are essential for light source manipulation because global information benefits the shadow and illumination consistency, and local information enhances the details. To utilize the information of different perception scales, we propose a novel multi-scale perception block that uses filters with different perceptive sizes (e.g., filter size of 3 × 3, 5 × 5, ...), which extracts rich features for the following process.</p><p>After processing the multi-scale perception, features with different spatial perception are merged into a single feature map, where each channel stores a type of spatial pattern. However, different patterns may have different importance for the re-rendering process. As designed in <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b35">33]</ref>, a recalibration process is designed to investigate the weights for different patterns, which selects the key features for the following painting process. Finally, a convolutional layer (with the filter size of 7 × 7, padding of 3, stride of 1 and a tanh activation function) paints the estimation from the feature space to the image space. Loss function. The loss function designed for the re-renderer consists of per-pixel reconstruction error and perceptual difference. The reconstruction error is measured by the wildly-used L1-norm loss. The perceptual similarity is calculated as <ref type="bibr" target="#b18">[17]</ref> based on the features extracted from the VGG-19 network. The network is pre-trained with ImageNet dataset for image classification. The extracted features have discriminative power for visual comparison, so that they are used to measure the perceptual similarity. The loss function is defined as:</p><formula xml:id="formula_10">L(Y,Ŷ) = ||Y −Ŷ|| + λ||f eat(Y) − f eat(Ŷ)||<label>(12)</label></formula><p>where Y andŶ denote the ground-truth target and estimated images respectively. The term f eat(·) is the feature maps extracted from the VGG-19 network. The symbol λ is a balanced coefficient and was set to 0.01 in our experiments.  <ref type="bibr" target="#b7">[6]</ref>) to get highresolution images. The objective of the VIDIT dataset is for illumination manipulation. Each scene is rendered with eight light directions and five color temperatures, which results in forty images with the resolution of 1024*1024. As we mentioned in Section 3.2, the exposure fusion method was used to generate shadow-free images for the scenes, which brings us 300 shadow-free images (work as ground-truth target) to guide the training of the scene reconversion network. We participated the "AIM Image Relighting Challenge -Track 1: any to one" <ref type="bibr" target="#b6">[5]</ref>. The objective is that, given an image under any types of illuminations, the method should give the estimation under a specific light source (color temperature is 4500k and light direction is from East). We used all possible pairs from the 300 training scenes to train the network, and the provided validation dataset (45 scenes) for evaluation.</p><p>Training Process. Limited by the GPU memory and computational power, our three sub-networks (scene reconversion, shadow prior estimation and re-renderer) were trained separately. Firstly, we trained the scene reconstruction network by using the paired inputs and shadow-free targets through the designed loss functions. Similarly, we trained the shadow prior estimation network with paired input and target images. Next, we fixed the scene reconstruction and shadow prior estimation networks and removed their last convolution layer and the discriminators (the green circle in <ref type="figure" target="#fig_4">Fig. 5</ref> and the pink circle in <ref type="figure" target="#fig_5">Fig. 6</ref>). Finally, the re-renderer network was trained with the designed loss functions. All training images were resized from 1024*1024 to 512*512, and the mini-batch size was set to six. We used the Adam optimization method with the momentum of 0.5 and learning rate of 0.0001. The networks were randomly initialized as <ref type="bibr" target="#b36">[34]</ref>. As we mentioned, the scene reconstruction and shadow prior estimation networks were firstly trained independently, where each network was trained for 20 epochs. Then, the two networks were fixed, and the re-renderer network was also trained for 20 epochs. All experiments were conducted through PyTorch <ref type="bibr" target="#b30">[29]</ref> on a PC with two NVIDIA GTX2080Ti GPUs. Codes have been released at https://github.com/WangLiwen1994/DeepRelight</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of the Proposed Method</head><p>There are no evaluation methods for light source measurement, which makes it difficult to evaluate the performance of different methods. Because we have the ground-truth images under the target light condition, and we believe the estimation should be close to these ground-truth targets. Hence, the Peak Signal-to-Noise Ratio (PSNR) and Structure SIMilarity (SSIM) <ref type="bibr" target="#b37">[35]</ref> are adopted to measure the similarity between the estimation and the ground-truth, where a larger value means better performance. To measure the perceptive quality, we use the Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b42">[40]</ref>, in which a smaller value means more perceptual similarity. Pix2pix <ref type="bibr" target="#b16">[15]</ref> has shown great success in the image-to-image translation tasks, like background removal, pose transfer, etc. The method is a conditional GAN structure that is trained through the adversarial strategy. The relighting problem can be regarded as an image-to-image translation task that translates the light source to the target settings. Because light source manipulation is a new topic, few methods are available for comparison. Therefore, the Pix2Pix can be considered as the baseline model to present the efficiency of the proposed method. The Pix2Pix method is based on an auto-encoder structure where the input image is firstly down-sampled four times (scale is reduced to 1/16), and then processed by nine residual blocks. Finally, a set of deconvolutional layers is used to up-sample the image back to the original size and the estimation is formed. <ref type="table" target="#tab_1">Table 1</ref> gives a comparison among different structures, where ShadAdv and BPAE are two variations of the Pxi2Pix network. The baseline method (Pix2Pix) achieves 16.28 dB in PSNR, 0.553 in SSIM and 0.482 in LPIPS. The performance of other structures (ShadAdv, BPAE, and the proposed DRN) will be discussed below. Effect of the Shadow-region Discriminator. Let us enhance the baseline, Pix2Pix, by adding the proposed shadow-region discriminator (as introduced in Section 3.3) to it, and entitled it as "ShadAdv". Compared with the original Pix2Pix method, the "ShadAdv" gives more focus on the appearance of the shadow regions. In other words, the shadow discriminator can provide better guidance for recasting the shadows of the target light source. With more accurate shadows, the PSNR is increased by 0.84(= 17.12 − 16.28) dB, and the perceptive quality is improved by 0.042 (= 0.482 − 0.440) in terms of LPIPS.</p><p>Effect of the Back-Projection (BP) Block. The "Pix2Pix" and "ShadAdv" methods are based on the auto-encoder structure. As we have mentioned, it down-and up-samples the image through stacked convolutional and deconvolutional layers. The "BPAE" method is an enhanced version of the auto-encoder, where the down-and up-sampling processes are done by the DBP and UBP blocks (as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>). The BP blocks are based on the back-projection theory, which remedies the lost information in the down-and up-sampling processes. Compared with the auto-encoder structure (used in "ShadAdv"), the "BPAE" method extracts more informative features, which enriches the structure of the estimation and increases the SSIM from 0.569 to 0.573.</p><p>Effect of the Relighting Assumption. As defined in Section 3.1, we regard the any-to-one relighting task as a two-stage problem, where the first stage finds the scene structure L −1 Φ (X) and light effect L (Φ→Ψ) (X) from the input image X. The second stage paints P (·) the estimation Y under the target light source. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the "Pix2Pix", "ShadAdv" and "BPAE" methods learns the mapping to the target light condition directly. The "DRN" is the proposed method that is based on our relighting assumption. It is clear that the proposed method achieves the best reconstruction with the highest PSNR (17.59 dB) and SSIM (0.596) scores, and comparable visual similarity (0.440 of LPIPS). These suggest the effects of the proposed relighting assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Other Approaches</head><p>Single image relighting is a new topic in the field of image processing. As we have mentioned, few methods are publicly available for our comparison. Besides comparing with the baseline method (Pix2Pix <ref type="bibr" target="#b16">[15]</ref>), we have also made comparisons with other representive methods. U-Net <ref type="bibr" target="#b24">[23]</ref> is a popular CNN structure that was initially designed for biomedical image segmentation. It consists of down-(encoder) and up-sampling (decoder) paths to form an auto-encoder structure, where several short-connections transmit the information from the encoder to the decoder part directly.</p><p>Retinex-Net <ref type="bibr" target="#b38">[36]</ref> was designed to enlighten the low-light images based on the Retinex theory. It firstly decomposes the low-light image into the reluctance and illumination elements, and then an adjustment sub-network refines the illumination to enlighten the input images. We retrained the methods with their desired settings at the VIDIT <ref type="bibr" target="#b13">[12]</ref> training dataset, and the comparison was made using the VIDIT validation dataset.  <ref type="table" target="#tab_2">Table 2</ref> shows the results of different approaches. Benefiting from the short-connections, the U-Net method preserves much information of the inputs, which achieves good SSIM performance. However, the shortconnections preserve too much structure (detailed) information which makes the network lazy to change the light source that limits its PSNR score. The Retinex-Net method can find the inherent scene structure but fails in manipulating the light source. The proposed DRN method is able to manipulate the light source, and shows superior performance (with the best PSNR of 17.59 dB and the LPIPS score of 0.440) compared with all other methods. Also, we made use of the proposed DRN network to join the competition "AIM2020 Image Relighting Challenge -Track 1: any to one" and have achieved the best PSNR score in the final testing phase.</p><p>Visual Comparison. <ref type="figure">Fig. 8</ref> gives a visual comparison among different methods. As shown in the first row of <ref type="figure">Fig. 8(a)</ref> that the Retinex-Net method <ref type="bibr" target="#b38">[36]</ref> fails to change the color temperature where the hue of the estimation is significantly different from the others. Although U-Net <ref type="bibr" target="#b24">[23]</ref> produces correct color temperature for the target light source, it fails to manipulate the light direction (see the arrows in <ref type="figure">Fig. 8(a)</ref>). The Pix2Pix <ref type="bibr" target="#b16">[15]</ref> can produce the correct light direction but brings in many artifacts (see the red rectangle area), which decreases the perceptual quality. The proposed method gives correct estimation for the light direction and color temperature with a good perceptual quality.</p><p>A challenging case is shown in <ref type="figure">Fig. 8(b)</ref>, where the input image is nearly all black (see the top-left image of the figure). For better visualization, we provide a binary mask to show the illuminated and shadow regions (see the 2 nd row of the <ref type="figure">figure)</ref>. The U-Net <ref type="bibr" target="#b24">[23]</ref> fails to enlighten the building with the new light source (the yellow circle in <ref type="figure">Fig. 8(b)</ref>). Pix2Pix <ref type="bibr" target="#b16">[15]</ref> brings many artifacts that cause inconsistency shadows (the <ref type="figure">Fig. 8</ref>. Visual comparison among different approaches (zoom in for better view). Images in the 2 nd row of (a) are the results after gamma correction to highlight the light effects and the arrows indicate the light directions. The 2 nd row of (b) contains the binary mask for illuminated and shadow regions. green circle in <ref type="figure">Fig. 8(b)</ref>). Benefited from our shadow-region discriminator, the proposed DRN enlightens the building and recasts the shadows for the new light source (the blue and red encircled regions, as shown in <ref type="figure">Fig. 8(b)</ref>), which suggests superior performance as compared to all other approaches. However, the structure of the center building is completely lost (the pixels are all zero) in the input image, which makes the relighting difficult. It can be seen from the figure that the proposed method attempts to recover the color and shape of the wall, but fails to construct the detailed structures. Limited by the time and the training data, the network is designed to focus on the global light effects and lacks investigation for more advanced topics, like inpainting the building. In the future, we will continue our work and would like to invite others to work on these challenging relighting cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have introduced our proposed Deep Relighting Network (DRN) that achieves excellent performance in single image relighting task. We formulate the image relighting as three parts: scene reconversion, shadow prior estimation and re-rendering. We embed the backprojection theory into auto-encoder structure that significantly improves the capacity of the deep network. Benefited from adversarial learning, the proposed DRN can recast the shadows and estimate the required image from the target light source, which confirms our formulation of separating the scene and shadow structures. It is useful and can be generalized to many light-related tasks, for example, cases with dual or blind light sources, reference-based image relighting (i.e., produce images with any light source settings). Experimental results show that the proposed DRN network outperforms all other methods. Also, it obtained the best PSNR in the competition "AIM2020 Image Relighting Challenge -Track 1: any to one" of the 2020 ECCV conference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example of the "any to one" relighting task. The 2500K, 3500K, ... are the color temperatures, and the E, N, ... are the light directions. Images in (a) are the inputs with any light settings, and (b) is target out with a specific light setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the proposed method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Structure of the scene reconversion network. The structures in the green circle are removed after the training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Structure of the Down-sampling Back-Projection (DBP, as shown in (a)) and Up-sampling Back-Projection (UBP, as shown in (b)) blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>An example of exposure fusion process (images with ID "239" of the VIDIT dataset<ref type="bibr" target="#b13">[12]</ref>). The fourty images in (a) are captured under eight different light directions and five color temperatures of the same scene. The fusion result is shown in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Structure of the shadow prior estimation network. The structures in the red circle are removed after the training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison among different structures</figDesc><table><row><cell>Method</cell><cell cols="3">ShadAdv Structure Stages PSNR SSIM LPIPS</cell></row><row><cell>Pix2Pix [15]</cell><cell>No</cell><cell>Auto-Encoder One</cell><cell>16.28 0.553 0.482</cell></row><row><cell>ShadAdv</cell><cell>Yes</cell><cell>Auto-Encoder One</cell><cell>17.12 0.569 0.440</cell></row><row><cell>BPAE</cell><cell>Yes</cell><cell>Back-Pojection One</cell><cell>17.22 0.573 0.439</cell></row><row><cell>DRN (proposed)</cell><cell>Yes</cell><cell cols="2">Back-Pojection Two 17.59 0.596 0.440</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison among different approaches</figDesc><table><row><cell>Methods</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS</cell></row><row><cell>U-Net [23]</cell><cell>16.72</cell><cell>0.616</cell><cell>0.441</cell></row><row><cell>Retinex-Net [36]</cell><cell>12.28</cell><cell>0.162</cell><cell>0.657</cell></row><row><cell>Pix2Pix [15]</cell><cell>16.28</cell><cell>0.553</cell><cell>0.482</cell></row><row><cell>DRN (proposed)</cell><cell>17.59</cell><cell>0.596</cell><cell>0.440</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Night-to-day image translation for retrieval-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anoosheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/icra.2019.8794387</idno>
		<ptr target="https://doi.org/10.1109/icra.2019.8794387" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recovering high dynamic range radiance maps from photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
	<note>SIG-GRAPH &apos;97</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/258734.258884</idno>
		<ptr target="https://doi.org/10.1145/258734.258884" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recovering high dynamic range radiance maps from photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2008 classes</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AIM 2020: Scene relighting and illumination estimation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops</title>
		<meeting>the European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<publisher>ECCVW</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unreal Engine | The most powerful real-time 3D creation platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Epic Games</surname></persName>
		</author>
		<ptr target="https://www.unrealengine.com/en-US/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Akita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Navarrete Michelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<title level="m">Aim 2019 challenge on image extreme super-resolution: Methods and results</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3556" to="3564" />
		</imprint>
	</monogr>
	<note>IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barthas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05460</idno>
		<title level="m">Vidit: Virtual image dataset for illumination transfer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Direction-aware spatial context features for shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7454" to="7462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Enlightengan: Deep light enhancement without paired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv abs/1906.06972</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shadow removal via shadow image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8577" to="8586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast monocular visual place recognition for non-uniform vehicle speed and varying lighting environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical back projection network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image super-resolution via attention based back projection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop</title>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ntire 2020 challenge on realworld image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exposure fusion: A simple and practical alternative to high dynamic range photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Reeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="161" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nestmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lehrmann</surname></persName>
		</author>
		<title level="m">Learning physics-guided face relighting under directional light. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Retinex processing for automatic image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">U</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="111" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single image portrait relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3306346.3323008</idno>
		<ptr target="https://doi.org/10.1145/3306346.3323008" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lightening network for lowlight image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Lun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2020.3008396</idno>
		<ptr target="https://doi.org/10.1109/TIP.2020.3008396" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7984" to="7996" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep retinex decomposition for lowlight enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv abs/1808.04560</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep high dynamic range imaging with large foreground motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contrast limited adaptive histogram equalization based enhancement for real time video system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2392" to="2397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02147</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep single-image portrait relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
