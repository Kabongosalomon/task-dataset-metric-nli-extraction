<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Deeper Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date>August 23-27, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
							<email>mengliu@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
							<email>hongyang.gao@tamu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Deeper Graph Neural Networks</title>
					</analytic>
					<monogr>
						<title level="j" type="main">KDD</title>
						<meeting> <address><addrLine>USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<biblScope unit="volume">20</biblScope>
							<date type="published">August 23-27, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403076</idno>
					<note>ACM Reference Format: Meng Liu, Hongyang Gao, and Shuiwang Ji. 2020. Towards Deeper Graph Neural Networks. In Proceedings of the 26th ACM SIGKDD Conference on ACM ISBN 978-1-4503-7998-4/20/08. . . $15.00 Knowledge Discovery and Data Mining USB Stick (KDD &apos;20), August 23-27, 2020, Virtual Event, USA. ACM, New York, NY, USA, 11 pages. https: //</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Mathematics of computing → Graph algorithms</term>
					<term>• Com- puting methodologies → Artificial intelligence</term>
					<term>Neural net- works KEYWORDS deep learning, graph representation learning, graph neural net- works</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, coauthorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs, representing entities and their relationships, are ubiquitous in the real world, such as social networks, point clouds, traffic networks, knowledge graphs, and molecular structures. Recently, many studies focus on developing deep learning approaches for graph data, leading to rapid development in the field of graph neural networks. Great successes have been achieved for many applications, such as node classification <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33]</ref>, graph classification <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b38">38]</ref> and link prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>. Graph convolutions adopt a neighborhood aggregation (or message passing) scheme to learn node representations by considering the node features and graph topology information together, among which the most representative method is Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b11">[11]</ref>. GCN learns representation for a node by aggregating representations of its neighbors iteratively. However, a common challenge faced by GCN and most other graph convolutions is that one layer of graph convolutions only consider immediate neighbors and the performance degrades greatly when we apply multiple layers to leverage large receptive fields. Several recent works attribute this performance degradation to the oversmoothing issue <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b33">33]</ref>, which states that representations from different classes become inseparable due to repeated propagation. In this work, we study this performance deterioration systematically and develop new insights towards deeper graph neural networks.</p><p>We first systematically analyze the performance degradation when stacking multiple GCN layers by using our quantitative metric for node representation smoothness measurement and a data visualization technique. We observe and argue that the main factor compromising the performance greatly is the entanglement of representation transformation and propagation. After decoupling these two operations, it is demonstrated that deeper graph neural networks can be deployed to learn graph node representations from larger receptive fields without suffering from performance deterioration. The over-smoothing issue is shown to affect performance only when extremely large receptive fields are utilized. We further give a theoretical analysis of the above observation when building very deep models, which shows that graph node representations will become indistinguishable when depth goes infinity. This aligns with the over-smoothing issue. The previous descriptions of the over-smoothing issue simplify the assumption of non-linear activation function <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b33">33]</ref> or make approximations of different probabilities <ref type="bibr" target="#b33">[33]</ref>. Our theoretical analysis can serve as a more rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose an efficient and effective network, termed as Deep Adaptive Graph Neural Network, to learn node representations by adaptively incorporating information from large receptive fields. Extensive experiments on citation, co-authorship, and co-purchase datasets demonstrate the reasonability of our insights and the superiority of our proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORKS</head><p>First, we introduce our notations used throughout this paper. Generally, we let bold uppercase letters represent matrices and bold lowercase letters denote vectors. A graph is formally defined as G = (V , E), where V is the set of nodes (vertices) that are indexed from 1 to n and E ⊆ V × V is the set of edges between nodes in V . n = |V | and m = |E| are the numbers of nodes and edges, respectively. In this paper, we consider unweighted and undirected graphs. The topology information of the whole graph is described by the adjacency matrix A ∈ R n×n , where A (i, j) = 1 if an edge exists between node i and node j, otherwise 0. The diagonal matrix of node degrees are denoted as D ∈ R n×n , where D (i,i) = j A <ref type="bibr">(i, j)</ref> . N i denotes the neighboring nodes set of node i. An attributed graph has a node feature matrix X ∈ R n×d , where each row x i ∈ R d represents the feature vector of node i and d is the dimension of node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Convolution Operations</head><p>Most popular graph convolution operations follow a neighborhood aggregation (or message passing) fashion to learn a node representation by propagating representations of its neighbors and applying transformation after that. The ℓ-th layer of a general graph convolution can be described as</p><formula xml:id="formula_0">a (ℓ) i = PROPAGATION (ℓ) x (ℓ−1) i , {x (ℓ−1) j |j ∈ N i } x (ℓ) i = TRANSFORMATION (ℓ) a (ℓ) i . (1) x (ℓ)</formula><p>i is the representation of node i at l-th layer and x (0) i is initialized as node feature x i . Most graph convolutions, like GCN <ref type="bibr" target="#b11">[11]</ref>, GraphSAGE <ref type="bibr" target="#b9">[9]</ref>, GAT <ref type="bibr" target="#b30">[30]</ref>, and GIN <ref type="bibr" target="#b32">[32]</ref>, can be obtained under this framework by deploying different propagation and transformation mechanisms.</p><p>Without losing generalization, we focus on the Graph Convolutional Network (GCN) <ref type="bibr" target="#b11">[11]</ref>, the most representative graph convolution operation, in the following analysis. The ℓ-th layer forwardpropagation process is formulated as</p><formula xml:id="formula_1">X (ℓ) = σ AX (ℓ−1) W (ℓ) ,<label>(2)</label></formula><p>where X (ℓ) ∈ R n×d (ℓ) and X (ℓ−1) ∈ R n×d (ℓ−1) are the output and input node representation matrices of layer ℓ.</p><formula xml:id="formula_2">A = D − 1 2 A D − 1 2 , where A = A + I is the adjacency matrix with added self-connections. D (i,i) = j A (i, j) is the diagonal node degree matrix. W (ℓ) ∈ R d (ℓ−1) ×d (ℓ)</formula><p>is a layer-specific trainable weight matrix. σ is a nonlinear activation function like ReLU <ref type="bibr" target="#b22">[22]</ref>. Intuitively, GCN learns representation for each node by propagating neighbors' representations and conducting non-linear transformation after that. GCN is originally applied for semi-supervised classification, where only partial nodes have training labels in a graph. Thanks to the propagation process, representation of a labeled node carries the information from its neighbors that are usually unlabeled, thus training signals can be propagated to the unlabeled nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Works</head><p>From Eq.(2), one layer GCN only considers immediate neighbors, i.e. one-hop neighborhood. Multiple layers should be applied if multi-hop neighborhood is needed. In practice, however, the performance of GCN degrades greatly when multiple layers are stacked. Several works reveal that stacking many layers can bring the oversmoothing issue, which means that representations of nodes converge to indistinguishable limits. To our knowledge, <ref type="bibr" target="#b15">[15]</ref> is the first attempt to demystify the over-smoothing issue in the GCN model. The authors first demonstrate that the propagation process of the GCN model is a special symmetric form of Laplacian smoothing <ref type="bibr" target="#b29">[29]</ref>, which makes the representations of nodes in the same class similar, thus significantly easing the classification task. Then they show that repeatedly stacking many layers may make representations of nodes from different classes indistinguishable. The same problem is studied in <ref type="bibr" target="#b33">[33]</ref> by analyzing the connection of nodes' influence distribution and random walk <ref type="bibr" target="#b17">[17]</ref>. Recently, SGC <ref type="bibr" target="#b31">[31]</ref> is proposed by reducing unnecessary complexity in GCN. The authors show that SGC corresponds to a low-pass-type filter on the spectral domain, thus deriving smoothing features across a graph. Another recent work <ref type="bibr" target="#b3">[3]</ref> verify that smoothing is the nature of most typical graph convolutions. It is showed that reasonable smoothing makes graph convolutions work and over-smoothing results in poor performance.</p><p>Due to the potential concern of the over-smoothing issue, a limited neighborhood is usually used in practice and it is difficult to extend. However, long-range dependencies should be taken into consideration, especially for peripheral nodes. Also, small receptive fields are not enough to propagate training signals to the whole graph when the number of training nodes is limited under a semi-supervised learning setting. <ref type="bibr" target="#b15">[15]</ref> applies co-training and self-training to overcome the limitation of shallow architectures. A smoothness regularizer term and adaptive edge optimization are proposed in <ref type="bibr" target="#b3">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Network <ref type="bibr" target="#b33">[33]</ref> deploys a layer-aggregation mechanism to adaptively select a nodeâĂŹs sub-graph features at different ranges rather than to capture equally smoothed representations for all nodes. <ref type="bibr" target="#b12">[12]</ref> utilizes the relationship between GCN and PageRank <ref type="bibr" target="#b23">[23]</ref> to develop a propagation mechanism based on personalize PageRank, which can preserve the node's local information while gather information from a large neighborhood. Recently, Geom-GCN <ref type="bibr" target="#b25">[25]</ref> and non-local GNNs <ref type="bibr" target="#b16">[16]</ref> are proposed to capture longrange dependencies for disassortative graph by designing non-local aggregators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EMPIRICAL AND THEORETICAL ANALYSIS OF DEEP GNNS</head><p>In this section, we first propose a quantitative metric to measure the smoothness of graph node representations. Then we utilize this metric, along with a data visualization technique, to rethink the performance degradation when utilizing GCN layer to build deep graph neural networks. We observe and argue that the entanglement of representation transformation and propagation is a prominent factor that compromises the network performance. After decoupling these two operations, deeper graph neural networks can be built to learn graph node representations from large receptive fields without suffering from performance degradation. The over-smoothing issue is shown to influence the performance only when extremely large receptive fields are adopted. Further, we provide a theoretical analysis of the above observation when building very deep models, which aligns with the conclusion of over-smoothing issue and can serve as a rigorous description of the over-smoothing issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantitative Metric for Smoothness</head><p>Smoothness is a metric that reflects the similarity of node representations. Here, we first define a similarity metric between the representations of node i and node j with their Euclidean distance:</p><formula xml:id="formula_3">D(x i , x j ) = 1 2 x i ∥x i ∥ − x j ∥x j ∥ ,<label>(3)</label></formula><p>where x i is the feature representation of node i and ∥·∥ denotes the Euclidean norm. The Euclidean distance is a simple but effective way to measure the similarity of two representations, especially in high dimensional space. Smaller Euclidean distance value incidates higher similarity of two representations. To remove the influence of the magnitude of feature representations, we use normalized node representations to compute their Euclidean distance, thus constraining D(x i , x j ) in the range of [0, 1]. Based on the similarity metric in Eq.(3), we further propose a smoothness metric SMV i for node i, which is computed as the average distance between node i to other nodes:</p><formula xml:id="formula_4">SMV i = 1 n − 1 j ∈V , j i D(x i , x j ).<label>(4)</label></formula><p>Hence, SMV i measures the similarity of node i's representations to the entire graph. For instance, a node in the periphery or a leaf node usually has a large smoothness metric value. Further, we can use SMV G to represent the smoothness metric value of the whole graph G. Its mathematical expression is defined as:</p><formula xml:id="formula_5">SMV G = 1 n i ∈V SMV i .<label>(5)</label></formula><p>Here, SMV G is negatively related to the overall smoothness of nodes' representations in graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why Deeper GNNs Fail?</head><p>In this section, we utilize our proposed smoothness metric to investigate the performance deterioration phenomenon in deep graph neural networks. Here, we mainly use the GCN layer for analysis, but the main results can be easily applied to other graph deep learning methods. Besides using our proposed metric from the quantitative perspective, we employ a data visualization technique t-SNE <ref type="bibr" target="#b19">[19]</ref>. t-SNE provides an interpretable visualization, especially on high-dimensional data. t-SNE is capable of capturing both the local structure and the global structure like clusters in high-dimensional data, which is consistent with the classification of nodes. Hence, we utilize t-SNE to demonstrate the discriminative power of node representations in the graph. We develop a series of graph neural networks (GNNs) with different depths in terms of the number of GCN layers, and evaluate them on three citation datasets; those are Cora, CiteSeer and PubMed <ref type="bibr" target="#b26">[26]</ref>. In particular, we also include a graph neural network with depth of 0, which is approximated with a multi-layer perceptron network. A GNN with depth of 0 can be viewed as aggregating information from a 0-hop neighborhood, which only considers node features while with the graph structure ignored. We conduct 100 runs for each model on each dataset, using the same data split scheme as <ref type="bibr" target="#b11">[11]</ref>.</p><p>The result on Cora is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We provide results on other datasets in Section A.1 in the appendix. We can observe that test accuracy increases as the rise of the number of layers in the beginning, but degrades dramatically from 3 layers. Besides, from the t-SNE visualization on Cora in <ref type="figure" target="#fig_0">Figure 1</ref> (t-SNE visualization results of other datasets are provided in Appendix A.3.), the discriminative power of the node representations derived by different numbers of GCN layers has the similar trend. The node representations generated by multiple GCN layers, like 6 layers, are very difficult to be separated.  Several studies <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b15">15]</ref> attribute this performance degradation phenomenon to the over-smoothing issue. However, we question this view for the following two reasons. First, we hold that the over-smoothing issue only happens when node representations propagate repeatedly for a large number of iterations, especially for a graph with sparsely connected edges. As shown in <ref type="table" target="#tab_0">Table 1</ref>, all these three citation datasets have a small value of edge density, hence several propagation iterations are not enough to make oversmoothing happen, theoretically. Second, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the smoothness metric value of graph node representations has a slight downward trend as the number of propagation iterations increases. According to <ref type="bibr" target="#b15">[15]</ref>, the node representations suffering from the oversmoothing issue will converge to the same value or be proportional to the square root of the node degree, where the corresponding smoothness metric value should be close to 0, computed by our quantitative metric. However, the metric value in our experiments is relatively far from the ideal over-smoothing situation.</p><p>In this work, we argue that it is the entanglement of transformation and propagation that significantly compromise the performance of deep graph neural networks. Our argument is originated from the following two intuitions. First, the entanglement of representation transformation and propagation makes the number of parameters in transformation intertwined with the receptive fields in propagation. As illustrated in Eq.(1), one hop propagation requires a transformation function, thus leading to a large number of parameters when considering a large receptive field. Hence, it might be hard to train a deep GNN with a large number of parameters. This can possibly explain why the performance of multiple GCN layers in <ref type="figure" target="#fig_1">Figure 2</ref> fluctuates greatly. Second, representation propagation and transformation should be viewed as two separate operations. Note that the class of a node can be totally predictable by its initial features, which explains why MLP, as shown in <ref type="figure" target="#fig_1">Figure  2</ref> and 1, performs well without using any graph structure information. Propagation based on the graph structure can help to ease the classification task by making node representations in the same class to be similar, under the assumption that connected nodes usually belong to the same class. For instance, intuitively, the class of a document is completely determined by its content (i.e. its feature derived by word embedding), instead of the references relationships with other documents. Utilizing its neighbors' features just eases the classification of documents. Hence, representation transformation and propagation play their distinct roles from feature and structure aspects, respectively.</p><p>To support and verify our argument, we decouple the propagation and transformation in Eq.(2), leading to the following model:</p><formula xml:id="formula_6">Z = MLP (X ) X out = softmax A k Z .<label>(6)</label></formula><p>Z ∈ R n×c denotes the new feature matrix transformed from the original feature matrix by an MLP network, where c is the number of classes. After the transformation, we apply a k-steps propagation to derive the output feature matrix X out ∈ R n×c . A softmax classifier is applied to compute the classification probabilities. Notably, the separation of transformation and propagation processes is also adopted in <ref type="bibr" target="#b12">[12]</ref> and <ref type="bibr" target="#b31">[31]</ref> but for the sake of reducing complexity.</p><p>In this work, we analyze this scheme systematically and reveal that it can help to build deeper models without suffering from performance degradation, which has not been prioritized by the community.</p><p>The test accuracy and smoothness metric value of representations with different numbers of layers adopted in Eq.(6) on Cora are illustrated in <ref type="figure" target="#fig_3">Figure 4</ref> (Results for other datasets are shown in Appendix A.2). After resolving the entanglement of feature transformation and propagation, deeper models is capable of leveraging larger receptive fields without suffering from performance degradation. We can observe that the over-smoothing issue starts compromising the performance at an extremely large receptive field, such as 75-hop on Cora. The smoothness metric value decreases greatly after that, which is demonstrated by the metric value of nearly 0. Besides, from the t-SNE visualization in <ref type="figure" target="#fig_2">Figure 3</ref> (The t-SNE visualization results of other datasets are provided in Appendix A.4), deep models with large receptive fields, like 50-hop, still generating distinguishable node representations, which is impressive compared to the regular GCN model. In practice, we usually do not need an extremely large receptive field because the highest shortest path distance in a connected component usually is an acceptable small number. Thus training signals can be propagated to the entire graph with a small number of layers. This is demonstrated by the fact that graph neural networks with 2 or 3 GCN layers usually perform competitively. However, deep models with large receptive fields are necessary to incorporate more information, especially with limited training nodes under a semi-supervised learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Theoretical Analysis of Very Deep Models</head><p>The empirical analysis in the previous section shows that decoupling transformation and propagation can help to build much deeper models which can leverage larger receptive fields to incorporate more information. In this section, we provide a theoretical analysis of the above observation when building very deep graph neural networks, which aligns with the over-smoothing issue. <ref type="bibr" target="#b15">[15]</ref> and <ref type="bibr" target="#b33">[33]</ref> study the over-smoothing issue from the perspective of Laplacian smoothing and nodes' influence distribution, with several simplified assumptions like non-linear transformation and probability approximations. After decoupling transformation from propagation, our theoretical analysis can serve as a more rigorous and gentle description of the over-smoothing issue. In this section, we strictly describe the over-smoothing issue for 2 typical propagation mechanisms.</p><p>A</p><formula xml:id="formula_7">⊕ = D −1 A and A ⊙ = D − 1 2 A D − 1 2 , where A = A + I ,</formula><p>are two frequently utilized propagation mechanisms. The row-averaging normalization A ⊕ is adopted in GraphSAGE <ref type="bibr" target="#b9">[9]</ref> and DGCNN <ref type="bibr" target="#b38">[38]</ref>. The symmetrical normalization scheme A ⊙ is applied in GCN <ref type="bibr" target="#b11">[11]</ref>. In the following, we describe the over-smoothing issue by proving the convergence of A k ⊕ and A k ⊙ , respectively, when k goes to infinity. Let e = [1, 1, · · · , 1] ∈ R 1×n be a row vector whose all entries are 1. Function Ψ(x) = x sum(x ) normalizes a vector to sum to 1 and function Φ(x) = x ∥x ∥ normalizes a vector such that its magnitude is 1.</p><formula xml:id="formula_8">Theorem 3.1. Given a connected graph G, lim k →∞ A k ⊕ = Π ⊕ ,</formula><p>where Π ⊕ is the matrix with all rows are π ⊕ and π ⊕ = Ψ(e D).</p><formula xml:id="formula_9">Theorem 3.2. Given a connected graph G, lim k →∞ A k ⊙ = Π ⊙ , where Π ⊙ = Φ( D 1 2 e T )(Φ( D 1 2 e T )) T .</formula><p>From the above two theorems, we can derive the exact convergence value of A k ⊕ and A k ⊙ , respectively, when k goes to infinity in an infinite deep model. Hence, applying infinite layers to propagate information iteratively is equivalent to utilizing Π ⊕ or Π ⊙ to propagate features by one step. Rows of Π ⊕ are the same and rows of Π ⊙ are proportional to the square root value of the corresponding nodes' degrees. Therefore, rows of Π ⊕ or Π ⊙ are linearly inseparable and utilizing them as propagation mechanism will generate indistinguishable representations, thereby leading to the over-smoothing issue.</p><p>To prove these two theorems, we first introduce the following two lemmas. The proofs of these two lemmas can be found in Appendix A.5 and A.6. Proof. (of Theorem 3.1) A ⊕ can be viewed as a transition matrix because all entries are nonnegative and each row sums to 1. The graph G can be further regarded as a Markov chain, whose transition matrix P is A ⊕ . This Markov chain is irreducible and aperiodic because the graph G is connected and self-loops are included in the connectivity. If a Markov chain is irreducible and aperiodic, then lim k →∞ P k = Π, where Π is the matrix with all rows equal to π and π can be computed by πP = π , s.t. i π i = 1 <ref type="bibr" target="#b13">[13]</ref>. It is obvious that π is the unique left eigenvector of P and is normalized such that all entries sum to 1. Hence, lim k →∞ A k ⊕ = Π ⊕ , where Π ⊕ is the matrix with all rows are π ⊕ and π ⊕ = Ψ(e D) from Lemma 3.4. □</p><p>Proof. (of Theorem 3.2) Although A ⊙ cannot be processed as a transition matrix like A ⊕ , it is a symmetric matrix, which is diagonalizable. We have A ⊙ = QΛQ T , where Q is an orthogonal matrix whose columns are normalized eigenvectors of A ⊙ and Λ is the diagonal matrix whose diagonal entries are the eigenvalues. Then the k-th power of A ⊙ can be computed by</p><formula xml:id="formula_10">A k ⊙ = QΛQ T · · · QΛQ T = QΛ k Q T = k i=1 λ n i v i v T i ,<label>(7)</label></formula><p>where v i is the normalized right eigenvector associated with λ i . From Lemma 3.4, A ⊙ always has an eigenvalue 1 with unique associated eigenvectors and all other eigenvalues λ satisfy |λ| &lt; 1.</p><formula xml:id="formula_11">Hence, lim k →∞ A k ⊙ = Φ( D 1 2 e T )(Φ( D 1 2 e T )) T . □</formula><p>These two theorems hold for connected graphs that are frequently studied in graph neural networks. For a disconnected graph, these theorems can also be applied to each of its connected components, which means that applying these propagation mechanisms infinite times will generate indistinguishable node representations in each connected components.</p><p>The above theorems reveal that over-smoothing will make node representations inseparable and provide the exact convergence value of frequently used propagation mechanisms. Theoretically, we have proved that the over-smoothing issue is inevitable in very deep models. Further, the convergence speed is a more important factor that we should consider in practice. Mathematically, according to Eq.(7), the convergence speed depends on the other eigenvalues except 1 of the propagation matrix, especially the second largest eigenvalue. Intuitively, the propagation matrix is determined by the topology information of the corresponding graph. This might be the reason for our observation in Section 3.2 that a sparsely connected graph suffers from the over-smoothing only when extremely deep models are applied. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEP ADAPTIVE GRAPH NEURAL NETWORK</head><p>In this section, we propose Deep Adaptive Graph Neural Network (DAGNN) based on the above insights. Our DAGNN contributes two prominent advantages. First, it decouples the representation transformation from propagation so that large receptive fields can be applied without suffering from performance degradation, which has been verified in Section 3.2. Second, it utilizes an adaptive adjustment mechanism that can adaptively balance the information from local and global neighborhoods for each node, thus leading to more discriminative node representations. The mathematical expression of DAGNN is defined as</p><formula xml:id="formula_12">Z = MLP (X ) ∈ R n×c H ℓ = A ℓ Z , ℓ = 1, 2, · · · , k ∈ R n×c H = stack (Z , H 1 , · · · , H k ) ∈ R n×(k +1)×c S = σ (Hs) ∈ R n×(k +1)×1 S = reshape (S) ∈ R n×1×(k +1) X out = softmax squeeze SH ∈ R n×c ,<label>(8)</label></formula><p>where c is the number of node classes. Z ∈ R n×c is the feature matrix derived by applying an MLP network to the original feature matrix. We utilize the symmetrical normalization propagation</p><formula xml:id="formula_13">mechanism A = D − 1 2 A D − 1 2 , where A = A + I .</formula><p>k is a hyperparameter that indicates the depth of the model. s ∈ R c×1 is a trainable projection vector. σ (·) is an activation function and we apply sigmoid. stack, reshape and squeeze are utilized to rearrange the data dimension so that dimension can be matched during computation.</p><p>An illustration of our proposed DAGNN is provided in <ref type="figure" target="#fig_5">Figure 5</ref>. There are three main steps in DAGNN: transformation, propagation and adaptive adjustment. We first utilize a shared MLP network for feature transformation. Theoretically, MLP can approximate any measurable function <ref type="bibr" target="#b10">[10]</ref>. Obviously, Z only contains the information of individual nodes themselves with no structure information included. After transformation, a propagation mechanism A is applied to gather neighborhood information. H ℓ denotes the representations obtained by propagating information from nodes that are ℓ-hop away, hence H ℓ captures the information from the subtree of height ℓ rooted at individual nodes. As the depth ℓ increase, more global information is included in H ℓ because the corresponding subtree is deeper. However, it is difficult to determine a suitable ℓ. Small ℓ may fail to capture sufficient and essential neighborhood information, while large ℓ may bring too much global information and dilute the special local information. Furthermore, each node has a different subtree structure rooted at this node and the most suitable receptive field for each node should be different. To this end, we include an adaptive adjustment mechanism after the propagation. We utilize a trainable projection vector s that is shared by all nodes to generate retainment scores. These scores are used for representations that carry information from various range of neighborhood. These retainment scores measure how much information of the corresponding representations derived by different propagation layers should be retained to generate the final representation for each node. Utilizing this adaptive adjustment mechanism, DAGNN can adaptively balance the information from local and global neighborhoods for each node. Obviously, the transformation process and the adaptive adjustment process have trainable parameters and there is no trainable parameter in the propagation process, leading to a parameter-efficient model. Note that DAGNN is trained end-to-end, which means that these three steps are considered together when optimizing the network.</p><p>Decoupling representation transformation from propagation and utilizing the learnable retainment score to adaptively adjust the information from local and global neighborhoods make DAGNN have the ability to generate suitable representations for specific nodes from large and adaptive receptive fields. Besides, removing the entanglement of representation transformation and propagation, we can derive a large neighborhood without introducing more trainable parameters. Also, transforming representations to a low dimensional space at an early stage makes DAGNN computationally   </p><formula xml:id="formula_14">L = − i ∈V L c p=1 Y [i,p] ln X out [i,p] ,<label>(9)</label></formula><p>where V L is the set of labeled nodes and Y ∈ R n×c is the label indicator matrix. c is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL STUDIES</head><p>In this section, we conduct extensive experiments on node classification tasks to evaluate the superiority of our proposed DAGNN. We begin by introducing datasets and experimental setup we utilized. We then compare DAGNN with prior state-of-the-art baselines to demonstrate the effectiveness of DAGNN. Also, we deploy some performance studies to further verify the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Setup</head><p>We conduct experiments on 7 datasets based on citation, co-authorship, or co-purchase graphs for semi-supervised node classification tasks; those are Cora <ref type="bibr" target="#b26">[26]</ref>, CiteSeer <ref type="bibr" target="#b26">[26]</ref>, PubMed <ref type="bibr" target="#b26">[26]</ref>, Coauthor CS <ref type="bibr" target="#b28">[28]</ref>, Coauthor Physics <ref type="bibr" target="#b28">[28]</ref>, Amazon Computers <ref type="bibr" target="#b28">[28]</ref>, and Amazon Photo <ref type="bibr" target="#b28">[28]</ref>. The statistics of these datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>. The detailed description of these datasets are provided in Appendix A.7. We implemented our proposed DAGNN and some necessary baselines using Pytorch <ref type="bibr" target="#b24">[24]</ref> and Pytorch Geometric <ref type="bibr" target="#b5">[5]</ref>, a library for deep learning on irregularly structured data built upon Pytorch. We consider the following baselines: Logistic Regression (LogReg), Multilayer Perceptron (MLP), Label Propagation (LabelProp) <ref type="bibr" target="#b1">[2]</ref>, Normalized Laplacian Label Propagation (LabelProp NL) <ref type="bibr" target="#b1">[2]</ref>, Cheb-Net <ref type="bibr" target="#b4">[4]</ref>, Graph Convolutional Network (GCN) <ref type="bibr" target="#b11">[11]</ref>, Graph Attention Network(GAT) <ref type="bibr" target="#b30">[30]</ref>, Mixture Model Network (MoNet) <ref type="bibr" target="#b21">[21]</ref>, Graph-SAGE <ref type="bibr" target="#b9">[9]</ref>, APPNP <ref type="bibr" target="#b12">[12]</ref>, and SGC <ref type="bibr" target="#b31">[31]</ref>. We aim to provide a rigorous and fair comparison between different models on each dataset by using the same dataset splits and training procedure. We tune hyperparameters for all models individually and some baselines even achieve better results than their original reports. For our DAGNN, we tune the following hyperparameters: (1) k ∈ {5, 10, 20}, (2) weight decay ∈ {0, 2e-2, 5e-3, 5e-4, 5e-5}, and (3) dropout rate ∈ {0.5, 0.8}. Our code is publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Results</head><p>The results on citation datasets are summarized in <ref type="table" target="#tab_1">Table 2</ref>. To ensure a fair comparison, we use 20 labeled nodes per class as the training set, 500 nodes as the validation set, and 1000 nodes as the test set for all models. For each model, we conduct 100 runs for the fixed training/validation/test split from <ref type="bibr" target="#b11">[11]</ref>, which is commonly used to evaluate performance by the community. Also, we conduct 100 runs for each model on randomly training/validation/test splits, where we additionally ensure uniform class distribution on the train split as <ref type="bibr" target="#b5">[5]</ref>. We compute the average test accuracy of 100 runs. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our DAGNN model performs better than the representative baselines by significant margins. Also, the fact that DAGNN achieves state-of-the-art performance on random splits demonstrates the strong robustness of DAGNN. Quantitatively, for the randomly split data, the improvements of DAGNN over GCN are 4.6%, 3.0%, and 3.0% on Cora, CiteSeer, and PubMed, respectively. The results on co-authorship and co-purchase datasets are summarized in <ref type="table" target="#tab_2">Table 3</ref>. We utilize 20 labeled nodes per class as the training set, 30 nodes per class as the validation set, and the rest as the test set. The results of baselines are obtained from <ref type="bibr" target="#b28">[28]</ref>. For DAGNN, we conduct 100 runs for randomly training/validation/test splits as <ref type="bibr" target="#b28">[28]</ref> to ensure a fair comparison with baselines. Our DAGNN model achieves better performance over the current state-of-the-art models by significant margins of 1.5%, 1.0%, 1.0%, and 0.6% on the Coauthor CS, Coauthor Physics, Amazon Computers, and Amazon Photo, respectively. Note that DAGNN reduces the error rate by 11% on average.</p><p>In summary, our DAGNN achieves superior performance on all these seven datasets, which significantly demonstrates the effectiveness of our proposed model. These results verify the superiority of learning node representations from large and adaptive receptive fields, which is achieved by decoupling transformation from propagation and utilizing an adaptive adjustment mechanism in DAGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training Set Sizes</head><p>The number of training samples is usually limited in the real world graph. Hence, it is necessary to explore how models perform with different training set sizes. To further demonstrate the advantage that our DAGNN is capable of capturing information from large and adaptive receptive fields, we conduct experiments with different training set sizes for several representative baselines. MLP only utilizes node features to learn representations. GCN and GAT include the structure information by propagation representations through edges, however, only limited receptive fields can be taken into consideration and it is shown in Section 3.2 that performance degrades when stacking multiple GCN layers to enable large receptive fields. APPNP, SGC, and our DAGNN all have the ability to deploy a large receptive field. Note that for a fair comparison, we set the depth in APPNP, SGC, and DAGNN as 10, where information in the 10-hop neighborhood can be included. For each model, we conduct 100 runs on randomly training/validation/test splits for every training set size on Cora dataset. The results are present in <ref type="table" target="#tab_3">Table 4</ref> where our improvements over GCN are also highlighted. Our DAGNN 7%. These considerable improvements are mainly attributed to the advantage that DAGNN can incorporate information from large receptive fields by removing the entanglement of representation transformation and propagation. Reachable large receptive fields are beneficial for propagating training signals to distant nodes, which is very essential when the number of training nodes is limited. APPNP and SGC also can gather information from a large neighborhood, but they perform not as good as DAGNN. This performance gap is mainly caused by the adaptive adjustment of DAGNN, which can adjust the information from different receptive fields for each node adaptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Model Depths</head><p>In order to investigate when over-smoothing happens in our DAGNN, we conduct experiments for DAGNN with different depths. For each dataset, we choose different hyperparameter k in DAGNN, which means the k-hop neighborhood is visible by each node, and conduct 100 runs for each setting. The results are illustrated in <ref type="figure" target="#fig_6">Figure 6</ref>. For citation and co-authorship datasets, very deep models with large numbers of propagation iterations can be applied with keeping stable or slightly decreasing performance, which can be attributed to the design that we decouple the transformation from propagation and utilize adaptive receptive fields to learn node representations in DAGNN. Note that performances on co-purchase datasets decrease obviously with the increment of depth. This should be resulted by their larger value of edge density than other datasets, as shown in <ref type="table" target="#tab_0">Table 1</ref>. Intuitively, when nodes are more densely connected, their representations will become indistinguishable by applying a smaller number of propagation iterations, which aligns with our assumption in Section 3.3 that further connection exists between the graph topology information and convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we consider the performance deterioration problem existed in current deep graph neural networks and develop new insights towards deeper graph neural networks. We first provide a systematical analysis on this issue and argue that the key factor that compromises the network performance is the entanglement of representation transformation and propagation. We propose to decouple these two operations and show that deep graph neural networks without this entanglement can leverage large receptive fields without suffering from performance deterioration. Further, we provide a theoretically analysis of the above strategy when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Utilizing our insights, DAGNN is proposed to conduct node representation learning with the ability to capture information from large and adaptive receptive fields. According to our comprehensive experiments, our DAGNN achieves a better performance than current state-of-the-art models by significant margins, especially when training samples are limited, which demonstrates its superiority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>In this section, we provide necessary information for reproducing our insights and experimental results. These include the quantitative results and qualitative visualization on more datasets that can further support our insights, the proofs of lemmas, and the detailed description of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Test Accuracy and Smoothness Metric Value of GCNs</head><p>Test accuracy and smoothness metric value of node representations with different numbers of GCN layers are shown in <ref type="figure" target="#fig_7">Figure 7</ref> for CiteSeer and PubMed. They have the same trends as we discussed in Section 3.2. Test accuracy and smoothness metric value of node representations with different numbers of layers adopted in models as Eq.(6) are shown in <ref type="figure" target="#fig_8">Figure 8</ref> for CiteSeer and PubMed. It is illustrated that after decoupling transformation from propagation, we can apply deeper models without suffering from performance degradation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Visualization of Representations Derived by GCNs</head><p>The t-SNE visualization of node representations derived by different numbers of GCN layers are shown in <ref type="figure" target="#fig_11">Figure 9</ref> and 10 for CiteSeer and PubMed, respectively. The node representations become indistinguishable when several layers are deployed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Visualization of Representations Derived by Models as Eq.(6)</head><p>The t-SNE visualization of node representations derived by model as Eq. <ref type="formula" target="#formula_6">(6)</ref> with different numbers of layers are shown in <ref type="figure" target="#fig_0">Figure 11</ref> and 12 for CiteSeer and PubMed, respectively. It is shown that after the entanglement of representation transformation and propagation is removed, the model with a large receptive field, such as 50-hop, still generating distinguishable node representations. The over-smoothing issue affects the distinguishability only when an extremely receptive field, like 200-hop, is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Proof for Lemma 3.3</head><p>Proof. If λ is an eigenvalue of A ⊕ with left eigenvector v l and right eigenvector v r , we have v l A ⊕ = λv l and A ⊕ v r = λv r , i.e. v l D −1 A = λv l and D −1 Av r = λv r . We right multiply the first eigenvalue equation with D  Proof. We first prove that A ⊕ and A ⊙ always have an eigenvalue 1 and all eigenvalues λ satisfy |λ| ≤ 1. We have A ⊕ e T = e T because each row of A ⊕ sums to 1. Therefore, 1 is an eigenvalue of A ⊕ . Suppose that there exists an eigenvalue λ that |λ| &gt; 1 with eigenvector v, then the length of the right side in A k ⊕ v = λ k v grows exponentially when k goes to infinity. This indicates that some entries of A k ⊕ shoulde be larger than 1. Nevertheless, all entries of A k ⊕ are positive and each row of A k ⊕ always sums to 1, hence no entry of A k ⊕ can be larger than 1, which leads to contradiction. From Lemma 3.3, A ⊕ and A ⊙ have the same eigenvalues. Therefore, A ⊕ and A ⊙ always have an eigenvalue 1 and all eigenvalues λ satisfy |λ| ≤ 1.</p><formula xml:id="formula_15">derive (v l D − 1 2 ) D − 1 2 A D − 1 2 = λ(v l D − 1 2 ) and D − 1 2 A D − 1 2 ( D 1 2 v r ) = λ( D 1 2 v r ).</formula><p>According to the Perron-Frobenius Theorem for Primitive Matrices <ref type="bibr" target="#b27">[27]</ref>, there exists an eigenvalue r for an n × n non-negative primitive matrix such that r &gt; |λ| for any eigenvalue λ r and the eigenvectors associated with r are unique. The property that the given graph is connected can guarantee that for ∀i, j: ∃k s.t.     to A ⊙ . We then compute the eigenvectors associated with eigenvalue 1. Obviously, e T is the right eigenvector of A ⊕ associated with eigenvalue 1. Next, assume v l is the left eigenvector of A ⊕ associated with eigenvalue 1 and thus v l D − 1 2 is the left eigenvector of A ⊙ associated with eigenvalue 1. We know A ⊙ is a symmetric matrix, whose left and right eigenvectors associated with the same eigenvalue are simply each other's transpose. Hence, we utilize v l D </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Datasets Description and Statistics</head><p>Citation datasets. Cora, CiteSeer and PubMed <ref type="bibr" target="#b26">[26]</ref> are representative citation network datasets where nodes and edges denote documents and their citation relationships, respectively. Node features are formed by bay-of-words representations for documents. Each node has a label indicating what field the corresponding document belongs to. Co-authorship datasets. Coauthor CS and Coauthor Physics <ref type="bibr" target="#b28">[28]</ref> are co-authorship graphs datasets. Nodes denote authors, which are connected by an edge if they co-authored a paper. Node features represent paper keywords for each author's papers. Each node has a label denoting the most active fields of study for the corresponding author.</p><p>Co-purchase datasets. Amazon Computers and Amazon Photo <ref type="bibr" target="#b28">[28]</ref> are segments of the Amazon co-purchase graph <ref type="bibr" target="#b20">[20]</ref> where nodes are goods and edges denote that two goods are frequently bought together. Node features are derived from bag-of-words representations for product reviews and class labels are given by the product category.</p><p>The datasets statistics are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>t-SNE visualization of node representations derived by different numbers of GCN layers on Cora. Colors represent node classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Test accuracy and smoothness metric value of node representations with different numbers of GCN layers on Cora. "Init" means the smoothness metric value of the original data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>t-SNE visualization of node representations derived by models as Eq.(6) with different numbers of layers on Cora. Colors represent node classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Test accuracy and smoothness metric value of node representations with different numbers of layers adopted in models as Eq.(6) on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 3 . 3 . 1 2 ∈ R 1×n and right eigenvector D 1 2 1 2 ∈ R 1×n and D 1 2</head><label>331111</label><figDesc>Given a graph G, λ is an eigenvalue of A ⊕ with left eigenvector v l ∈ R 1×n and right eigenvector v r ∈ R n×1 if and only if λ is an eigenvalue of A ⊙ with left eigenvector v l D − v r ∈ R n×1 . Lemma 3.4. Given a connected graph G, A ⊕ and A ⊙ always have an eigenvalue 1 with unique associated eigenvectors and all other eigenvalues λ satisfy |λ| &lt; 1. The left and right eigenvectors of A ⊕ associated with eigenvalue 1 are e D ∈ R 1×n and e T ∈ R n×1 , respectively. For A ⊙ , they are e D e T ∈ R n×1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>An illustration of the proposed Deep Adaptive Graph Neural Network (DAGNN) . For clarity, we show the pipeline to generate the prediction for one node. Notation letters are consistent with Eq.(8) but bold lowercase versions are applied to denote representation vectors. s is the projection vector that computes retainment scores for representations generating from various receptive fields. s 0 , s 1 , s 2 , and s k represent the retainment scores of z, h 1 , h 2 , and h k , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Results of DAGNN with different depths. achieves the best performance under different training set sizes. Notably, The superiority of our DAGNN can be demonstrated more obviously when fewer training nodes are used. The improvement of DAGNN over GCN increases greatly when the number of training nodes decreases. Extremely, only utilizing one training node per class, DAGNN achieves an overwhelming result over GCN by a significant margin of 23.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Test accuracy and smoothness metric value of node representations with different numbers of GCN layers on CiteSeer and PubMed. "Init" means the smoothness metric value of the original data.A.2 Test Accuracy and Smoothness MetricValue of Models as Eq.(6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Test accuracy and smoothness metric value of node representations with different numbers of layers adopted in models as Eq.(6) on CiterSeer and PubMed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>− 1 2</head><label>1</label><figDesc>and left multiply the second eigenvalue equation with D 1 2 , respectively. Then we can</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>A k ⊕ [i, j] &gt; 0. Furthermore, there must exsit some k that can make all entries of A k to be simultaneously positive because self-loops are included in the graph. Formally, ∃k: A k ⊕ [i, j] &gt; 0 for ∀i, j. Hence, A ⊕ is a non-negative primitive matrix. From the Perron-Frobenius Theorem for Primitive Matrices, A ⊕ always has an eigenvalue 1 with unique associated eigenvectors and all other eigenvalues λ satisfy |λ| &lt; 1. Based on Lemma 3.3, this property can be extended</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>t-SNE visualization of node representations derived by different numbers of GCN layers on CiteSeer. Colors represent node classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>t-SNE visualization of node representations derived by different numbers of GCN layers on PubMed. Colors represent node classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>t-SNE visualization of node representations derived by models as Eq.(6) with different numbers of layers on CiteSeer. Colors represent node classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>t-SNE visualization of node representations derived by models as Eq.(6) with different numbers of layers on PubMed. Colors represent node classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>− 1 2 = ( D 1 2</head><label>11</label><figDesc>e T ) T to obtain v l = e D. After deriving the eigenvectors of A ⊕ associated with eigenvalue 1, corresponding eigenvectors of A ⊙ can be computed by Lemma 3.3. □</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets. The edge density is computed by 2m n 2 . Note that for a fair comparison with other baselines, we only consider the largest connected component in co-purchase graphs as<ref type="bibr" target="#b28">[28]</ref>.Dataset #Classes #Nodes #Edges Edge Density #Features #Training Nodes #Validation Nodes #Test Nodes</figDesc><table><row><cell>Cora</cell><cell>7</cell><cell>2708</cell><cell>5278</cell><cell>0.0014</cell><cell>1433</cell><cell>20 per class</cell><cell>500</cell><cell>1000</cell></row><row><cell>CiteSeer</cell><cell>6</cell><cell>3327</cell><cell>4552</cell><cell>0.0008</cell><cell>3703</cell><cell>20 per class</cell><cell>500</cell><cell>1000</cell></row><row><cell>PubMed</cell><cell>3</cell><cell>19717</cell><cell>44324</cell><cell>0.0002</cell><cell>500</cell><cell>20 per class</cell><cell>500</cell><cell>1000</cell></row><row><cell>Coauthor CS</cell><cell>15</cell><cell>18333</cell><cell>81894</cell><cell>0.0005</cell><cell>6805</cell><cell>20 per class</cell><cell>30 per class</cell><cell>Rest nodes</cell></row><row><cell>Coauthor Physics</cell><cell>5</cell><cell>34493</cell><cell>247962</cell><cell>0.0004</cell><cell>8415</cell><cell>20 per class</cell><cell>30 per class</cell><cell>Rest nodes</cell></row><row><cell>Amazon Computers</cell><cell>10</cell><cell>13381</cell><cell>245778</cell><cell>0.0027</cell><cell>767</cell><cell>20 per class</cell><cell>30 per class</cell><cell>Rest nodes</cell></row><row><cell>Amazon Photo</cell><cell>8</cell><cell>7487</cell><cell>119043</cell><cell>0.0042</cell><cell>745</cell><cell>20 per class</cell><cell>30 per class</cell><cell>Rest nodes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on citation datasets with both fixed and random splits in terms of classification accuracy (in percent). ± 0.5 83.7 ± 1.4 73.3 ± 0.6 71.2 ± 1.4 80.5 ± 0.5 80.1 ± 1.7</figDesc><table><row><cell>Models</cell><cell>Fixed</cell><cell>Cora Random</cell><cell>CiteSeer Fixed Random</cell><cell>PubMed Fixed Random</cell></row><row><cell>MLP</cell><cell cols="4">61.6 ± 0.6 59.8 ± 2.4 61.0 ± 1.0 58.8 ± 2.2 74.2 ± 0.7 70.1 ± 2.4</cell></row><row><cell>ChebNet</cell><cell cols="4">80.5 ± 1.1 76.8 ± 2.5 69.6 ± 1.4 67.5 ± 2.0 78.1 ± 0.6 75.3 ± 2.5</cell></row><row><cell>GCN</cell><cell cols="4">81.3 ± 0.8 79.1 ± 1.8 71.1 ± 0.7 68.2 ± 1.6 78.8 ± 0.6 77.1 ± 2.7</cell></row><row><cell>GAT</cell><cell cols="4">83.1 ± 0.4 80.8 ± 1.6 70.8 ± 0.5 68.9 ± 1.7 79.1 ± 0.4 77.8 ± 2.1</cell></row><row><cell>APPNP</cell><cell cols="4">83.3 ± 0.5 81.9 ± 1.4 71.8 ± 0.4 69.8 ± 1.7 80.1 ± 0.2 79.5 ± 2.2</cell></row><row><cell>SGC</cell><cell cols="4">81.7 ± 0.1 80.4 ± 1.8 71.3 ± 0.2 68.7 ± 2.1 78.9 ± 0.1 76.8 ± 2.6</cell></row><row><cell cols="2">DAGNN (Ours) 84.4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on co-authorship and co-purchase datasets in terms of classification accuracy (in percent). Notably, there are no fully-connected layers utilized as a classifier at the end of this model. In our DAGNN, the final representations X out are used as the final prediction. Thus, the cross-entropy loss for all labeled examples can be calculated as</figDesc><table><row><cell>Models</cell><cell>Coauthor CS</cell><cell>Coauthor Physics</cell><cell cols="2">Amazon Computers</cell><cell>Amazon Photo</cell></row><row><cell>LogReg</cell><cell cols="2">86.4 ± 0.9 86.7 ± 1.5</cell><cell cols="2">64.1 ± 5.7</cell><cell>73.0 ± 6.5</cell></row><row><cell>MLP</cell><cell cols="2">88.3 ± 0.7 88.9 ± 1.1</cell><cell cols="2">44.9 ± 5.8</cell><cell>69.6 ± 3.8</cell></row><row><cell>LabelProp</cell><cell cols="2">73.6 ± 3.9 86.6 ± 2.0</cell><cell cols="2">70.8 ± 8.1</cell><cell>72.6 ± 11.1</cell></row><row><cell>LabelProp NL</cell><cell cols="2">76.7 ± 1.4 86.8 ± 1.4</cell><cell cols="2">75.0 ± 2.9</cell><cell>83.9 ± 2.7</cell></row><row><cell>GCN</cell><cell cols="2">91.1 ± 0.5 92.8 ± 1.0</cell><cell cols="2">82.6 ± 2.4</cell><cell>91.2 ± 1.2</cell></row><row><cell>GAT</cell><cell cols="2">90.5 ± 0.6 92.5 ± 0.9</cell><cell cols="3">78.0 ± 19.0 85.7 ± 20.3</cell></row><row><cell>MoNet</cell><cell cols="2">90.8 ± 0.6 92.5 ± 0.9</cell><cell cols="2">83.5 ± 2.2</cell><cell>91.2 ± 1.3</cell></row><row><cell>GraphSAGE-mean</cell><cell cols="2">91.3 ± 2.8 93.0 ± 0.8</cell><cell cols="2">82.4 ± 1.8</cell><cell>91.4 ± 1.3</cell></row><row><cell>GraphSAGE-maxpool</cell><cell cols="2">85.0 ± 1.1 90.3 ± 1.2</cell><cell>N/A</cell><cell></cell><cell>90.4 ± 1.3</cell></row><row><cell cols="3">GraphSAGE-meanpool 89.6 ± 0.9 92.6 ± 1.0</cell><cell cols="2">79.9 ± 2.3</cell><cell>90.7 ± 1.6</cell></row><row><cell>DAGNN (Ours)</cell><cell cols="2">92.8 ± 0.9 94.0 ± 0.6</cell><cell cols="2">84.5 ± 1.2</cell><cell>92.0 ± 0.8</cell></row><row><cell cols="4">efficient and memory-saving. In order to compute A</cell><cell cols="2">ℓ Z efficiently,</cell></row><row><cell cols="6">we choose to compute it sequentially from right to left with time</cell></row><row><cell cols="6">complexity O(n 2 c), which saves the computational cost compared</cell></row><row><cell cols="2">to O(n 3 ) of calculating A</cell><cell></cell><cell></cell><cell></cell></row></table><note>ℓ first.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results with different training set sizes on Cora in terms of classification accuracy (in percent). Results in brackets are the improvements of DAGNN over GCN.</figDesc><table><row><cell>#Training nodes per class</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>100</cell></row><row><cell>MLP</cell><cell>30.3</cell><cell>35.0</cell><cell>38.3</cell><cell>40.8</cell><cell>44.7</cell><cell>53.0</cell><cell>59.8</cell><cell>63.0</cell><cell>64.8</cell><cell>65.4</cell><cell>64.0</cell></row><row><cell>GCN</cell><cell>34.7</cell><cell>48.9</cell><cell>56.8</cell><cell>62.5</cell><cell>65.3</cell><cell>74.3</cell><cell>79.1</cell><cell>80.8</cell><cell>82.2</cell><cell>82.9</cell><cell>84.7</cell></row><row><cell>GAT</cell><cell>45.3</cell><cell>58.8</cell><cell>66.6</cell><cell>68.4</cell><cell>70.7</cell><cell>77.0</cell><cell>80.8</cell><cell>82.6</cell><cell>83.4</cell><cell>84.0</cell><cell>86.1</cell></row><row><cell>APPNP</cell><cell>44.7</cell><cell>58.7</cell><cell>66.3</cell><cell>71.2</cell><cell>74.1</cell><cell>79.0</cell><cell>81.9</cell><cell>83.2</cell><cell>83.8</cell><cell>84.3</cell><cell>85.4</cell></row><row><cell>SGC</cell><cell>43.7</cell><cell>59.2</cell><cell>67.2</cell><cell>70.4</cell><cell>71.5</cell><cell>77.5</cell><cell>80.4</cell><cell>81.3</cell><cell>81.9</cell><cell>82.1</cell><cell>83.6</cell></row><row><cell>DAGNN (Ours)</cell><cell cols="11">58.4(23.7↑) 67.7(18.8↑) 72.4(15.6↑) 75.5(13.0↑) 76.7(11.4↑) 80.8(6.5↑) 83.7(4.6↑) 84.5(3.7↑) 85.6(3.4↑) 86.0(3.1↑) 87.1(2.4↑)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Hence, λ is also an eigenvalue of A ⊙ with left eigenvector . From A ⊙ to A ⊕ , we can prove it in the same way.</figDesc><table><row><cell>v l D</cell><cell>− 1 2 and right eigenvector D</cell><cell>1 2 v r</cell></row></table><note>□ A.6 Proof for Lemma 3.4</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/divelab/DeeperGNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by National Science Foundation grants DBI-1922969, IIS-1908166, and IIS-1908198.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Multi-Scale Approach for Graph Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Four AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semi-supervised learning (chapelle, o</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<editor>. et al.</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Four AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph U-Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramana</forename><surname>Panqanamala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varaiya</surname></persName>
		</author>
		<title level="m">Stochastic systems: Estimation, identification, and adaptive control</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-Attention Graph Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14612</idno>
		<title level="m">Non-Local Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random walks on graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Paul erdos is eighty</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Combinatorics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with eigenpooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Stanford InfoLab</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems Autodiff Workshop</title>
		<meeting>Neural Information Processing Systems Autodiff Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Non-negative matrices and Markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Seneta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A signal processing approach to fair surface design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gabriel Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 22nd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="351" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">StructPool: Structured Graph Pooling via Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An endto-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
