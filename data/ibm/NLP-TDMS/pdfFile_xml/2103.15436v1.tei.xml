<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Zhu</surname></persName>
							<email>jiawen@mail.dlut.edu.cnwdice@dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
							<email>xyang@remarkholdings.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Peng Cheng Laboratory 3 Remark AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Correlation acts as a critical role in the tracking field, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion manner to consider the similarity between the template and the search region. However, the correlation operation itself is a local linear matching process, leading to lose semantic information and fall into local optimum easily, which may be the bottleneck of designing high-accuracy tracking algorithms. Is there any better feature fusion method than correlation? To address this issue, inspired by Transformer, this work presents a novel attention-based feature fusion network, which effectively combines the template and search region features solely using attention. Specifically, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. Finally, we present a Transformer tracking (named TransT) method based on the Siamese-like feature extraction backbone, the designed attention-based fusion mechanism, and the classification and regression head. Experiments show that our TransT achieves very promising results on six challenging datasets, especially on largescale LaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively 50 f ps on GPU. Code and models are available at https://github.com/chenxindlut/TransT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual object tracking is a fundamental task in computer vision, which aims to predict the position and shape of a given target in each video frame. It has a wide range of applications in robot vision, video surveillance, unmanned driving, and other fields. The main challenges of tracking are large occlusion, severe deformation, interference from * Equal contribution † Corresponding author: Dr. Dong Wang, wdice@dlut.edu.cn <ref type="bibr">Self-attention</ref> Screen Shots Cross-attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth Ours</head><p>Ocean-online DiMP50 <ref type="figure">Figure 1</ref>. Tracking results of TransT and two state-of-the-art trackers. Our tracker is more robust and accurate in handling various challenges, such as occlusion, similar object interference, motion blur. similar objects, to name a few. Many efforts have been done in recent years <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b29">28]</ref>, however, designing a high-accuracy and real-time tracker is still a challenging task. For most of the popular trackers (such as SiamFC <ref type="bibr" target="#b2">[1]</ref>, SiamRPN <ref type="bibr" target="#b23">[22]</ref>, and ATOM <ref type="bibr" target="#b10">[9]</ref>), correlation plays a critical role in integrating the template or target information into the regions of interest (ROI). However, the correlation operation itself is a linear matching process and leads to semantic information loss, which limits the tracker to capture the complicated non-linear interaction between the template and ROIs. Thus, previous models have to improve the nonlinear representation ability by introducing fashion structures <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b50">48]</ref>, using additional modules <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b14">13]</ref>, designing effective online updaters <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b11">10]</ref>, to name a few. This naturally introduces an interesting question: is there any better feature fusion method than correlation?</p><p>In this work, inspired by the core idea of Transformer <ref type="bibr" target="#b39">[38]</ref>, we address the aforementioned problem by designing an attention-based feature fusion network and propose a novel Transformer tracking algorithm (named TransT). The proposed feature fusion network consists of an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. This fusion mechanism effectively integrates the template and ROI features, producing more semantic feature maps than correlation. <ref type="figure">Figure 1</ref> provides some representative visual results, illustrating that our TransT method produces insightful attention maps regarding the target and performs better than other competing trackers. Our main contributions are summarized as follows.</p><p>• We propose a novel Transformer tracking framework, consisting of feature extraction, Transformer-like fusion, and head prediction modules. The Transformerlike fusion combines the template and search region features solely using attention, without correlation. • We develop our feature fusion network based on an ego-context augment module with self-attention as well as a cross-feature augment module with crossattention. Compared with correlation-based feature fusion, our attention-based method adaptively focuses on useful information, such as edges and similar targets, and establishes associations between distant features, to make the tracker obtain better classification and regression results. • Numerous experimental results on many benchmarks show that the proposed tracker performs significantly better than the state-of-the-art algorithms, especially on large-scale LaSOT, TrackingNet, GOT-10k datasets. Besides, our tracker runs at about 50 f ps in GPU, which meets the real-time requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Object Tracking. In recent years, Siamese-based methods have been more popular in the tracking field <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b2">1,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b50">48]</ref>. SiamFC <ref type="bibr" target="#b2">[1]</ref>, the pioneering work, combines naive feature correlation with Siamese framework. After that, SiamRPN <ref type="bibr" target="#b23">[22]</ref> combines the Siamese network with RPN <ref type="bibr" target="#b34">[33]</ref> and conducts feature fusion using depthwise correlation, to obtain more precise tracking results. Some further improvements have been made, such as adding additional branches <ref type="bibr" target="#b44">[42,</ref><ref type="bibr" target="#b47">45]</ref>, using deeper architectures <ref type="bibr" target="#b22">[21]</ref>, exploiting anchor-free architectures <ref type="bibr" target="#b46">[44,</ref><ref type="bibr" target="#b50">48]</ref>, and so on. These mainstream tracking architectures can be divided into two parts: a backbone network to extract image features, followed by a correlation-based network to compute the similarity between the template and the search region. Moreover, some popular online trackers (e.g., ECO <ref type="bibr" target="#b9">[8]</ref>, ATOM <ref type="bibr" target="#b10">[9]</ref>, and DiMP <ref type="bibr" target="#b3">[2]</ref>) also heavily rely on the correlation operation. However, two issues have been overlooked. First, the correlation-based network does not make full use of global context, so it is easy to fall into the local optimum. Second, through correlation, the semantic information has been lost to some degree, which may lead to an imprecise prediction regarding the target's boundaries. Therefore, in this work, we design a variant structure of Transformer based on attention to replace the correlation-based network for conducting feature fusion.</p><p>Transformer and Attention. Transformer <ref type="bibr" target="#b39">[38]</ref> was first introduced by Vaswani et al. and applied in machine translation. Briefly, Transformer is an architecture for transforming one sequence into another one with the help of attention-based encoders and decoders. The attention mechanism looks at an input sequence and decides at each step which other parts of the sequence are important, and therefore facilitates capturing the global information from the input sequence. Transformer has replaced recurrent neural networks in many sequential tasks (natural language processing <ref type="bibr" target="#b12">[11]</ref>, speech processing <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b37">36]</ref>, and computer vision <ref type="bibr" target="#b33">[32]</ref>), and gradually extended to handle non-sequential problems <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b5">4]</ref>. In <ref type="bibr" target="#b5">[4]</ref>, Carion et al. considers object detection as a set prediction problem and adopts the encoderdecoder architecture in <ref type="bibr" target="#b39">[38]</ref> as the detection head. Experiments on COCO <ref type="bibr" target="#b25">[24]</ref> demonstrate that the DETR approach achieves comparable results to an optimized Faster R-CNN baseline <ref type="bibr" target="#b34">[33]</ref>. Motivated by the success of DETR as well as the close relationship between detection and tracking (like RPN <ref type="bibr" target="#b34">[33]</ref> and SiamRPN <ref type="bibr" target="#b23">[22]</ref>), we attempt to introduce Transformer into the tracking field. Different from DETR, we do not directly follow the encoder-decoder architecture in the original Transformer as it is not very matched with the tracking task. We adopt the core idea of Transformer and exploit the attention mechanism to design the ego-context augment (ECA) and cross-feature augment (CFA) modules. The integration of ECA and CFA focuses on feature fusion between template and search region, rather than extracting information from only one image in <ref type="bibr" target="#b5">[4]</ref>. This design philosophy is more suitable for visual object tracking.</p><p>Several efforts have been made to introduce the attention mechanism in the tracking field. ACF <ref type="bibr" target="#b7">[6]</ref> learns an attention network to do switching among different correlation filters. MLT <ref type="bibr" target="#b8">[7]</ref> adopts channel-wise attention to provide the matching network with target-specific information. These two works merely borrow the concept of attention to conduct model or feature selection. For improving the tracking performance, different attention layers (such as channel-wise attention <ref type="bibr" target="#b43">[41,</ref><ref type="bibr" target="#b18">17]</ref>, spatial-temporal attention <ref type="bibr" target="#b52">[50]</ref>, and residual attention <ref type="bibr" target="#b43">[41]</ref>) are utilized to enhance the template information within the correlation matching framework. SiamAttn <ref type="bibr" target="#b48">[46]</ref> explores both self-attention and cross branch attention to improve the discriminative ability of target features before applying depth-wise cross correlation. CGACD <ref type="bibr" target="#b14">[13]</ref> learns attention from the correlation result of the template and search region, and then adopts the learned attention to enhance the search region features for further classification and regression. These works have improved the tracking accuracy with the attention mechanism, but they still highly rely on the correlation operation in fusing the template and search region features. In this work, we exploit the core idea of Transformer and design a new attention-based network to directly fuse template and search region features without using any correlation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transformer Tracking</head><p>This section presents the proposed Transformer Tracking method, named TransT. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, our TransT is very concise, consisting of three components: backbone network, feature fusion network and prediction head. The backbone network separately extracts the features of the template and the search region. Then, the features are enhanced and fused by the proposed feature fusion network. Finally, the prediction head performs the binary classification and bounding box regression on the enhanced features to generate the tracking results 1 . We introduce the details of each component of our TransT, introduce the two important modules in the feature fusion network, and then provide some illustrations and discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>Feature Extraction. Like Siamese-based trackers <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b23">22]</ref>, the proposed TransT method takes a pair of image patches (i.e., the template image patch z ∈ R 3×Hz0×Wz0 and the search region image patch x ∈ R 3×Hx0×Wx0 ) as the inputs of the backbone network. The template patch is expanded by twice the side length from the center of the tar-get in the first frame of a video sequence, which includes the appearance information of the target and its local surrounding scene. The search region patch is expanded four times the side length from the center coordinate of the target in the previous frame, and the search region typically covers the possible moving range of the target. Search region and template are reshaped to squares, then be processed by the backbone. We use a modified version of ResNet50 <ref type="bibr" target="#b19">[18]</ref> for feature extraction. Specifically, we remove the last stage of ResNet50 and take the outputs of the fourth stage as final outputs. We also change the convolution stride of the down-sampling unit of the fourth stage from 2 to 1, to obtain a larger feature resolution. Besides, we modify the 3×3 convolution in the fourth stage to dilation convolution with stride of 2 to increase the receptive field. The backbone processes the search region and the template to obtain their features maps f z ∈ R C×Hz×Wz and f</p><formula xml:id="formula_0">x ∈ R C×Hx×Wx . H z , W z = Hz0 8 , Wz0 8 , H x , W x = Hx0 8 , Wx0 8 and C = 1024. Feature Fusion Network.</formula><p>We design a feature fusion network to effectively fuse the features f z and f x . First, a 1 × 1 convolution reduces the channel dimension of f z and f x , obtaining two lower dimension feature maps, f z0 ∈ R d×Hz×Wz and f x0 ∈ R d×Hx×Wx . We employ d = 256 in our implementation. Since the attention-based feature fusion network takes a set of feature vectors as input, we flatten f z0 and f x0 in spatial dimension, obtaining f z1 ∈ R d×HzWz and f x1 ∈ R d×HxWx . Both f z1 and f x1 can be regarded as a set of feature vectors of length d. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the feature fusion network takes f z1 and f x1 as the inputs to the template branch and the search region branch respectively. First, two ego-context augment (ECA) modules focus on the useful semantic context adaptively by multi-head self-attention, to enhance the feature representation. Then, two cross-feature augment (CFA) modules receive the feature maps of their own and the other branch at the same time and fuse these two feature maps through multi-head cross-attention. In this way, two ECAs and two CFAs form a fusion layer, as shown in the dotted box in <ref type="figure" target="#fig_0">Figure 2</ref>. The fusion layer repeats N times, followed by an additional CFA to fuse the feature map of two branches, decoding a feature map f ∈ R d×HxWx (we employ N = 4 in this work). The details of ECA and CFA modules are introduced in Section 3.2. Prediction Head Network. The prediction head consists of a classification branch and a regression branch, where each branch is a three-layer perceptron with hidden dimension d and a ReLU activation function. For the feature map f ∈ R d×HxWx generated by the feature fusion network, the head makes predictions on each vector to get H x W x foreground/background classification results, and H x W x normalized coordinates with respect to the search region size. Our tracker directly predicts the normalized coordinates instead of adjusting the anchor points or anchor boxes, completely discarding the anchor points or anchor boxes based on prior knowledge, thereby making the tracking framework more concise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ego-Context Augment and Cross-Feature Augment Modules</head><p>Multi-head Attention. Attention is the fundamental component in designing our feature fusion network. Given queries Q, keys K and values V, the attention function is the scale dot-product attention, defined in equation <ref type="formula" target="#formula_1">(1)</ref>.</p><formula xml:id="formula_1">Attention(Q, K, V) = softmax( QK √ d k )V,<label>(1)</label></formula><p>where d k is the key dimensionality. As described in <ref type="bibr" target="#b39">[38]</ref>, extending the attention mechanism (1) into multiple heads enable the mechanism to consider various attention distributions and make the model pay attention to different aspects of information. The mechanism of multi-head attention is defined in equation <ref type="bibr" target="#b3">(2)</ref>. We refer the reader to the literature <ref type="bibr" target="#b39">[38]</ref> for more detailed descriptions.</p><formula xml:id="formula_2">MultiHead(Q, K, V) = Concat(H 1 , ..., H n h )W O ,<label>(2)</label></formula><formula xml:id="formula_3">H i = Attention(QW Q i , KW K i , VW V i ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">W Q i ∈ R dm×d k , W K i ∈ R dm×d k , W V i ∈ R dm×dv , and W O ∈ R n h dv×dm are parameter matri- ces.</formula><p>In this work, we employ n h = 8, d m = 256 and d k = d v = d m /n h = 32 as default values. Ego-Context Augment (ECA). The structure of ECA is shown in the left of <ref type="figure" target="#fig_2">Figure 3</ref>. ECA adaptively integrates the information from different positions of the feature map, by using multi-head self-attention in the form of residual. As shown in equation <ref type="formula" target="#formula_1">(1)</ref>, the attention mechanism has no ability to distinguish the position information of the input feature sequence. Thus, we introduce a spatial positional  encoding process to the input X ∈ R d×Nx . Following <ref type="bibr" target="#b5">[4]</ref>, we use a sine function to generate spatial positional encoding. Finally, the mechanism of ECA can be summarized as</p><formula xml:id="formula_5">X EC = X + MultiHead(X + P x , X + P x , X),<label>(4)</label></formula><p>where P x ∈ R d×Nx is the spatial positional encodings and X EC ∈ R d×Nx is the output of ECA. Cross-Feature Augment (CFA). The structure of CFA is shown in the right of <ref type="figure" target="#fig_2">Figure 3</ref> . CFA fuses the feature vectors from two inputs by using multi-head cross-attention in the form of residual. Similar to ECA, spatial positional encoding is also used in CFA. In addition, a FFN module is used to enhance the fitting ability of the model, which is a fully connected feed-forward network that consists of two linear transformation with a ReLU in between, that is,</p><formula xml:id="formula_6">FFN (x) = max (0, xW 1 + b 1 ) W 2 + b 2 ,<label>(5)</label></formula><p>the symbols W and b stand for weight matrices and basis vectors, respectively. The subscripts denote different layers. Thus, the mechanism of CFA can be summarized as</p><formula xml:id="formula_7">X CF = X CF + FFN X CF , X CF = X q + MultiHead (X q + P q , X kv + P kv , X kv ) ,<label>(6)</label></formula><p>where X q ∈ R d×Nq is the input of the branch where the module is applied, P q ∈ R d×Nq is the spatial positional encoding corresponding to X q . X kv ∈ R d×N kv is the input  from another branch, and P kv ∈ R d×N kv is the spatial encoding for the coordinate of X kv . X CF ∈ R d×Nq is the output of CFA. According to equation <ref type="formula" target="#formula_7">(6)</ref>, CFA calculates the attention map according to multiple scaled products between X kv and X q , then reweighs X kv according to the attention map, and adds it to X q to enhance the representation ability of the feature map.</p><p>Differences with the original Transformer. Our method draws on the core idea of Transformer, i.e., employing the attention mechanism. But we do not directly adopt the structure of the Transformer in DETR <ref type="bibr" target="#b5">[4]</ref>. Instead, we design a new structure to make it more suitable for tracking framework. The cross-attention operation in our method plays a more important role than that in DETR, since the tracking task focuses on fusing the template and search region features. Experimental comparisons of the trackers with our method and the original Transformer are shown in Section 4.3.</p><p>What does attention want to see? To explore how the attention module works in our framework, we visualized the attention maps of all attention modules in a representative tracking clip, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>, to see what the attention wants to see. We use the number n (1≤n≤4) to represent the current number of the fusion layer. There are four layers in total, and the fusion layer goes deeper from left to right. The last single attention map is obtained from the last crossattention, which is used for decoding. The first line shows self-attention maps of the search region. When n = 1, there is no information from the template, the attention module tries to see all objects that are different from the environment. The same thing happens in the second line, i.e., self-attention map of template. Interestingly, attention focuses more on key information, such as the red dot on the tail of the ant. The third and fourth lines are cross-attention maps applied to the search region and template respectively. At this point, attention modules receive features from both template and search region. To locate the target under the interference of similar targets, attention modules tend to pay attention to the important information, i.e., the colored points on the tail of ants. When n = 2, at this point, the inputs of every attention module have fused the search region and template information. The focus of the search region self-attention map on similar distractors has been reduced, the model appears to have recognized the target. The cross-attention map to the search region seems quite sure of its estimation. For the template, attention modules begin to focus on boundary information.</p><p>As the fusion layers go deeper, the search region selfattention map tends to strengthen the location of the target, while the cross-attention map to the search region focuses on the boundary of the identified target. In this way, the template feature becomes an information bank that contains a large amount of the target's boundary information, while the search region feature still keeps its spatial information. We notice that the last few attention maps for the template no longer follow the initial spatial position, but a puzzling distribution. Perhaps this is because, after the target has been identified, the features of the template branch no longer need to keep the information of the template itself, but store a lot of the target's boundary information, becoming a feature library serving for regression. Through the visualization of the attention maps, we can see that the attention modules automatically look for global useful information, thereby making the tracker achieve good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Loss</head><p>The prediction head receives H x × W x feature vectors, and outputs H x × W x binary classification and regression results. We select the prediction of feature vectors corresponding to pixels in the ground-truth bounding box as positive samples, the rest are negative samples. All samples contribute to the classification loss, while only positive samples contribute to the regression loss. In order to reduce the imbalance between positive and negative samples, we down-weigh the loss produced by negative samples by a factor 16. We employ the standard binary cross-entropy loss for classification, which is defined as</p><formula xml:id="formula_8">L cls = − j [y j log(p j ) + (1 − y j )log(1 − p j )],<label>(7)</label></formula><p>where y j denotes the ground-truth label of the j-th sample, y j = 1 denotes foreground, and p j denotes the probability belong to the foreground predicted by the learned model. For regression, we employ a linear combination of 1 -norm loss L 1 (., .) and the generalized IoU loss L GIoU (., .) <ref type="bibr" target="#b35">[34]</ref>. The regression loss can be formulated as</p><formula xml:id="formula_9">L reg = j 1 {yj =1} [λ G L GIoU (b j ,b) + λ 1 L 1 (b j ,b)] ,<label>(8)</label></formula><p>where y j = 1 denotes the positive sample, b j denotes the j-th predicted bounding box, andb denotes the normalized ground-truth bounding box. λ G = 2 and λ 1 = 5 are the regularization parameters in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Offline Training. We train our model on the training splits of COCO <ref type="bibr" target="#b25">[24]</ref>, TrackingNet <ref type="bibr" target="#b31">[30]</ref>, LaSOT <ref type="bibr" target="#b15">[14]</ref>, and GOT-10k <ref type="bibr" target="#b20">[19]</ref> datasets. For the video datasets (TrackingNet, La-SOT, and GOT-10k), we directly sample the image pairs from one video sequence to collect training samples. For COCO detection datasets, we apply some transformations on the original image to generate image pairs. The common data augmentation (such as translation and brightness jitter) is applied to enlarge the training set. The sizes of search region patch and template patch are 256 × 256 and 128 × 128, respectively. The backbone parameters are initialized with ImageNet-pretrained <ref type="bibr" target="#b36">[35]</ref> ResNet-50 <ref type="bibr" target="#b19">[18]</ref>, other parameters of our model are initialized with Xavier init <ref type="bibr" target="#b16">[15]</ref>. We train the model with AdamW <ref type="bibr" target="#b26">[25]</ref>, setting backbone's learning rate to 1e-5, other parameters' learning rate to 1e-4, and weight decay to 1e-4. We train the network on two Nvidia Titan RTX GPUs with the batch size of 38, for a total of 1000 epochs with 1000 iterations per epoch. The learning rate decreases by factor 10 after 500 epochs. Online Tracking. In online tracking, the prediction head outputs 1024 boxes with their confidence scores, and then the window penalty is adopted for post-processing these scores. Specifically, the Hanning window with the shape of 32 × 32 is applied to scores, weighted by a parameter w (chosen as 0.49 in this work). The final score score w can be defined as</p><formula xml:id="formula_10">score w = (1 − w) × score + w × score h ,<label>(9)</label></formula><p>where score is the original score of the tracker's output. score h is the value of the corresponding position on the Hanning window. Based on the window penalty, the confidence of feature points far from the target in the previous frames will be punished. Finally, we select the box with the highest confidence score as the tracking result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on TrackingNet, LaSOT and GOT-10k Datasets</head><p>In this subsection, we compare our TransT method with twelve state-of-the-art trackers published in 2020 (SiamR-CNN <ref type="bibr" target="#b40">[39]</ref>, Ocean <ref type="bibr" target="#b50">[48]</ref>, KYS <ref type="bibr" target="#b4">[3]</ref>, DCFST <ref type="bibr" target="#b51">[49]</ref>, SiamFC++ <ref type="bibr" target="#b46">[44]</ref>, PrDiMP <ref type="bibr" target="#b11">[10]</ref>, CGACD <ref type="bibr" target="#b14">[13]</ref>, Sia-mAttn <ref type="bibr" target="#b48">[46]</ref>, MAML <ref type="bibr" target="#b42">[40]</ref>, D3S <ref type="bibr" target="#b27">[26]</ref>, SiamCAR <ref type="bibr" target="#b17">[16]</ref>, and SiamBAN <ref type="bibr" target="#b6">[5]</ref>) and six representative trackers presented before (DiMP <ref type="bibr" target="#b3">[2]</ref>, SiamPRN++ <ref type="bibr" target="#b22">[21]</ref>, ATOM <ref type="bibr" target="#b10">[9]</ref>, ECO <ref type="bibr" target="#b9">[8]</ref>, MDNet <ref type="bibr" target="#b32">[31]</ref> and SiamFC [1]) 2 . We report the detailed comparison results on the large-scale LaSOT <ref type="bibr" target="#b15">[14]</ref>, Track-ingNet <ref type="bibr" target="#b31">[30]</ref>, and GOT-10k <ref type="bibr" target="#b20">[19]</ref> datasets in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>LaSOT. LaSOT <ref type="bibr" target="#b15">[14]</ref> is a recent large-scale dataset with high-quality annotations, which contains 1400 challenging videos: 1120 for training and 280 for testing. We follow the one-pass evaluation (Success and Precision) to compare different tracking algorithms on the LaSOT test set. Then, we report the Success (AUC) and Precision (P and P Norms ) scores in <ref type="table" target="#tab_0">Table 1</ref>. This table shows that the proposed method obtains the best performance, better than other trackers by a significant margin except SiamR-CNN <ref type="bibr" target="#b40">[39]</ref>, but SiamR-CNN merely runs less than 5fps in our machine, while our tracker runs at 50 f ps. <ref type="figure" target="#fig_5">Figure 5</ref> reports an attribute-based evaluation of representative stateof-the-art algorithms, illustrating that the TransT performs much better than other competing trackers on all attributes. TrackingNet. TrackingNet <ref type="bibr" target="#b31">[30]</ref> is a large-scale tracking dataset, which covers diverse object classes and scenes. Its test set contains 511 sequences publicly available groundtruth. We submit our tracker's outputs to the official online evaluation server, and report the Success (AUC) and Precision (P and P N orm ) results in <ref type="table" target="#tab_0">Table 1</ref>. our TransT obtains 81.4%, 86.7% and 80.3% in terms of AUC, P Norms and P respectively, surpassing all previous methods. GOT-10k. The GOT-10k <ref type="bibr" target="#b20">[19]</ref> dataset contains 10k sequences for training and 180 for testing. We follow the defined protocol presented in <ref type="bibr" target="#b20">[19]</ref>, and submit the tracking <ref type="bibr" target="#b3">2</ref> Many trackers have different variants, such as DiMP50 and DiMP18, in the original paper. For fair comparison, We simply select the variant with highest performance. For example, DiMP means DiMP50 (DiMP with the ResNet50 backbone) in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="table">Table 2</ref>. Ablation study on TrackingNet, LaSOT, and GOT-10k. The best results are shown in the red font.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>LaSOT <ref type="bibr" target="#b15">[14]</ref> TrackingNet <ref type="bibr" target="#b31">[30]</ref> GOT-10k <ref type="bibr">[</ref> outputs to the official evaluation server. Then, we report the obtained results (i.e., AO and SR T ) in <ref type="table" target="#tab_0">Table 1</ref>. TransT-GOT denotes training with only the GOT-10k traning set.</p><p>TransT and TransT-GOT method achieve the best performance. TransT-GOT method performs 2.2% higher than SiamR-CNN in the main AO metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study and Analysis</head><p>Post-processing. In the prior work such as SiamRPN <ref type="bibr" target="#b23">[22]</ref>, SiamRPN++ <ref type="bibr" target="#b22">[21]</ref> and Ocean <ref type="bibr" target="#b50">[48]</ref>, the final tracking results are selected by post-processing schemes including cosine window penalty, scale change penalty and bounding box smoothing. However, these post-processing schemes are parameter-sensitive, since three hyparameters that need to be adjusted carefully for different test sets. To avoid this problem, in this work, we merely adopt the window penalty to conduct post-processing using the default parameter for all test sets.</p><p>To show the effect of post-processing, we compare the TransT variants with and without the post-processing step in <ref type="table">Table 2</ref>. TransT denotes our tracker and TransT-np is our tracker without post-processing. First, from <ref type="table">Table 2</ref>, we can conclude that our TransT without post-processing still achieves state-of-the-art performance, being attributed to the Transformer-like fusion method. Second, the postprocessing step further improves the tracking accuracy, producing the best record among almost all metrics on these benchmarks. Comparison with the original Transformer. To show the superiority of our feature fusion network, we design a tracker using the original Transformer. Specifically, we replace the feature fusion network in <ref type="figure" target="#fig_0">Figure 2</ref> with the original Transformer structure and keep the other components unchanged. Because the size of the output of the Transformer is consistent with the size of the decoder input, we input the template feature to the encoder and the search region feature to the decoder. The training data and strategy are the same as our TransT in Section 3. The comparison results are shown in <ref type="table">Table 2</ref>. TransT(ori) denotes the tracker with the original Transformer and TransT(ori)-np is the TransT(ori) method without post-processing. First, the TransT(ori)-np variant achieves an AUC score of 60.9% on LaSOT, an AUC score of 80.9% on TrackingNet and an AO score of 68.6% on GOT-10k, which is also better than many state-of-the-art algorithms. This indicates that the Transformer structure works better than the simple correlation operation in dealing with feature fusion. Second, by observ- <ref type="table">Table 3</ref>. Comparison with correlation on TrackingNet, LaSOT, and GOT-10k. The best results are shown in the red font.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ECA CFA Correlation LaSOT <ref type="bibr" target="#b15">[14]</ref> TrackingNet <ref type="bibr" target="#b31">[30]</ref> GOT-10k <ref type="bibr" target="#b20">[19]</ref>  Prior Siamese trackers use cross correlation to compute similarity between template and search region. However, correlation is a linear local comparison, outputting a similarity map. This simple method leads to semantic loss and lacks global information. Compared with correlation-based methods, first, our attention-based method can establish long-distance feature associations, which effectively aggregates the global information of the template and search region. Second, our method outputs features with rich semantic information, not just a similarity map. We conduct experiments to compare CFA with correlation and explore the impact of ECA. To make a fair comparison, for the TransT without CFA, we keep the FFN in CFA unchanged, only remove the cross-attention layers, and replace the last CFA module with depth-wise correlation. The comparison results are shown in <ref type="table">Table 3</ref>. The comparison results show that after replacing CFA with correlation layer, the performance significantly decreases. Without ECA, the performance of tracker drops. Without both ECA and CFA, the performance further drops, and the impact of post-processing becomes greater. These results show that without attention modules, the localization ability of the tracker significantly decreases, and it needs to rely more on the prior information in post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Other Datasets</head><p>We evaluate our tracker on some commonly used smallscale datasets, including NFS <ref type="bibr" target="#b21">[20]</ref>, OTB2015 <ref type="bibr" target="#b45">[43]</ref>, and UAV123 <ref type="bibr" target="#b30">[29]</ref>. We also collect some state-of-the-art and baseline trackers for comparison. The results are shown in <ref type="table">Table 4</ref>. NFS. We evaluate the proposed tracker on the 30 fps version of the NFS <ref type="bibr" target="#b21">[20]</ref> dataset, which contains challenging videos with fast-moving objects. The previous best method, <ref type="bibr">Table 4</ref>. Comparison with state-of-the-art on the OTB100, NFS and UAV123 datasets in terms of overall AUC score. The best two results are shown in red and blue fonts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we propose a novel, simple, and highperformance tracking framework based on the Transformerlike feature fusion network. The proposed network conducts feature fusion solely using the attention mechanism, which includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. The attention mechanism establishes longdistance feature associations, making the tracker adaptively focus on useful information and extract abundant semantic information. The proposed fusion network could replace correlation to composite the template and search region features, thereby facilitating object localization and bounding box regression. Numerous experimental results on many benchmarks show that the proposed tracker performs significantly better than the state-of-the-art algorithms while running at a real-time speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of our Transformer tracking framework. This framework contains three fundamental components: feature extraction backbone, feature fusion network, and prediction head. The proposed attention-based feature fusion network is naturally applied on the Siamese-based feature extraction backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Left: ECA module. Right: CFA module. The ECA module is based on multi-head self-attention in a residual form. The CFA module is based on multi-head cross-attention and FFN in a residual form. The input Xq receives the feature from the branch where CFA is located, and X kv receives the feature from the other branch. Spatial positional encodings are used to encode position information. ECA enhances the contextual information of the input and CFA adaptively fuses the features from two branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of the attention maps for a representative pair. From left to right, the feature fusion layer goes deeper. From top to bottom, they are self-attention maps in the search region, self-attention maps in the template, cross-attention maps in the search region, and cross-attention maps in the template, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>AUC scores of different attributes on the LaSOT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>State-of-the-art comparison on TrackingNet, LaSOT, and GOT-10k. The best two results are shown in red and blue fonts.</figDesc><table><row><cell>Method</cell><cell>Source</cell><cell cols="2">LaSOT [14] AUC P N orm</cell><cell>P</cell><cell cols="3">TrackingNet [30] AUC P N orm P</cell><cell cols="3">GOT-10k [19] AO SR 0.5 SR 0.75</cell></row><row><cell>TransT</cell><cell>Ours</cell><cell>64.9</cell><cell>73.8</cell><cell cols="2">69.0 81.4</cell><cell>86.7</cell><cell cols="3">80.3 72.3 82.4</cell><cell>68.2</cell></row><row><cell>TransT-GOT</cell><cell>Ours</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">67.1 76.8</cell><cell>60.9</cell></row><row><cell>SiamR-CNN [39]</cell><cell>CVPR2020</cell><cell>64.8</cell><cell>72.2</cell><cell>-</cell><cell>81.2</cell><cell>85.4</cell><cell cols="3">80.0 64.9 72.8</cell><cell>59.7</cell></row><row><cell>Ocean [48]</cell><cell>ECCV2020</cell><cell>56.0</cell><cell>65.1</cell><cell>56.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">61.1 72.1</cell><cell>47.3</cell></row><row><cell>KYS [3]</cell><cell>ECCV2020</cell><cell>55.4</cell><cell>63.3</cell><cell>-</cell><cell>74.0</cell><cell>80.0</cell><cell cols="3">68.8 63.6 75.1</cell><cell>51.5</cell></row><row><cell>DCFST [49]</cell><cell>ECCV2020</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.2</cell><cell>80.9</cell><cell cols="3">70.0 63.8 75.3</cell><cell>49.8</cell></row><row><cell>SiamFC++ [44]</cell><cell>AAAI2020</cell><cell>54.4</cell><cell>62.3</cell><cell cols="2">54.7 75.4</cell><cell>80.0</cell><cell cols="3">70.5 59.5 69.5</cell><cell>47.9</cell></row><row><cell>PrDiMP [10]</cell><cell>CVPR2020</cell><cell>59.8</cell><cell>68.8</cell><cell cols="2">60.8 75.8</cell><cell>81.6</cell><cell cols="3">70.4 63.4 73.8</cell><cell>54.3</cell></row><row><cell>CGACD [13]</cell><cell>CVPR2020</cell><cell>51.8</cell><cell>62.6</cell><cell>-</cell><cell>71.1</cell><cell>80.0</cell><cell>69.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SiamAttn [46]</cell><cell>CVPR2020</cell><cell>56.0</cell><cell>64.8</cell><cell>-</cell><cell>75.2</cell><cell>81.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MAML [40]</cell><cell>CVPR2020</cell><cell>52.3</cell><cell>-</cell><cell>-</cell><cell>75.7</cell><cell>82.2</cell><cell>72.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>D3S [26]</cell><cell>CVPR2020</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.8</cell><cell>76.8</cell><cell cols="3">66.4 59.7 67.6</cell><cell>46.2</cell></row><row><cell>SiamCAR [16]</cell><cell>CVPR2020</cell><cell>50.7</cell><cell>60.0</cell><cell>51.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">56.9 67.0</cell><cell>41.5</cell></row><row><cell>SiamBAN [5]</cell><cell>CVPR2020</cell><cell>51.4</cell><cell>59.8</cell><cell>52.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DiMP [2]</cell><cell>ICCV2019</cell><cell>56.9</cell><cell>65.0</cell><cell cols="2">56.7 74.0</cell><cell>80.1</cell><cell cols="3">68.7 61.1 71.7</cell><cell>49.2</cell></row><row><cell>SiamPRN++ [21]</cell><cell>CVPR2019</cell><cell>49.6</cell><cell>56.9</cell><cell cols="2">49.1 73.3</cell><cell>80.0</cell><cell cols="3">69.4 51.7 61.6</cell><cell>32.5</cell></row><row><cell>ATOM [9]</cell><cell>CVPR2019</cell><cell>51.5</cell><cell>57.6</cell><cell cols="2">50.5 70.3</cell><cell>77.1</cell><cell cols="3">64.8 55.6 63.4</cell><cell>40.2</cell></row><row><cell>ECO [8]</cell><cell>ICCV2017</cell><cell>32.4</cell><cell>33.8</cell><cell cols="2">30.1 55.4</cell><cell>61.8</cell><cell cols="3">49.2 31.6 30.9</cell><cell>11.1</cell></row><row><cell>MDNet [31]</cell><cell>CVPR2016</cell><cell>39.7</cell><cell>46.0</cell><cell cols="2">37.3 60.6</cell><cell>70.5</cell><cell cols="3">56.5 29.9 30.3</cell><cell>9.9</cell></row><row><cell>SiamFC [1]</cell><cell cols="2">ECCVW2016 33.6</cell><cell>42.0</cell><cell cols="2">33.9 57.1</cell><cell>66.3</cell><cell cols="3">53.3 34.8 35.3</cell><cell>9.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The tracking results are also post-processed by the window penalty, which will be introduced in Section 4.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Our method performs better than PrDiMP with a gain of 2.2%. OTB2015. OTB2015 [43] contains 100 sequences in total and 11 challenge attributes. Table 4 shows that our method achieves comparable results with state-of-the-art algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prdimp</surname></persName>
		</author>
		<idno>achieves an AUC score of 63.5%</idno>
		<imprint/>
	</monogr>
	<note>such as PrDiMP and SiamRPN++</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">low altitude aerial videos captured from a UAV and adopts success and precision metrics for evaluation</title>
		<idno>UAV123. UAV123 [29] includes 123</idno>
		<imprint/>
	</monogr>
	<note>As shown in Table 4, the pro</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip H S</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Know Your Surroundings: Exploiting scene information for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Siamese box adaptive network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zedu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attentional correlation filter network for adaptive visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep meta learning for real-time target-aware visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ATOM: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Correlation-guided attention for corner detection based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LaSOT: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAIS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SiamCAR: Siamese fully convolutional classification and regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A twofold siamese network for real-time object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SiamRPN++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep visual tracking: Review and experimental comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">D3S -A discriminative single shot segmentation tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">RWTH ASR Systems for LibriSpeech: hybrid vs attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep learning for visual tracking: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Seyed Mojtaba Marvasti-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Ghanei-Yakhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kasaei</surname></persName>
		</author>
		<idno>abs/1912.00535</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for UAV tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TrackingNet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ImageNet Large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">End-toend ASR: from supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno>abs/1911.08460</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual tracking by redetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Siam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tracking by Instance Detection: A metalearning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Attentions: Residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SiamFC++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Alpha-refine: Boosting tracking performance by precise bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deformable siamese attention networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuechen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning the model update for siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad Shahbaz</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning feature embeddings for discriminant model based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end flow correlation tracking with spatial-temporal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
