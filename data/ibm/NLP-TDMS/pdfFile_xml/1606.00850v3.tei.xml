<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face Detection with End-to-End Integration of a ConvNet and a 3D Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="laboratory" key="lab1">Nat&apos;l Engineering Laboratory for Video Technology</orgName>
								<orgName type="laboratory" key="lab2">Key Laboratory of Machine Perception (MoE)</orgName>
								<orgName type="institution" key="instit1">Shanghai Sch&apos;l of EECS</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benyuan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="laboratory" key="lab1">Nat&apos;l Engineering Laboratory for Video Technology</orgName>
								<orgName type="laboratory" key="lab2">Key Laboratory of Machine Perception (MoE)</orgName>
								<orgName type="institution" key="instit1">Shanghai Sch&apos;l of EECS</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
							<email>tianfuwu@ncsu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of ECE and the Visual Narrative Cluster</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<settlement>Raleigh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<email>yizhou.wang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="laboratory" key="lab1">Nat&apos;l Engineering Laboratory for Video Technology</orgName>
								<orgName type="laboratory" key="lab2">Key Laboratory of Machine Perception (MoE)</orgName>
								<orgName type="institution" key="instit1">Shanghai Sch&apos;l of EECS</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Face Detection with End-to-End Integration of a ConvNet and a 3D Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face Detection</term>
					<term>Face 3D Model</term>
					<term>ConvNet</term>
					<term>Deep Learning</term>
					<term>Multi-task Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a method for face detection in the wild, which integrates a ConvNet and a 3D mean face model in an end-to-end multi-task discriminative learning framework. The 3D mean face model is predefined and fixed (e.g., we used the one provided in the AFLW dataset <ref type="bibr" target="#b19">[20]</ref>). The ConvNet consists of two components: (i) The face proposal component computes face bounding box proposals via estimating facial key-points and the 3D transformation (rotation and translation) parameters for each predicted key-point w.r.t. the 3D mean face model. (ii) The face verification component computes detection results by pruning and refining proposals based on facial key-points based configuration pooling. The proposed method addresses two issues in adapting stateof-the-art generic object detection ConvNets (e.g., faster R-CNN [32]) for face detection: (i) One is to eliminate the heuristic design of predefined anchor boxes in the region proposals network (RPN) by exploiting a 3D mean face model. (ii) The other is to replace the generic RoI (Region-of-Interest) pooling layer with a configuration pooling layer to respect underlying object structures. The multi-task loss consists of three terms: the classification Softmax loss and the location smooth l1-losses [14] of both the facial key-points and the face bounding boxes. In experiments, our ConvNet is trained on the AFLW dataset [20] only and tested on the FDDB benchmark [19] with fine-tuning and on the AFW benchmark [41] without fine-tuning. The proposed method obtains very competitive state-of-the-art performance in the two benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation and Objective</head><p>Face detection has been used as a core module in a wide spectrum of applications such as surveillance, mobile communication and human-computer interaction. It Y. Li and B. Sun contributed equally to this work and are joint first authors. arXiv:1606.00850v3 [cs.CV] 29 Aug 2016 is arguably one of the most successful applications of computer vision. Face detection in the wild continues to play an important role in the era of visual big data (e.g., images and videos on the web and in social media). However, it remains a challenging problem in computer vision due to the large appearance variations caused by nuisance variabilities including viewpoints, occlusion, facial expression, resolution, illumination and cosmetics, etc.  <ref type="figure">Fig. 1</ref>. Some example results in the FDDB face benchmark <ref type="bibr" target="#b18">[19]</ref> computed by the proposed method. For each testing image, we show the detection results (left) and the corresponding heat map of facial key-points with the legend shown in the right most column. (Best viewed in color) It has been a long history that computer vision researchers study how to learn a better representation for unconstrained faces <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">34]</ref>. Recently, together with large-scale annotated image datasets such as the ImageNet <ref type="bibr" target="#b7">[8]</ref>, deep ConvNets <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> have made significant progress in generic object detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16]</ref>, as well as in face detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>. The success is generally considered to be due to the region proposal methods and region-based ConvNets (R-CNN) <ref type="bibr" target="#b14">[15]</ref>. The two factors used to be addressed separately (e.g., the popular combination of the Selective Search <ref type="bibr" target="#b36">[37]</ref> and R-CNNs pretrained on the ImageNet), and now they are integrated through introducing the region proposal networks (RPNs) as done in the faster-RCNN <ref type="bibr" target="#b31">[32]</ref> or are merged into a single pipeline for speeding up the detection as done in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26]</ref>. In R-CNNs, one key layer is the so-called RoI (Region-of-Interest) pooling layer <ref type="bibr" target="#b13">[14]</ref>, which divides a valid RoI (e.g., an object bounding box proposal) evenly into a grid with a fixed spatial extent (e.g., 7 Ã— 7) and then uses max-pooling to convert the features inside the RoI into a small feature map. In this paper, we are interested in adapting state-ofthe-art ConvNets of generic object detection (e.g., the faster R-CNN <ref type="bibr" target="#b31">[32]</ref>) for face detection by overcoming the following two limitations: i) RPNs need to predefine a number of anchor boxes (with different aspect ratios and sizes), which requires potentially tedious parameter tuning in training and is sensitive to the (unknown) distribution of the aspect ratios and sizes of the object instances in a random testing image.  <ref type="figure">Figure 1</ref>. The 3D mean face model is predefined and fixed in both training and testing. The key idea of the proposed method is to learn a ConvNet to estimate the 3D transformation parameters (rotation and translation) w.r.t. the 3D mean face model to generate accurate face proposals and predict the face key points. The proposed ConvNet is trained in a multi-task discriminative training framework consisting of the classification Softmax loss and the location smooth l1-losses <ref type="bibr" target="#b13">[14]</ref> of both the facial key-points and the face bounding boxes. It is surprisingly simple w.r.t. its competitive state-of-the-art performance compared to the other methods in the popular FDDB benchmark <ref type="bibr" target="#b18">[19]</ref> and the AFW benchmark <ref type="bibr" target="#b40">[41]</ref>. See text for details.</p><p>ii) The RoI pooling layer in R-CNNs is predefined and generic to all object categories without exploiting the underlying object structural configurations, which either are available from the annotations in the training dataset (e.g., the facial landmark annotations in the AFLW dataset <ref type="bibr" target="#b19">[20]</ref>) as done in <ref type="bibr" target="#b40">[41]</ref> or can be pursued during learning (such as the deformable part-based models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>).</p><p>To address the two above issues in learning ConvNets for face detection, we propose to integrate a ConvNet and a 3D mean face model in an end-to-end multi-task discriminative learning framework. <ref type="figure">Figure 1</ref> shows some results of the proposed method. <ref type="figure">Figure 2</ref> illustrates the proposed method. We use 10 facial key-points in this paper, including "LeftEyeLeftCorner", "RightEyeRightCorner", "LeftEar", "Nose-Left", "NoseRight", "RightEar", "MouthLeftCorner", "MouthRightCorner", "Chin-Center", "CenterBetweenEyes" (see an example image in the left-top of <ref type="figure">Figure 2)</ref>. The 3D mean face model is then represented by the corresponding ten 3D facial key-points. The architecture of our ConvNet is straight-forward when taking into account a 3D model (see Section 3.2 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Method Overview</head><p>The key idea is to learn a ConvNet to (i) estimate the 3D transformation parameters (rotation and translation) w.r.t. the 3D mean face model for each detected facial key-point so that we can generate face bounding box proposals and (ii) predict facial key-points for each face instance more accurately. Leveraging the 3D mean face model is able to "kill two birds with one stone": Firstly, we can eliminate the manually heuristic design of anchor boxes in RPNs. Secondly, instead of using the generic RoI pooling, we devise a "configuration pooling" layer so as to respect the object structural configurations in a meaningful and principled way. In other words, we propose to learn to compute the proposals in a straight-forward top-down manner, instead of to design the bottom-up heuristic and then learn related regression parameters. To do so, we assume a 3D mean face model is available and facial key-points are annotated in the training dataset. Thanks to many excellent existing work in collecting and annotating face datasets, we can easily obtain both for faces nowadays. In learning, we have multiple types of losses involved in the objective loss function, including classification Softmax loss and location smooth l 1 -loss <ref type="bibr" target="#b13">[14]</ref> of facial key-points, and location smooth l 1 -loss of face bounding boxes respectively, so we formulate the learning of the proposed ConvNet under the multi-task discriminative deep learning framework (see Section 3.3).</p><p>In summary, we provide a clean and straight-forward solution for end-to-end integration of a ConvNet and a 3D model for face detection 1 . In addition to the competitive performance w.r.t the state-of-the-art face detection methods on the FDDB and AFW benchmarks, the proposed method is surprisingly simple and it is able to detect challenging faces (e.g., small, blurry, heavily occluded and extreme poses).</p><p>Potentially, the proposed method can be utilized to learn to detect other rigid or semi-rigid object categories (such as cars) if the required information (such as the 3D model and key-point/part annotation) are provided in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are a tremendous amount of existing works on face detection or generic object detection. We refer to <ref type="bibr" target="#b39">[40]</ref> for a more thorough survey on face detection. We discuss some of the most relevant ones in this section.</p><p>In human/animal vision, how the brain distills a representation of objects from retinal input is one of the central challenges for systems neuroscience, and many works have been focused on the ecologically important class of objectsfaces. Studies using fMRI experiments in the macaque reveal that faces are represented by a system of six discrete, strongly interconnected regions which illustrates hierarchical information processing in the brain <ref type="bibr" target="#b11">[12]</ref>, as well as some other results <ref type="bibr" target="#b33">[34]</ref>. These findings provide some biologically-plausible evidences for supporting the usage of deep learning based approaches in face detection and analysis.</p><p>The seminal work of Viola and Jones <ref type="bibr" target="#b37">[38]</ref> made face detection by a computer vision system feasible in real world applications, which trained a cascade of Ad-aBoost classifiers using Haar wavelet features. Many works followed this direction with different extensions proposed in four aspects: appearance features (beside Haar) including Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b6">[7]</ref>, Aggregate Channel Features (ACF) <ref type="bibr" target="#b8">[9]</ref>, Local Binary Pattern (LBP) features <ref type="bibr" target="#b0">[1]</ref> and SURF <ref type="bibr" target="#b2">[3]</ref>, etc.; detector structures (beside cascade) including the the scalar tree <ref type="bibr" target="#b10">[11]</ref> and the width-first-search tree <ref type="bibr" target="#b17">[18]</ref>, etc.; strong classifier learning (beside AdaBoost) including RealBoost <ref type="bibr" target="#b32">[33]</ref> and GentleBoost <ref type="bibr" target="#b12">[13]</ref>, ect; weak classifier learning (beside stump function) including the histogram method <ref type="bibr" target="#b24">[25]</ref> and the joint binarizations of Haar-like feature <ref type="bibr" target="#b27">[28]</ref>, etc..</p><p>Most of the recent face detectors are based on the deformable part-based model (DPM) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b26">27]</ref> with HOG features used, where a face is represented by a collection of parts defined based on either facial landmarks or heuristic pursuit as done in the original DPM. <ref type="bibr" target="#b26">[27]</ref> showed that a properly trained vanilla DPM can yield significant improvement for face detection.</p><p>More recent advances in deep learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> further boosted the face detection performance by learning more discriminative features from large-scale raw data, going beyond those handcrafted ones. In the FDDB benchmark, most of the face detectors with top performance are based on ConvNets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref>, combining with cascade <ref type="bibr" target="#b22">[23]</ref> and more explicit structure <ref type="bibr" target="#b38">[39]</ref>.</p><p>3D information has been exploited in learning object models in different ways. Some works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref> used a mixture of 3D view based templates by dividing the view sphere into a number of sectors. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref> utilized 3D models in extracting features and inferring the object pose hypothesis based on EM or DP. <ref type="bibr" target="#b35">[36]</ref> used a 3D face model for aligning faces in learning ConvNets for face recognition. Our work resembles <ref type="bibr" target="#b1">[2]</ref> in exploiting 3D model in face detection, which obtained very good performance in the FDDB benchmark. <ref type="bibr" target="#b1">[2]</ref> computes meaningful 3D pose candidates by image-based regression from detected face key-points with traditional handcrafted features, and verifies the 3D pose candidates by a parameter sensitive classifier based on difference features relative to the 3D pose. Our work integrates a ConvNet and a 3D model in an end-to-end multi-task discriminative learning fashion, which is more straightforward and simpler compared to <ref type="bibr" target="#b1">[2]</ref>.</p><p>Our Contributions. The proposed method contributes to face detection in three aspects.</p><p>i) It presents a simple yet effective method to integrate a ConvNet and a 3D model in an end-to-end learning with multi-task loss used for face detection in the wild. ii) It addresses two limitations in adapting the state-of-the-art faster RCNN <ref type="bibr" target="#b31">[32]</ref> for face detection: eliminating the heuristic design of anchor boxes by leveraging a 3D model, and replacing the generic and predefined RoI pooling with a configuration pooling which exploits the underlying object structural configurations. iii) It obtains very competitive state-of-the-art performance in the FDDB <ref type="bibr" target="#b18">[19]</ref> and AFW <ref type="bibr" target="#b40">[41]</ref> benchmarks.</p><p>Paper Organization. The remainder of this paper is organized as follows. Section 3 presents the method of face detection using a 3D model and details of our ConvNet including its architecture and training procedure. Section 4 presents details of experimental settings and shows the experimental results in the FDDB and AFW benchmarks. Section 5 first concludes this paper and then discuss some on-going and future work to extend the proposed work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>In this section, we introduce the notations and present details of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">3D Mean Face Model and Face Representation</head><p>In this paper, a 3D mean face model is represented by a collection of n 3D key-points in the form of (x, y, z) and then is denoted by a n Ã— 3 matrix, F <ref type="bibr" target="#b2">(3)</ref> . Usually, each key-point has its own semantic name. We use the 3D mean face model in the AFLW dataset <ref type="bibr" target="#b19">[20]</ref> which consists of 21 key-points. We select 10 key-points as stated above. A face, denoted by f , is presented by its 3D transformation parameters, Î˜, for rotation and translation, and a collection of 2D key-points, F <ref type="bibr" target="#b1">(2)</ref> , in the form of (x, y) (with the number being less than or equal to n). Hence, f = (Î˜, F <ref type="bibr" target="#b1">(2)</ref> ). The 3D transformation parameters Î˜ are defined by,</p><formula xml:id="formula_0">Î˜ = (Âµ, s, A (3) ),<label>(1)</label></formula><p>where Âµ represents a 2D translation (dx, dy), s a scaling factor, and A (3) a 3 Ã— 3 rotation matrix. We can compute the predicted 2D key-points by,</p><formula xml:id="formula_1">F (2) = Âµ + s Â· Ï€(A (3) Â· F (3) ),<label>(2)</label></formula><p>where Ï€() projects a 3D key-point to a 2D one, that is, Ï€ : R 3 â†’ R 2 and Ï€(x, y, z) = (x, y). Due to the projection Ï€(), we only need 8 parameters out of the original 12 parameters. Let A (2) denote a 2 Ã— 3 matrix, which is composed by the top two rows of A <ref type="bibr" target="#b2">(3)</ref> . We can re-produce the predicted 2D key-points by,</p><formula xml:id="formula_2">F (2) = Âµ + A (2) Â· F (3)<label>(3)</label></formula><p>which makes it easy to implement the computation of back-propagation in training our ConvNet. Note that we use the first sector in a 4-sector X-Y coordinate system to define all the positions, that is, the origin point (0, 0) is defined by the leftbottom corner in an image lattice.</p><p>In face datasets, faces are usually annotated with bounding boxes. In the FDDB benchmark <ref type="bibr" target="#b18">[19]</ref>, however, faces are annotated with ellipses and detection performance are evaluated based on ellipses. Given a set of predicted 2D key-pointsF <ref type="bibr" target="#b1">(2)</ref> , we can compute proposals in both ellipse form and bounding box form.</p><p>Computing a Face Ellipse and a Face Bounding Box based on a set of Predicted 2D Key-Points. For a givenF <ref type="bibr" target="#b1">(2)</ref> , we first predict the position of the top of head by,</p><formula xml:id="formula_3">x y TopOfHead = 2 Ã— x y CenterBetweenEyes âˆ’ x y ChinCenter .</formula><p>Based on the keypoints of a face proposal, we can compute its ellipse and bounding box. Face Ellipse. We first compute the outer rectangle. We use as one axis the line segment between the top-of-the-head key-point and the chin key-point, and then compute the minimum rectangle, usually a rotated rectangle, which covers all the key-points. Then, we can compute the ellipse using the two edges of the (rotated) rectangle as the major and minor axes respectively.</p><p>Face Bounding Box. We compute a face bounding box by the minimum upright rectangle which covers all the key-points, which is also adopted in the FDDB benchmark <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Architecture of Our ConvNet</head><p>As illustrated in <ref type="figure">Figure 2</ref>, the architecture of our ConvNet consists of: i) Convolution, ReLu and MaxPooling Layers. We adopt the VGG <ref type="bibr" target="#b3">[4]</ref> design in our experiments which has shown superior performance in a series of tasks. There are 5 groups and each group has 3 convolution and ReLu consecutive layers followed by a MaxPooling layer except for the 5th group. The spatial extent of the final feature map is of 16 times smaller than that of an input image due to the sub-sampling.</p><p>ii) An Upsampling Layer. Since we will measure the location difference between the input facial key-points and the predicted ones, we add an upsampling layer to compensate the sub-sampling effects in previous layers. It is implemented by deconvolution. We upsample the feature maps to 8 times bigger in size (i.e., the upsampled feature maps are still quarter size of an input image) considering the trade-off between key-point location accuracy, memory consumption and computation efficiency.</p><p>iii) A Facial Key-point Label Prediction Layer. There are 11 labels (10 facial key-points and 1 background class). It is used to compute the classification Softmax loss based on the input in training.</p><p>iv) A 3D Transformation Parameter Estimation Layer. This is the key observation in this paper. Originally, there are 12 parameters in total consisting of 2D translation, scaling and 3 Ã— 3 rotation matrix. Since we focus on the 2D projected key-points, we only need to account for 8 parameters (see the derivation above). v) A Face Proposal Layer. At each position, based on the 3D mean face model and the estimated 3D transformation parameters, we can compute a face proposal consisting of 10 predicted facial key-points and the corresponding face bounding box. The score of a face proposal is the sum of log probabilities of the 10 predicted facial key-points. The predicated key-points will be used to compute the smooth l 1 loss <ref type="bibr" target="#b13">[14]</ref> w.r.t. the ground-truth key-points. We apply the non-maximum suppression (NMS) to the face proposals in which the overlap between two bounding boxes a and b is computed by |a|âˆ©|b| |b| (where | Â· | represents the area of a bounding box), instead of the traditional intersection-over-union, accounting for the fact that it is rarely observed that one face is inside another one.</p><p>vi) A Configuration Pooling Layer. After NMS, for each face proposal, we pool the features based on the predicted 10 facial key-points. Here, for simplicity, we use all the 10 key-points without considering the invisibilities of certain keypoints in different face examples.</p><p>vii) A Face Bounding Box Regression Layer. It is used to further refine face bounding boxes in the spirit similar to the method <ref type="bibr" target="#b13">[14]</ref>. Based on the configuration pooling, we add two fully-connected layers to implement the regression. It is used to compute the smooth l 1 loss of face bounding boxes.</p><p>Denote by Ï‰ all the parameters in our ConvNet, which will be estimated through multi-task discriminative end-to-end learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The End-to-End Training</head><p>Input Data. Denote by C = {0, 1, Â· Â· Â· , 10} as the key-point labels where = 0 represents the background class. We use the image-centric sampling trick as done in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>. Without loss of generality, considering a training image with only one face appeared, we have its bounding box, B = (x, y, w, h) and m 2D key-points (m â‰¤ 10),</p><formula xml:id="formula_4">{(x i , i ) m i=1 } where x i = (x i , y i )</formula><p>is the 2D position of the ith key-point and i â‰¥ 1 âˆˆ C. We randomly sample m locations outside the face bounding box B as the background class,</p><formula xml:id="formula_5">{(x i , i ) 2m</formula><p>i=m+1 } (where i = 0, âˆ€i &gt; m). Note that in our ConvNet, we use the coordinate of the upsampled feature map which is half size along both axes of the original input. All the key-points and bounding boxes are defined accordingly based on ground-truth annotation.</p><p>The Classification Softmax Loss of Key-point Labels. At each position x i , our ConvNet outputs a discrete probability distribution, p xi = (p xi 0 , p xi 1 , Â· Â· Â· , p xi 10 ), over the 11 classes, which is computed by the Softmax over the 11 scores as usual <ref type="bibr" target="#b20">[21]</ref>. Then, we have the loss,</p><formula xml:id="formula_6">L cls (Ï‰) = âˆ’ 1 2m 2m i=1 log(p xi i ) (4)</formula><p>The Smooth l 1 Loss of Key-point Locations. At each key-point location x i ( i â‰¥ 1), we compute a face proposal based on the estimated 3D parameters and the 3D mean face, denoted by {(x</p><formula xml:id="formula_7">(i) j ,Ë† (i) j ) 10</formula><p>j=1 } the predicted 10 keypoints. So, for each key-point location x i , we will have m predicted locations, denoted byx i,j (j = 1, Â· Â· Â· , m). We follow the definition in <ref type="bibr" target="#b13">[14]</ref> to compute the smooth l 1 loss for each axis individually.</p><formula xml:id="formula_8">L pt loc (Ï‰) = 1 m 2 m i=1 m j=1 tâˆˆ{x,y} Smooth l1 (t i âˆ’t i,j )<label>(5)</label></formula><p>where the smooth term is defined by,</p><formula xml:id="formula_9">Smooth l1 (a) = 0.5a 2 if |a| &lt; 1 |a| âˆ’ 0.5 otherwise.<label>(6)</label></formula><p>Faceness Score. The faceness score of a face proposal in our ConvNet is computed by the sum of log probabilities of the predicted key-points,</p><formula xml:id="formula_10">Score(x i ,Ë† i ) = 10 i=1 log(px Ã® i )<label>(7)</label></formula><p>where for simplicity we do not account for the invisibilities of certain key-points. So the current faceness score has the issue of potential double-counting, especially for low-resolution faces. We observed that it hurts the quantitative performance in our experiments. We will address this issue in future work. See some heat maps of key-points in <ref type="figure">Figure 1</ref>.</p><p>The Smooth l 1 Loss of Bounding Boxes. For each face bounding box proposalB (after NMS), our ConvNet computes its bounding box regression offsets, t = (t x , t y , t w , t h ), where t specifies a scale-invariant translation and log-space height/width shift relative to a proposal, as done in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>. For the ground-truth bounding box B, we do the same parameterization and have v = (v x , v y , v w , v h ). Assuming that there are K bounding box proposals, we have,</p><formula xml:id="formula_11">L box loc (Ï‰) = 1 K K k=1 iâˆˆ{x,y,w,h} Smooth l1 (t i âˆ’ v i )<label>(8)</label></formula><p>So, the overall loss function is defined by,</p><formula xml:id="formula_12">L(Ï‰) = L cls (Ï‰) + L pt loc (Ï‰) + L box loc (Ï‰),<label>(9)</label></formula><p>where the third term depends on the output of the first two terms, which makes the loss minimization more challenging. We adopt a method to implement the differentiable bounding box warping layer, similar to <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the training procedure and implementation details and then show evaluation results on the FDDB <ref type="bibr" target="#b18">[19]</ref> and AFW <ref type="bibr" target="#b40">[41]</ref> benchmarks.  <ref type="table">Table 1</ref>. Classification accuracy of the key-points in the AFLW validation set at the end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>The Training Dataset. The only dataset we used for training our model is the AFLW dataset <ref type="bibr" target="#b19">[20]</ref>, which contains 25, 993 annotated faces in real-world images. The facial key-points are annotated upon visibility w.r.t. a 3D mean face model with 21 landmarks. Of the images 70% are used for training while the remaining is reserved as a validation set.</p><p>Training process. For convenience, the short edge of every image is resized to 600 pixels while preserving the aspect ratio (as done in the faster RCNN <ref type="bibr" target="#b31">[32]</ref>), thus our model learns how to handle faces under various scale. To handle faces of different resolution, we randomly blur images using Gaussian filters in preprocessing. Apart from the rescaling and blurring, no other preprocessing mechanisms (e.g., random crop or left-right flipping) are used.</p><p>We adopt the method of image-centric sampling <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> which uses one image at a time in training. Under the consideration that grids around the labeled position share almost the same context information, thus the 3 Ã— 3 grids around every labeled key-point's position are also regarded as the same positive examples, and we randomly choose the same amount of background examples outside the bounding boxes. The convolution filters are initialized by the VGG-16 <ref type="bibr" target="#b3">[4]</ref> pretrained on the ImageNet <ref type="bibr" target="#b7">[8]</ref>. We train the network for 13 epoch, and during the process, the learning rate is modified from 0.01 to 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of the intermediate results</head><p>Key-points classification in the validation dataset. As are shown by the heat maps in <ref type="figure">Figure 1</ref>, our model is capable of detecting facial key-points with rough face configurations preserved, which shows the effectiveness of exploiting the 3D mean face model. <ref type="table">Table 1</ref> shows the key-point classification accuracy on the validation set in the last epoch in training.</p><p>Face proposals. To evaluate the quality of our face proposals, we first show some qualitative results on the FDDB dataset in <ref type="figure">Fig. 3</ref>. These ellipses are directly calculated from the predicted 3D transformation parameters, forming several clusters around face instances. We also evaluate the quantitative results of face proposals. After a non-maximum suppression of IoU 0.7, the recall rate of 93.67% is obtained with average 34.4 proposals per image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Face Detection Results</head><p>To show the effectiveness of our method, we test our model on two popular face detection benchmarks: FDDB <ref type="bibr" target="#b18">[19]</ref> and AFW <ref type="bibr" target="#b40">[41]</ref>. Results on FDDB. FDDB is a challenge benchmark for face detection in unconstrained environment, which contains the annotations for 5171 faces in a set of 2845 images. We evaluate our results by using the evaluation code provided by the FDDB authors. The results on the FDDB dataset are shown in <ref type="figure">Figure 4</ref>. Our result is represented by "Ours-Conv3D", which surpasses the recall rate of 90% when encountering 2000 false positives and is competitive to the state-ofthe-art methods. We compare with published methods only. Only DP2MFD <ref type="bibr" target="#b29">[30]</ref> is slightly better than our model on discrete scores. It's worth noting that we beat all other methods on continuous scores. This is partly caused by the predefined 3D face model helps us better describe the pose and part locations of faces. We refer to the FDDB result webpage 2 for details of the published methods evaluated on it ( <ref type="figure" target="#fig_2">Fig. 4 and Fig. 5</ref>).</p><p>When comparing with recent work Faceness <ref type="bibr" target="#b38">[39]</ref>, we both recognize that one of the central issues to alleviate the problems of the occlusion and pose variation is to introduce facial part detector. However, our mechanism of computing face bounding box candidates is more straight forward since we explicitly integrate the structural information of a 3D mean face model instead of using a heuristic way of assuming the facial part distribution over a bounding box.</p><p>Results on AFW. AFW dataset contains 205 images with faces in various poses and view points. We use the evaluation toolbox provided by <ref type="bibr" target="#b26">[27]</ref>, which contains updated annotations for the AFW dataset where the original annotations are not comprehensive enough. Since the method of labeling face bounding boxes in AFW is different from that of in FDDB, we only use face proposals without configuration pooling and bounding box regression. The results on AFW are shown in <ref type="figure" target="#fig_3">Figure 6</ref>.</p><p>In our current implementation, there is one major limitation that prevents us from achieving better results. We do not explicitly handle invisible facial parts, which would be harmful when calculating the faceness score according to Eqn. 7, we will refine the method and introduce mechanisms of handling the invisible problem in future work. More detection results on both datasets are shown in <ref type="figure" target="#fig_4">Figure 7</ref> and <ref type="figure" target="#fig_5">Figure 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>We have presented a method of end-to-end integration of a ConvNet and a 3D model for face detection in the wild. Our method is a clean and straightforward solution when taking into account a 3D model in face detection. It also addresses two issues in state-of-the-art generic object detection ConvNets: eliminating heuristic design of anchor boxes by leveraging a 3D model, and overcoming generic and predefined RoI pooling by configuration pooling which exploits underlying object configurations. In experiments, we tested our method on two benchmarks, the FDDB dataset and the AFW dataset, with very compatible state-of-the-art performance obtained. We analyzed the experimental results and pointed out some current limitations.</p><p>In our on-going work, we are working on addressing the doubling-counting issue of the faceness score in the current implementation. We are also working on extending the proposed method for other types of rigid/semi-rigid object classes (e.g., cars). We expect that we will have a unified model for cars and faces which can achieve state-of-the-art performance, which will be very useful in a lot of practical applications such as surveillance and driveless cars.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Examples of face proposals computed using predicted 3D transformation parameters without non-maximum suppression. For clarity, we randomly sample 1/30 of the original number of proposals. ACF-multiscale (0.860762) Yan et al. (0.861535) MultiresHPM (0.863276) Joint Cascade (0.866757) HeadHunter (0.880874) Barbu et al. (0.893444) Faceness (0.909882) Ours-Conv3D (0.911816) DP2MFD (0.917392) FDDB results based on discrete scores using face bounding boxes in evaluation. The recall rates are computed against 2000 false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>FDDB results based on continuous scores using face ellipses in evaluation. The recall rates are computed against 2000 false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>97.21) Faceness AFW (AP 97.20) Headhunter (AP 97.14) Ours Conv3D (AP 95.97) SquaresChnFtrs-5 (AP 95.24) Structured Models (AP 95.19) Shen et al. (AP 89.03) TSM (AP 87.99) Picasa Face.com Face++ Precision-recall curves on the AFW dataset (AP = average precision) without configuration pool and face bounding box regression used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Some qualitative results on the FDDB dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Some qualitative results on the AFW dataset</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the open source deep learning package, MXNet<ref type="bibr" target="#b4">[5]</ref>, in our implementation. The full source code is released at https://github.com/tfwu/FaceDetection-ConvNet-3D</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://vis-www.cs.umass.edu/fddb/results.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>PietikÃ¤inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Face detection using a 3d model on face keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gramajo</surname></persName>
		</author>
		<idno>abs/1404.3596</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speeded-up robust features (SURF)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coarse-to-fine face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="85" to="107" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Functional compartmentalization and viewpoint generalization within the macaque face-processing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Freiwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">6005</biblScope>
			<biblScope unit="page" from="845" to="851" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Additive logistic regression: a statistical view of boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Fast R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning 3d object templates by quantizing geometry and appearance spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1190" to="1205" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-performance rotation invariant multiview face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="671" to="686" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-view object class detection with a 3d geometric model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Kullback-leibler boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02325</idno>
		<title level="m">SSD: Single shot multibox detector</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Joint haar-like features for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">From contours to 3d object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Payet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A deep pyramid deformable part model for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 7th International Conference on Biometrics Theory, Applications and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved boosting algorithms using confidence-rated predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Face recognition by humans: 19 results all computer vision researchers should know about</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Balas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1948" to="1962" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning a dense multi-view representation for detection, viewpoint classification and synthesis of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deepface: Closing the gap to humanlevel performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey on face detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
