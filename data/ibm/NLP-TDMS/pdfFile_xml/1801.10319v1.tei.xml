<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology Nanjing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology Nanjing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology Nanjing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<email>yingtai@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab Tencent Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>super resolution</term>
					<term>squeeze and excitation</term>
					<term>recursive networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image super resolution is a very important computer vision task, with a wide range of applications. In recent years, the depth of the super-resolution model has been constantly increasing, but with a small increase in performance, it has brought a huge amount of computation and memory consumption. In this work, in order to make the super resolution models more effective, we proposed a novel single image super resolution method via recursive squeeze and excitation networks (SESR). By introducing the squeeze and excitation module, our SESR can model the interdependencies and relationships between channels and that makes our model more efficiency. In addition, the recursive structure and progressive reconstruction method in our model minimized the layers and parameters and enabled SESR to simultaneously train multi-scale super resolution in a single model. After evaluating on four benchmark test sets, our model is proved to be above the state-of-the-art methods in terms of speed and accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Single image super resolution (SISR) is a hot topic in computer vision and has high practical value in many fields such as video, photography, games and medical imaging. The task of super resolution is to restore the low-resolution (LR) image to high-resolution (HR) images. When the upscaling factor is large, it is hard to learn the mapping from LR to HR and restore visual pleasing images. In recent years, neural networks were utilized in super resolution and showed great improvement in the reconstruction quality. To gain better restoration performance the super resolution models become deeper and deeper by stacking convolutional layers, many models with the depth exceeding 80 layers have appeared. Although increasing the depth of the model spatially can improve the performance of super resolution quality, it will bring a huge amount of computation and memory consumption. In order to improve the efficiency of super resolution networks, inspired by the SENet <ref type="bibr" target="#b0">[1]</ref> we proposed a novel single image super resolution method with recursive squeeze and excitation network named SESR.</p><p>The squeeze and excitation (SE) module is used to model the interdependencies among channels and reweight the features. The information among channels would be selected and the efficiency of the model is greatly improved. We found that after <ref type="figure">Fig. 1</ref>. Overview of our proposed model adding the SE structure, the model could achieve very high reconstruction performance only with few residual blocks. Although the SE structure adds a small number of weighting layers, the number of layers and parameters in SESR is far fewer than that of other models when achieving similar level of super resolution performance. As shown in <ref type="figure">figure 1</ref>, we designed the model with a recursive structure in which the data continuously pass through the recursive unit. In addition, our model is end to end which means our model can input the low-resolution images directly. Different to DRRN <ref type="bibr" target="#b1">[2]</ref> and other previous methods, our model do not need a bicubic input and we used a deconvolution layer as the upsample module in SESR which could decrease extra computation. Moreover, for large upscaling factors, our model used a progressively reconstruction method which means our model first reconstruct the lower scale image from the LR and share the information to the larger branch. This method also enables us to train multi scale super resolution in a single model. We summarize our contribution in the following points:</p><p>• We proposed a novel method for single image super resolution via squeeze and excitation module and recursive structure. Our model is proved to be over stateof-the-art methods in scale x4 benchmark not only in accuracy but also in speed.</p><p>• We found that adding the squeeze and excitation module can significantly improve the model performance, at least 0.1dB gain in PSNR in each test dataset compared with models removed the SE module.</p><p>• We designed the model with recursive structure and progressive reconstruction method which minimized the layers and parameters in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classical super resolution methods</head><p>Super resolution is a hot topic in the field of computer vision. Although interpolation methods are widely used nowadays, the quality is hard to meet a satisfied level. Yang et al. <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> developed a set of super-resolution model based on sparse coding. Timofte et al. proposed A + <ref type="bibr" target="#b5">[6]</ref> and IA <ref type="bibr" target="#b6">[7]</ref> based on anchored neighborhood regression. Huang et al. proposed the SelfExSR <ref type="bibr" target="#b7">[8]</ref> via transformed self-exemplars. The above methods achieved better results than bicubic but still hard to restore high quality images for higher upscaling factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep learning based super resolution methods</head><p>In recent years, with the development of deep learning <ref type="bibr" target="#b8">[9]</ref> and convolutional neural networks <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, many deeplearning-based super resolution methods have been proposed. The SRCNN <ref type="bibr" target="#b11">[12]</ref> proposed by Dong et al. for the first time used convolutional neural networks on super resolution tasks. Simonyan et al. <ref type="bibr" target="#b12">[13]</ref> found that the deepening of the network can bring about performance improvement. Then He et al. proposed ResNet <ref type="bibr" target="#b13">[14]</ref> to make the deep models available for training. Inspired by the research above, the networks used in a super resolution tasks are also deepening. <ref type="bibr">Kim</ref>   <ref type="bibr" target="#b16">[17]</ref> by introducing the Laplacian pyramid which allows a network to do multiple-scale superresolution simultaneously in one feed forward. To gain higher performance, more researches on the spatial structure were conducted. The networks are becoming more sophisticated instead of simply stacking the convolutional layers. Later MemNet <ref type="bibr" target="#b17">[18]</ref> and SR DenseNet <ref type="bibr" target="#b18">[19]</ref> which were designed to have different dense skip-connections <ref type="bibr" target="#b19">[20]</ref> were proposed. The above methods are prone to be deeper and deeper. However, for some super resolution tasks the 84-layer MemNet <ref type="bibr" target="#b17">[18]</ref> is not much better than the 52-layer DRRN <ref type="bibr" target="#b1">[2]</ref>, explosive growth of the size of the network could bring little improvement in super resolution quality but large amount of computation and GPU memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Perceptual loss and GANs</head><p>In order to make the image more visual pleasing, perceptual loss <ref type="bibr" target="#b20">[21]</ref> was proposed and widely used in the style transfer <ref type="bibr" target="#b21">[22]</ref> and super resolution field. In addition, Generative Adversarial Networks(GAN) can also produce visually beautiful images, recently many GAN based models were developed for single image super resolution such as SRGAN <ref type="bibr" target="#b22">[23]</ref> and Neural Enhance <ref type="bibr" target="#b23">[24]</ref>. Although GANs would produce good looking samples, the accuracy which evaluated by peak signal to noise ratio (PSNR) and structural similarity (SSIM) <ref type="bibr" target="#b24">[25]</ref> is decreased compared with those supervised by L1 or L2 loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Recursive structure</head><p>We build the model with a recursive structure which enables SESR to increase recursion depth without bring more parameters. In our model low resolution images are fed directly into the network and passed through a convolution layer, then entered the recursive unit. When output from squeeze and excitation enhanced residual blocks (SE-ResBlock), the features reinput to the recursive unit. Finally, the output entered the reconstruction network to obtain high resolution images. Different from DRCN <ref type="bibr" target="#b15">[16]</ref>, our model employed progressive reconstruction method and only get supervised at each end of different scale super resolution branch instead of supervised at each recursion which significantly decreased the amount of computation. Our model has local residual learning and global residual learning, the skip connections in our model makes it easy to converge. The total layers in one branch is 27 but we got higher performance than those deeper models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Squeeze and Excitation Residual Block</head><p>Inspired by the SENet <ref type="bibr" target="#b0">[1]</ref>, we introduced the SE module to make the super resolution models more efficient. Squeeze and excitation would make the network more powerful by emphasizing important features and suppress useless features among the channels. In order to squeeze global spatial information for each channel, we followed the SENet <ref type="bibr" target="#b0">[1]</ref>, using global average pooling <ref type="bibr" target="#b25">[26]</ref> in our model. The squeeze function in SESR is shown as below:</p><formula xml:id="formula_0">= ( ) = 1 ∑ ∑ ( , )<label>(1)</label></formula><p>Where is the c-th element of the squeezed channels and S(.) is the squeeze function. is the c-th channel of the input. H and W denotes the height and width of the input.</p><p>The excitation function is shown as the following formula:</p><p>( ) = ( ( )) (2) Where E(.) is the excitation function and x is the input squeezed signal from the previous layer. denotes the Sigmoid, and denotes the 1x1 convolutional layer Conv Up and Conv Down shown in figure2.  We build our basic structure in the recursive unit on the base of residual blocks. Lim et al. <ref type="bibr" target="#b6">[7]</ref> found that removing the batch normalization (BN) <ref type="bibr" target="#b26">[27]</ref> layer would improve the performance of the super resolution network. We reproduced the experiment in the discussion section and we noticed the phenomenon. Therefore, we removed all the BN layers from the residual block which was proposed by He et al. <ref type="bibr" target="#b13">[14]</ref>. and we added the squeeze and excitation module into the block. The SE-ResBlock we used in our SESR is shown in <ref type="figure" target="#fig_0">figure 2</ref>.</p><p>We first build the blocks by stacking convolutional layers interleaved with Leaky ReLU <ref type="bibr" target="#b27">[28]</ref> then we put the SE module into the block. In contrast to original SENet <ref type="bibr" target="#b0">[1]</ref>, we used 1x1 convolutional layers instead of fully connected layers in the SE module. The number of channels in the first two convolution layers in each SE-Residual Block is 64, while the number of channels in the third convolution layer is increased by a factor of 4, followed by the SE module. In the SE module, squeeze was done by global average pooling. We used Conv Down to reduce the number of output channels to 16 and then Conv Up to increase the number of channels to 256 to form a Bottle Neck structure and then pass the sigmoid layer for modeling the correlations between the channels. The weights for channels were then multiplied with the residual. Finally, pass a 1x1 transition convolution to retransform the number of channels to 64, and add the output of the previous block to obtain Xi+1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Progressive Reconstruction</head><p>In order to improve the training efficiency of the model, a model needs to be trained with multiple scale for up sampling at the same time, inspired by the LapSRN <ref type="bibr" target="#b16">[17]</ref>, we introduced the laplacian pyramid in our model and the structure is shown as <ref type="figure" target="#fig_1">figure 3</ref>. LR images first input to the lower scale branch and reconstruct the HR 2x image then share the residual and image to the higher scale branch to reconstruct the HR 4x image.</p><p>Compared with those direct reconstruction methods, progressive methods could lead to better quality for higher upscaling factors and decreasing parameters by sharing information between each super resolution branch. Also, our progressive reconstruction enabled our model to do multi scale super resolution in a single model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Reconstruction Network 1) Global residual learning</head><p>We first upscaling our low-resolution image via a deconvolution layer outside the recursive unit. The quality of the upscaled image from the tiny deconvolution layer is usually not very high but we used it to take the place of bicubic input. In the reconstruction network we add the low-quality upscaled image with the residual from the bottom recursive unit to obtain highquality high-resolution image, the process is shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Loss function</head><p>Even though directly optimizing the mean squared error (MSE) could get high peak signal to noise ratio (PSNR), the L2 loss always results in over smooth hard to restore visual pleasing images, so the Charbonnier Loss <ref type="bibr" target="#b16">[17]</ref> is used as a loss function in the model. The loss function is shown as below:</p><formula xml:id="formula_1">Loss = 1 ∑ ∑ √ 2 − 2 =1 (̂− ) (3) =1</formula><p>Where N is the batch size and set ε to 0.001, L is the number of up sampling branch. The s means the scale while the and ̂ are the ground truth and generated high resolution image in a branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structure of residual blocks</head><p>As shown in <ref type="figure">figure 5</ref>, we researched on three different structures of residual blocks. Structure (a) is the same as the original ResNet <ref type="bibr" target="#b13">[14]</ref> proposed by He et al. and this architecture was utilized in SR ResNet <ref type="bibr" target="#b22">[23]</ref>. Structure (b) is the residual block similar to which was used in EDSR <ref type="bibr" target="#b28">[29]</ref>, all the BN layers were removed on the basis of structure (a), simplifying the network. Structure(c) is the SE-ResBlock we proposed in SESR which removed all the BN layer and added the SE module. The kernel size of convolutional layers in SE and the transition were set to 1x1 so it would not bring much parameters and that makes these three structures have similar number of parameters. Similarly, we put these three residual blocks into our recursive unit and set the recursion depth to four. Then we iterate 300 <ref type="figure">Fig.5</ref>. We researched on three different structure of residual blocks epochs on the same training set and test these trained models on Set5 <ref type="bibr" target="#b29">[30]</ref> and Set14 <ref type="bibr" target="#b30">[31]</ref>. The results are shown in <ref type="table">Table 1</ref>.</p><p>As shown in the table, our SE-ResBlock achieves the highest score of PSNR and SSIM in both of the test datasets. When comparing PSNR, in Set5 our model is 0.22dB higher than (b) and 2.26 dB higher than (a). In Set15, our SESR is 0.1 dB higher than (b) and 1.08 dB higher than (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recursion depth</head><p>In this section, we study the effect of recursion depth with the model's reconstruction quality. We trained models with recursion depth of 2,3,4,5,6 with 391 training images and iterate for 300 epochs. We test these models for scale x4 on Set5 and Set14. The results are shown in <ref type="table">Table 2</ref>. Recursion depth directly impact the performance of the recursive network. As we can see in <ref type="table">table 3</ref>, the reconstruction quality is highest when the recursion depth is set to four. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>In this work we used Yang91 <ref type="bibr" target="#b3">[4]</ref>, BSD200 <ref type="bibr" target="#b31">[32]</ref> and General100 <ref type="bibr" target="#b32">[33]</ref> dataset for training. The model was evaluated with some public available and popular benchmark datasets including Set5 <ref type="bibr" target="#b29">[30]</ref> and Set14 <ref type="bibr" target="#b30">[31]</ref>. We also included the Berkeley segmentation dataset <ref type="bibr" target="#b31">[32]</ref> (BSD100) and a dataset of urban landscape named Urban100 <ref type="bibr" target="#b7">[8]</ref>. All the RGB images of these four benchmark datasets were converted to YCbCr color space with OpenCV, and we only input the Y channel to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Setup</head><p>In the experiment, we used a NVIDIA Tesla P40 for training our proposed models. We build the model using Pytorch version 0.2.0. The operating system of our server is Ubuntu16.10, CUDA8 and CUDNN5.1 were installed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with state-of-the-art models 1) Visual comparison</head><p>Figure6 shows the reconstruction results and the ground truth from our test sets and we compare our proposed SESR with other state-of-the-art super resolution methods including A+ <ref type="bibr" target="#b5">[6]</ref>, SelfExSR <ref type="bibr" target="#b7">[8]</ref>, SRCNN <ref type="bibr" target="#b11">[12]</ref>, VDSR <ref type="bibr" target="#b14">[15]</ref> and DRCN <ref type="bibr" target="#b15">[16]</ref>. We cropped a 64x64 sub image from each reconstructed highresolution images and compute the PSNR and SSIM of each sub image with the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Reconstruction Accuracy</head><p>We use PSNR and SSIM as evaluation methods to evaluate the model on the above benchmark dataset. Same amounts of pixels of the border were ignored. The test images were first down sampling by bicubic and restored by the super resolution models.</p><p>The reconstruction quality for scale x2 and scale x4 of our SESR and other state-of-the-art models can be obtained from <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref>, we marked the best quality in red, the second in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Speed</head><p>In this part, we researched on the running time of models. We reproduced LapSRN <ref type="bibr" target="#b16">[17]</ref> and VDSR <ref type="bibr" target="#b14">[15]</ref> with PyTorch. We test these methods on a Tesla P40 GPU. We tested on BSD100 <ref type="bibr" target="#b31">[32]</ref> for scale x4. As shown in figure 9 SESR could run at a very high speed, less than 0.02 second per image and achieves the best accuracy among the state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Model Parameters</head><p>Both the SESR and LapSRN <ref type="bibr" target="#b16">[17]</ref> contains two branches for different scale super resolution due to the progressive reconstruction method in the model. Our recursive model only contains 624k parameters. SESR is set with the recursion depth of 4. We compared the parameters and Set14 <ref type="bibr" target="#b30">[31]</ref> results of SESR with other state-of-the-art models. From figure10, our proposed SESR is shown to be the most powerful model with a small number of parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this study, we proposed a novel single image super resolution method. Compared with other methods, our model could achieve good results with fewer residual blocks and shallow recursion depth, effectively reducing the number of model parameters and calculating time. In addition, we also absorbed many excellent super resolution methods in the early stage and utilizing the progressive reconstruction methods so that our model could train higher scale better and could do a variety of super resolution scales in a single model. Our model was evaluated on serval testing datasets and we achieved the performance over the state-of-the-art methods not only in accuracy but also in speed. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>We added squeeze and excitation into a common residual block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The model could do multi scale upscaling task via Laplacian Pyramid</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Global residual learning for image reconstruction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Quality comparison of our model with other work for scale x4 super resolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Running time comparison with other state-of-the-art models Model parameters comparison with state-of-the-art models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I .</head><label>I</label><figDesc>RECONSTRUCTION PERFORMANCE OF DIFFERENT STRUCTURES</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>V. EXPERIMENT</cell></row><row><cell></cell><cell>Model</cell><cell cols="2">PSNR/SSIM Set5 Set14</cell></row><row><cell></cell><cell cols="3">Structure (a) 29.58/ 0.840 27.24/ 0.753</cell></row><row><cell></cell><cell cols="3">Structure (b) 31.66/ 0.888 28.22/ 0.781</cell></row><row><cell></cell><cell cols="3">Structure (c) 31.84/ 0.891 28.32/ 0.784</cell></row><row><cell>TABLE II.</cell><cell cols="3">NUMBER OF BLOCKS IN EACH SUPER RESOLUTION BRANCH</cell></row><row><cell></cell><cell>Recursion</cell><cell cols="2">PSNR/SSIM</cell></row><row><cell></cell><cell>Depth</cell><cell>Set5</cell><cell>Set14</cell></row><row><cell></cell><cell>2</cell><cell cols="2">31.69/0.888 28.24/0.782</cell></row><row><cell></cell><cell>3</cell><cell cols="2">31.74/0.889 28.28/ 0.783</cell></row><row><cell></cell><cell>4</cell><cell cols="2">31.84/0.891 28.32/ 0.784</cell></row><row><cell></cell><cell>5</cell><cell cols="2">31.78/0.891 28.32/ 0.783</cell></row><row><cell></cell><cell>6</cell><cell cols="2">31.75/0.889 28.29/ 0.783</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III .</head><label>III</label><figDesc>PSNR AND SSIM FOR SCALE X4 ON SET5, SET14, BSD100 AND URBAN100 Method</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PSNR/SSIM</cell><cell></cell></row><row><cell></cell><cell>Set5</cell><cell>Set14</cell><cell>Bsd100</cell><cell>Urban100</cell></row><row><cell>Bicubic</cell><cell>28.43/0.811</cell><cell>26.01/ 0.704</cell><cell>25.97/ 0.670</cell><cell>23.15/ 0.660</cell></row><row><cell>A+ [6]</cell><cell>30.32/ 0.860</cell><cell>27.34/ 0.751</cell><cell>26.83/ 0.711</cell><cell>24.34/ 0.721</cell></row><row><cell>SRCNN [12]</cell><cell>30.50/ 0.863</cell><cell>27.52/ 0.753</cell><cell>26.91/ 0.712</cell><cell>24.53/ 0.725</cell></row><row><cell>FSRCNN [33]</cell><cell>30.72/ 0.866</cell><cell>27.61/ 0.755</cell><cell>26.98/ 0.715</cell><cell>24.62/ 0.728</cell></row><row><cell>SelfExSR [8]</cell><cell>30.34/ 0.862</cell><cell>27.41/ 0.753</cell><cell>26.84/ 0.713</cell><cell>24.83/ 0.740</cell></row><row><cell>VDSR [15]</cell><cell>31.35/ 0.883</cell><cell>28.02/ 0.768</cell><cell>27.29/ 0.726</cell><cell>25.18/ 0.754</cell></row><row><cell>DRCN [16]</cell><cell>31.54/ 0.884</cell><cell>28.03/ 0.768</cell><cell>27.24/ 0.725</cell><cell>25.14/ 0.752</cell></row><row><cell>LapSRN [17]</cell><cell>31.54/ 0.885</cell><cell>28.19/ 0.772</cell><cell>27.32/ 0.727</cell><cell>25.21/ 0.756</cell></row><row><cell>DRRN [2]</cell><cell>31.68/ 0.888</cell><cell>28.21/ 0.772</cell><cell>27.38/ 0.728</cell><cell>25.44/ 0.764</cell></row><row><cell>SESR(ours)</cell><cell>31.84/ 0.891</cell><cell>28.32/ 0.784</cell><cell>27.42/ 0.737</cell><cell>25.42/ 0.771</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation net-works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image Super-Resolution via Deep Recursive Residual Network[C]// IEEE Confer-ence on Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society</title>
		<imprint>
			<biblScope unit="page" from="2790" to="2798" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Image super-resolution as sparse representation of raw image patches[C]//Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image su-per-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transac-tions on image processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adjusted anchored neighborhood regression for fast su-per-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>A+</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="111" to="126" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Seven ways to improve examplebased single image super resolu-tion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1865" to="1873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural net-works[C]//Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J S</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recogni-tion</title>
		<meeting>the IEEE conference on computer vision and pattern recogni-tion</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional net-works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image su-per-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Confer-ence on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Confer-ence on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep Laplacian Pyramid Networks for Fast and Accurate Su-per-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03915</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Dense Skip Connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and su-per-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image style trans-fer using convolutional neural net-works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Photo-realistic sin-gle image superresolution using a generative adver-sarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszá R F</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirsch</forename><forename type="middle">M</forename><surname>Enhancenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Con-ference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H R</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">/International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. 2017</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Low-complexity singleimage super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On single image scale-up using sparserepresentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="711" to="730" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]//Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
