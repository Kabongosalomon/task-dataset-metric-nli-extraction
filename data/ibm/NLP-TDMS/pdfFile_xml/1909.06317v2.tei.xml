<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A COMPARATIVE STUDY ON TRANSFORMER VS RNN IN SPEECH APPLICATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Communication Science Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Alphabetical Order)</roleName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Human Dataware Lab. Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Someki</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Enrique</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalta</forename><surname>Soplin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">LINE Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takenori</forename><surname>Yoshimura</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Human Dataware Lab. Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangyou</forename><surname>Zhang</surname></persName>
							<affiliation key="aff8">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A COMPARATIVE STUDY ON TRANSFORMER VS RNN IN SPEECH APPLICATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transformer</term>
					<term>Recurrent Neural Networks</term>
					<term>Speech Recognition</term>
					<term>Text-to-Speech</term>
					<term>Speech Translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequence-to-sequence models have been widely used in end-toend speech processing, for example, automatic speech recognition (ASR), speech translation (ST), and text-to-speech (TTS). This paper focuses on an emergent sequence-to-sequence model called Transformer, which achieves state-of-the-art performance in neural machine translation and other natural language processing applications. We undertook intensive studies in which we experimentally compared and analyzed Transformer and conventional recurrent neural networks (RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS benchmarks. Our experiments revealed various training tips and significant performance benefits obtained with Transformer for each task including the surprising superiority of Transformer in 13/15 ASR benchmarks in comparison with RNN. We are preparing to release Kaldi-style reproducible recipes using open source and publicly available datasets for all the ASR, ST, and TTS tasks for the community to succeed our exciting outcomes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Transformer is a sequence-to-sequence (S2S) architecture originally proposed for neural machine translation (NMT) <ref type="bibr" target="#b0">[1]</ref> that rapidly replaces recurrent neural networks (RNN) in natural language processing tasks. This paper provides intensive comparisons of its performance with that of RNN for speech applications; automatic speech recognition (ASR), speech translation (ST), and text-tospeech (TTS).</p><p>One of the major difficulties when applying Transformer to speech applications is that it requires more complex configurations (e.g., optimizer, network structure, data augmentation) than the conventional RNN based models. Our goal is to share our knowledge on the use of Transformer in speech tasks so that the community can fully succeed our exciting outcomes with reproducible open source tools and recipes.</p><p>Currently, existing Transformer-based speech applications <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref> still lack an open source toolkit and reproducible experiments while previous studies in NMT <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> provide them. Therefore, we work on an open community-driven project for end-to-end speech applications using both Transformer and RNN by following the success of Kaldi for hidden Markov model (HMM)-based ASR <ref type="bibr" target="#b6">[7]</ref>. Specifically, our experiments provide practical guides for tuning Transformer in speech tasks to achieve state-of-the-art results.</p><p>In our speech application experiments, we investigate several aspects of Transformer and RNN-based systems. For example, we measure the word/character/regression error from the ground truth, training curve, and scalability for multiple GPUs.</p><p>The contributions of this work are: • We conduct a larges-scale comparative study on Transformer and RNN with significant performance gains especially for the ASR related tasks. • We explain our training tips for Transformer in speech applications: ASR, TTS and ST. • We provide reproducible end-to-end recipes and models pretrained on a large number of publicly available datasets in our open source toolkit ESPnet [8] <ref type="bibr" target="#b0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related studies</head><p>As Transformer was originally proposed as an NMT system <ref type="bibr" target="#b0">[1]</ref>, it has been widely studied on NMT tasks including hyperparameter search <ref type="bibr" target="#b8">[9]</ref>, parallelism implementation <ref type="bibr" target="#b4">[5]</ref> and in comparison with RNN <ref type="bibr" target="#b9">[10]</ref>. On the other hand, speech processing tasks have just provided their preliminary results in ASR <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, ST <ref type="bibr" target="#b2">[3]</ref> and TTS <ref type="bibr" target="#b3">[4]</ref>. Therefore, this paper aims to gather the previous basic research and to explore wider topics (e.g., accuracy, speed, training tips) in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SEQUENCE-TO-SEQUENCE RNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unified formulation for S2S</head><p>S2S is a variant of neural networks that learns to transform a source sequence X to a target sequence Y <ref type="bibr" target="#b11">[12]</ref>. In <ref type="figure">Fig. 1</ref>, we illustrate a common S2S structure for ASR, TTS and ST tasks. S2S consists of two neural networks: an encoder</p><formula xml:id="formula_0">X0 = EncPre(X), (1) Xe = EncBody(X0),<label>(2)</label></formula><p>and a decoder</p><formula xml:id="formula_1">Y0[1 : t − 1] = DecPre(Y [1 : t − 1]), (3) Y d [t] = DecBody(Xe, Y0[1 : t − 1]), (4) Ypost[1 : t] = DecPost(Y d [1 : t]),<label>(5)</label></formula><p>where X is the source sequence (e.g., a sequence of speech features (for ASR and ST) or characters (for TTS)), e is the number of layers </p><formula xml:id="formula_2">L = Loss(Ypost, Y )<label>(6)</label></formula><p>between the generated sequence Ypost and the target sequence Y . The remainder of this section describes RNN-based universal modules: "EncBody" and "DecBody". We regard "EncPre", "DecPre", "DecPost" and "Loss" as task-specific modules and we describe them in the later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">RNN encoder</head><p>EncBody(·) in Eq. (2) transforms a source sequence X0 into an intermediate sequence Xe. Existing RNN-based EncBody(·) implementations <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref> typically adopt a bi-directional long short-term memory (BLSTM) that can perform such an operation thanks to its recurrent connection. For ASR, an encoded sequence Xe can also be used for source-level frame-wise prediction using connectionist temporal classification (CTC) <ref type="bibr" target="#b15">[16]</ref> for joint training and decoding <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">RNN decoder</head><p>DecBody(·) in Eq. (4) generates a next target frame with the encoded sequence Xe and the prefix of target prefix Y0[1 : t − 1]. For sequence generation, the decoder is mostly unidirectional. For example, uni-directional LSTM with an attention mechanism <ref type="bibr" target="#b12">[13]</ref> is often used in RNN-based DecBody(·) implementations. That attention mechanism emits source frame-wise weights to sum the encoded source frames Xe as a target frame-wise vector to be transformed with the prefix Y0[0 : t−1]. We refer to this type of attention as "encoder-decoder attention".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TRANSFORMER</head><p>Transformer learns sequential information via a self-attention mechanism instead of the recurrent connection employed in RNN. This section describes the self-attention based modules in Transformer in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-head attention</head><p>Transformer consists of multiple dot-attention layers <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_3">att(X q , X k , X v ) = softmax X q X k √ d att X v ,<label>(7)</label></formula><p>where X k , X v ∈ R n k ×d att and X q ∈ R n q ×d att are inputs for this attention layer, d att is the number of feature dimensions, n q is the length of X q , and n k is the length of X k and X v . We refer to X q X k as the "attention matrix". Vaswani et al. <ref type="bibr" target="#b0">[1]</ref> considered these inputs X q , X k and X v to be a query and a set of key-value pairs, respectively.</p><p>In addition, to allow the model to deal with multiple attentions in parallel, Vaswani et al. <ref type="bibr" target="#b0">[1]</ref> extended this attention layer in Eq. <ref type="bibr" target="#b6">(7)</ref> to multi-head attention (MHA):</p><formula xml:id="formula_4">MHA(Q, K, V ) = [H1, H2, . . . , H d head ]W head ,<label>(8)</label></formula><formula xml:id="formula_5">H h = att(QW q h , KW k h , V W v h ),<label>(9)</label></formula><p>where K, V ∈ R n k ×d att and Q ∈ R n q ×d att are inputs for this MHA layer,</p><formula xml:id="formula_6">H h ∈ R n q ×d att is the h-th attention layer output (h = 1, . . . , d head ), W q h , W k h , W v h ∈ R d att ×d att and W head ∈ R d att d head ×d att</formula><p>are learnable weight matrices and d head is the number of attentions in this layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-attention encoder</head><p>We define Transformer-based EncBody(·) used for Eq. (2) unlike the RNN encoder in Section 2.2 as follows:</p><formula xml:id="formula_7">X i = Xi + MHAi(Xi, Xi, Xi), Xi+1 = X i + FFi(X i ),<label>(10)</label></formula><p>where i = 0, . . . , e − 1 is the index of encoder layers, and FFi is the i-th two-layer feedforward network:</p><formula xml:id="formula_8">FF(X[t]) = ReLU(X[t]W ff 1 + b ff 1 )W ff 2 + b ff 2 ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_9">X[t] ∈ R d att is the t-th frame of the input sequence X, W ff 1 ∈ R d att ×d ff , W ff 2 ∈ R d ff ×d att are learnable weight matrices, and b ff 1 ∈ R d ff , b ff 2 ∈ R d att</formula><p>are learnable bias vectors. We refer to MHAi(Xi, Xi, Xi) in Eq. (10) as "self attention".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-attention decoder</head><p>Transformer-based DecBody(·) used for Eq. (4) consists of two attention modules:</p><formula xml:id="formula_10">Yj[t] = Yj[t] + MHA self j (Yj[t], Yj[1 : t], Yj[1 : t]), Y j = Yj + MHA src j (Y j , Xe, Xe), Yj+1 = Y j + FFj(Y j ),<label>(12)</label></formula><p>where j = 0, . . . , d − 1 is the index of the decoder layers. We refer to the attention matrix between the decoder input and the encoder output in MHA src j (Y j , Xe, Xe) as "encoder-decoder attention' as same as the one in RNN in Sec 2.3. Because the unidirectional decoder is useful for sequence generation, its attention matrices at the t-th target frame are masked so that they do not connect with future frames later than t. This masking of the sequence can be done in parallel using an elementwise product with a triangular binary matrix. Because it requires no sequential operation, it provides a faster implementation than RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Positional encoding</head><p>To represent the time location in the non-recurrent model, Transformer adopts sinusoidal positional encoding:</p><formula xml:id="formula_11">PE[t] = sin t 10000 t/d att if t is even, cos t 10000 t/d att if t is odd.<label>(13)</label></formula><p>The input sequences X0, Y0 are concatenated with (PE <ref type="bibr" target="#b0">[1]</ref>, PE <ref type="bibr" target="#b1">[2]</ref>, . . . ) before EncBody(·) and DecBody(·) modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ASR EXTENSIONS</head><p>In our ASR framework, the S2S predicts a target sequence Y of characters or SentencePiece <ref type="bibr" target="#b18">[19]</ref> from an input sequence X fbank of logmel filterbank speech features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ASR encoder architecture</head><p>The source X in ASR is represented as a sequence of 83-dim log-mel filterbank frames with pitch features <ref type="bibr" target="#b19">[20]</ref>. First, EncPre(·) transforms the source sequence X into a subsampled sequence X0 ∈ R n sub ×d att by using two-layer CNN with 256 channels, stride size 2 and kernel size 3 in <ref type="bibr" target="#b1">[2]</ref>, or VGG-like max pooling in <ref type="bibr" target="#b20">[21]</ref>, where n sub is the length of the output sequence of the CNN. This CNN corresponds to EncPre(·) in Eq. (1). Then, EncBody(·) transforms X0 into a sequence of encoded features Xe ∈ R n sub ×d att for the CTC and decoder networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ASR decoder architecture</head><p>The decoder network receives the encoded sequence Xe and the prefix of a target sequence Y [1 : t − 1] of token IDs: characters or SentencePiece <ref type="bibr" target="#b18">[19]</ref>. First, DecPre(·) in Eq. (3) embeds the tokens into learnable vectors. Next, DecBody(·) and single-linear layer DecPost(·) predicts the posterior distribution of the next token prediction Ypost[t] given Xe and Y [1 : t − 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ASR training and decoding</head><p>During ASR training, both the decoder and the CTC module predict the frame-wise posterior distribution of Y given corresponding source X: ps2s(Y |X) and pctc(Y |X), respectively. We simply use the weighted sum of those negative log likelihood values:</p><formula xml:id="formula_12">L ASR = −α log ps2s(Y |X) − (1 − α) log pctc(Y |X),<label>(14)</label></formula><p>where α is a hyperparameter.</p><p>In the decoding stage, the decoder predicts the next token given the speech feature X and the previous predicted tokens using beam search, which combines the scores of S2S, CTC and the RNN language model (LM) <ref type="bibr" target="#b21">[22]</ref> as follows:</p><formula xml:id="formula_13">Y = argmax Y ∈Y * {λ log ps2s(Y |Xe) + (1 − λ) log pctc(Y |Xe) + γ log p lm (Y )},<label>(15)</label></formula><p>where Y * is a set of hypotheses of the target sequence, and γ, λ are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ST EXTENSIONS</head><p>In ST, S2S receives the same source speech feature and target token sequences in ASR but the source and target languages are different. Its modules are also defined in the same ways as in ASR. However, ST cannot cooperate with the CTC module introduced in Section 4.3 because the translation task does not guarantee the monotonic alignment of the source and target sequences unlike ASR <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">TTS EXTENSIONS</head><p>In the TTS framework, the S2S generates a sequence of log-mel filterbank features and predicts the probabilities of the end of sequence (EOS) given an input character sequence <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">TTS encoder architecture</head><p>The input of the encoder in TTS is a sequence of IDs corresponding to the input characters and the EOS symbol. First, the character ID sequence is converted into a sequence of character vectors with an embedding layer, and then the positional encoding scaled by a learnable scalar parameter is added to the vectors <ref type="bibr" target="#b3">[4]</ref>. This process is a TTS implementation of EncPre(·) in Eq. (1). Finally, the encoder EncBody(·) in Eq. (2) transforms this input sequence into a sequence of encoded features for the decoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">TTS decoder architecture</head><p>The inputs of the decoder in TTS are a sequence of encoder features and a sequence of log-mel filterbank features. In training, ground-truth log-mel filterbank features are used with an teacherforcing manner while in inference, predicted ones are used with an autoregressive manner. First, the target sequence of 80-dim log-mel filterbank features is converted into a sequence of hidden features by Prenet <ref type="bibr" target="#b14">[15]</ref> as a TTS implementation of DecPre(·) in Eq. (3). This network consists of two linear layers with 256 units, a ReLU activation function, and dropout followed by a projection linear layer with d att units. Since it is expected that the hidden representations converted by Prenet are located in the similar feature space to that of encoder features, Prenet helps to learn a diagonal encoder-decoder attention <ref type="bibr" target="#b3">[4]</ref>. Then the decoder DecBody(·) in Eq. (4), whose architecture is the same as the encoder, transforms the sequence of encoder features and that of hidden features into a sequence of decoder features. Two linear layers are applied for each frame of Y d to calculate the target feature and the probability of the EOS, respectively. Finally, Postnet <ref type="bibr" target="#b14">[15]</ref> is applied to the sequence of predicted target features to predict its components in detail. Postnet is a five-layer CNN, each layer of which is a 1d convolution with 256 channels and a kernel size of 5 followed by batch normalization, a tanh activation function, and dropout. These modules are a TTS implementation of DecPost(·) in Eq. (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">TTS training and decoding</head><p>In TTS training, the whole network is optimized to minimize two loss functions in TTS; 1) L1 loss for the target features and 2) binary cross entropy (BCE) loss for the probability of the EOS. To address the issue of class imbalance in the calculation of the BCE, a constant weight (e.g. 5) is used for a positive sample <ref type="bibr" target="#b3">[4]</ref>.</p><p>Additionally, we apply a guided attention loss <ref type="bibr" target="#b23">[24]</ref> to accelerate the learning of diagonal attention to only the two heads of two layers from the target side. This is because it is known that the encoderdecoder attention matrices are diagonal in only certain heads of a few layers from the target side <ref type="bibr" target="#b3">[4]</ref>. We do not introduce any hyperparameters to balance the three loss values. We simply add them all together.</p><p>In inference, the network predicts the target feature of the next frame in an autoregressive manner. And if the probability of the EOS becomes higher than a certain threshold (e.g. 0.5), the network will stop the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ASR EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Dataset</head><p>In <ref type="table">Table 1</ref>, we summarize the 15 datasets we used in our ASR experiment. Our experiment covered various topics in ASR including recording (clean, noisy, far-field, etc), language (English, Japanese, Mandarin Chinese, Spanish, Italian) and size (10 -960 hours). Except for JSUT <ref type="bibr" target="#b24">[25]</ref> and Fisher-CALLHOME Spanish, our data preparation scripts are based on Kaldi's "s5x" recipe <ref type="bibr" target="#b6">[7]</ref>. Technically, we tuned all the configurations (e.g., feature extraction, SentencePiece <ref type="bibr" target="#b18">[19]</ref>, language modeling, decoding, data augmentation <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>) except for the training stage to their optimum in the existing RNN-based system. We used data augmentation for several corpora. For example, we applied speed perturbation <ref type="bibr" target="#b26">[27]</ref> at ratio 0.9, 1.0 and 1.1 to CSJ, CHiME4, Fisher-CALLHOME Spanish, HKUST, and TED-LIUM2/3, and we also applied SpecAugment <ref type="bibr" target="#b25">[26]</ref> to Aurora4, LibriSpeech, TED-LIUM2/3 and WSJ. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Settings</head><p>We adopted the same architecture for Transformer in <ref type="bibr" target="#b40">[41]</ref> (e = 12, d = 6, d ff = 2048, d head = 4, d att = 256) for every corpus except for the largest, LibriSpeech (d head = 8, d att = 512). For RNN, we followed our existing best architecture configured on each corpus as in previous studies <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b41">[42]</ref>.</p><p>Transformer requires a different optimizer configuration from RNN because Transformer's training iteration is eight times faster and its update is more fine-grained than RNN. For RNN, we followed existing best systems for each corpus using Adadelta <ref type="bibr" target="#b42">[43]</ref> with early stopping. To train Transformer, we basically followed the previous literature <ref type="bibr" target="#b1">[2]</ref> (e.g., dropout, learning rate, warmup steps). We did not use development sets for early stopping in Transformer. We simply ran 20 -200 epochs (mostly 100 epochs) and averaged the model parameters stored at the last 10 epochs as the final model.</p><p>We conducted our training on a single GPU for larger corpora such as LibriSpeech, CSJ and TED-LIUM3. We also confirmed that the emulation of multiple GPUs using accumulating gradients over multiple forward/backward steps <ref type="bibr" target="#b4">[5]</ref> could result in similar performance with those corpora. In the decoding stage, Transformer and RNN share the same configuration for each corpus, for example, beam size (e.g., 20 -40), CTC weight λ (e.g., 0.3), and LM weight γ (e.g., 0.3 -1.0) introduced in Section 4.3.  <ref type="table" target="#tab_1">Table 2</ref> summarizes the ASR results in terms of character/word error rate (CER/WER) on each corpora. It shows that Transformer outperforms RNN on 13/15 corpora in our experiment. Although our system has no pronunciation dictionary, part-of-speech tag nor alignment-based data cleaning unlike Kaldi, our Transformer provides comparable CER/WERs to the HMM-based system, Kaldi on 7/12 corpora. We conclude that Transformer has ability to outperform the RNN-based end-to-end system and the DNN/HMM-based system even in low resource (JSUT), large resource (LibriSpeech, CSJ), noisy (AURORA4) and far-field (REVERB) tasks. <ref type="table" target="#tab_2">Table 3</ref> also summarizes the LibriSpeech ASR benchmark with ours and other reports because it is one of the most competitive task. Our transformer results are comparable to the best performance in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. <ref type="figure" target="#fig_0">Fig. 2</ref> shows an ASR training curve obtained with multiple GPUs on LibriSpeech. We observed that Transformer trained with a larger minibatch became more accurate while RNN did not. On the other hand, when we use a smaller minibatch for Transformer, it typically became under-fitted after the warmup steps. In this task, Transformer achieved the best accuracy provided by RNN about eight times faster than RNN with a single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Discussion</head><p>We summarize the training tips we observed in our experiment:</p><p>• When Transformer suffers from under-fitting, we recommend increasing the minibatch size because it also results in a faster training time and better accuracy simultaneously unlike any other hyperparameters. • The accumulating gradient strategy <ref type="bibr" target="#b4">[5]</ref> can be adopted to emulate the large minibatch if multiple GPUs are unavailable. • While dropout did not improve the RNN results, it is essential for Transformer to avoid over-fitting. • We tried several data augmentation methods <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. They greatly improved both Transformer and RNN. • The best decoding hyperparameters γ, λ for RNN are generally the best for Transformer.</p><p>Transformer's weakness is decoding. It is much slower than Kaldi's system because the self-attention requires O(n 2 ) in a naive implementation, where n is the speech length. To directly compare the performance with DNN-HMM based ASR systems, we need to develop a faster decoding algorithm for Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">MULTILINGUAL ASR EXPERIMENTS</head><p>This section compares the ASR performance of RNN and Transformer in a multilingual setup given the success of Transformer for <ref type="table">Table 1</ref>. ASR dataset description. Names listed in "test sets" correspond to ASR results in <ref type="table" target="#tab_1">Table 2</ref>. We enlarged corpora marked with (*) by the external WSJ train si284 dataset (81 hours). <ref type="bibr">dataset</ref> language hours speech test sets AISHELL <ref type="bibr" target="#b27">[28]</ref> zh 170 read dev / test AURORA4 <ref type="bibr" target="#b28">[29]</ref> (*) en 15 noisy read (dev 0330) A / B / C / D CSJ <ref type="bibr" target="#b29">[30]</ref> ja 581 spontaneous eval1 / eval2 / eval3 CHiME4 <ref type="bibr" target="#b30">[31]</ref> (*) en 108 noisy far-field multi-ch read dt05 simu / dt05 real / et05 simu / et05 real CHiME5 <ref type="bibr" target="#b31">[32]</ref> en 40 noisy far-field multi-ch conversational dev worn / kinect Fisher-CALLHOME Spanish es 170 telephone conversational dev / dev2 / test / devtest / evltest HKUST <ref type="bibr" target="#b32">[33]</ref> zh 200 telephone conversational dev JSUT <ref type="bibr" target="#b24">[25]</ref> ja 10 read (our split) LibriSpeech <ref type="bibr" target="#b33">[34]</ref> en 960 clean/noisy read dev clean / dev other / test clean / test other REVERB <ref type="bibr" target="#b34">[35]</ref> (*) en 124 far-field multi-ch read et near / et far SWITCHBOARD <ref type="bibr" target="#b35">[36]</ref> en 260 telephone conversational (eval2000) callhm / swbd TED-LIUM2 <ref type="bibr" target="#b36">[37]</ref> en 118 spontaneous dev / test TED-LIUM3 <ref type="bibr" target="#b37">[38]</ref> en 452 spontaneous dev / test VoxForge <ref type="bibr" target="#b38">[39]</ref> it 16 read (our split) WSJ <ref type="bibr" target="#b39">[40]</ref> en 81 read dev93 / eval92  the monolingual ASR tasks in the previous section. In accordance with <ref type="bibr" target="#b45">[46]</ref>, we prepared 10 different languages, namely WSJ (English), CSJ (Japanese) <ref type="bibr" target="#b29">[30]</ref>, HKUST <ref type="bibr" target="#b32">[33]</ref> (Mandarin Chinese), and VoxForge (German, Spanish, French, Italian, Dutch, Portuguese, Russian). The model is based on a single multilingual model, where the parameters are shared across all the languages and whose output units include the graphemes of all 10 languages (totally 5,297 graphemes and special symbols). We used a default setup for both RNN and Transformer introduced in Section 7.2 without RNNLM shallow fusion <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure">Figure 3</ref> clearly shows that our Transformer significantly outperformed our RNN in 9 languages. It realized a more than 10% relative improvement in 8 languages and with the largest value of 28.0% for relative improvement in VoxForge Italian. When compared with the RNN result reported in <ref type="bibr" target="#b45">[46]</ref>, which used a deeper BLSTM (7 layer) and RNNLM, our Transformer still provided superior performance in 9 languages. From this result, we can conclude that Transformer also outperforms RNN in multilingual end-to-end ASR.  <ref type="figure">Fig. 3</ref>. Comparison of multilingual end-to-end ASR with the RNN in Watanabe et al. <ref type="bibr" target="#b45">[46]</ref>, ESPnet RNN, and ESPnet Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">SPEECH TRANSLATION EXPERIMENTS</head><p>Our baseline end-to-end ST RNN is based on <ref type="bibr" target="#b22">[23]</ref>, which is similar to the RNN structure used in our ASR system, but we did not use a convolutional LSTM layer in the original paper. The configuration of our ST Transformer was the same as that of our ASR system. We conducted our ST experiment on the Fisher-CALLHOME English-Spanish corpus <ref type="bibr" target="#b46">[47]</ref>. Our Transformer improved the BLEU score to 17.2 from our RNN baseline BLEU 16.5 on the CALL-HOME "evltest" set. While training Transformer, we observed more serious under-fitting than with RNN. The solution for this is to use the pretrained encoder from our ASR experiment since the ST dataset contains Fisher-CALLHOME Spanish corpus used in our ASR experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">TTS EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.">Settings</head><p>Our baseline RNN-based TTS model is Tacotron 2 <ref type="bibr" target="#b14">[15]</ref>. We followed its model and optimizer setting. We reuse existing TTS recipes including those for data preparation and waveform generation that we configured to be the best for RNN. We configured our Transformer-based configurations introduced in Section 3 as follows: e = 6, d = 6, d att = 384, d ff = 1536, d head = 4. The input for both systems was the sequence of characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.">Results</head><p>We compared Transformer and RNN based TTS using two corpora: M-AILABS <ref type="bibr" target="#b47">[48]</ref> (Italian, 16 kHz, 31 hours) and LJSpeech <ref type="bibr" target="#b48">[49]</ref> (English, 22 kHz, 24 hours). A single Italian male speaker (Riccardo) was used in the case of M-AILABS. <ref type="figure">Figures 4 and 5</ref> show training curves in the two corpora. In these figures, Transformer and RNN provide similar L1 loss convergence. As seen in ASR, we observed that a larger minibatch results in better validation L1 loss for Transformer and faster training, while it has a detrimental effect on the L1 loss for RNN. We also provide generated speech melspectrograms in <ref type="figure" target="#fig_3">Fig. 6 and 7</ref>  <ref type="bibr" target="#b2">3</ref> . We conclude that Transformer-based TTS can achieve almost the same performance as RNN-based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3.">Discussion</head><p>Our lessons for training Transformer in TTS are as follows:</p><p>• It is possible to accelerate TTS training by using a large minibatch as well as ASR if a lot of GPUs are available. • The validation loss value, especially BCE loss, could be overfitted more easily with Transformer. We recommend monitoring attention maps rather than the loss when checking its convergence. • Some heads of attention maps in Transformer are not always diagonal as found with Tacotron 2. We needed to select where to apply the guided attention loss <ref type="bibr" target="#b23">[24]</ref>.  • Decoding filterbank features with Transformer is also slower than with RNN (6.5 ms vs 78.5 ms per frame, on CPU w/ single thread). We also tried FastSpeech <ref type="bibr" target="#b49">[50]</ref>, which realizes nonautoregressive Transformer-based TTS. It greatly improves the decoding speed (0.6 ms per frame, on CPU w/ single thread) and generates comparable quality of speech with the autoregressive Transformer. • A reduction factor introduced in <ref type="bibr" target="#b50">[51]</ref> was also effective for Transformer. It can greatly reduce training and inference time but slightly degrades the quality.</p><p>As future work, we need further investigation of the trade off between training speed and quality, and the introduction of ASR techniques (e.g., data augmentation, speech enhancement) for TTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">SUMMARY</head><p>We presented a comparative study of Transformer and RNN in speech applications with various corpora, namely ASR (15 monolingual + one multilingual), ST (one corpus), and TTS (two corpora). In our experiments on these tasks, we obtained the promising results including huge improvements in many ASR tasks and explained how we improved our models. We believe that the reproducible recipes, pretrained models and training tips described in this paper will accelerate Transformer research directions on speech applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>best acc 0.930060) Transformer 2GPU (best acc 0.931801) Transformer 4GPU (best acc 0.948303) RNN 1GPU (best acc 0.905068) RNN 2GPU (best acc 0.885973) RNN 4GPU (best acc 0.895651) ASR training curve with LibriSpeech dataset. Minibatches had the maximum number of utterances for each models on GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>TTS training curve on M-AILABS. TTS training curve on LJSpeech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Samples of mel-spectrograms on M-AILABs. (top) groundtruth, (middle) Tacotron 2 sample, (bottom) Transformer sample. The input text is "E PERCHÈ SUBITO VIENE IN MENTE CHE IDDIO NON PUÒ AVER FATTO UNA COSA INGIUSTA".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Samples of mel-spectrograms on LJSpeech. (top) groundtruth, (middle) Tacotron 2 sample, (bottom) Transformer sample. The input text is "IS NOT CONSISTENT WITH THE STANDARDS WHICH THE RESPONSIBILITIES OF THE SECRET SERVICE RE-QUIRE IT TO MEET.".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>ASR results of char/word error rates. Results marked with (*) were evaluated in our environment because the official results were not provided. Kaldi official results were retrieved from the version "c7876a33".</figDesc><table><row><cell>dataset</cell><cell cols="2">token error</cell><cell>Kaldi (s5)</cell><cell>ESPnet RNN (ours)</cell><cell>ESPnet Transformer (ours)</cell></row><row><cell>AISHELL</cell><cell>char</cell><cell>CER</cell><cell>N/A / 7.4</cell><cell>6.8 / 8.0</cell><cell>6.0 / 6.7</cell></row><row><cell>AURORA4</cell><cell>char</cell><cell cols="2">WER (*) 3.6 / 7.7 / 10.0 / 22.3</cell><cell>3.5 / 6.4 / 5.1 / 12.3</cell><cell>3.3 / 6.0 / 4.5 / 10.6</cell></row><row><cell>CSJ</cell><cell>char</cell><cell>CER</cell><cell>(*) 7.5 / 6.3 / 6.9</cell><cell>6.6 / 4.8 / 5.0</cell><cell>5.7 / 4.1 / 4.5</cell></row><row><cell>CHiME4</cell><cell>char</cell><cell>WER</cell><cell>6.8 / 5.6 / 12.1 / 11.4</cell><cell>9.5 / 8.9 / 18.3 / 16.6</cell><cell>9.6 / 8.2 / 15.7 / 14.5</cell></row><row><cell>CHiME5</cell><cell>char</cell><cell>WER</cell><cell>47.9 / 81.3</cell><cell>59.3 / 88.1</cell><cell>60.2 / 87.1</cell></row><row><cell>Fisher-CALLHOME Spanish</cell><cell>char</cell><cell>WER</cell><cell cols="3">N/A 27.9 / 27.8 / 25.4 / 47.2 / 47.9 27.0 / 26.3 / 24.4 / 45.3 / 46.2</cell></row><row><cell>HKUST</cell><cell>char</cell><cell>CER</cell><cell>23.7</cell><cell>27.4</cell><cell>23.5</cell></row><row><cell>JSUT</cell><cell>char</cell><cell>CER</cell><cell>N/A</cell><cell>20.6</cell><cell>18.7</cell></row><row><cell>LibriSpeech</cell><cell>BPE</cell><cell>WER</cell><cell>3.9 / 10.4 / 4.3 / 10.8</cell><cell>3.1 / 9.9 / 3.3 / 10.8</cell><cell>2.2 / 5.6 / 2.6 / 5.7</cell></row><row><cell>REVERB</cell><cell>char</cell><cell>WER</cell><cell>18.2 / 19.9</cell><cell>24.1 / 27.2</cell><cell>15.5 / 19.0</cell></row><row><cell>SWITCHBOARD</cell><cell>BPE</cell><cell>WER</cell><cell>18.1 / 8.8</cell><cell>28.5 / 15.6</cell><cell>18.1 / 9.0</cell></row><row><cell>TED-LIUM2</cell><cell>BPE</cell><cell>WER</cell><cell>9.0 / 9.0</cell><cell>11.2 / 11.0</cell><cell>9.3 / 8.1</cell></row><row><cell>TED-LIUM3</cell><cell>BPE</cell><cell>WER</cell><cell>6.2 / 6.8</cell><cell>14.3 / 15.0</cell><cell>9.7 / 8.0</cell></row><row><cell>VoxForge</cell><cell>char</cell><cell>CER</cell><cell>N/A</cell><cell>12.9 / 12.6</cell><cell>9.4 / 9.1</cell></row><row><cell>WSJ</cell><cell>char</cell><cell>WER</cell><cell>4.3 / 2.3</cell><cell>7.0 / 4.7</cell><cell>6.8 / 4.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of the Librispeech ASR benchmark</figDesc><table><row><cell></cell><cell cols="4">dev clean dev other test clean test other</cell></row><row><cell>RWTH (E2E) [44]</cell><cell>2.9</cell><cell>8.8</cell><cell>3.1</cell><cell>9.8</cell></row><row><cell>RWTH (HMM) [45]</cell><cell>2.3</cell><cell>5.2</cell><cell>2.7</cell><cell>5.7</cell></row><row><cell>Google SpecAug. [26]</cell><cell>N/A</cell><cell>N/A</cell><cell>2.5</cell><cell>5.8</cell></row><row><cell>ESPnet Transformer (ours)</cell><cell>2.2</cell><cell>5.6</cell><cell>2.6</cell><cell>5.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/espnet/espnet arXiv:1909.06317v2 [cs.CL] 28 Sep 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We chose datasets to apply these data augmentation methods by preliminary experiments with our RNN-based system.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our audio samples generated by Tacotron 2, Transformer, and Fast-Speech are available at https://bit.ly/329gif5</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech-transformer: A norecurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5884" to="5888" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end speech translation with the transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Cross</forename><surname>Vila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A R</forename><surname>Fonollosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IberSPEECH</title>
		<meeting>IberSPEECH</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="60" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation</title>
		<meeting>the Third Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tensor2Tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the 13th Conference of the Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ESPnet: End-toend speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Enrique</forename><surname>Yalta Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training tips for the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prague Bull. Math. Linguistics</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="43" to="70" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparison of transformer and recurrent neural networks on multilingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lakew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Syllable-based sequence-to-sequence speech recognition with the transformer in Mandarin Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="791" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4960" to="4964" />
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning wavenet on MEL spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4779" to="4783" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. ACM International Conference Proceeding Series</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition with word-based rnn language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop, SLT 2018</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A pitch extraction algorithm tuned for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babaali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Riedhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2494" to="2498" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="949" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models can directly translate foreign speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2625" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Uenoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4784" to="4788" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">JSUT corpus: Free large-scale japanese speech corpus for end-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takamichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saruwatari</surname></persName>
		</author>
		<idno>abs/1711.00354</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>ArXiv</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aurora working group: Dsr front end lvcsr evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Picone</surname></persName>
		</author>
		<idno>au/384/02</idno>
	</analytic>
	<monogr>
		<title level="j">Inst. for Signal &amp; Inform. Process</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Mississippi State Univ., Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spontaneous speech corpus of Japanese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maekawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koiso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Language Resources and Evaluation (LREC&apos;00)</title>
		<meeting>the Second International Conference on Language Resources and Evaluation (LREC&apos;00)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The third CHiME speech separation and recognition challenge: Analysis and outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="605" to="626" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The fifth &apos;CHiME&apos; speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1561" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">HKUST/MTS: A very large scale mandarin telephone speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="724" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lib-riSpeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The REVERB challenge: A benchmark task for reverberation-robust asr techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kellermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Leutnant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Era for Robust Speech Recognition: Exploiting Deep Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SWITCH-BOARD: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">TED-LIUM: An automatic speech recognition dedicated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deleglise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jokisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Potapova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Computer</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voxforge</surname></persName>
		</author>
		<ptr target="http://www.voxforge.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The design for the Wall Street Journal-based CSR corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Speech and Natural Language, ser. HLT &apos;91</title>
		<meeting>the Workshop on Speech and Natural Language, ser. HLT &apos;91</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1408" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Language modeling with deep transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04226</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">RWTH ASR systems for Lib-riSpeech: Hybrid vs attention-w/o data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03072</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Language independent end-to-end architecture for joint language identification and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="265" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved speech-to-text translation with the Fisher and Callhome Spanish-English speech translation corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Solak</surname></persName>
		</author>
		<ptr target="https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/" />
		<title level="m">The M-AILABS speech dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<title level="m">The LJ Speech dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">FastSpeech: Fast, robust and controllable text to speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09263</idno>
		<idno>arXiv:1905.09263</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tacotron: Towards End-to-End Speech Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4006" to="4010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
