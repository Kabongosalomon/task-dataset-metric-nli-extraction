<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
							<email>jjylee@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
							<email>francky@mit.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted as a conference paper at NAACL 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Short-text classification is an important task in many areas of natural language processing, including sentiment analysis, question answering, or dialog management. Many different approaches have been developed for short-text classification, such as using Support Vector Machines (SVMs) with rule-based features <ref type="bibr" target="#b17">(Silva et al., 2011)</ref>, combining SVMs with naive Bayes <ref type="bibr" target="#b18">(Wang and Manning, 2012)</ref>, and building dependency trees with Conditional Random Fields <ref type="bibr" target="#b13">(Nakagawa et al., 2010)</ref>. Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs) <ref type="bibr" target="#b10">(Kim, 2014;</ref><ref type="bibr" target="#b1">Blunsom et al., 2014;</ref><ref type="bibr" target="#b1">Kalchbrenner et al., 2014)</ref> and recursive neural networks <ref type="bibr" target="#b17">(Socher et al., 2012)</ref>.</p><p>Most ANN systems classify short texts in isolation, i.e., without considering preceding short texts. * These authors contributed equally to this work. However, short texts usually appear in sequence (e.g., sentences in a document or utterances in a dialog), therefore using information from preceding short texts may improve the classification accuracy. Previous works on sequential short-text classification are mostly based on non-ANN approaches, such as Hidden Markov Models (HMMs) (Reithinger and <ref type="bibr" target="#b14">Klesen, 1997)</ref>, <ref type="bibr">(Stolcke et al., 2000)</ref>, maximum entropy <ref type="bibr" target="#b0">(Ang et al., 2005)</ref>, and naive Bayes <ref type="bibr">(Lendvai and Geertzen, 2007)</ref>.</p><p>Inspired by the performance of ANN-based systems for non-sequential short-text classification, we introduce a model based on recurrent neural networks (RNNs) and CNNs for sequential short-text classification, and evaluate it on the dialog act classification task. A dialog act characterizes an utterance in a dialog based on a combination of pragmatic, semantic, and syntactic criteria. Its accurate detection is useful for a range of applications, from speech recognition to automatic summarization <ref type="bibr">(Stolcke et al., 2000)</ref>. Our model achieves state-of-the-art results on three different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our model comprises two parts. The first part generates a vector representation for each short text using either the RNN or CNN architecture, as discussed in Section 2.1 and <ref type="figure" target="#fig_0">Figure 1</ref>. The second part classifies the current short text based on the vector representations of the current as well as a few preceding short texts, as presented in Section 2.2 and <ref type="figure">Figure 2</ref>.</p><p>We denote scalars with italic lowercases (e.g., k, b f ), vectors with bold lowercases (e.g., s, x i ), and matrices with italic uppercases (e.g., W f ). We    <ref type="figure">Figure 2</ref>: Four instances of the two-layer feedforward ANN used for predicting the probability distribution over the classes zi for the i th short-text Xi. S2V stands for short text to vector, which is the RNN/CNN architecture that generates si from Xi. From left to right, the history sizes (d1, d2) are (0, 0), (2, 0), (0, 2) and (1, 1). (0, 0) corresponds to the non-sequential classification case. use the colon notation v i:j to denote the sequence of vectors (v i , v i+1 , . . . , v j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Short-text representation</head><p>A given short text of length is represented as the sequence of m-dimensional word vectors x 1: , which is used by the RNN or CNN model to produce the n-dimensional short-text representation s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">RNN-based short-text representation</head><p>We use a variant of RNN called Long Short Term Memory (LSTM) <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref>. For the t th word in the short-text, an LSTM takes as input x t , h t−1 , c t−1 and produces h t , c t based on the following formulas:</p><formula xml:id="formula_0">i t = σ(W i x t + U i h t−1 + b i ) f t = σ(W f x t + U f h t−1 + b f ) c t = tanh(W c x t + U c h t−1 + b c ) c t = f t c t−1 + i t c t o t = σ(W o x t + U o h t−1 + b o ) h t = o t tanh(c t )</formula><p>where W j ∈ R n×m , U j ∈ R n×n are weight matrices and b j ∈ R n are bias vectors, for j ∈ {i, f, c, o}.</p><p>The symbols σ(·) and tanh(·) refer to the elementwise sigmoid and hyperbolic tangent functions, and is the element-wise multiplication. h 0 = c 0 = 0.</p><p>In the pooling layer, the sequence of vectors h 1: output from the RNN layer are combined into a single vector s ∈ R n that represents the short-text, using one of the following mechanisms: last, mean, and max pooling. Last pooling takes the last vector, i.e., s = h , mean pooling averages all vectors, i.e., s = 1 t=1 h t , and max pooling takes the elementwise maximum of h 1: .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">CNN-based short-text representation</head><p>Using a filter W f ∈ R h×m of height h, a convolution operation on h consecutive word vectors starting from t th word outputs the scalar feature</p><formula xml:id="formula_1">c t = ReLU(W f • X t:t+h−1 + b f ) where X t:t+h−1 ∈ R h×m is the matrix whose i th row is x i ∈ R m , and b f ∈ R is a bias.</formula><p>The symbol • refers to the dot product and ReLU(·) is the elementwise rectified linear unit function.</p><p>We perform convolution operations with n different filters, and denote the resulting features as c t ∈ R n , each of whose dimensions comes from a distinct filter. Repeating the convolution operations for each window of h consecutive words in the shorttext, we obtain c 1: −h+1 . The short-text representation s ∈ R n is computed in the max pooling layer, as the element-wise maximum of c 1: −h+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequential short-text classification</head><p>Let s i be the n-dimensional short-text representation given by the RNN or CNN architecture for the i th short text in the sequence. The sequence s i−d 1 −d 2 : i is fed into a two-layer feedforward ANN that predicts the class for the i th short text. The hyperparameters d 1 , d 2 are the history sizes used in the first and second layers, respectively.</p><p>The first layer takes as input s i−d 1 −d 2 : i and outputs the sequence y i−d 2 : i defined as</p><formula xml:id="formula_2">y j = tanh d 1 d=0 W −d s j−d + b 1 , ∀j ∈ [i − d 2 , i]</formula><p>where W 0 , W −1 , W −2 ∈ R k×n are the weight matrices, b 1 ∈ R k is the bias vector, y j ∈ R k is the class representation, and k is the number of classes for the classification task.</p><p>Similarly, the second layer takes as input the sequence of class representations y i−d 2 :i and outputs z i ∈ R k :</p><formula xml:id="formula_3">z i = softmax   d 2 j=0 W −j y i−j + b 2  </formula><p>where U 0 , U −1 , U −2 ∈ R k×k and b 2 ∈ R k are the weight matrices and bias vector.</p><p>The final output z i represents the probability distribution over the set of k classes for the i th shorttext: the j th element of z i corresponds to the probability that the i th short-text belongs to the j th class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets and Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate our model on the dialog act classification task using the following datasets:</p><p>•  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>The model is trained to minimize the negative loglikelihood of predicting the correct dialog acts of the utterances in the train set, using stochastic gradient descent with the Adadelta update rule <ref type="bibr" target="#b19">(Zeiler, 2012)</ref>. At each gradient descent step, weight matrices, bias vectors, and word vectors are updated. For regularization, dropout is applied after the pooling layer, and early stopping is used on the validation set with a patience of 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>To find effective hyperparameters, we varied one hyperparameter at a time while keeping the other ones fixed.  We initialized the word vectors with the 300dimensional word vectors pretrained with word2vec on Google News <ref type="bibr" target="#b12">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013b)</ref>   <ref type="table">Table 3</ref>: Accuracy (%) on different architectures and history sizes d1, d2. For each setting, we report average (minimum, maximum) computed on 5 runs. Sequential classification (d1 + d2 &gt; 0) outperforms non-sequential classification (d1 = d2 = 0). Overall, the CNN model outperformed the LSTM model for all datasets, albeit by a small margin except for SwDA. We also tried a variant of the LSTM model, gated recurrent units <ref type="bibr" target="#b2">(Cho et al., 2014)</ref>, but the results were generally lower than LSTM.</p><p>word vectors pretrained with GloVe on Twitter <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref> for MRDA and SwDA, as these choices yielded the best results among all publicly available word2vec, GloVe, SENNA <ref type="bibr" target="#b3">(Collobert, 2011;</ref><ref type="bibr" target="#b3">Collobert et al., 2011)</ref> and RNNLM <ref type="bibr" target="#b11">(Mikolov et al., 2011)</ref> word vectors. The effects of the history sizes d 1 and d 2 for the short-text and the class representations, respectively, are presented in <ref type="table">Table 3</ref> for both the LSTM and CNN models. In both models, increasing d 1 while keeping d 2 = 0 improved the performances by 1.3-4.2 percentage points. Conversely, increasing d 2 while keeping d 1 = 0 yielded better results, but the performance increase was less pronounced: incorporating sequential information at the short-text representation level was more effective than at the class representation level.</p><p>Using sequential information at both the shorttext representation level and the class representation level does not help in most cases and may even lower the performances. We hypothesize that shorttext representations contain richer and more general information than class representations due to their larger dimension. Class representations may not convey any additional information over shorttext representations, and are more likely to propagate errors from previous misclassifications. Interlabeler agreement --84.0 <ref type="table" target="#tab_6">Table 4</ref>: Accuracy (%) of our models and other methods from the literature. The majority class model predicts the most frequent class. SVM: <ref type="bibr" target="#b4">(Dernoncourt et al., 2016)</ref>. Graphical model: <ref type="bibr" target="#b7">(Ji and Bilmes, 2006)</ref>. Naive Bayes: <ref type="bibr">(Lendvai and Geertzen, 2007)</ref>. HMM: <ref type="bibr">(Stolcke et al., 2000)</ref>. Memory-based Learning: <ref type="bibr" target="#b15">(Rotaru, 2002)</ref>. All five models use features derived from transcribed words, as well as previous predicted dialog acts except for Naive Bayes. The interlabeler agreement could be obtained only for SwDA. For the CNN and LSTM models, the presented results are the test set accuracy of the run with the highest accuracy on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this article we have presented an ANN-based approach to sequential short-text classification. We demonstrate that adding sequential information improves the quality of the predictions, and the performance depends on what sequential information is used in the model. Our model achieves state-of-theart results on three different datasets for dialog act prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>RNN (left)  and CNN (right) architectures for generating the vector representation s of a short text x 1: . For CNN, Conv refers to convolution operations, and the filter height h = 3 is used in thisfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FF</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MRDA, we use the train/validation/test splits provided with the datasets. For DSTC 4 and SwDA, only the train/test splits are provided. 1Table 1presents statistics on the datasets.</figDesc><table><row><cell cols="2">For Dataset |C| |V |</cell><cell>Train</cell><cell>Validation</cell><cell>Test</cell></row><row><cell cols="2">DSTC 4 89 6k</cell><cell>24 (21k)</cell><cell>5 (5k)</cell><cell>6 (6k)</cell></row><row><cell>MRDA</cell><cell>5 12k</cell><cell>51 (78k)</cell><cell cols="2">11 (16k) 11 (15k)</cell></row><row><cell>SwDA</cell><cell cols="4">43 20k 1003 (193k) 112 (23k) 19 (5k)</cell></row><row><cell>DSTC 4: Dialog State Tracking Challenge 4 (Kim</cell><cell></cell><cell></cell><cell></cell></row><row><cell>et al., 2015; Kim et al., 2016).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>• MRDA: ICSI Meeting Recorder Dialog Act Cor-</cell><cell></cell><cell></cell><cell></cell></row></table><note>pus (Janin et al., 2003; Shriberg et al., 2004). The 5 classes are introduced in (Ang et al., 2005).• SwDA: Switchboard Dialog Act Corpus (Jurafsky et al., 1997).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset overview. |C| is the number of classes, |V | the vocabulary size. For the train, validation and test sets, we indicate the number of dialogs (i.e., sequences) followed by the number of utterances (i.e., short texts) in parenthesis.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>presents our hyperparameter choices.</figDesc><table><row><cell>Hyperparameter</cell><cell>Choice</cell><cell>Experiment Range</cell></row><row><cell>LSTM output dim. (n)</cell><cell>100</cell><cell>50 -1000</cell></row><row><cell>LSTM pooling</cell><cell>max</cell><cell>max, mean, last</cell></row><row><cell>LSTM direction</cell><cell>unidir.</cell><cell>unidir., bidir.</cell></row><row><cell>CNN num. of filters (n)</cell><cell>500</cell><cell>50 -1000</cell></row><row><cell>CNN filter height (h)</cell><cell>3</cell><cell>1 -10</cell></row><row><cell>Dropout rate</cell><cell>0.5</cell><cell>0 -1</cell></row><row><cell>Word vector dim. (m)</cell><cell>200, 300</cell><cell>25 -300</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Experiments ranges and choices of hyperparameters.</figDesc><table><row><cell>Unidir refers to the regular RNNs presented in Section 2.1.1,</cell></row><row><cell>and bidir refers to bidirectional RNNs introduced in (Schuster</cell></row><row><cell>and Paliwal, 1997).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>compares our results with the state-of-theart. Overall, our model shows competitive results, while requiring no human-engineered features. Rigorous comparisons are challenging to draw, as many important details such as text preprocessing and train/valid/test split may vary, and many studies fail to perform several runs despite the randomness in some parts of the training process, such as weight initialization.</figDesc><table><row><cell>Model</cell><cell cols="3">DSTC 4 MRDA SwDA</cell></row><row><cell>CNN</cell><cell>65.5</cell><cell>84.6</cell><cell>73.1</cell></row><row><cell>LSTM</cell><cell>66.2</cell><cell>84.3</cell><cell>69.6</cell></row><row><cell>Majority class</cell><cell>25.8</cell><cell>59.1</cell><cell>33.7</cell></row><row><cell>SVM</cell><cell>57.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Graphical model</cell><cell>-</cell><cell>81.3</cell><cell>-</cell></row><row><cell>Naive Bayes</cell><cell>-</cell><cell>82.0</cell><cell>-</cell></row><row><cell>HMM</cell><cell>-</cell><cell>-</cell><cell>71.0</cell></row><row><cell>Memory-based Learning</cell><cell>-</cell><cell>-</cell><cell>72.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All train/validation/test splits can be found at https:// github.com/Franck-Dernoncourt/naacl2016</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic dialog act segmentation and classification in multiparty meetings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP (1)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1061" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<idno type="arXiv">arXiv:1409.1259</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<editor>Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa</editor>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Encoderdecoder approaches. arXiv preprint</note>
	<note>On the properties of neural machine translation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno>number EPFL-CONF-192374</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adobe-MIT submission to the DSTC 4 Spoken Language Understanding pilot task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Workshop on Spoken Dialogue Systems (IWSDS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Dernoncourt et al.2016</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The ICSI meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Janin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">364</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings.(ICASSP&apos;03)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual. Institute of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">] Gang</forename><surname>Bilmes2006</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<editor>Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca</editor>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A convolutional neural network for modelling sentences</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>and Matthew Henderson. 2015. Dialog State Tracking Challenge 4: Handbook</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Fourth Dialog State Tracking Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Spoken Dialogue Systems (IWSDS)</title>
		<meeting>the 7th International Workshop on Spoken Dialogue Systems (IWSDS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Token-based chunking of turninternal dialogue act sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Lendvai and Geertzen2007] Piroska Lendvai and Jeroen Geertzen</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
	<note>Proceedings of the 8th SIGDIAL Workshop on Discourse and Dialogue</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rnnlm-recurrent neural network language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2011 ASRU Workshop</title>
		<meeting>of the 2011 ASRU Workshop</meeting>
		<imprint>
			<date type="published" when="2011-01" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Efficient estimation of word representations in vector space</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dependency tree-based sentiment classification using CRFs with hidden variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dialogue act classification using language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Reithinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Klesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<editor>EuroSpeech. Citeseer</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>GloVe: global vectors for word representation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dialog act tagging using memory-based learning. Term project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Rotaru</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="255" to="276" />
		</imprint>
		<respStmt>
			<orgName>University of Pittsburgh</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Paliwal1997</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="339" to="373" />
		</imprint>
		<respStmt>
			<orgName>Carol Van Ess-Dykema, and Marie Meteer</orgName>
		</respStmt>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2012] Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: An adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
