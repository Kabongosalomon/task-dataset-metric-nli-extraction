<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
							<email>faghri@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto and Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
							<email>fleet@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto and Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
							<email>kiros@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Brain Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<email>fidler@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto and Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>FAGHRI ET AL.: VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH H. N. 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new technique for learning visual-semantic embeddings for cross-modal retrieval. Inspired by hard negative mining, the use of hard negatives in structured prediction, and ranking loss functions, we introduce a simple change to common loss functions used for multi-modal embeddings. That, combined with fine-tuning and use of augmented data, yields significant gains in retrieval performance. We showcase our approach, VSE++, on MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods. On MS-COCO our approach outperforms state-ofthe-art methods by 8.8% in caption retrieval and 11.3% in image retrieval (at R@1).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Joint embeddings enable a wide range of tasks in image, video and language understanding. Examples include shape-image embeddings ( <ref type="bibr" target="#b19">[20]</ref>) for shape inference, bilingual word embeddings ( <ref type="bibr" target="#b37">[38]</ref>), human pose-image embeddings for 3D pose inference ( <ref type="bibr" target="#b18">[19]</ref>), fine-grained recognition ( <ref type="bibr" target="#b24">[25]</ref>), zero-shot learning ( <ref type="bibr" target="#b8">[9]</ref>), and modality conversion via synthesis ( <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>). Such embeddings entail mappings from two (or more) domains into a common vector space in which semantically associated inputs (e.g., text and images) are mapped to similar locations. The embedding space thus represents the underlying domain structure, where location and often direction are semantically meaningful.</p><p>Visual-semantic embeddings have been central to image-caption retrieval and generation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, and visual question-answering <ref type="bibr" target="#b21">[22]</ref>. One approach to visual question-answering, for example, is to first describe an image by a set of captions, and then to find the nearest caption in response to a question ( <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref>). For image synthesis from text, one could map from text to the joint embedding space, and then back to image space ( <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>).</p><p>Here we focus on visual-semantic embeddings for cross-modal retrieval; i.e. the retrieval of images given captions, or of captions for a query image. As is common in retrieval, we measure performance by R@K, i.e., recall at K -the fraction of queries for which the correct c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</p><p>* Work done while a Ph.D. student at the University of Toronto.</p><p>arXiv:1707.05612v4 <ref type="bibr">[cs.</ref>LG] 29 Jul 2018 item is retrieved in the closest K points to the query in the embedding space (K is usually a small integer, often 1). More generally, retrieval is a natural way to assess the quality of joint embeddings for image and language data ( <ref type="bibr" target="#b10">[11]</ref>). The basic problem is one of ranking; the correct target(s) should be closer to the query than other items in the corpus, not unlike learning to rank problems (e.g., <ref type="bibr" target="#b17">[18]</ref>), and maxmargin structured prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>. The formulation and model architecture in this paper are most closely related to those of <ref type="bibr" target="#b14">[15]</ref>, learned with a triplet ranking loss. In contrast to that work, we advocate a novel loss, the use of augmented data, and fine-tuning, which, together, produce a significant increase in caption retrieval performance over the baseline ranking loss on well-known benchmark data. We outperform the best reported result on MS-COCO by almost 9%. We also show that the benefit of a more powerful image encoder, with finetuning, is amplified with the use of our stronger loss function. We refer to our model as VSE++. To ensure reproducibility, our code is publicly available 1 .</p><p>Our main contribution is to incorporate hard negatives in the loss function. This was inspired by the use of hard negative mining in classification tasks ( <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>), and by the use of hard negatives for improving image embeddings for face recognition ( <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>). Minimizing a loss function using hard negative mining is equivalent to minimizing a modified non-transparent loss function with uniform sampling. We extend the idea with the explicit introduction of hard negatives in the loss for multi-modal embeddings, without any additional cost of mining.</p><p>We also note that our formulation complements other recent articles that propose new architectures or similarity functions for this problem. To this end, we demonstrate improvements to <ref type="bibr" target="#b30">[31]</ref>. Among other methods that could be improved with a modified loss, <ref type="bibr" target="#b31">[32]</ref> propose an embedding network to fully replace the similarity function used for the ranking loss. An attention mechanism on both images and captions is used by <ref type="bibr" target="#b23">[24]</ref>, where the authors sequentially and selectively focus on a subset of words and image regions to compute the similarity. In <ref type="bibr" target="#b11">[12]</ref>, the authors use a multi-modal context-modulated attention mechanism to compute the similarity between images and captions. Our proposed loss function and triplet sampling could be extended and applied to other such problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Visual-Semantic Embeddings</head><p>For image-caption retrieval the query is a caption and the task is to retrieve the most relevant image(s) from a database. Alternatively, the query may be an image, and the task is to retrieves relevant captions. The goal is to maximize recall at K (R@K), i.e., the fraction of queries for which the most relevant item is ranked among the top K items returned.</p><p>Let S = {(i n , c n )} N n=1 be a training set of image-caption pairs. We refer to (i n , c n ) as positive pairs and (i n , c m =n ) as negative pairs; i.e., the most relevant caption to the image i n is c n and for caption c n , it is the image i n . We define a similarity function s(i, c) ∈ R that should, ideally, give higher similarity scores to positive pairs than negatives. In caption retrieval, the query is an image and we rank a database of captions based on the similarity function; i.e., R@K is the percentage of queries for which the positive caption is ranked among the top K captions using s(i, c). Likewise for image retrieval. In what follows the similarity function is defined on the joint embedding space. This differs from other formulations, such as <ref type="bibr" target="#b31">[32]</ref>, which use a similarity network to directly classify an image-caption pair as matching or non-matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual-Semantic Embedding</head><p>Let φ (i; θ φ ) ∈ R D φ be a feature-based representation computed from image i (e.g. the representation before logits in VGG19 ( <ref type="bibr" target="#b27">[28]</ref>) or ResNet152 ([10])). Similarly, let ψ(c; θ ψ ) ∈ R D ψ be a representation of caption c in a caption embedding space (e.g. a GRU-based text encoder). Here, θ φ and θ ψ denote model parameters for the respective mappings to these initial image and caption representations.</p><p>Then, let the mappings into the joint embedding space be defined by linear projections:</p><formula xml:id="formula_0">f (i;W f , θ φ ) = W T f φ (i; θ φ ) (1) g(c;W g , θ ψ ) = W T g ψ(c; θ ψ )<label>(2)</label></formula><p>where W f ∈ R D φ ×D and W g ∈ R D ψ ×D . We further normalize f (i;W f , θ φ ), and g(c;W g , θ ψ ), to lie on the unit hypersphere. Finally, we define the similarity function in the joint embedding space to be the usual inner product:</p><formula xml:id="formula_1">s(i, c) = f (i;W f , θ φ ) · g(c;W g , θ ψ ) .<label>(3)</label></formula><p>Let θ = {W f ,W g , θ ψ } be the model parameters. If we also fine-tune the image encoder, then we would also include θ φ in θ .</p><p>Training entails the minimization of empirical loss with respect to θ , i.e., the cumulative loss over training data S = {(i n , c n )} N n=1 :</p><formula xml:id="formula_2">e(θ , S) = 1 N N ∑ n=1 (i n , c n )<label>(4)</label></formula><p>where (i n , c n ) is a suitable loss function for a single training exemplar. Inspired by the use of a triplet loss for image retrieval (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>), recent approaches to joint visual-semantic embeddings have used a hinge-based triplet ranking loss <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:id="formula_3">SH (i, c) = ∑ c [α − s(i, c) + s(i,ĉ)] + + ∑ i [α − s(i, c) + s(î, c)] + ,<label>(5)</label></formula><p>where α serves as a margin parameter, and [x] + ≡ max(x, 0). This hinge loss comprises two symmetric terms. The first sum is taken over all negative captionsĉ, given query i. The second is taken over all negative imagesî, given caption c. Each term is proportional to the expected loss (or violation) over sets of negative samples. If i and c are closer to one another in the joint embedding space than to any negative, by the margin α, the hinge loss is zero. In practice, for computational efficiency, rather than summing over all negatives in the training set, it is common to only sum over (or randomly sample) the negatives in a mini-batch of stochastic gradient descent <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>. The runtime complexity of computing this loss approximation is quadratic in the number of image-caption pairs in a mini-batch.</p><p>Of course there are other loss functions that one might consider. One is a pairwise hinge loss in which elements of positive pairs are encouraged to lie within a hypersphere of radius ρ 1 in the joint embedding space, while negative pairs should be no closer than ρ 2 &gt; ρ 1 . This is problematic as it constrains the structure of the latent space more than does the ranking loss, and it entails the use of two hyper-parameters which can be very difficult to set. Another possible approach is to use Canonical Correlation Analysis to learn W f and W g , thereby trying to preserve correlation between the text and images in the joint embedding (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>). By comparison, when measuring performance as R@K, for small K, a correlation-based loss will not give sufficient influence to the embedding of negative items in the local vicinity of positive pairs, which is critical for R@K. Notice that the hardest negative sample c is closer to i in (a). Assuming a zero margin, (b) has a higher loss with the SH loss compared to (a). The MH loss assigns a higher loss to (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Emphasis on Hard Negatives</head><p>Inspired by common loss functions used in structured prediction ( <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>), we focus on hard negatives for training, i.e., the negatives closest to each training query. This is particularly relevant for retrieval since it is the hardest negative that determines success or failure as measured by R@1.</p><p>Given a positive pair (i, c), the hardest negatives are given by i = arg max j =i s( j, c) and c = arg max d =c s(i, d). To emphasize hard negatives we define our loss as</p><formula xml:id="formula_4">MH (i, c) = max c α + s(i, c ) − s(i, c) + + max i α + s(i , c) − s(i, c) + .<label>(6)</label></formula><p>Like Eq. 5, this loss comprises two terms, one with i and one with c as queries. Unlike Eq. 5, this loss is specified in terms of the hardest negatives, c and i . We refer to the loss in Eq. 6 as Max of Hinges (MH) loss, and the loss in Eq. 5 as Sum of Hinges (SH) loss. There is a spectrum of loss functions from the SH loss to the MH loss. In the MH loss, the winner takes all the gradients, where instead we use re-weighted gradients of all the triplets. We only discuss the MH loss as it was empirically found to perform the best. One case in which the MH loss is superior to SH is when multiple negatives with small violations combine to dominate the SH loss. For example, <ref type="figure" target="#fig_0">Fig. 1</ref> depicts a positive pair together with two sets of negatives. In <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, a single negative is too close to the query, which may require a significant change to the mapping. However, any training step that pushes the hard negative away, might cause a number of small violating negatives, as in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Using the SH loss, these 'new' negatives may dominate the loss, so the model is pushed back to the first example in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. This may create local minima in the SH loss that may not be as problematic for the MH loss, which focuses on the hardest negative.</p><p>For computational efficiency, instead of finding the hardest negatives in the entire training set, we find them within each mini-batch. This has the same quadratic complexity as the complexity of the SH loss. With random sampling of the mini-batches, this approximation yields other advantages. One is that there is a high probability of getting hard negatives that are harder than at least 90% of the entire training set. Moreover, the loss is potentially robust to label errors in the training data because the probability of sampling the hardest negative over the entire training set is somewhat low.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Probability of Sampling the Hardest Negative</head><p>Let S = {(i n , c n )} N n=1 denote a training set of image-caption pairs, and let C = {c n } denote the set of captions. Suppose we draw M samples in a mini-batch, Q = {(i m , c m )} M m=1 , from S. Let the permutation, π m , on C refer to the rankings of captions according to the similarity function s(i m , c n ) for c n ∈ S \ {c m }. We can assume permutations, π m , are uncorrelated.</p><p>Given a query image, i m , we are interested in the probability of getting no captions from the 90th percentile of π m in the mini-batch. Assuming IID samples, this probability is simply .9 (M−1) , the probability that no sample in the mini-batch is from the 90th percentile. This probability tends to zero exponentially fast, falling below 1% for M ≥ 44. Hence, for large enough mini-batchs, with high probability we sample negative captions that are harder than 90% of the entire training set. The probability for the 99.9th percentile of π m tends to zero more slowly; it falls below 1% for M ≥ 6905, which is a relatively large mini-batch.</p><p>While we get strong signals by randomly sampling negatives within mini-batches, such sampling also provides some robustness to outliers, such as negative captions that better describe an image compared to the ground-truth caption. Mini-batches as small as 128 can provide strong enough training signal and robustness to label errors. Of course by increasing the mini-batch size, we get harder negative examples and possibly a stronger training signal. However, by increasing the mini-batch size, we lose the benefit of SGD in finding good optima and exploiting the gradient noise. This can lead to getting stuck in local optima or as observed by <ref type="bibr" target="#b26">[27]</ref>, extremely long training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Below we perform experiments with our approach, VSE++, comparing it to a baseline formulation with SH loss, denoted VSE0, and other state-of-the-art approaches. Essentially, the baseline formulation, VSE0, is similar to that in <ref type="bibr" target="#b14">[15]</ref>, denoted UVS.</p><p>We experiment with two image encoders: VGG19 by <ref type="bibr" target="#b27">[28]</ref> and ResNet152 by <ref type="bibr" target="#b9">[10]</ref>. In what follows, we use VGG19 unless specified otherwise. As in previous work we extract image features directly from FC7, the penultimate fully connected layer. The dimensionality of the image embedding, D φ , is 4096 for VGG19 and 2048 for ResNet152.</p><p>In more detail, we first resize the image to 256 × 256, and then use either a single crop of size 224 × 224 or the mean of feature vectors for multiple crops of similar size. We refer to training with one center crop as 1C, and training with 10 crops at fixed locations as 10C. These image features can be pre-computed once and reused. We also experiment with using a single random crop, denoted by RC. For RC, image features are computed on the fly. Recent works have mostly used RC/10C. In our preliminary experiments, we did not observe significant differences between RC/10C. As such, we perform most experiments with RC.</p><p>For the caption encoder, we use a GRU similar to the one used in <ref type="bibr" target="#b14">[15]</ref>. We set the dimensionality of the GRU, D ψ , and the joint embedding space, D, to 1024. The dimensionality of the word embeddings that are input to the GRU is set to 300.</p><p>We further note that in <ref type="bibr" target="#b14">[15]</ref>, the caption embedding is normalized, while the image embedding is not. Normalization of both vectors means that the similarity function is cosine similarity. In VSE++ we normalize both vectors. Not normalizing the image embedding changes the importance of samples. In our experiments, not normalizing the image embedding helped the baseline, VSE0, to find a better solution. However, VSE++ is not significantly affected by this normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate our method on the Microsoft COCO dataset ( <ref type="bibr" target="#b20">[21]</ref>) and the Flickr30K dataset ( <ref type="bibr" target="#b33">[34]</ref>). Flickr30K has a standard 30, 000 images for training. Following <ref type="bibr" target="#b12">[13]</ref>, we use 1000 images for validation and 1000 images for testing. We also use the splits of <ref type="bibr" target="#b12">[13]</ref> for MS-COCO. In this split, the training set contains 82, 783 images, 5000 validation and 5000 test images. However, there are also 30, 504 images that were originally in the validation set of MS-COCO but have been left out in this split. We refer to this set as rV. Some papers use rV for training (113, 287 training images in total) to further improve accuracy. We report results using both training sets. Each image comes with 5 captions. The results are reported by either averaging over 5 folds of 1K test images or testing on the full 5K test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Details of Training</head><p>We use the Adam optimizer <ref type="bibr" target="#b13">[14]</ref>. Models are trained for at most 30 epochs. Except for fine-tuned models, we start training with learning rate 0.0002 for 15 epochs, and then lower the learning rate to 0.00002 for another 15 epochs. The fine-tuned models are trained by taking a model trained for 30 epochs with a fixed image encoder, and then training it for 15 epochs with a learning rate of 0.00002. We set the margin to 0.2 for most experiments. We use a mini-batch size of 128 in all experiments. Notice that since the size of the training set for different models is different, the actual number of iterations in each epoch can vary. For evaluation on the test set, we tackle over-fitting by choosing the snapshot of the model that performs best on the validation set. The best snapshot is selected based on the sum of the recalls on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on MS-COCO</head><p>The results on the MS-COCO dataset are presented in <ref type="table" target="#tab_1">Table 1</ref>. To understand the effect of training and algorithmic variations we report ablation studies for the baseline VSE0 (see <ref type="table" target="#tab_3">Table 2</ref>). Our best result with VSE++ is achieved by using ResNet152 and fine-tuning the image encoder (row 1.11), where we see 21.2% improvement in R@1 for caption retrieval  Comparing VSE++ (ResNet, FT) to the current state-of-the-art on MS-COCO, 2WayNet (row 1.11 and row 1.5), we see 8.8% improvement in R@1 for caption retrieval and compared to sm-LSTM (row 1.11 and row 1.4), 11.3% improvement in image retrieval. We also report results on the full 5K test set of MS-COCO in rows 1.13 and 1.14.</p><p>Effect of the training set. We compare VSE0 and VSE++ by incrementally improving the training data. Comparing the models trained on 1C (rows 1.1 and 1.6), we only see 2.7% improvement in R@1 for image retrieval but no improvement in caption retrieval performance. However, when we train using RC (rows 1.7 and 2.2) or RC+rV (rows 1.8 and 2.3), we see that VSE++ gains an improvement of 5.9% and 5.1%, respectively, in R@1 for caption retrieval compared to VSE0. This shows that VSE++ can better exploit the additional data.</p><p>Effect of a better image encoding. We also investigate the effect of a better image encoder on the models. Row 1.9 and row 2.4 show the effect of fine-tuning the VGG19 image encoder. We see that the gap between VSE0 and VSE++ increases to 6.1%. If we use ResNet152 instead of VGG19 (row 1.10 and row 2.5), the gap is 5.6%. As for our best result, if we use ResNet152 and also fine-tune the image encoder (row 1.11 and row 2.6) the gap becomes 8.6%. The increase in the performance gap shows that the improved loss of VSE++ can better guide the optimization when a more powerful image encoder is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on Flickr30K</head><p>Tables 3 summarizes the performance on Flickr30K. We obtain 23.1% improvement in R@1 for caption retrieval and 17.6% improvement in R@1 for image retrieval (rows 3.1 and 3.17). We observed that VSE++ over-fits when trained with the pre-computed features of 1C. The reason is potentially the limited size of the Flickr30K training set. As explained in Sec. 3.2, we select a snapshot of the model before over-fitting occurs, based on performance with the validation set. Over-fitting does not occur when the model is trained using the RC training data. Our results show the improvements incurred by our MH loss persist across datasets, as well as across models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Improving Order Embeddings</head><p>Given the simplicity of our approach, our proposed loss function can complement the recent approaches that use more sophisticated model architectures or similarity functions.</p><p>Here we demonstrate the benefits of the MH loss by applying it to another approach to joint embeddings called order-embeddings <ref type="bibr" target="#b30">[31]</ref>. The main difference with the formulation above is the use of an asymmetric similarity function, i.e., s(i, c) = − max(0, g(c;W g , θ ψ ) − f (i;W f , θ φ )) 2 . Again, we simply replace their use of the SH loss by our MH loss. Like their experimental setting, we use the training set 10C+rV. For our Order++, we use the same learning schedule and margin as our other experiments. However, we use their training settings to train Order0. We start training with a learning rate of 0.001 for 15 epochs and lower the learning rate to 0.0001 for another 15 epochs. Like <ref type="bibr" target="#b30">[31]</ref> we use a margin of 0.05. Additionally, <ref type="bibr" target="#b30">[31]</ref> takes the absolute value of embeddings before computing the similarity function which we replicate only for Order0. <ref type="table" target="#tab_7">Table 4</ref> reports the results when the SH loss is replaced by the MH loss. We replicate their results using our Order0 formulation and get slightly better results (row 4.1 and row 4.3). We observe 4.5% improvement from Order0 to Order++ in R@1 for caption retrieval (row 4.3 and row 4.5). Compared to the improvement from VSE0 to VSE++, where the improvement on the 10C+rV training set is 1.8%, we gain an even higher improvement here. This shows that the MH loss can potentially improve numerous similar loss functions used in retrieval and ranking tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Behavior of Loss Functions</head><p>We observe that the MH loss can take a few epochs to 'warm-up' during training. <ref type="figure" target="#fig_1">Fig. 2</ref> depicts such behavior on the Flickr30K dataset using RC. Notice that the SH loss starts off faster, but after approximately 5 epochs MH loss surpasses SH loss. To explain this, the MH loss depends on a smaller set of triplets compared to the SH loss. Early in training the gradient of the MH loss is influenced by a relatively small set of triples. As such, it can take more iterations to train a model with the MH loss. We explored a simple form of curriculum learning ( <ref type="bibr" target="#b1">[2]</ref>) to speed-up the training. We start training with the SH loss for a few epochs, then switch to the MH loss for the rest of the training. However, it did not perform much better than training solely with the MH loss.  <ref type="table" target="#tab_5">(Table 3</ref>, row 3.9 and row 3.11). Notice that, in the first 5 epochs the SH loss achieves a better performance, however, from there-on the MH loss leads to much higher recall rates.</p><p>In <ref type="bibr" target="#b26">[27]</ref>, it is reported that with a mini-batch size of 1800, training is extremely slow. We experienced similar behavior with large mini-batches up to 512. However, mini-batches of size 128 or 256 exceeded the performance of the SH loss within the same training time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Examples of Hard Negatives</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper focused on learning visual-semantic embeddings for cross-modal, image-caption retrieval. Inspired by structured prediction, we proposed a new loss based on violations incurred by relatively hard negatives compared to current methods that used expected errors ( <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref>). We performed experiments on the MS-COCO and Flickr30K datasets and showed that our proposed loss significntly improves performance on these datasets. We observed that the improved loss can better guide a more powerful image encoder, ResNet152, and also guide better when fine-tuning an image encoder. With all modifications, our VSE++ model achieves state-of-the-art performance on the MS-COCO dataset, and is slightly below the GT: A little girl wearing pink pants, pink and white tennis shoes and a white shirt with a little girl on it puts her face in a blue Talking Tube.</p><p>HN: [0.26] Blond boy jumping onto deck.</p><p>GT: A teal-haired woman in a very short black dress, pantyhose, and boots standing with right arm raised and left hand obstructing her mouth in microphonesinging fashion is standing.</p><p>HN: [0.08] Two dancers in azure appear to be performing in an alleyway.</p><p>GT: Two men, one in a dark blue button-down and the other in a light blue tee, are chatting as they walk by a small restaurant.</p><p>HN: [0.41] Two men with guitars strapped to their back stand on the street corner with two other people behind them.</p><p>GT: A man wearing a black jacket and gray slacks, stands on the sidewalk holding a sheet with something printed on it in his hand.</p><p>HN: [0.26] Two men with guitars strapped to their back stand on the street corner with two other people behind them.</p><p>GT: There is a wall of a building with several different colors painted on it and in the distance one person sitting down and another walking.</p><p>HN: [0.06] A woman with luggage walks along a street in front of a large advertisement.</p><p>GT: A man is laying on a girl's lap, she is looking at him, she also has her hand on her notebook computer.</p><p>HN: [0.18] A woman sits on a carpeted floor with a baby.</p><p>GT: A young blond girl in a pink sweater, blue skirt, and brown boots is jumping over a puddle on a cloudy day.</p><p>HN: [0.51] An Indian woman is sitting on the ground, amongst drawings, rocks and shrubbery.</p><p>GT: One man dressed in black is stretching his leg up in the air, behind him is a massive cruise ship in the water.</p><p>HN: [0.24] A topless man straps surfboards on top of his blue car. <ref type="figure" target="#fig_2">Figure 3</ref>: Examples from the Flickr30K training set along with their hard negatives in a random mini-batch according to the loss of a trained VSE++ model. The value in brackets is the cost of the hard negative and is in the range [0, 2] in our implementation. HN is the hardest negative in a random sample of size 128. GT is the positive caption used to compute the cost of NG. best recent model on the Flickr30K dataset. Our proposed loss function can be used to train more sophisticated models that have been using a similar ranking loss for training. GT: Two elephants are standing by the trees in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Examples of Hard Negatives</head><p>VSE0: <ref type="bibr" target="#b8">[9]</ref> Three elephants kick up dust as they walk through the flat by the bushes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VSE++: [1]</head><p>A couple elephants walking by a tree after sunset.</p><p>GT: A large multi layered cake with candles sticking out of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VSE0: [1]</head><p>A party decoration containing flowers, flags, and candles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VSE++: [1]</head><p>A party decoration containing flowers, flags, and candles.</p><p>GT: The man is walking down the street with no shirt on.</p><p>VSE0: <ref type="bibr" target="#b23">[24]</ref> A person standing on a skate board in an alley. VSE++: <ref type="bibr" target="#b9">[10]</ref> Two young men are skateboarding on the street.</p><p>GT: A row of motorcycles parked in front of a building. GT: a brown cake with white icing and some walnut toppings VSE0: <ref type="bibr" target="#b5">[6]</ref> A large slice of angel food cake sitting on top of a plate. VSE++: <ref type="bibr" target="#b15">[16]</ref> A baked loaf of bread is shown still in the pan.</p><p>GT: A woman holding a child and standing near a bull.</p><p>VSE0: [1] A woman holding a child and standing near a bull.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VSE++: [1]</head><p>A woman holding a child looking at a cow.</p><p>GT: A woman in a short pink skirt holding a tennis racquet.</p><p>VSE0: <ref type="bibr" target="#b5">[6]</ref> A man playing tennis and holding back his racket to hit the ball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VSE++: [1]</head><p>A woman is standing while holding a tennis racket. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of typical positive pairs and the nearest negative samples. Here assume similarity score is the negative distance. Filled circles show a positive pair (i, c), while empty circles are negative samples for the query i. The dashed circles on the two sides are drawn at the same radii.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Analysis of the behavior of the MH loss on the Flickr30K dataset training with RC. This figure compares the SH loss to the MH loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>shows the hard negatives in a random mini-batch. These examples illustrate that hard negatives from a mini-batch can provide useful gradient information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. A. 1</head><label>1</label><figDesc>compares the outputs of VSE++ and VSE0 for a few examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>VSE0: [2] a parking area for motorcycles and bicycles along a street VSE++: [1] A number of motorbikes parked on an alley GT: some skateboarders doing tricks and people watching them VSE0: [39] Young skateboarder displaying skills on sidewalk near field. VSE++: [3] Two young men are outside skateboarding together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A. 1 :</head><label>1</label><figDesc>Examples of MS-COCO test images and the top 1 retrieved captions for VSE0and VSE++ (ResNet)-finetune. The value in brackets is the rank of the highest ranked ground-truth caption. GT is a sample from the ground-truth captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of experiments on MS-COCO.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The effect of data augmentation and fine-tuning. We copy the relevant results for VSE++ fromTable 1to enable an easier comparison. Notice that after applying all the modifications, VSE0 model reaches 56.0% for R@1, while VSE++ achieves 64.6%.</figDesc><table /><note>and 21% improvement in R@1 for image retrieval compared to UVS (rows 1.1 and 1.11). Notice that using ResNet152 and fine-tuning can only lead to 12.6% improvement using the VSE0 formulation (rows 2.6 and 1.1), while our MH loss function brings a significant gain of 8.6% (rows 1.11 and 2.6).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on the Flickr30K dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparison on MS-COCO. Training set for all the rows is 10C+rV.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/fartashf/vsepp</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by funding to DF from NSERC Canada, the Vector Institute, and the Learning in Brains and Machines Program of the Canadian Institute for Advanced Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large margin optimization of ranking measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop: Machine learning for Web search</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Linking image and text with 2-way nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning globallyconsistent local distance functions for shape-based image retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0704.3359</idno>
		<title level="m">Direct optimization of ranking measures</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval and natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="121" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Joint embeddings of shapes and images via cnn image purification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="234" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neuralbased approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble of exemplarsvms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005-09" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krähenbühl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Measuring machine intelligence through visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>C Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AI Magazine</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
