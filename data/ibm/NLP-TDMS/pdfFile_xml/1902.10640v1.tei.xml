<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Video Classification Using Fewer Frames</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shweta</forename><surname>Bhardwaj</surname></persName>
							<email>shweta@cse.iitm.ac.in</email>
							<affiliation key="aff1">
								<orgName type="department">Indian Institute of Technology Madras and Robert Bosch Centre for Data Science and AI (RBC-DSAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukundhan</forename><surname>Srinivasan</surname></persName>
							<email>msrinivasan@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
							<email>miteshk@cse.iitm.ac.in</email>
							<affiliation key="aff1">
								<orgName type="department">Indian Institute of Technology Madras and Robert Bosch Centre for Data Science and AI (RBC-DSAI)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Video Classification Using Fewer Frames</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, there has been a lot of interest in building compact models for video classification which have a small memory footprint (&lt; 1 GB) <ref type="bibr" target="#b15">[15]</ref>. While these models are compact, they typically operate by repeated application of a small weight matrix to all the frames in a video. For example, recurrent neural network based methods compute a hidden state for every frame of the video using a recurrent weight matrix. Similarly, cluster-and-aggregate based methods such as NetVLAD have a learnable clustering matrix which is used to assign soft-clusters to every frame in the video. Since these models look at every frame in the video, the number of floating point operations (FLOPs) is still large even though the memory footprint is small. In this work, we focus on building compute-efficient video classification models which process fewer frames and hence have less number of FLOPs. Similar to memory efficient models, we use the idea of distillation albeit in a different setting. Specifically, in our case, a compute-heavy teacher which looks at all the frames in the video is used to train a compute-efficient student which looks at only a small fraction of frames in the video. This is in contrast to a typical memory efficient Teacher-Student setting, wherein both the teacher and the student look at all the frames in the video but the student has fewer parameters. Our work thus complements the research on memory efficient video classification. We do an extensive evaluation with three types of models for video classification, viz., (i) recurrent models (ii) cluster-and-aggregate models and (iii) memory-efficient cluster-and-aggregate models and show that in each of these cases, a see-it-all teacher can be used to train a compute efficient see-very-little student. Overall, we show that the proposed student network can reduce the inference time by 30% and the number of FLOPs by approximately 90% with a negligible drop in the performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Today video content has become extremely prevalent on the internet influencing all aspects of our life such as education, entertainment, communication etc. This has led to an increasing interest in automatic video processing with the aim of identifying activities <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b38">37]</ref>, generating textual descriptions <ref type="bibr" target="#b8">[9]</ref>, generating summaries <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">24]</ref>, answering questions <ref type="bibr" target="#b14">[14]</ref> and so on. On one hand, with the availability of large-scale datasets <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">36]</ref> for various video processing tasks, it has now become possible to train increasingly complex models which have high memory and computational needs but on the other hand there is a demand for running these models on low power devices such as mobile phones and tablets with stringent constraints on latency, memory and computational cost. It is important to balance the two and design models which can learn from large amounts of data but still be computationally cheap at inference time.</p><p>In this context, the recently concluded ECCV workshop on YouTube-8M Large-Scale Video Understanding (2018) <ref type="bibr" target="#b15">[15]</ref> focused on building memory efficient models which use less than 1GB of memory. The main motivation was to discourage the use of ensemble based methods and instead focus on memory efficient single models. One of the main ideas explored by several participants <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27]</ref> in this workshop was to use knowledge distillation to build more compact student models. More specifically, they first train a teacher network which has a large number of parameters and then use this network to guide a much smaller student network which has limited memory requirements and can thus be employed at inference time. Of course, in addition to requiring less memory, such a model would also require fewer FLOPs as the size of weight matrices, hidden representations, etc. would be smaller. However, there is scope for reducing the FLOPs further because existing models process all frames in the video which may be redundant.</p><p>Based on the results of the ECCV workshop <ref type="bibr" target="#b15">[15]</ref>, we found that the two most popular paradigms for video classification are (i) recurrent neural network based methods and (ii) cluster-and-aggregate based methods. Not surprisingly, the third type of approaches based on C3D (3D convolu-tions) <ref type="bibr" target="#b2">[3]</ref> were not so popular because they are expensive in terms of their memory and compute requirements. For example, the popular I3D model <ref type="bibr" target="#b2">[3]</ref> is trained using 64 GPUs as mentioned in the original paper. Hence, in this paper, we focus only on the first two paradigms. We first observe that, RNN based methods <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">28]</ref> compute a hidden representation for every frame in the video and then compute a final representation for the video based on these frame representations. Hence, even if the model is compact due to smaller weight matrix and/or hidden representations, the number of FLOPs would still be large because this computation needs to be done for every frame in the video. Similarly, clusterand-aggregate based methods <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b16">16]</ref> have a learnable clustering matrix which is used for assigning soft clusters to every frame in the video. Even if the model is made compact by reducing the size of the clustering matrix and/or hidden representations, the number of FLOPs would still be large. To alleviate this problem, in this work, we focus on building models which have fewer FLOPs and are thus computationally efficient. Our work thus complements existing work on memory efficient models for video classification.</p><p>We propose to achieve this by again using the idea of distillation wherein we train a computationally expensive teacher network which computes a representation for the video by processing all frames in the video. We then train a relatively inexpensive student network whose objective is to process only a few frames of the video and produce a representation which is very similar to the representation computed by the teacher. This is achieved by minimizing (i) the squared error loss between the representations of the student network and the teacher network and/or (ii) by minimizing the difference between the output distributions (class probabilities) predicted by the two networks. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates this idea where the teacher sees every frame of the video but the student sees fewer frames, i.e., every j-th frame of the video. At inference time, we then use the student network for classification thereby reducing the time required for processing the video.</p><p>We experiment with two different methods of training the Teacher-Student network. In the first method (which we call Serial Training), the teacher is trained independently and then the student is trained to match the teacher with or without an appropriate regularizer to account for the classification loss. In the second method (which we call Parallel Training), the teacher and student are trained jointly using the classification loss as well as the matching loss. This parallel training method is similar to on-the-fly knowledge distillation from a dynamic teacher as mentioned in <ref type="bibr" target="#b18">[18]</ref>. We experiment with different students, viz., (i) a hierarchical RNN based model (ii) NetVLAD and (iii) NeXtVLAD which is a memory efficient version of NetVLAD and was the best single model in the ECCV'18 workshop. We experiment with the YouTube-8M dataset and show that the smaller student network reduces the inference time by upto 30% while still achieving a classification performance which is very close to that of the expensive teacher network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Since we focus on the task of video classification in the context of the YouTube-8M dataset <ref type="bibr" target="#b0">[1]</ref>, we first review some recent work on video classification and then some relevant work on model compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Classification</head><p>One of the popular datasets for video classification is the YouTube-8M dataset which contains videos having an average length of 200 seconds. We use this dataset in all our experiments. The authors of this dataset proposed a simple baseline model which treats the entire video as a sequence of one-second frames and uses a Long Short-Term Memory network (LSTM) to encode this sequence. Apart from this, they also propose some simple baseline models like Deep Bag of Frames (DBoF) and Logistic Regression <ref type="bibr" target="#b0">[1]</ref>. Various other classification models <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">30]</ref> have been proposed and evaluated on this dataset (2017 version) which explore different methods of: (i) feature aggregation in videos (temporal as well as spatial) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22]</ref>, (ii) capturing the interactions between labels <ref type="bibr" target="#b34">[34]</ref> and (iii) learning new non-linear units to model the interdependencies among the activations of the network <ref type="bibr" target="#b22">[22]</ref>. The state-of-the-art model on the 2017 version of the Youtube-8M dataset uses NetVLAD pooling <ref type="bibr" target="#b22">[22]</ref> to aggregate information from all the frames of a video.</p><p>In the recently concluded competition (2018 version), many methods <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b15">15]</ref> were proposed to compress the models such that they fit in 1GB of memory. As mentioned by <ref type="bibr" target="#b15">[15]</ref>, the major motivation behind this competition was to avoid the late-stage model ensemble techniques and focus mainly on single model architectures at inference time <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b7">8]</ref>. One of the top performing systems in this competition was NeXtVLAD <ref type="bibr" target="#b27">[27]</ref>, which modifies NetVLAD <ref type="bibr" target="#b22">[22]</ref> to squeeze the dimensionality of modules (embeddings). However, this model still processes all the frames of the video and hence has a large number of FLOPs. In this work, we take this compact NeXtVLAD model and make it compute efficient by using the idea of distillation. One clear message from this workshop was that the emphasis should be on single model architectures and not ensembles. Hence, in this work, we focus on single model-based solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Compression</head><p>the reader to the survey paper by <ref type="bibr" target="#b6">[7]</ref> for a thorough review of the field. For brevity, here we refer to only those papers which use the idea of distillation. For example, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b3">4]</ref> use Knowledge Distillation to learn a more compact student network from a computationally expensive teacher network. The key idea is to train a shallow student network using soft targets (or class probabilities) generated by the teacher instead of the hard targets present in the training data. There are several other variants of this technique such as, <ref type="bibr" target="#b26">[26]</ref> extend this idea to train a student model which not only learns from the outputs of the teacher but also uses the intermediate representations learned by the teacher as additional hints. This idea of Knowledge Distillation has also been tried in the context of pruning networks for multiple object detection <ref type="bibr" target="#b3">[4]</ref>, speech recognition <ref type="bibr" target="#b35">[35]</ref> and reading comprehension <ref type="bibr" target="#b13">[13]</ref>.</p><p>In the context of video classification, there is some work <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b20">20]</ref> on using Quantization for model compression. Some work on video-based action recognition <ref type="bibr" target="#b39">[38]</ref> tries to accelerate processing in a two-stream CNN architecture by transferring knowledge from motion modality to optical modality. In some very recent work on video classification <ref type="bibr" target="#b9">[10]</ref> and video captioning <ref type="bibr" target="#b5">[6]</ref> the authors use a reinforcement learning agent to select which frames to process. We do not focus on the problem of frame selection but instead focus on distilling knowledge from the teacher once fewer frames have been selected for the student. While in this work we simply select frames uniformly, the same ideas can also be used on top of an RL agent which selects the best frames but we leave this as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Classification Models</head><p>Given a fixed set of m classes y 1 , y 2 , y 3 , ..., y m ∈ Y and a video V containing N frames (F 0 , F 1 , . . . , F N −1 ), the goal of video classification is to identify all the classes to which the video belongs. In other words, for each of the m classes we are interested in predicting the probability P (y i |V). This probability can be parameterized using a neural network f which looks at all the frames in the video to predict:</p><formula xml:id="formula_0">P (y i |V) = f (F 0 , F 1 , . . . , F N −1 )</formula><p>Given the setup, we now briefly discuss the two state-ofthe-art models that we have considered as teacher/student for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Recurrent Network Based Models</head><p>We consider the Hierarchical Recurrent Neural Network (H-RNN) based model which assumes that each video contains a sequence of b equal sized blocks. Each of these blocks in turn is a sequence of l frames thereby making the entire video a sequence of sequences. In the case of the YouTube-8M dataset, these frames are one-second shots of the video and each block b is a collection of l such onesecond frames. The model contains a lower level RNN to encode each block (sequence of frames) and a higher level RNN to encode the video (sequence of blocks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cluster and Aggregate Based Models</head><p>We consider the NetVLAD model with Context Gating (CG) as proposed in <ref type="bibr" target="#b22">[22]</ref>. This model does not treat the video as a sequence of frames but simply as a bag of frames. For every frame in this bag, it first assigns a soft cluster to the frame which results is a M × k dimensional representation for the frame where k is the number of clusters considered and M is the size of the initial representation of the frame (say, obtained from a CNN). Instead of using a standard clustering algorithm such as k-means, the authors introduce a learnable clustering matrix which is trained along with all the parameters of the network. The cluster assignments are thus a function of a parameter which is learned during training. The video representation is then computed by aggregating all the frame representations obtained after clustering. This video representation is then fed to multiple fully connected layers with Context Gating (CG) which help to model interdependencies among network activations. We also experiment with NeXtVLAD <ref type="bibr" target="#b27">[27]</ref> which is a compact version of NetVLAD wherein the M ×k dimensional representation is downsampled by grouping which effectively reduces the total number of parameters in the network.</p><p>Note that all the models described above look at all the frames in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Approach</head><p>The focus of this work is to design a simpler network g which looks at only a fraction of the N frames at inference time while still allowing it to leverage the information from all the N frames at training time. To achieve this, we propose a Teacher-Student network as described below wherein the teacher has access to more frames than the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TEACHER:</head><p>The teacher network can be any state-of-theart model described above (H-RNN, NetVLAD, NeXtVLAD). This teacher network looks at all the N frames of video (F 0 , F 1 , . . . , F N −1 ) and computes an encoding E T of the video, which is then fed to a simple feedforward neural network with a multi-class output layer containing a sigmoid neuron for each of the Y classes. The parameters of the teacher network are learnt using a standard multi-label classification loss L CE , which is a sum of the cross-entropy loss for each of the Y classes. We refer to this loss as L CE where the subscript CE stands for cross entropy between </p><formula xml:id="formula_1">0 1 2 3 4 N − 1 F0 F1 F2 F3 F4 FN−1 TEACHER (N frames) ET 0 j 2j N j − 1 F0 Fj F2j F N j −1 STUDENT (every j th frame) ES VIDEO CLASSIFIER-1 VIDEO CLASSIFIER-2 L rep L pred L CE backprop L rep through STUDENT backprop L pred through STUDENT backprop L CE through STUDENT</formula><formula xml:id="formula_2">L CE = − |Y | i=1 y i log(ŷ i ) + (1 − y i ) log(1 −ŷ i )<label>(1)</label></formula><p>STUDENT: In addition to this teacher network, we introduce a student network which only processes every j th frame (F 0 , F j , F 2j , . . . , F N j −1 ) of the video and computes a representation E S of the video from these N j frames. We use only same family distillation wherein both the teacher and the student have the same architecture. For example, <ref type="figure" target="#fig_0">Figure 1</ref> shows the setup when the teacher is H-RNN and the student is also H-RNN (please refer to the supplementary material for a similar diagram when the teacher and student are NetVLAD). Further, the parameters of the output layer are shared between the teacher and the student. The student is trained to minimize the squared error loss between the representation computed by the student network and the representation computed by the teacher. We refer to this loss as L rep where the subscript rep stands for representations.</p><formula xml:id="formula_3">L rep = ||E T − E S || 2<label>(2)</label></formula><p>We also try a simple variant of the model, where in addition to ensuring that the final representations E S and E T are similar, we also ensure that the intermediate representations (I S and I T ) of the models are similar. In particular, we ensure that the representation of the frames j, 2j and so on computed by the teacher and student network are very similar by minimizing the squared error distance between the corresponding intermediate representations. We refer to this loss as L I rep where the superscript I stands for interme-diate.</p><formula xml:id="formula_4">L I rep = N j −1 i=j,2j,.. ||I i T − I i S || 2<label>(3)</label></formula><p>Alternately, the student can also be trained to minimize the difference between the class probabilities predicted by the teacher and the student. We refer to this loss as L pred where the subscript pred stands for predicted probabilities. More specifically if P T = {p 1 T , p 2 T , ...., p m T } and P S = {p 1 S , p 2 S , ...., p m S } are the probabilities predicted for the m classes by the teacher and the student respectively, then</p><formula xml:id="formula_5">L pred = d(P T , P S )<label>(4)</label></formula><p>where d is any suitable distance metric such as KL divergence or squared error loss. TRAINING: Intuitively, it makes sense to train the teacher first and then use this trained teacher to guide the student. We refer to this as the Serial mode of training as the student is trained after the teacher as opposed to jointly. For the sake of analysis, we use different combinations of loss function to train the student as described below:</p><p>(a) L rep : Here, we operate in two stages. In the first stage, we train the student network to minimize the L rep as defined above, i.e., we train the parameters of the student network to produce representations which are very similar to the teacher network. The idea is to let the student learn by only mimicking the teacher and not worry about the final classification loss. In the second stage, we then plug in the classifier trained along with the teacher (see <ref type="figure" target="#fig_0">Equation 1</ref>) and fine-tune all the parameters of the student and the classifier using the cross entropy loss, L CE . In practice, we found that the fine-tuning done in the second stage helps to improve the performance of the student.</p><p>(b) L rep + L CE : Here, we train the student to jointly minimize the representation loss as well as the classification loss. The motivation behind this was to ensure that while mimicking the teacher, the student also keeps an eye on the final classification loss from the beginning (instead of being fine-tuned later as in the case above).</p><p>(c) L pred : Here, we train the student to only minimize the difference between the class probabilities predicted by the teacher and the student.</p><p>(d) L pred +L CE : Here, in addition to mimicking the probabilities predicted by the teacher, the student is also trained to minimize the cross entropy loss.</p><p>(e) L rep +L CE +L pred : Finally, we combine all the 3 loss functions. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the process of training the student with different loss functions.</p><p>For the sake of completeness, we also tried an alternate mode in which we train the teacher and student in parallel such that the objective of the teacher is to minimize L CE and the objective of the student is to minimize one of the 3 combinations of loss functions described above. We refer to this as Parallel training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>In this section, we describe the dataset used for our experiments, the hyperparameters that we considered, the baseline models that we compared with and the effect of different loss functions and training methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Dataset:</head><p>The YouTube-8M dataset (2017 version) <ref type="bibr" target="#b0">[1]</ref> contains 8 million videos with multiple classes associated with each video. The average length of a video is 200s and the maximum length of a video is 300s. The authors of the dataset have provided pre-extracted audio and visual features for every video such that every second of the video is encoded as a single frame. The original dataset consists of 5,786,881 training (70%), 1,652,167 validation (20%) and 825,602 test examples (10%). Since the authors did not release the test set, we used the original validation set as test set and report results on it. In turn, we randomly sampled 48,163 examples from the training data and used these as validation data for tuning the hyperparameters of the model. We trained our models using the remaining 5,738,718 training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hyperparameters: For all our experiments, we used</head><p>Adam Optimizer with the initial learning rate set to 0.001 and then decreased it exponentially with 0.95 decay rate. We used a batch size of 256. For both the student and teacher networks we used a 2-layered MultiLSTM Cell with cell size of 1024 for both the layers of the hierarchical model. For regularization, we used dropout (0.5) and L 2 regularization penalty of 2 for all the parameters. We trained all the models for 5 epochs and then picked the best model-based on validation performance. We did not see any benefit of training beyond 5 epochs. For the teacher network we chose the value of l (number of frames per block ) to be 20 and for the student network, we set the value of l to 5 or 3 depending on the reduced number of frames considered by the student.</p><p>In the training of NetVLAD model, we have used the standard hyperparameter settings as mentioned in <ref type="bibr" target="#b22">[22]</ref>. We consider 256 clusters and 1024 dimensional hidden layers. Similarly, in the case of NeXtVLAD, we have considered the hyperparameters of the single best model as reported by <ref type="bibr" target="#b27">[27]</ref>. In this network, we are working with a cluster size of 128 with hidden size as 2048. The input is reshaped and downsampled using 8 groups in the cluster as done in the original paper. For all these networks, we have worked with a batch size of 80 and an initial learning rate of 0.0002 exponentially decayed at the rate of 0.8. Additionally, we have applied dropout of 0.5 on the output of NeXtVLAD layer which helps for better regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation Metrics:</head><p>We used the following metrics as proposed in <ref type="bibr" target="#b0">[1]</ref> for evaluating the performance of different models :</p><p>• GAP (Global Average Precision): is defined as</p><formula xml:id="formula_6">GAP = P i=1 p(i)∇r(i)</formula><p>where p(i) is the precision at prediction i, ∇r(i) is the change in recall at prediction i and P is the number of top predictions that we consider. Following the original YouTube-8M Kaggle competition we use the value of P as 20.</p><p>• mAP (Mean Average Precision) : The mean average precision is computed as the unweighted mean of all the perclass average precisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Models Compared:</head><p>We compare our Teacher-Student network with the following models which helps us to better contextualize our results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion And Results</head><p>Since we have 3 different base models (H-RNN, NetVLAD, NeXtVLAD), 5 different combinations of loss functions (see section 4), 2 different training paradigms (Serial and Parallel) and 5 different baselines for each base model, the total number of experiments that we needed to run to report all these results was very large. To reduce the number of experiments we first consider only the H-RNN model to identify the (a) best baseline (Uniform-k, Random-k, First-k, Middle-k, Last-k, First-Middle-Last-k) (b) best training paradigm (Serial v/s Parallel) and (c) best combination of loss function. We then run the experiments on NetVLAD and NeXtVLAD using only the best baseline, best training paradigm and best loss function thus identified. The results of our experiments using the H-RNN model are summarized in <ref type="table">Table 1</ref> to <ref type="table" target="#tab_2">Table 3</ref> and are discussed first followed by a discussion of the results using NetVLAD and NeXtVLAD as summarized in <ref type="table" target="#tab_4">Tables 5 and 6:</ref> 1. Comparisons of different baselines: First, we simply compare the performance of different baselines listed in the top half of <ref type="table">Table 1</ref>. As is evident, the Uniform-k baseline which looks at equally spaced k frames performs better than all the other baselines. The performance gap between Uniform-k and the other baselines is even more significant when the value of k is small. The main purpose of this experiment was to decide the right way of selecting frames for the student network. Based on these results, we ensured that for all our experiments, we fed equally spaced k frames to the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Comparing</head><p>Teacher-Student Network with Uniformk Baseline: As mentioned above, the Uniform-k baseline is a simple but effective way of reducing the number of frames to be processed. We observe that all the teacherstudent models outperform this strong baseline. Further, in a separate experiment as reported in <ref type="table" target="#tab_0">Table 2</ref> we observe that when we reduce the number of training examples seen by the teacher and the student, then the performance of the Uniform-k baseline drops and is much lower than that of the corresponding Teacher-Student network. This suggests that the Teacher-Student network can be even more useful when the amount of training data is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Serial Versus Parallel Training of Teacher-Student:</head><p>While the best results in <ref type="table">Table 1</ref> are obtained using Serial training, if we compare the corresponding rows of Serial and Parallel training we observe that there is not much difference between the two. We found this to be surprising and investigated this further. In particular, we compared the performance of the teacher after different epochs in the Parallel training setup with the performance of a static teacher trained independently (Serial). We plotted this performance    <ref type="figure" target="#fig_3">Figure 2</ref> and observed that after 3-4 epochs of training, the Parallel teacher is able to perform at par with Serial teacher (the constant blue line). As a result, the Parallel student now learns from this trained teacher for a few more epochs and is almost able to match the performance of the Serial student. This trend is same across the different combinations of loss functions that we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Visualization of Teacher and Student Representations:</head><p>Apart from evaluating the final performance of the model in terms of mAP and GAP, we also wanted to check if the representations learned by the teacher and student are indeed similar. To do this, we chose top-5 classes (class1: Vehicle, class2: Concert, class3: Association football, class4: Animal, class5: Food) in the Youtube-8M dataset and visualized the TSNE-embeddings of the representations computed by the student and the teacher for the same video (see <ref type="figure" target="#fig_4">Figure 3</ref>). We use the darker shade of a color to represent teacher embeddings of a video and a lighter shade of the same color to represent the student embeddings of the same video. We observe that the dark shades and the light shades of the same color almost completely overlap showing that the student and teacher representations are indeed very close to each other. This shows that introducing the L rep indeed brings the teacher and student representations       <ref type="table" target="#tab_3">Table 4</ref>, we did not see any benefit of matching intermediate representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Computation time of different models:</head><p>The main aim of this work was to ensure that the computational cost and time is minimized at inference time. The computational cost can be measured in terms of the number of FLOPs. As shown in <ref type="table" target="#tab_2">Table 3</ref> when k=30, the inference time drops by 30% and the number of FLOPs reduces by approximately 90%, but the performance of the model is not affected. In particular, as seen in <ref type="table">Table 1</ref>, when k = 30, the GAP and mAP drop by 0.5-0.9% and 0.9-2% respectively as compared to the teacher skyline. 7. Performance using NetVLAD models: In <ref type="table" target="#tab_4">Table 5</ref> we summarize the results obtained using NetVLAD as the base model in the Teacher-Student network. Here the student network was trained using the best loss function ( L rep , L pred , L CE ) and the best training paradigm (Serial) as identified from the experiments done using the H-RNN model. Further, we consider only the Uniform-k baseline as that was the best baseline as observed in our previous experiments. Here again we observe that the student network does better than the Uniform-k baseline. 8. Combining with memory efficient models: Lastly, we experiment with the compact NeXtVLAD model and show that the student network performs slightly better than the Uniform-k baseline in terms of mAP but not so much in terms of GAP (note that mAP gives equal importance to all classes but GAP is influenced more by the most frequent classes in the dataset). Once again, there is a significant reduction in the number of FLOPs (approximately 89%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>We proposed a method to reduce the computation time for video classification using the idea of distillation. Specifically, we first train a teacher network which computes a representation of the video using all the frames in the video. We then train a student network which only processes k frames of the video. We use different combinations of loss functions which ensures that (i) the final representation produced by the student is the similar as that produced by the teacher and (ii) the output probability distributions produced by the student are similar to those produced by the teacher. We compare the proposed models with a strong baseline and skyline and show that the proposed model outperforms the baseline and gives a significant reduction in terms of computational time and cost when compared to the skyline. In particular, we evaluate our model on the YouTube-8M dataset and show that the computationally less expensive student network can reduce the computation time by 30% while giving an approximately similar performance as the teacher network.</p><p>As future work, we would like to evaluate our model on other video processing tasks such as summarization, question answering and captioning. We would also like to train a student with an ensemble of teachers (preferably from different families). Lastly, we would like to train a reinforcement learning agent to first select the most favorable k (or even fewer) frames in the video and use these as opposed to simply using equally spaced k frames.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of TEACHER-STUDENT network for video classification the true labels y and predictionsŷ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a) Teacher-Skyline: The original teacher model which processes all the frames of the video. This, in some sense, acts as the upper bound on the performance. b) Baseline Methods: As baseline, we consider a model (H-RNN, NetVLAD or NeXtVLAD) which is trained MODEL 0.062 0.267 0.067 0.282 0.077 0.294 0.083 0.317 0.094 First-Middle-Last-k 0.640 0.215 0.671 0.242 0.680 0.249 0.698 0.268 0.721 0.287 Training Student-Loss Teacher-Student METHODS Parallel L rep 0.724 0.280 0.762 0.331 0.785 0.365 0.794 0.380 0.803 0.392 Parallel L rep , L CE 0.726 0.285 0.766 0.334 0.785 0.362 0.795 0.381 0.804 0.396 Parallel L rep , L pred , L CE 0.729 0.292 0.770 0.337 0.789 0.371 0.796 0.388 0.806 0.404 Serial L rep 0.727 0.288 0.768 0.339 0.786 0.365 0.795 0.381 0.802 0.394 Serial L pred 0.722 0.287 0.766 0.341 0.784 0.367 0.793 0.383 0.798 0.390 Serial L rep , L CE 0.728 0.291 0.769 0.341 0.786 0.368 0.794 0.383 0.803 0.399 Serial L pred , L CE 0.724 0.289 0.763 0.341 0.785 0.369 0.795 0.386 0.799 0.391 Serial L rep , L pred , L CE 0.731 0.297 0.771 0.349 0.789 0.375 0.798 0.390 0.806 0.405 Table 1: Performance comparison of proposed Teacher-Student models using different Student-Loss variants, with their corresponding baselines using k frames. Teacher-Skyline refers to the default model which process all the frames in a video. from scratch but uses only k frames of the video. However, unlike the student model this model is not guided by a teacher. These k frames can be (i) separated by a constant interval and are thus equally spaced (Uniformk) or (ii) sampled randomly from the video (Random-k) or (iii) the first k frames of the video (First-k) or (iv) the middle k frames of the video (Middle-k) or (v) the last k frames of the video (Last-k) or (i) the first k 3 , middle k 3 and last k 3 frames of the video (First-Middle-Last-k). We report results with different values of k : 6, 10, 15, 20, 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Training: Lrep,LCE,L pred</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Performance comparison (GAP score) of different variants of Serial and Parallel methods in Teacher-Student training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>TSNE-Embedding of teacher and student representations. Here, class c refers to the cluster representation obtained corresponding to c th class, whereas t and s denote teacher and student embedding respectively. close to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Effect of amount of training data on performance of Serial and Uniform models using 30 frames in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of FLOPs and evaluation time of models using k frames with Skyline model on original validation set using Tesla k80s GPU MODEL Intermediate Final GAP mAP GAP mAP Parallel L rep 0.803 0.393 0.803 0.392 Parallel L rep + L CE 0.803 0.396 0.804 0.396 Parallel L rep + L pred 0.804 0.400 0.806 0.404</figDesc><table><row><cell>Serial Serial Serial</cell><cell>L rep L rep + L CE L rep + L pred 0.806 0.405 0.806 0.405 0.804 0.395 0.802 0.394 0.803 0.397 0.803 0.399</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of Final and Intermediate representation matching by Student network using k=30 frames</figDesc><table><row><cell>Model: NetVLAD</cell><cell>k=10</cell><cell>k=30</cell></row><row><cell></cell><cell cols="2">mAP GAP mAP GAP</cell></row><row><cell>Skyline</cell><cell></cell><cell>0.462 0.823</cell></row><row><cell>Uniform</cell><cell cols="2">0.364 0.773 0.421 0.803</cell></row><row><cell>Student</cell><cell cols="2">0.383 0.784 0.436 0.812</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance of NetVLAD model with k= 10, 30</figDesc><table><row><cell cols="3">frames in proposed Teacher-Student Framework</cell></row><row><cell>Model: NeXtVLAD</cell><cell cols="2">k=30 mAP GAP (in Billion) FLOPs</cell></row><row><cell>Skyline</cell><cell>0.464 0.831</cell><cell>1.337</cell></row><row><cell>Uniform</cell><cell>0.424 0.812</cell><cell>0.134</cell></row><row><cell>Student</cell><cell>0.439 0.818</cell><cell>0.134</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance and FLOPs comparison in NeXtVLAD model with k=30 frames in proposed Teacher-Student Framework 5. Matching Intermediate v/s Final representations: Intuitively, it seemed that the student should benefit more if we train it to match the intermediate representations of the teacher at different timesteps as opposed to only the final representation at the last time step. However, as reported in</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Aggregating frame-level features for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/1707.00803</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Less is more: Picking informative frames for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A survey of model compression and acceleration for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1710.09282</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Temporal attention mechanism with conditional inference for large-scale multi-label video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-D</forename><forename type="middle">L</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Watching a small portion could be as good as watching all: Towards efficient video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="705" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Diverse sequential subset selection for supervised video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2069" to="2077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attention-guided answer distillation for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1808.07644</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The 2nd youtube-8m large-scale video understanding challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Joonseok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><surname>Readewalter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding (ECCV 2018)</title>
		<meeting>of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding (ECCV 2018)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kmiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>An</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00530</idno>
		<title level="m">Learnable pooling methods for video classification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hmdb: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge distillation by onthe-fly native ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Lan Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Xiatian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To Appear In Proceedings of Thirtysecond Annual Conference on Neural Information Processing Systems NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Temporal modeling approaches for large-scale youtube-8m video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<idno>abs/1707.04555</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constrained-size tensorflow models for youtube-8m video understanding challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Liu Tianqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</title>
		<meeting>of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification. CoRR, abs/1706.06905</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building a size constrained predictive model for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</title>
		<meeting>of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Label denoising with large ensembles of heterogeneous neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Elizaveta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gleb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Sergey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</title>
		<meeting>of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nextvlad: An efficient neural network to aggregate frame-level features for largescale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rongcheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</title>
		<meeting>of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning video features for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shivam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</title>
		<meeting>of the 2nd Workshop on YouTube-8M Large-Scale Video Understanding</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep learning methods for efficient large scale video labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Skalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pekalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">E</forename><surname>Pan</surname></persName>
		</author>
		<idno>abs/1706.04572</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Non-local netvlad encoding for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00207</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Videoset: A large-scale compressed video quality dataset based on jnd measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The monkeytyping solution to the youtube-8m video understanding challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1706.05150</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence student-teacher training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2761" to="2765" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sun database: Exploring a large collection of scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
