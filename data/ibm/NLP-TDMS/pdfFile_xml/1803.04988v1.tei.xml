<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LCANet: End-to-End Lipreading with Cascaded Attention-CTC</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85281</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung Research America</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Cassimatis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung Research America</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung Research America</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LCANet: End-to-End Lipreading with Cascaded Attention-CTC</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lipreading</term>
					<term>ASR</term>
					<term>attention mechanism</term>
					<term>CTC</term>
					<term>cas- caded attention-CTC</term>
					<term>deep neural network</term>
					<term>3D CNN</term>
					<term>highway network</term>
					<term>Bi-GRU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine lipreading is a special type of automatic speech recognition (ASR) which transcribes human speech by visually interpreting the movement of related face regions including lips, face, and tongue. Recently, deep neural network based lipreading methods show great potential and have exceeded the accuracy of experienced human lipreaders in some benchmark datasets. However, lipreading is still far from being solved, and existing methods tend to have high error rates on the wild data. In this paper, we propose LCANet, an end-to-end deep neural network based lipreading system. LCANet encodes input video frames using a stacked 3D convolutional neural network (CNN), highway network and bidirectional GRU network. The encoder effectively captures both short-term and long-term spatio-temporal information. More importantly, LCANet incorporates a cascaded attention-CTC decoder to generate output texts. By cascading CTC with attention, it partially eliminates the defect of the conditional independence assumption of CTC within the hidden neural layers, and this yields notably performance improvement as well as faster convergence. The experimental results show the proposed system achieves a 1.3% CER and 3.0% WER on the GRID corpus database, leading to a 12.3% improvement compared to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Lipreading, the ability to understand what people are saying from only visual information, has long been believed to be a superpower as shown in some science fiction movies. The recent development of deep learning techniques has begun to unveil the secret behind it and demonstrated the potential to make it available for commons. Compared to the acoustic-based speech recognition technique, lipreading solves a much broader range of practical problems, such as aids for hearing-impaired persons, speech recognition in a noisy environment, silent movie analysis, etc.</p><p>Lipreading is a challenging problem because the differences among some of the phonemes or visemes are subtle (e.g., the visemes corresponding to "p" and "b") although their corresponding utterances can be easily distinguished. Such fine distinction is the major obstacle for humans to read from lips and as reported previously only around 20% reading accuracy can be achieved <ref type="bibr" target="#b0">[1]</ref>. On the other hand, recent work shows that machine lipreading has got remarkable progress especially with the advancement of deep learning techniques <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Deep learning techniques have been the cornerstone for recent success <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b10">[11]</ref> of many traditionally hard machine learning tasks <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b16">[17]</ref>. Generally, deep learning based lipreading methods follow the state-of-the-art sequential modeling solutions that have been widely applied to problems like acoustic based speech recognition <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> and neural machine translation <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Two most widely used approaches are Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b24">[25]</ref> and the attention-based sequence to sequence (seq2seq) model <ref type="bibr" target="#b20">[21]</ref>.</p><p>CTC approach works well on acoustic-based speech recognition. For lipreading problem, <ref type="bibr" target="#b2">[3]</ref> uses the CTC loss function <ref type="bibr" target="#b24">[25]</ref> to train an end-to-end deep neural network, named "LipNet", and the model outperforms experienced human lipreaders on the GRID benchmark dataset <ref type="bibr" target="#b25">[26]</ref>. However, CTC loss function assumes conditional independence of separate labels (i.e., the individual character symbols), and each output unit is the probability of observing one particular label at a time. Therefore, although CTC is applied on top of Recurrent Neural Networks (RNNs), it tends to focus too much on local information (nearby frames) <ref type="bibr" target="#b4">[5]</ref>. This may work well for acoustic phonemes, but is not well fit for predicting visemes which require longer context information to discriminate their subtle differences.</p><p>Attention-based seq2seq models were initially introduced for neural machine translation <ref type="bibr" target="#b20">[21]</ref>, for which the input sequence and output sequence can be chronologically inconsistent. Such kind of model has also been adapted to solve the lipreading problem <ref type="bibr" target="#b4">[5]</ref>: an encoder encodes the input frames (optionally with audios) into hidden features which are fed into an attention-based decoder to obtain the text output. By eliminating the conditional independence assumption, the attention-based method achieves better performance than CTC. However, recent work <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> shows that the attention model is too flexible to predict proper alignments for speech recognition, and is hard to train from scratch due to the misalignment on longer input sequences.</p><p>In this paper, we propose a cascaded attention-CTC deep learning model, LCANet, for end-to-end lipreading. LCANet incorporates an encoder that is composed of 3D convolution, highway network and bi-directional GRU layers, which is able to effectively capture both short-term and long-term spatio-temporal information from the input video frames. To deal with the conditional independence assumption of the CTC loss, the encoded features are fed into a cascaded attention-CTC decoder. The proposed decoder explicitly absorbs information from the longer context from the hidden layers using the attention mechanism. Extensive evaluation result shows that LCANet achieves the state-of-the-art performance and faster convergence than existing methods.</p><p>The contribution of this paper is as follows:</p><p>• We propose an end-to-end lipreading deep neural network architecture, LCANet, which relies purely on the visual information. • LCANet leverages a cascaded attention-CTC decoder to generate text from encoded spatio-temporal features. Such design partially eliminate the defect of the conditional independence assumption in the vanilla CTCbased method. • We demonstrate the performance improvement compared with existing work. The benefits of the cascaded attention-CTC decoder are also discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Deep Learning for Speech Recognition</head><p>Recent advancement of speech recognition technique is primarily due to the prosperity of deep learning research <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b32">[33]</ref>, and the reported speech recognition error rate is already below that of a human. This makes speech a convenient input method for computers as well as mobile or embedded devices. Together with the advancement of natural language understanding techniques <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b36">[37]</ref> and recent software <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref> and hardware acceleration <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref> methods, the intelligent virtual personal assistant has become a hot market (e.g., Siri of iPhone, Alexa of Amazon Echo, Google Now, Samsung Bixby, etc.).</p><p>The most successful deep speech recognition models are built with either CTC <ref type="bibr" target="#b24">[25]</ref> or the sequence-to-sequence (seq2seq) model. The CTC based methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> predict a label (i.e., a character label) for each audio frame and is trained to optimize the alignment between the prediction and the target label. Seq2seq models for speech recognition <ref type="bibr" target="#b31">[32]</ref> improved significantly with attention mechanism <ref type="bibr" target="#b20">[21]</ref> which enables more effective flow of information than a recurrent neural network. Attention-based seq2seq model has exceeded the CTC based model on several benchmark datasets. A recent work <ref type="bibr" target="#b43">[43]</ref> shows that attention-based model may not perform well on noisy speech data, and proposes a joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture. The hybrid method minimizes a joint loss which is the weighted sum of both CTC loss and attention loss, and this effectively utilizes both advantages in decoding. Different from <ref type="bibr" target="#b43">[43]</ref>, LCANet uses a cascaded attention-CTC decoder. The benefit of such design includes (1) it avoids tuning the weight parameter in the loss function and (2) it generates the output directly from the softmax output instead of merging the results from both CTC and attention branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Machine Lipreading</head><p>Generally, machine lipreading contains two phases: visual feature extraction from the lip movement and text prediction based on classifiers. The traditional visual features extracted from the mouth region of interest (ROI) can be broadly classified into four categories: 1) visual feature extracted directly from images through image transformations such as DCT, DWT [44]- <ref type="bibr" target="#b47">[47]</ref>; 2) geometry-based features, e.g., height, width, area of the lip region; 3) motion-based features, e.g., optical flow of the motion of the lip region <ref type="bibr" target="#b48">[48]</ref>; 4) modelbased features. The shape and appearance of the lip ROI are explicitly modeled by active shape models (ASMs) <ref type="bibr" target="#b49">[49]</ref> or active appearance models (AAMs) <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b51">[51]</ref>. The quality of such model-based features are highly dependent on the accuracy of hand-labeled training data which requires lots of efforts to obtain. To utilize the temporal dynamics in the extracted visual features, a dynamic classifier such as hidden Markov models (HMMs) is used for classification <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b52">[52]</ref>. However, such classifiers are based on the conditional independent assumption and usually fails to model the longterm dependencies.</p><p>Recently, deep neural network emerges to be used for lipreading and achieves promising performance compared to the conventional methods. <ref type="bibr" target="#b5">[6]</ref> propose to use CNN for visual feature extraction, and a Gaussian mixture observation model for an isolated word recognition task by modeling the temporal dependence of the generated phoneme label sequences. <ref type="bibr" target="#b2">[3]</ref> proposes an end-to-end sentence-level lipreading model. A stacked 3D CNN are used for dynamic visual feature extraction. The mapping between lip movement and the corresponding text representation is modeled </p><formula xml:id="formula_0">l ˽ i p ˽ r e a ˽ d ˽ 3D Conv Bi-GRU + T 1-T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highway Network</head><p>Att Att Att C T C Cascaded Attention-CTC ... <ref type="figure">Figure 2</ref>: The architecture of LCANet. The pre-processed mouth-region frames are used as input, which are processed by the following 3D-CNN, the highway network, the Bi-GRU and the cascaded attention-CTC decoder. A softmax layer calculates the probability distribution of the output of the attention module. Each attention unit is related to one character output. The CTC decoder generates text predictions from the probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Decoder</head><p>by the Gated Recurrent Unit (GRU). CTC loss is used for calculating the difference between the predictions and the text labels which have different lengths. <ref type="bibr" target="#b1">[2]</ref> presents an end-to-end lipreading system which admits two input streams: mouth images that encode static information and mouth differences that capture local temporal dynamics. The encoding layers are pre-trained using RBMs and the temporal dynamics are modeled by an LSTM. The fusion of the two streams is performed through a Bi-directional LSTM (BLSTM). <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b53">[53]</ref> propose to use combined visual speech features for multi-modal speech recognition where video and audio features are fused as input. The former method is based on the 'EESEN' framework <ref type="bibr" target="#b18">[19]</ref>, which uses CTC loss function to solve the temporal alignment problem. The latter approach is based on the 'Watch, Listen, Attend and Spell' network <ref type="bibr" target="#b32">[33]</ref>, which leverages the attention mechanism to model rich contextual information between the input video/audio frames and the character predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LCANET ARCHITECTURE</head><p>In this section, we describe the neural network architecture of the proposed LCANet ( <ref type="figure">Figure 2</ref>). The network is composed of a spatio-temporal video encoder which maps a sequence of video frames x = (x 1 , ..., x n ) to a sequence of continuous latent representations h = (h 1 , ..., h n ), and a cascaded CTC-attention decoder which generates the output sequence of character symbols y = (y 1 , ..., y m ) 1 . In the following, we first introduce the spatio-temporal video encoder, and then we present the cascaded attention-CTC decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Spatio-temporal Video Encoder</head><p>The video encoder of LCANet has three components: 3D-CNN, highway network, and Bidirectional GRU (Bi-GRU).</p><p>3D-CNN: LCANet feeds the input video frames into a 3D convolution neural network (3D-CNN) <ref type="bibr" target="#b54">[54]</ref> to encode both visual and short temporal information. Compared to 2D-CNN which only deals with the spatial dimensions , 3D-CNN also convolves across time steps: a group of adjacent k frames x t:t+k are processed by a set of P temporal convolutional filters C p (w, h, k, c), where w and h are the width and height of the filter in the spatial dimension and c is the number of channels. The value of k in the filter is small (e.g., 3 or 5) to capture only short temporal information, which in lipreading can be mapped to an individual or two nearby visemes.</p><p>Highway Network: We stack two layers of highway networks <ref type="bibr" target="#b55">[55]</ref> on top of the 3D-CNN. The highway network module (as shown in <ref type="figure">Figure 3</ref>) has a pair of transform gate t and carry gate 1-t that allows the deep neural network to carry some input information directly to the output. The highway network is formulated as follows:</p><formula xml:id="formula_1">t = σ(W T x + b T ), (1) g = t σ(W H x + b H ) + (1 − t) x.<label>(2)</label></formula><p>where x is the input, g is the network output, and denotes element-wise multiplication. A recent study on language modeling <ref type="bibr" target="#b33">[34]</ref> shows that, by adaptively combining local features detected by the individual filters of CNN, highway network enables encoding of much richer semantic features.</p><p>We show that such a design stably enhances the modeling capability of the encoder. An alternative design is to use a 3D CNN with a ResNet, which however highly increases the complexity of the encoder (e.g., a 34-layer ResNet is used in <ref type="bibr" target="#b56">[56]</ref>). Bi-GRU: LCANet encodes long-term temporal information with the Bi-GRU <ref type="bibr" target="#b57">[57]</ref>. 3D-CNN only captures short viseme-level features, however for lipreading problem, there are some visemes visually very similar to each other. Such ambiguity can only be distinguished with longer temporal context which can be well modeled with the state-of-the-art recurrent neural networks such as GRU. A GRU has two gates: 1) a reset gate r which determines how much of the previous memory state should be mixed with the current input; 2) an update gate z which determines how much of the previous memory state should be kept with the current hidden state. The standard formulation is:</p><formula xml:id="formula_2">z t = σ(W z g t + U z h t−1 + C z c t + b z ), r t = σ(W r g t + U r h t−1 + C r c t + b r ), h t = tanh(W h g t + U h (r t h t−1 ) + C p c t ), h t = (1 − z t ) h t−1 + z t h t ,<label>(3)</label></formula><p>where r t and z t is the reset and update gate, respectively, C is the linear projection matrix for integrating the context vector, and g is the output of the highway network. To fully exploit both the past and future context, we use Bi-GRU to capture both the forward and backward information flow. Compared to the vanilla GRU, Bi-GRU can leverage longrange context to improve the motion modeling capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cascaded Attention-CTC Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Connectionist temporal classification (CTC):</head><p>Many state-of-the-art ASR systems train an end-to-end deep neural network using the Connectionist Temporal Classification (CTC) approach <ref type="bibr" target="#b24">[25]</ref>. CTC is an objective loss function that eliminates the requirement of explicit alignment between the input sequence and the target sequence during the model training process. Given an input x of length T , the output of the neural network is a softmax layer which indicates the possibility of emitting the symbol (either a character or 'blank ) at every time-step. For the same target sequence, there can be many alignment candidates since the blank symbol can be inserted at any position. For example, the paths (x, -, -, y, -, z), (-, x, -, y, -, z) and (-, x, -, y, z, -) all W H W T 1 <ref type="figure">Figure 3</ref>: Information flow in the highway network. W t is the weight of the transform gate and 1 − W t is the weight of the carry gate. The highway network leverages adaptive gating units to regulate the information flow, which allows gradients to flow through many layers.</p><p>map to the same target label of (x, y, z). Now, the possibility of generating one correct path π for the given input x is:</p><formula xml:id="formula_3">p(π|x) = T t=1 p(π t |x).<label>(4)</label></formula><p>The probability of generating the target sequence l is the sums over all possible paths π:</p><formula xml:id="formula_4">p(l|x) = i p(π i |x).<label>(5)</label></formula><p>A forward-backward dynamic programming algorithm is proposed to calculate equation (5) efficiently and makes the network training possible. Finally, the CTC network is trained to minimize the negative log-likelihood of the target sequence l as:</p><formula xml:id="formula_5">L CT C = − ln p(l|x).<label>(6)</label></formula><p>According to equation 4, the CTC approach assumes conditional independence of separate labels (i.e., the character symbols predicted by p(π t |x)), and thus obstructing further performance improvement.</p><p>2) Cascaded Attention-CTC: To capture information explicitly from longer context, LCANet feeds the encoded spatiotemperal features into the cascaded attention-CTC decoder <ref type="figure">(Figure 4</ref>). As illustrated here, the decoder uses an attention mechanism to find an alignment between each element of the decoded sequence (s 1 , s 2 , s 3 ) and the hidden states from the encoder (h 1 , h 2 , h 3 ). The attention score is calculated as:</p><formula xml:id="formula_6">e j,t = V a tanh(W a s t−1 + U a h j ),<label>(7)</label></formula><p>α j,t = exp(e j ) T k=1 exp(e k )</p><p>.</p><p>The context vector is computed as the weighted average over all the source hidden states.</p><formula xml:id="formula_8">c t = T k=1 α k,t h k<label>(9)</label></formula><p>The prediction result for the frame at time step t is computed based on the context, hidden state, and the previous prediction:</p><formula xml:id="formula_9">y t = σ(W o Ey t−1 + U o h t−1 + C o c t ).<label>(10)</label></formula><p>where E is the character-level embedding matrix which is much simpler than the word embedding matrix used and learned in neural machine translation tasks. The output prediction is fed into the CTC loss function to guide the model training process.</p><formula xml:id="formula_10">GRU GRU GRU + GRU GRU GRU i1 i2 i3 GRU GRU o1 o2</formula><p>Bi-GRU Attention mechanism debilitates the constraint of the conditional independence assumption in CTC loss. Therefore it improves the modeling capability on the lipreading problem and can give better predictions on visually similar visemes. On the other hand, compared to the attention only architecture, the combination of attention mechanism and CTC substantially reduces the irregular alignments during the training stage, as CTC will guide the attention decoder to eliminate the unnecessary non-sequential predictions between the hypothesis and reference. Therefore, the attention module in LCANet requires fewer glimpses to find the correct focus-then-decode pattern. From the experimental result, the cascaded attention-CTC decoder outperforms existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate the performance of the proposed LCANet and compare it with other state-of-the-art lipreading methods on public benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>GRID The GRID dataset contains 28 hours of videos which are recorded from 34 different speakers each speaks 1000 sentences in a controlled lab environment. Each sentence has a fixed 6-word structure: command + color + preposition + letter + digit + adverb, e.g., "set blue with h seven again". There are 51 different words including four commands, four colors, four prepositions, 25 letters, ten digits and four adverbs, yielding 64,000 possible sentences. For each sentence, the six words are randomly combined. Until the paper submission, GRID is still the largest available public sentence level lip-reading dataset. We follow the evaluation protocol of <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref> to randomly divide the dataset into train, validation and test sets, where 255 sentences of each speaker are used for testing. All videos are recorded with a frame rate of 25fps and each lasts for 3 seconds (i.e., 75 frames per sample). Based on 68 facial landmarks extracted by Dlib <ref type="bibr" target="#b58">[58]</ref>, we crop 100x50 affine-transformed mouth-centered region as the input data. We then apply an affine transformation to extract an aligned mouth region and each has a resolution of 100 × 50. The obtained RGB mouth images are meanreduced and normalized. We perform data augmentation on the training data similar to that used in <ref type="bibr" target="#b2">[3]</ref> to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>We use character error rate (CER), word error rate (WER), and BLEU <ref type="bibr" target="#b59">[59]</ref> to benchmark the proposed framework. CER is computed with the minimum number of operations required to transform the predicted text to the ground truth (e.g., the Levenshtein distance), divided by the number of characters in the ground truth. WER is similar to CER except it operates on a word level. Smaller CER/WER means higher prediction accuracy, while a larger BLEU score is preferred. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We implement and train LCANet using Tensorflow <ref type="bibr" target="#b60">[60]</ref>. The detailed parameters of each layer in the architecture are summarized in <ref type="table" target="#tab_0">Table I</ref>. ADAM optimizer <ref type="bibr" target="#b61">[61]</ref> is used with an initial learning rate of 0.0001 and the batch size is 64.</p><p>We train the propose model 50 epochs on GRID with early stopping.</p><p>The proposed approach is compared with <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b62">[62]</ref>, which is referred as 'Lipnet', 'LRSW' and 'LRW', respectively. LipNet is the first end-to-end sentence-level lipreading model. LRW and LRSW achieve state-of-the-art performance on the "Lip Reading Sentences" dataset and "Lip Reading in the wild" dataset, respectively. In addition, we compare LCANet with several of its variances to explore the impact of the highway network and the cascaded attention-CTC decoder. The structures of these models are described in <ref type="table" target="#tab_0">Table II</ref> and the simplified diagrams are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The model architectures in subfigures 5a, 5c and 5d all use a single CTC loss but varies on whether the highway network layers or the attention mechanism is used. We also compare a multi-task learning architecture <ref type="bibr" target="#b43">[43]</ref> (subfigure 5b) which optimizes a joint loss in two branches: CTC processes one branch, and the joint attention/crossentropy loss handle another. The objective loss function is defined as:</p><formula xml:id="formula_11">L total = λ * L ctc + (1 − λ) * L atten .<label>(11)</label></formula><p>The final prediction result is generated with a joint decoder as described in <ref type="bibr" target="#b43">[43]</ref>. In the experiment, we tried different λ values (from 0.1 to 0.9) and only report the best result here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>The performance comparison between the proposed method and the baselines on the GRID dataset is summarized in <ref type="table" target="#tab_0">Table III</ref>. Our architecture performs characterlevel prediction on input frames. A word is regarded as correctly predicted when "every" character is correct. LCANet <ref type="table" target="#tab_0">Table II: Structure description of the proposed method and  its variants. AH-CTC</ref> achieves 1.3% CER and 2.9% WER. Compared to Lipnet, LCANet has 31.6% and 37.5% improvement on CER and WER, receptively. The LRSW model is pre-trained on the large-scale "Lip Reading Sentences" dataset proposed in <ref type="bibr" target="#b4">[5]</ref>. However, our method still achieves better CER and WER.</p><p>The effectiveness of the highway network layers and the cascaded attention-CTC decoder is demonstrated by the comparison among models {3, 4, 5, 6} in <ref type="table" target="#tab_0">Table III</ref>. First, we show that the LCANet (model {6}) is better than the multitask or multi-branch variant (model {3}). The reason is partially because only the shared encoder is jointly updated by both the attention and CTC module in the multi-task model. Also, since the multi-task model relies on the CTC loss and the cross-entropy loss, it is difficult to coordinate the behavior of the two modules. In contrast, the proposed cascaded architecture uses a single CTC loss to update both the encoder and decoder. The leverage of the attention mechanism guarantees output sequences generated from input sources with long-term coherence. The CTC decoder enforces monotonic alignment to guide the attention module to concentrate on the salient sequential parts of the input video frames. Second, the benefit of the highway network is illustrated by comparing LCANet with model {5}, which yields 0.4% improvement on CER and 1.1% improvement on WER. The function of the highway network is similar to the ResNet <ref type="bibr" target="#b63">[63]</ref>. The difference is the highway network leverages adaptive gating units to regulate the information flow, which allows gradients to flow through many layers without attenuation. Finally, removing the attention mechanism (model {4}) also greatly compromises the overall performance. In addition, model {5} can be regarded as the attention-only model. The comparison between model {5} and model {6} illustrates the proposed cascaded attention-CTC model is better than its attention-only variant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Convergence speed</head><p>The training and validation loss on GRID dataset for the proposed approach and Lipnet are shown in <ref type="figure" target="#fig_4">Figure 6</ref>. In the figure, we mark the required number of epochs for the loss to reduce to 4. LCANet reaches the loss value 4 in 14 epochs for training and 20 epochs for validation. On the contrary, the converge is much slower for Lipnet, which reaches the same loss in 38 epochs for training and 40 epochs for validation. The speed-up comes from the cascaded attention-CTC module. The attention mechanism generates output sequences with long-term temporal correlation, which overcomes the disadvantage of CTC that assumes inputs are conditionally independent. Besides, the attention mechanism can be treated as a pre-alignment operation that eliminates unnecessary paths for the CTC decoder. Meanwhile, CTC guarantees a monotonic alignment between video frames and text labels, which helps to guide the attention mechanism to focus on the sequential order video-text pairs. Therefore, less irrelevant samples will be generated that leads to the observed speed-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Confusion Matrix</head><p>We use the mapping proposed by the IBM ViaVoice database <ref type="bibr" target="#b64">[64]</ref>. 43 phonemes are grouped into 13 visemes classes (including a silence class), denoted by: lip-rounding based vowels (V), Alveolar-semivowels (A), Alveolarfricatives (B), Alveolar (C), Palato-alveolar (D), Bilabial (E),  <ref type="figure" target="#fig_5">Figure 7c</ref>. The misclassification between different viseme categories is not statistically significant. There is a tiny uncertainty between viseme class V and D, which demonstrates the proposed method is effective on the lipreading task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we have presented LCANet, an end-to-end deep neural network architecture for machine lipreading. To accurately predict visemes, LCANet introduced a cascaded attention-CTC decoder which effectively compensates the defect of the conditional independence assumption of the CTC approach. In addition, LCANet stacks highway network layers over 3D-CNN layers to further improve performance. Extensive experiment results show that LCANet achieves the state-of-the-art accuracy as well as faster convergence.</p><p>Our future work lies in two directions. First, LCANet currently deals with the conditional independence assumption of CTC using attention mechanisms, but the loss function itself is not modified. Therefore, we will investigate solutions that can be directly applied on the loss function. Second, we would like to extend LCANet with joint audio and visual input as <ref type="bibr" target="#b4">[5]</ref> to explore further performance improvement. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*Figure 1 :</head><label>1</label><figDesc>Corresponding author. Work was done during Kai's internship at Samsung Research America. 978-1-5386-2335-0/18/$31.00 c 2018 IEEE The proposed end-to-end lipreading system includes three major steps. (1) The mouth regions are cropped from the input video frames from aligned faces. (2) An encoder extracts spatio-temporal features from the input sequence. (3) The cascaded attention-CTC decoder generates text from encoded hidden features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 sFigure 4 :</head><label>24</label><figDesc>Diagram of the proposed cascaded attention-CTC decoder. i and o are the input and output sequence, respectively. h i is the intermediate hidden state of the Bi-GRU. e i,j is the weight of the attention unit, where i, j denotes the index of the output and input node, respectively. Each attention unit generate a fixed-lenght context vector based on the input sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>, H-CTC, and A-CTC use only the CTC loss function. While the AH-CTC-CE model leverages the cross-entropy loss on the attention branch and the CTC loss on another branch. Diagram of the variant models for comparison. (a) AH-CTC (LCANet), (b) AH-CTC-CE, (c) A-CTC, (d) H-CTC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Training curve. The value pair (#iteration, loss) in the figure denotes the required iterations to achieve the specified loss. Dental (F), Labio-dental (G), Velar (H). We plot the confusion matrix for the viseme class of lip-rounding based vowels, Bilabial in Figure 7 since they capture the most confusing phonemes. For example, in Figure 7a, {/AA/, /AY /}, {/AE/, /IH/} and {/AO/, /U W/} are frequently incorrectly classified during the text decoding process. This phenomenon is caused by the similar pronunciation between the text pair of {r, i}, {at, bin}, {f our, two}. The intravisemes categorical confusion matrix is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(a) Lip-rounding based vowels, (b) Bilabial , (c) Intra-visemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>The detailed architecture of the proposed method.</figDesc><table><row><cell>Layer name</cell><cell>Output size</cell><cell>Kernel</cell><cell>Stride</cell><cell>Pad</cell></row><row><cell>3d-conv1</cell><cell>75x50x25x32</cell><cell>3, 5, 5</cell><cell>1, 2, 2</cell><cell>1, 2, 2</cell></row><row><cell>bn/relu/drop</cell><cell>75x50x25x32</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pool1</cell><cell>75x25x12x32</cell><cell>1, 2, 2</cell><cell>1, 2, 2</cell><cell></cell></row><row><cell>3d-conv2</cell><cell>75x25x12x64</cell><cell>3, 5, 5</cell><cell>1, 1, 1</cell><cell>1, 2, 2</cell></row><row><cell>bn/relu/drop</cell><cell>75x25x12x64</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pool2</cell><cell>75x12x6x64</cell><cell>1, 2, 2</cell><cell>1, 2, 2</cell><cell></cell></row><row><cell>3d-conv3</cell><cell>75x12x6x96</cell><cell>3, 5, 5</cell><cell>1, 2, 2</cell><cell>1, 2, 2</cell></row><row><cell>bn/relu/drop</cell><cell>75x12x6x96</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pool3</cell><cell>75x6x3x96</cell><cell>1, 2, 2</cell><cell>1, 2, 2</cell><cell></cell></row><row><cell>highway1</cell><cell>75x1728</cell><cell></cell><cell></cell><cell></cell></row><row><cell>highway2</cell><cell>75x1728</cell><cell></cell><cell></cell><cell></cell></row><row><cell>gru1</cell><cell>75x512</cell><cell></cell><cell></cell><cell></cell></row><row><cell>gru2</cell><cell>75x512</cell><cell></cell><cell></cell><cell></cell></row><row><cell>atten1</cell><cell>75x28</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CTC Loss</cell><cell>75</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table III :</head><label>III</label><figDesc>Performance on the GRID dataset.</figDesc><table><row><cell>#</cell><cell>Method</cell><cell>CER %</cell><cell>WER %</cell><cell>BLEU %</cell></row><row><cell>1</cell><cell>Lipnet [3]</cell><cell>1.9</cell><cell>4.8</cell><cell>96.0</cell></row><row><cell>2</cell><cell>LRSW [5]</cell><cell></cell><cell>3.0</cell><cell></cell></row><row><cell>3</cell><cell>AH-CTC-CE</cell><cell>2.1</cell><cell>4.9</cell><cell>95.8</cell></row><row><cell>4</cell><cell>H-CTC</cell><cell>1.8</cell><cell>4.3</cell><cell>96.4</cell></row><row><cell>5</cell><cell>A-CTC</cell><cell>1.7</cell><cell>4.1</cell><cell>96.7</cell></row><row><cell>6</cell><cell>AH-CTC(LCANet)</cell><cell>1.3</cell><cell>2.9</cell><cell>97.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For lipreading, m is normally smaller than n, meaning each character symbol corresponds to 1 or more video frames.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparison of human and machine-based lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hilder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end visual speech recognition with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lipnet: Sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename></persName>
		</author>
		<idno>abs/1611.01599</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lipreading with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lipreading using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel image tag completion method based on convolutional neural transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning convolutional ranking-score function by query preference regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Data Engineering and Automated Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A cnn regression approach for real-time 2d/3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1352" to="1363" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reconstruction for feature disentanglement in pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully automatic initialization of two-dimensional-three-dimensional medical image registration using hybrid classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Abdel-Fatah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mahfouz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24007</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust x-ray image segmentation by spectral clustering and active shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mahfouz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">34005</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Articulated and generalized gaussian kernel correlation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="776" to="789" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilateral two-dimensional neighborhood preserving discriminant embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic segmentation of method code into meaningful blocks: Design and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software: Evolution and Process</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatically generating natural language descriptions for object-related statement sequences</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)</title>
		<meeting>the 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)</meeting>
		<imprint>
			<date type="published" when="2017-02" />
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02720</idno>
		<title level="m">Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<idno type="arXiv">aarXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The natural statistics of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1000436</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Progressive joint modeling in unsupervised single-channel overlapped speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07048</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-channel speech recognition: Lstms all the way through</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHiME-4 workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01781</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeprebirth: Accelerating deep neural network execution on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<idno>abs/1708.04728</idno>
		<ptr target="http://arxiv.org/abs/1708.04728" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>abs/1510.00149</idno>
		<ptr target="http://arxiv.org/abs/1510.00149" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;1mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>abs/1602.07360</idno>
		<ptr target="http://arxiv.org/abs/1602.07360" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ese: Efficient speech recognition engine with sparse lstm on fpga</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA &apos;17</title>
		<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<idno type="DOI">http:/doi.acm.org/10.1145/3020078.3021745</idno>
		<ptr target="http://doi.acm.org/10.1145/3020078.3021745" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Draps: Dynamic and resource-aware placement scheme for docker containers in a heterogeneous cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pompili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 36th International Performance Computing and Communications Conference (IPCCC)</title>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint ctc/attention decoding for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lip reading based on cascade feature extraction and hmm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Signal Processing</title>
		<imprint>
			<publisher>ICSP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lip reading using dwt and lsda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Morade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patnaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Advance Computing Conference (IACC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning multi-boosted hmms for lip-password based speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Local spatiotemporal descriptors for visual recognition of spoken phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Human-centered Multimedia</title>
		<meeting>the International Workshop on Human-centered Multimedia</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lip reading using optical flow and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Shaikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Congress on Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visual speech recognition using active shape models and hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Comparison of depth-based features for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Palecek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Telecommunications and Signal Processing (TSP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An image transform approach for hmm based automatic lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Robust end-to-end deep audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanabria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06986</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Combining residual networks with lstms for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical report. Center for Language and Speech Processing</title>
		<meeting><address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
