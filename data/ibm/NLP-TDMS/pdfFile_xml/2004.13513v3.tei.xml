<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
							<email>arthur.douillard@heuritech.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Heuritech, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<email>matthieu.cord@sorbonne-universite.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Ollion</surname></persName>
							<email>charles.ollion@heuritech.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Heuritech, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Robert</surname></persName>
							<email>thomas.robert@heuritech.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Heuritech, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
							<email>dovalle@dca.fee.unicamp.br</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<settlement>Campinas</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>3 valeo.ai</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>incremental-learning, representation-learning pooling</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks -a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatialbased distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively. 5</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lifelong machine learning <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b37">37]</ref> focuses on models that accumulate and refine knowledge over large timespans. Incremental learning -the ability to aggregate different learning objectives seen over time into a coherent whole -is paramount to those models. To achieve incremental learning, models must fight catastrophic forgetting <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b8">8]</ref> of previous knowledge. Lifelong and incremental learning have attracted much attention in the past few years, but existing works still struggle to preserve acquired knowledge over many cycles of short incremental learning steps.</p><p>We will focus on image classifiers, which are ordinarily trained once on a fixed set of classes. In incremental learning, however, the classifier must learn the classes by steps, in training cycles called tasks. At each task, we expose the classifier to a new set of classes. Incremental learning would reduce trivially to ordinary classification if we were allowed to store all training samples, but we are imposed a limited memory: a maximum number of samples for previously learned classes. This limitation is motivated by practical applications, in which privacy issues, or storage and computing limitations prevent us from simply retraining the entire model for each new task <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>. Furthermore, incremental learning is different from transfer learning in that we aim to have good performance in both old and new classes.</p><p>To overcome catastrophic forgetting, different approaches have been proposed: reusing a limited amount of previous training data <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b2">3]</ref>; learning to generate the training data <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b36">36]</ref>; extending the architecture for new phases of data <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b22">22]</ref>; using a sub-network for each phase <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11]</ref>; or constraining the model divergence as it evolves <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In this work, we propose PODNet, approaching incremental learning as representation learning, with a distillation loss that constrains the evolution of the representation. By carefully balancing the compromise between remembering the old classes and learning new ones, we learn a representation that fights catastrophic forgetting, remaining stable over long runs of small incremental tasks. Our model innovates on existing art with (1) an efficient spatial-based distillation-loss applied throughout the model ; and (2) as a refinement, a representation comprising multiple proxy vectors for each class, resulting in a more flexible representation.</p><p>In this paper, we first present the existing state of the art (Sec. 2), which we close by detailing our contributions. We then describe our model (Sec. 3), and evaluate it in an extensive set of experiments (Sec. 4) on CIFAR100, Ima-geNet100, and ImageNet1000, including ablation studies assessing each contribution, and extensive comparisons with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To approach the problem of incremental learning, consider a single incremental task: one has a classifier already trained over a set of old classes and must adapt it to learn a set of new classes. To perform that single task, we will consider: (1) the data/class representation model; <ref type="bibr" target="#b1">(2)</ref> the set of constraints to prevent catastrophic forgetting; (3) the experimental context (including the constraints over the memory for previous training data) for which to design the model.</p><p>Data/class representation model. Representation learning was already implicitly present in iCaRL <ref type="bibr" target="#b33">[33]</ref>: it introduced the Nearest Mean Exemplars (NME) strategy which averages the outputs of the deep convolutional network to create a single proxy feature vector per class that are then used by a nearest-neighbor classifier predict the final classes. Hou et al. <ref type="bibr" target="#b14">[14]</ref> adopted this method and also introduced another, named CNN, which uses the output class probabilities to classify incoming samples, freezing (during training) the classifier weights associated with old classes, and then fine-tuning them on an under-sampled dataset.</p><p>Hou et al. <ref type="bibr" target="#b14">[14]</ref>, in the method called here UCIR, made representation learning explicit, by noticing that the limited memory imposed a severe imbalance on the training samples available for the old and for the new classes. To overcome that difficulty, they designed a metric-learning model instead of a classification model. That strategy is often used in few-shot learning <ref type="bibr" target="#b9">[9]</ref> because of its robustness to few data. Because classical metric architectures require special training sampling (e.g., semi-hard sampling for triplets), Hou et al. chose instead to redesign the classifier's last layer of their model to use the cosine similarity <ref type="bibr" target="#b27">[27]</ref>.</p><p>Model constraints to prevent catastrophic forgetting. Constraining the model's evolution to prevent forgetting is a fruitful idea proposed by several methods <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b2">3]</ref>. Preventing the model's parameters from diverging too much forces it to remember the old classes, but care must be taken to still allow it to learn the new ones. We call this balance the rigidity-plasticity trade-off.</p><p>Existing art on knowledge distillation/compression <ref type="bibr" target="#b13">[13]</ref> was an important source of inspiration for constraints on models. The goal is to distill a large trained model (called teacher) into a new smaller model (called student). The distillation loss forces the features of the student to approach those of its teacher. In our case, the student is the current model and the teacher-with same capacityis its version at the previous task. Zagoruyko and Komodakis <ref type="bibr" target="#b19">[19]</ref> investigated attention-based distillation for image classifiers, by pooling the intermediate features of convolutional networks into attention maps, then used in their distillation losses. Li and Hoiem [23] --and several authors after them <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b38">38]</ref> -used a binary cross-entropy between the output probabilities by the models. Hou et al. <ref type="bibr" target="#b14">[14]</ref>, used instead Less-Forget, a cosine-similarity constraint on the flat feature embeddings after the global average pooling. Dhar et al. <ref type="bibr" target="#b4">[5]</ref> proposed to constrain the gradient-based attentions generated by GradCam <ref type="bibr" target="#b35">[35]</ref>, a visualization method. Wu et al. <ref type="bibr" target="#b38">[38]</ref> proposed BiC, an algorithm oriented towards large-scale datasets, which employs a small linear model learned on validation data to recalibrate the output probabilities before applying a distillation loss.</p><p>Experimental context. A critical component of incremental learning is the convention used for the memory storing samples of previous data. An usual convention is to consider a fixed amount of samples allowed in that memory, as illustrated in <ref type="figure">Fig. 1</ref>.</p><p>Still, there are two experimental protocols for such fixed-sample convention: we may either use the memory budget at will (M total ), or add a constraint on the number of samples per class for the old classes (M per ). When M total = M per ×# of classes, both settings have equivalent final memory size, but the latter, that we adopt, is much more challenging since early tasks cannot benefit from the full memory size. The granularity of the increments is another critical element: with a fixed number of classes, increasing the number of tasks decreases the number of classes per task. More tasks imply stronger forgetting of the earliest classes, and pushing that number creates a challenging protocol, so far unexplored by</p><formula xml:id="formula_0">! E ∘ f 0 C 1 ⋮ C 50</formula><p>Train Initial classes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Train existing art. Hou et al. evaluate at most 10 tasks on CIFAR100, while we propose as much as 50 tasks. Finally, to score the experiments, Rebuffi et al. <ref type="bibr" target="#b33">[33]</ref> proposed a global metric that they called average incremental accuracy, taking into account the entire history of the run, averaging the accuracy at the end of each task (including the first).</p><formula xml:id="formula_1">! ' ∘ f 1 m 1 ⋮ m 50 C 51 ⋮ C 60 New classes Memory Train ! * ∘ f 2 m 1 ⋮ m 60 C 61 ⋮ C 70 ! &lt; ∘ f 3 m 1 ⋮ m 70 C 71 ⋮ C 80</formula><p>Contributions. As seen, associating representation learning to model constraints is a particularly fruitful idea for incremental learning, but requires carefully balancing the goals of rigidity (to avoid catastrophic forgetting) and plasticity (to learn new classes).</p><p>Employing a distillation-based loss to constrain the evolution of the representation has also resulted in leading results <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b4">5]</ref>. Our model improves existing art by employing a novel and efficient spatial-based distillation loss, which we are able to apply throughout the model. Implicit or explicit proxy vectors representing each class inside the models have lead to state of the art results <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b14">14]</ref>. Our model extends that idea allowing for multiple proxy vectors per class, resulting in a more flexible representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Formally, we learn the model in T tasks, task t comprising a set of new classes C t N , and a set of old classes C t O , and aiming at classifying all seen classes C t O ∪ C t N . Between tasks, the new set C t O will be set to C t−1 O ∪ C t−1 N , but the amount of training samples from C t O (called memory) is constrained to exactly M per samples per class, while all training samples in the dataset are allowed for the classes in C t N , as shown in <ref type="figure">Fig. 1</ref>. The resulting imbalance, if unmanaged, leads to catastrophic forgetting <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b8">8]</ref>, i.e., learning the new classes at the cost of forgetting the old ones.</p><p>Our base model is a deep convolutional networkŷ = g(f (x)), where x is the input image, y is the output vector of class probabilities, h = f (x) is the "feature extraction" part of the network (all layers up to the next-to-last),ŷ = g(h) is the final classification layer, and h is the final embedding of the network before classification <ref type="figure" target="#fig_2">(Fig. 3</ref>). The superscript t denotes the model learned at task t:f t , g t , h t , etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">POD: Pooled Outputs Distillation loss</head><p>Constraining the evolution of the weights is crucial to reduce forgetting. Each new task t learns a new (student) model, whose weights are not only initialized with those of the previous (teacher) model, but also constrained by a distillation loss. That loss must be carefully balanced to prevent forgetting (rigidity), while allowing the learning of new classes (plasticity).</p><p>To this goal, we propose a set of constraints we call Pooled Outputs Distillation (POD), applied not only over the final embedding output by h t = f t (x), but also over the output of its intermediate layers h t = f t (x) (where by notation</p><formula xml:id="formula_2">overloading f t (x) ≡ f t • . . . • f t 1 (x), and thus f t (x) ≡ f t L . . . • f t • . . . f t 1 (x)</formula><p>). The convolutional layers of the network output tensors h t with components h t ,c,w,h , where c stands for channel (filter), and w × h for column and row of the spatial coordinates. The loss used by POD may pool (sum over) one or several of those indexes, more aggressive poolings ( <ref type="figure" target="#fig_1">Fig. 2</ref>) providing more freedom, and thus, plasticity: the lowest possible plasticity imposes an exact similarity between the previous and current model while higher plasticity relaxes the similarity definition.</p><p>Pooling is an important operation in Computer Vision, with a strong theoretical motivation. In the past, pooling has been introduced to obtain invariant representations <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b21">21]</ref>. Here, the justification is similar, but the goal is different: as we will see, the pooled indexes are aggregated in the proposed loss, allowing plasticity. Instead of the model acquiring invariance to the input image, the desired loss acquires invariance to model evolution, and thus, representation. The proposed pooling-based formalism has two advantages: first, it organizes disparately proposed distillation losses into a neat, general formalism. Second, as we will see, it allowed us to propose novel distillation losses, with better plasticity-rigidity compromises. Those topics are explored next.</p><p>Pooling of convolutional outputs. As explained before, POD constrains the output of each intermediate convolutional layer h t ,c,w,h = f t (·) (in practice, each stage of a ResNet <ref type="bibr" target="#b12">[12]</ref>). As a reminder, c is the channel and w × h are the spatial coordinates. All POD variants use the Euclidean distance of 2 -normalize tensors, here noted as · − · . They differ on the type of pooling applied before that distance is computed. On one extreme, one can apply no pooling at all, resulting in the most strict loss, the most rigid constrains, and the lowest plasticity:</p><formula xml:id="formula_3">L POD-pixel (h t−1 , h t ) = C c=1 W w=1 H h=1 h t−1 ,c,w,h − h t ,c,w,h 2 .<label>(1)</label></formula><p>By pooling the channels, one preserves only the spatial coordinates, resulting in a more permissive loss, allowing the activations to reorganize across the channels, but penalizing global changes of those activations across the space,</p><formula xml:id="formula_4">L POD-channel (h t−1 , h t ) = W w=1 H h=1 C c=1 h t−1 ,c,w,h − C c=1 h t ,c,w,h 2 ;<label>(2)</label></formula><p>or, contrarily, by pooling the space (equivalent, up to a factor, to a Global Average Pooling), one preserves only the channels:</p><formula xml:id="formula_5">L POD-gap (h t−1 , h t ) = C c=1 W w=1 H h=1 h t−1 ,c,w,h − W w=1 H h=1 h t ,c,w,h 2 .<label>(3)</label></formula><p>Note that the only difference between the variants is in the position of the summation. For example, contrast equations Equation 1 and 2: in the former the differences are computed between activation pixels, and then totaled; in the latter, first the channel axis is flattened, then the differences are computed, resulting in a more permissive loss.</p><p>We can trade a little plasticity for rigidity, with less aggressive pooling by aggregating statistics across just one of the spatial dimensions:</p><formula xml:id="formula_6">L POD-width (h t−1 , h t ) = C c=1 H h=1 W w=1 h t−1 ,c,w,h − W w=1 h t ,c,w,h 2 ;<label>(4)</label></formula><p>or, likewise, for the vertical dimension, resulting in POD-height. Each of those variants measure the distribution of activation pixels across their respective axis. These two complementary intermediate statistics can be further combined together:</p><formula xml:id="formula_7">L POD-spatial (h t−1 , h t ) = L POD-width (h t−1 , h t ) + L POD-height (h t−1 , h t ) . (5)</formula><p>L POD-spatial is minimal when the average statistics over the dataset, on both width and height axes, are similar for the previous and current model. It brings the right balance between being too rigid (Equation 1) and being too permissive (Equation 2 and 3).</p><p>Constraining the final embedding. After the convolutional layers, the network, by design, flattens the spatial coordinates, and the formalism above needs adjustment, as a summation over w and h is no longer possible. Instead, we set a flat constraint on the final embedding h t = f t (x):</p><formula xml:id="formula_8">L POD-flat (h t−1 , h t ) = h t−1 − h t 2 .<label>(6)</label></formula><p>Combining the losses, analysis. The final POD loss combines the two components:</p><formula xml:id="formula_9">L POD-final (x) = λ c L − 1 L−1 =1 L POD-spatial f t−1 (x), f t (x) + λ f L POD-flat f t−1 (x), f t (x) . (7)</formula><p>The hyperparameters λ c and λ f are necessary to balance the two terms, due to the different nature of the intermediate outputs (spatial and flat).</p><p>As mentioned, the strategy above generalizes disparate propositions existing both in the literature of incremental learning, and elsewhere. When λ c = 0, it reduces to the cosine constraint of Less-Forget, proposed by Hou et al. for incremental learning, which constrains only the final embedding <ref type="bibr" target="#b14">[14]</ref>. When λ f = 0 and POD-spatial is replaced by POD-pixel, it suggests the Perceptual Features loss, proposed for style transfer <ref type="bibr" target="#b16">[16]</ref>. When λ f = 0 and POD-spatial is replaced by POD-channel, the strategy hints at the loss proposed by Komodakis et al. <ref type="bibr" target="#b19">[19]</ref> to allow distillation across different networks, a situation in which the channel pooling responds to the very practical need to allow the comparison of architectures with different number of channels.</p><p>As we will see in our evaluations of pooling strategies (Sec. 4.2), what proved optimal was a completely novel idea, POD-spatial, combining two poolings, each of which flattens one of the spatial coordinates. That relatively rigid strategy (channels and one of the spatial coordinates are considered in each half of the loss) makes intuitive sense in our context, which is small-task incremental learning, and thus where we expect a slow drift of the model across a single task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Similarity Classifier</head><p>Hou et al. <ref type="bibr" target="#b14">[14]</ref> observed that the class imbalance of incremental learning have concrete manifestations on the parameters of the final layer on classifiers, namely the weights for the over-represented (new) classes becoming much larger than those for the underrepresented (old) classes. To overcome this issue, their method (called here UCIR) 2 -normalizes both the weights and the activations, which corresponds to taking the cosine similarity instead of the dot product. For each class c, their last layer becomeŝ</p><formula xml:id="formula_10">y c = exp (η θ c , h ) i exp (η θ i , h ) ,<label>(8)</label></formula><formula xml:id="formula_11">f t -1 h x f t h t ℒ !"# ℒ )*+,-./0 ℒ )*+,12/03/.</formula><p>Multi-mode classification POD Distillation where θ c are the last-layer weights for class c, η is a learned scaling parameter, and ·, · is the cosine similarity. However, this strategy optimizes a global similarity: its training objective increases the similarity between the extracted features and their associated weights. For each class, the normalized weight vector acts as a single proxy <ref type="bibr" target="#b28">[28]</ref>, towards which the learning procedure pushes all samples in the class.</p><formula xml:id="formula_12">! " 1 ! " 2 ⋮ ! " T {% $ % } {% ' % } ⋮ {% ( % } Attention maps Classification module t -1 4 ' h ( ) h ( )*+</formula><p>We observed that such global strategy is hard to optimize in an incremental setting. To avoid forgetting, the distillation losses (Sec. 3.1) tries to keep the final embedding h consistent through time so that the class proxies stay relevant for the classifier. Unfortunately catastrophic forgetting, while alleviated by current methods, is not solved and thus the distribution of h may change. The cosine classifier is very sensitive to those changes as it models a unique majority mode through its class proxies.</p><p>Local Similarity Classifier. The problem above lead us to amend the classification layer during training, in order to consider multiple proxies/modes per class. A shift in the distribution of h will have less impact on the classifier as more modes are covered.</p><p>Our redesigned classification layer, which we call Local Similarity Classifier (LSC), allows for K multiple proxies/modes during training. Like before, the proxies are a way to interpret the weight vector in the cosine similarity, thus we allow for K vectors θ c,k for each class c. The similarity s c,k to each proxy/mode is first computed. An averaged class similarityŷ c is the output of the classification layer:</p><formula xml:id="formula_13">s c,k = exp θ c,k , h i exp θ c,i , h ,ŷ c = k s c,k θ c,k , h .<label>(9)</label></formula><p>The multi-proxies classifier optimizes the similarity of each sample to its ground truth class representation and minimizes all others. A simple cross-entropy loss would work, but we found empirically that the NCA loss <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b28">28]</ref> converged faster. We added to the original loss a hinge [ · ] + to keep it bounded, and a small margin δ to enforce stronger class separation, resulting in the final formulation:</p><formula xml:id="formula_14">L LSC = − log exp (η(ŷ y − δ)) i =y exp ηŷ i + .<label>(10)</label></formula><p>Weight initialization for new classes. The incremental learning setting imposes detecting new classes at each new task t. New weights {θ c,k | ∀c ∈ C t N , ∀k ∈ 1...K} must be added to predict them. We could initialize them randomly, but the class-agnostic features of the ConvNet f , extracted by the model trained so far offer a better prior. Thus, we employ a generalization of Imprinted Weights <ref type="bibr" target="#b31">[31]</ref> procedure to multiple modes: for each new class c, we extract the features of its training samples, use a k-means algorithm to split them into K clusters, and use the centroids of those clusters as initial values for θ c,k . This procedure ensures mode diversity at the beginning of a new task and resulted in a one percentage point improvement on CIFAR100 <ref type="bibr" target="#b20">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complete model formulation</head><p>Our model has the classical structure of a convolutional network f (·) acting as a features extractor, and a classifier g(·) producing a score per class. We introduced two innovations to this model: (1) our main contribution is a novel distillation loss (POD) applied all over the ConvNet, from the spatial features h to the final flat embedding h; (2) as further refinement we propose that the classifier learns a multi-modal representation that explicitly keeps multiple proxy vectors per class, increasing the model expressiveness and thus making it less sensible to shift in the distribution of h. The final loss for current model g t • f t , i.e., the model trained for task t, is simply their addition L {f t ;g t } = L LSC + L POD-final .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare our technique (PODNet) with three state-of-the-art models. Those models are particularly comparable to ours since they all employ a sample memory with a fixed capacity. Both iCaRL <ref type="bibr" target="#b33">[33]</ref> and UCIR <ref type="bibr" target="#b14">[14]</ref> use the same inference method -Nearest-Mean-Examplars (NME), although UCIR also proposes a second inference method based on the classifier probabilities (called here UCIR-CNN). We evaluate PODNet with both inference methods for a small scale dataset, and the later for larger scale datasets. BiC <ref type="bibr" target="#b38">[38]</ref>, while not focused on representation learning, is specially designed to be effective on large scale datasets, and thus provided an interesting baseline.</p><p>Datasets. We employ three images datasets -extensively used in the literature of incremental learning -for our experiments: CIFAR100 <ref type="bibr" target="#b20">[20]</ref>, Ima-geNet100 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b38">38]</ref>, and ImageNet1000 <ref type="bibr" target="#b3">[4]</ref>. ImageNet100 is a subset of Ima-geNet1000 with only 100 classes, randomly sampled from the original 1000. Protocol. We validate our model and the compared baselines using the challenging protocol introduced by Hou et al. <ref type="bibr" target="#b14">[14]</ref>: we start by training the models on half the classes (i.e., 50 for CIFAR100 and ImageNet100, and 500 for Ima-geNet1000). Then the classes are added incrementally in steps. We divide the remaining classes equally among the steps, e.g., for CIFAR100 we could have 5 steps of 10 classes or 50 steps of 1 class. Note that a training of 50 steps is actually made of 51 different tasks: the initial training followed by the incremental steps. Models are evaluated after each step on all the classes seen until then. To facilitate comparison, the accuracies at the end of each step are averaged into a unique score called average incremental accuracy <ref type="bibr" target="#b33">[33]</ref>. If not specified otherwise, the average incremental accuracy is the score reported in all our results. Following Hou et al. <ref type="bibr" target="#b14">[14]</ref>, for all datasets, and all compared models, we limit the memory M per to 20 images per old class. For results with different memory settings, refer to Sec. 4.2.</p><p>Implementation details. For fair comparison, all compared models employ the same ConvNet backbone: ResNet-32 for CIFAR100, and ResNet-18 for Ima-geNet. We remove the ReLU activation at the last block of each ResNet end-ofstage to provide a signed input to POD (Sec. 3.1). We implemented our method (called here PODNet) in PyTorch <ref type="bibr" target="#b30">[30]</ref>. We compare both ours and UCIR's implementation <ref type="bibr" target="#b14">[14]</ref> of iCaRL. Results of UCIR come from the implementation of Hou et al. <ref type="bibr" target="#b14">[14]</ref>. We provide their reported results and also run their code ourselves. We used our implementation of BiC in order to compare with the same backbone. We sample our memory images using herding selection <ref type="bibr" target="#b33">[33]</ref> and perform the inference with two different methods: the Nearest-Mean-Examplars (NME) proposed for iCarl, and also adopted on one of the variants of UCIR <ref type="bibr" target="#b14">[14]</ref>, and the "CNN" method introduced for UCIR (see Sec. 2). Please see the supplementary materials for the full implementation details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative results</head><p>The comparisons with all the state of the art are tabulated in <ref type="table" target="#tab_0">Table 1</ref> for CI-FAR100 and <ref type="table" target="#tab_1">Table 2</ref> for ImageNet100 and ImageNet1000. All tables shows the average incremental accuracy for each considered models with various number of steps on the incremental learning run. The "New classes per step" row shows the amount of new classes introduced per task.</p><p>CIFAR100. We run our comparisons on 5, 10, 25, and 50 steps with respectively 10, 5, 2, and 1 classes per step. We created three random class orders to ran each experiment thrice, reporting averages and standard deviations. For CIFAR100 only, we evaluated our model with two different kind of inference: NME and CNN. With both methods, our model surpasses all previous state of the art models on all steps. Moreover, our model relative improvement grows as the number the steps increases, surpassing existing models by 0.82, 2.81, 5.14, and 12.1 percent points ( p.p.) for respectively 5, 10, 25, and 50 steps. Larger numbers of steps imply stronger forgetting; those results confirm that PODNet manages to reduce drastically the said forgetting. While PODNet with NME has the largest gain, PODNet with CNN also outperforms the previous state of the art by up to 8.68 p.p. See <ref type="figure">Fig. 4</ref> for a plot of the incremental accuracies on this dataset. In the extreme setting of 50 increments of 1 class <ref type="figure" target="#fig_3">(Fig. 4a)</ref>, our model showcases large differences, with slow degradation ("gradual forgetting" <ref type="bibr" target="#b8">[8]</ref>) due to forgetting throughout the run, while the other models show a quick performance collapse ("catastrophic forgetting") at the start of the run.</p><p>ImageNet100. We run our comparisons on 5, 10, 25, and 50 steps with respectively 10, 5, 2, and 1 classes per step. For both ImageNet100, and ImageNet1000 we report only PODNet with CNN, as the kNN-based NME classifier did not generalize as well to larger-scale datasets. With the more complex images of Im- <ref type="table">Table 3</ref>. Ablation studies performed on CIFAR100 with 50 steps. We report the average incremental accuracy  ageNet100, our model also outperforms the state of the art on all tested runs, by up to 6.51 p.p.</p><p>ImageNet1000. This dataset is the most challenging, with much greater image complexity than CIFAR100, and ten times the number of classes as ImageNet100. We evaluate the models in 5 and 10 steps, and results confirm the consistent improvement of PODNet against existing arts by up to 2.85 p.p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Further analysis &amp; ablation studies</head><p>Ablation Studies. Our model has two components: the distillation loss POD and the LSC classifier. An ablation study showcasing the contribution of each component is displayed in <ref type="table">Table 3a</ref>: each additional component improves the model performance. We evaluate every ablation on CIFAR100 with 50 steps of 1 new class each. The reported metric is the average incremental accuracy. The table shows that our novel method of constraining the whole ConvNet is beneficial. Furthermore applying only POD-spatial still beats the previous state of the art by a significant margin. Using both POD-spatial and POD-flat then further increases results with a large gain. We also compare the results with the Cosine classifier <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b14">14]</ref> against the Local Similarity Classifier (LSC) with NCA loss. Finally, we add LSC-CE: our classifier with multi-mode but with a simple cross-entropy loss instead of our modified NCA loss. This version brings to mind SoftTriple <ref type="bibr" target="#b32">[32]</ref> and Infinited Mixture Prototypes <ref type="bibr" target="#b1">[2]</ref>, used in the different context of few-shot learning. The latter only considers the closest mode of each class in its class assignment, while LSC considers all modes of a class, thus, taking into account the intra-class variance. That allows LSC to decrease class similarity when intra-class variance is high (which could signal a lack of confidence in the class). Spatial-based distillation. We apply our distillation loss POD differently for the flat final embedding h (POD-flat) and the ConvNet's intermediate features maps h (POD-spatial). We designed and evaluated several alternative for the latter whose results are shown in <ref type="table">Table 3b</ref>. Refer to Sec. 3.1 and <ref type="figure" target="#fig_1">Fig. 2</ref> for their definition. All losses are evaluated with POD-flat. "None" is using only PODflat. Overall, we see that not using pooling results in bad performance (PODpixels). Our final loss, POD-spatial, surpasses all others by taking advantages of the statistics aggregated from both spatial axis. For the sake of completness we also included losses not designed by us: GradCam distillation <ref type="bibr" target="#b4">[5]</ref> and Perceptual Style <ref type="bibr" target="#b16">[16]</ref>. The former uses a gradient-based attention while the later -used for style transfer -computes a gram matrix for each channel.</p><p>Forgetting and plasticity balance. Forgetting can be drastically reduced by imposing a high factor on the distillation losses. Unfortunately, it will also degrade the capacity (its plasticity) to learn new classes. When POD-spatial is added on top of POD-flat, we manage to increase the oldest classes performance (+7 percentage points) while the newest classes performance were barely reduced (-0.2 p.p.). Because our loss POD-spatial constraints only statistics, it is less stringent than a loss based on exact pixels values as POD-pixel. The latter hurts the newest classes (-2 p.p.) for a smaller improvement of old classes (+5 p.p.).</p><p>Furthermore our experiments confirmed that LSC reduced the sensibility of the model to distribution shift, as the performance it brings was localized on the old classes.</p><p>Robustness of our model. While previous results showed that PODNet improved significantly over the state-of-the-arts, we wish here to demonstrate here the robustness of our model to various factors. In <ref type="table">Table 4</ref>, we compared how PODNet behaves against the baseline when the memory size per class M per changes: PODNet improvements increase as the memory size decrease, up to a gain of 26.20 p.p. with NME (resp. 13.42 p.p. for CNN) with M per = 5. Notice that by default, the memory size is 20 in Sec. 4.1. We also compared our model against baselines with a more flexible memory M total = 2000 <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b38">38]</ref>, and with various initial task size (by default it is 50 on CIFAR100). In the former case <ref type="table">(Table 5a)</ref>, models benefit from a larger memory per class in the early tasks. In the later case <ref type="table">(Table 5b)</ref>, models initialization is worse because of a smaller initial task size. In these settings very different from Sec. 4.1, PODNet still outperformed significantly the compared models, proving the robustness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced in this paper a novel distillation loss (POD) constraining the whole convolutional network. This loss strikes a balance between reducing forgetting of old classes and learning new classes, essential for long incremental runs, by carefully chosen pooling. As a further refinement, we proposed a multi-mode similarity classifier, more robust to shift in the distribution inherent to incremental learning. Those innovations allow PODNet to outperform the previous state of the art in a challenging experimental context, with severe sample-per-class memory limitation, and long runs of many small-sized tasks, by a large margin. Extensive experiments over three datasets show the robustness of our model on different settings.</p><p>Acknowledgement. E. Valle is funded by FAPESP grant 2019/05018-1 and CNPq grants 424958/2016-3 and 311905/2017-0. This work was performed using HPC resources from GENCI-IDRIS (Grant 2020-AD011011706). We also wish to thanks Estelle Thou for the helpful discussion.</p><p>6 Supplementary Material 6.1 Spatial-based distillation without POD-flat</p><p>In <ref type="table">Table 3</ref>.b of the main paper, we compared distillation loss alternatives to our final POD-spatial. In this table, the spatial-based losses were evaluated with POD-flat. In <ref type="table" target="#tab_5">Table 6</ref>, we evaluate those same losses without POD-flat. "None" is using only our LSC classifier without any distillation losses. Notice that PODspatial -and its sub-components POD-width and POD-height-are the only losses barely affected by POD-flat's absence. Note that all alternative losses were tuned on the validation set to get the best performance, including those from external papers. Still, our proposed loss, POD-spatial, outperforms all, both with and without POD-flat. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation details</head><p>For all datasets, images are augmented with random crops and flips. For CI-FAR100, we additionally change image intensity by a random value in the range <ref type="bibr">[-63, 63]</ref>. We train our model for 160 epochs for CIFAR100, and 90 epochs for both ImageNet100 and ImageNet100, with a SGD optimizer with momentum of 0.9. For all datasets, we start with a learning rate of 0.1, a batch size of 128, and cosine annealing scheduling. The weight decay is 5 · 10 −4 for CIFAR100, and 1 · 10 −4 for ImageNet100 and ImageNet1000. For CIFAR100 we set model hyperparameters λ c = 3 and λ f = 1, while for ImageNet100 and 1000 we set λ c = 8 and λ f = 10. Our model uses POD-spatial and POD-flat except when explicitly stated otherwise. Following Hou et al. <ref type="bibr" target="#b14">[14]</ref>, we multiply both losses by the adaptive scaling factor: λ = N /T with N being the number of seen classes and T the number of classes in the current task.</p><p>For POD-spatial, before sum-pooling we take the features to the power of 2 element-wise. The vector resulting from the pooling is then L2 normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Number of proxies per class</head><p>While our model's expressiveness increases with more proxies in L LSC , it remains fairly stable for values between 5 and 15, thus, for simplicity, we kept it fixed to 10 in all experiments.</p><p>In initial experiments, we had the following pairs for the number of clusters (k) and average incremental accuracy (acc): k=1, acc=56.80%; k=2, 57.14%; k=4, acc=57.40%; k=6, acc=57.46%; k=8, acc=57.95%, and k=10, acc=57.98% -i.e., a 1.18 p.p. improvement moving from k=1 to k=10. On ImageNet100, with 10 steps/tasks (increments of give classes per task), moving from k=1 to k=10 improved 1.51 p.p. on acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Reproducibility</head><p>Code Dependencies The Python version is 3.7.6. We used the PyTorch <ref type="bibr" target="#b30">[30]</ref> (version 1.2.0) deep learning framework and the libraries Torchvision (version 0.4.0), NumPy <ref type="bibr" target="#b29">[29]</ref> (version 1.17.2), Pillow (version 6.2.1), and Matplotlib <ref type="bibr" target="#b15">[15]</ref> (version 3.1.0). The CUDA version is 10.2. Initial experiments were done with the data loaders library Continuum <ref type="bibr" target="#b5">[6]</ref>. PODNet's full code is released at: github.com/arthurdouillard/incremental learning.pytorch. We provide all configuration files necessary to reproduce results, including seeds and class ordering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 …Fig. 1 .</head><label>31</label><figDesc>Training protocol for incremental learning. At each training task we learn a new set of classes, and the model must retain knowledge about all classes. The model is allowed a limited memory of samples of old classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Different possible poolings. The output from a convolutional layer h t ,c,w,h may be pooled (summed over) one or more axes. The resulting loss considers only the pooled activations instead of the individual components, allowing more plasticity across the pooled axes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of PODNet: the distillation loss POD prevent excessive model drift by constraining intermediate outputs of the ConvNet f and the LSC classifier g learns a more expressive multi-modal representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Comparison of the performance of the model when disabling parts of the complete PODNet lossClassifier POD-flat POD-spatial NME CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average incremental accuracy for PODNet vs. state of the art. We run experiments three times (random class orders) on CIFAR100 and report averages ± standard deviations. Models with an asterisk * are reported directly from Hou et al [14] ± 0.68 62.71 ± 1.26 64.03 ± 1.30 64.48 ± 1.32 PODNet (CNN) 57.98 ± 0.46 60.72 ± 1.36 63.19 ± 1.16 64.83 ± 0.98</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CIFAR100</cell><cell></cell></row><row><cell></cell><cell>50 steps</cell><cell>25 steps</cell><cell>10 steps</cell><cell>5 steps</cell></row><row><cell>New classes per step</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell></row><row><cell>iCaRL* [33]</cell><cell>-</cell><cell>-</cell><cell>52.57</cell><cell>57.17</cell></row><row><cell>iCaRL</cell><cell>44.20 ± 0.98</cell><cell>50.60 ± 1.06</cell><cell>53.78 ± 1.16</cell><cell>58.08 ± 0.59</cell></row><row><cell>BiC [38]</cell><cell>47.09 ± 1.48</cell><cell>48.96 ± 1.03</cell><cell>53.21 ± 1.01</cell><cell>56.86 ± 0.46</cell></row><row><cell>UCIR (NME)* [14]</cell><cell>-</cell><cell>-</cell><cell>60.12</cell><cell>63.12</cell></row><row><cell>UCIR (NME) [14]</cell><cell>48.57 ± 0.37</cell><cell>56.82 ± 0.19</cell><cell>60.83 ± 0.70</cell><cell>63.63 ± 0.87</cell></row><row><cell>UCIR (CNN)* [14]</cell><cell>-</cell><cell>-</cell><cell>60.18</cell><cell>63.42</cell></row><row><cell>UCIR (CNN) [14]</cell><cell>49.30 ± 0.32</cell><cell>57.57 ± 0.23</cell><cell>61.22 ± 0.69</cell><cell>64.01 ± 0.91</cell></row><row><cell>PODNet (NME)</cell><cell>61.40</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average incremental accuracy, PODNet vs. state of the art. Models with an asterisk * are reported directly from Hou et al.<ref type="bibr" target="#b14">[14]</ref> </figDesc><table><row><cell></cell><cell></cell><cell cols="2">ImageNet100</cell><cell></cell><cell cols="2">Imagenet1000</cell></row><row><cell></cell><cell cols="6">50 steps 25 steps 10 steps 5 steps 10 steps 5 steps</cell></row><row><cell>New classes per step</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>50</cell><cell>100</cell></row><row><cell>iCaRL* [33]</cell><cell>-</cell><cell>-</cell><cell>59.53</cell><cell>65.04</cell><cell>46.72</cell><cell>51.36</cell></row><row><cell>iCaRL [33]</cell><cell>54.97</cell><cell>54.56</cell><cell>60.90</cell><cell>65.56</cell><cell>-</cell><cell>-</cell></row><row><cell>BiC [38]</cell><cell>46.49</cell><cell>59.65</cell><cell>65.14</cell><cell>68.97</cell><cell>44.31</cell><cell>45.72</cell></row><row><cell>UCIR (NME)* [14]</cell><cell>-</cell><cell>-</cell><cell>66.16</cell><cell>68.43</cell><cell>59.92</cell><cell>61.56</cell></row><row><cell>UCIR (NME) [14]</cell><cell>55.44</cell><cell>60.81</cell><cell>65.83</cell><cell>69.07</cell><cell>-</cell><cell>-</cell></row><row><cell>UCIR (CNN)* [14]</cell><cell>-</cell><cell>-</cell><cell>68.09</cell><cell>70.47</cell><cell>61.28</cell><cell>64.34</cell></row><row><cell>UCIR (CNN) [14]</cell><cell>57.25</cell><cell>62.94</cell><cell>67.82</cell><cell>71.04</cell><cell>-</cell><cell>-</cell></row><row><cell>PODNet (CNN)</cell><cell>62.48</cell><cell>68.31</cell><cell>74.33</cell><cell>75.54</cell><cell>64.13</cell><cell>66.95</cell></row><row><cell></cell><cell>± 0.59</cell><cell>± 2.45</cell><cell>± 0.93</cell><cell>± 0.26</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Effect of the memory size per class Mper on the models performance. Results from CIFAR100 with 50 steps, we report the average incremental accuracy 21.97 47.09 55.01 62.23 67.47 UCIR (NME) [14] 21.81 41.92 48.57 56.09 60.31 64.24 UCIR (CNN) [14] 22.17 42.70 49.30 57.02 61.37 65.99 PODNet (NME) 48.37 57.20 61.40 62.27 63.14 63.63 PODNet (CNN) 35.59 48.54 57.98 63.69 66.48 67.62 Effect of the initial task size and the M total on the models performance. We report the average incremental accuracy [14] 54.08 62.89 UCIR (CNN) [14] 55.20 63.62 PODNet (NME) 62.47 64.60 PODNet (CNN) 61.87 64.68 (b) Varying initial task size, with Mper = 20, and followed by 50 to 90 tasks made of a single class 40.95 42.27 45.18 47.09 UCIR (NME) [14] 42.33 40.81 46.80 46.71 48.57 UCIR (CNN) [14] 43.25 41.69 47.85 47.51 49.30 PODNet (NME) 45.09 49.03 55.30 57.89 61.40 PODNet (CNN) 44.95 47.68 52.88 55.42 57.98</figDesc><table><row><cell>Mper</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>iCaRL [33]</cell><cell cols="6">16.44 28.57 44.20 48.29 54.10 57.82</cell></row><row><cell cols="2">BiC [38] 20.84 (a) Evaluation of an easier memory</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>constraint (M total = 2000)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Nb. steps 50 10 42.34 56.52 48.44 55.03 UCIR (NME) Initial task size Loss iCaRL [33] BiC [38] Loss 10 20 30 40 iCaRL [33] 40.97 41.28 43.38 44.35 44.20 50 BiC [38] 41.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison of distillation losses based on intermediary features. All losses evaluated without POD-flat.</figDesc><table><row><cell>Loss</cell><cell>NME CNN</cell></row><row><cell>None</cell><cell>41.56 40.76</cell></row><row><cell>POD-pixels</cell><cell>42.21 40.81</cell></row><row><cell>POD-channels</cell><cell>55.91 50.34</cell></row><row><cell>POD-gap</cell><cell>57.25 53.87</cell></row><row><cell>POD-width</cell><cell>61.25 57.51</cell></row><row><cell>POD-height</cell><cell>61.24 57.50</cell></row><row><cell>POD-spatial</cell><cell>61.42 57.64</cell></row><row><cell>GradCam [5]</cell><cell>41.89 42.07</cell></row><row><cell cols="2">Perceptual Style [16] 41.74 40.80</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Code is available at: github.com/arthurdouillard/incremental learning.pytorch arXiv:2004.13513v3 [cs.CV] 6 Oct 2020</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Infinite mixture prototypes for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Marín-Jiménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning without memorizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 3, 4</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 3, 4</meeting>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Continuum, data loaders for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lesort</surname></persName>
		</author>
		<ptr target="https://github.com/Continvvm/continuum" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.5281/zenodo.3759673</idno>
		<ptr target="https://doi.org/10.5281/zenodo.375967316" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">PathNet: Evolution Channels Gradient Descent in Super Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>arXiv preprint library</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Continual learning via neural pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matplotlib: A 2d graphics environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fearnet: Brain-inspired model for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. Object Categorization: Computer and Human Vision Perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Core50: a new dataset and benchmark for continuous object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cosine normalization: Using cosine similarity instead of dot product in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Artificial Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A guide to NumPy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Trelgol Publishing USA</publisher>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Softtriple loss: Deep metric learning without triplet sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting, rehearsal and pseudorehearsal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer Learning to Learn</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 3, 4, 9</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 3, 4, 9</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">M2kd: Multi-model and multi-level knowledge distillation for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>arXiv preprint library</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
