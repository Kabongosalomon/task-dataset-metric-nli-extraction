<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Memmesheimer</surname></persName>
							<email>raphael@uni-koblenz.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Active Vision Group</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Theisen</surname></persName>
							<email>nicktheisen@uni-koblenz.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Active Vision Group</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
							<email>paulus@uni-koblenz.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Active Vision Group</orgName>
								<orgName type="institution">University of Koblenz-Landau</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing an activity with a single reference sample using metric learning approaches is a promising research field. The majority of few-shot methods focus on object recognition or face-identification. We propose a metric learning approach to reduce the action recognition problem to a nearest neighbor search in embedding space. We encode signals into images and extract features using a deep residual CNN. Using triplet loss, we learn a feature embedding. The resulting encoder transforms features into an embedding space in which closer distances encode similar actions while higher distances encode different actions. Our approach is based on a signal level formulation and remains flexible across a variety of modalities. It further outperforms the baseline on the large scale NTU RGB+D 120 dataset for the one-shot action recognition protocol by 5.6%. With just 60% of the training data, our approach still outperforms the baseline approach by 3.7%. With 40% of the training data, our approach performs comparably well to the second follow up. Further, we show that our approach generalizes well in experiments on the UTD-MHAD dataset for inertial, skeleton and fused data and the Simitate dataset for motion capturing data. Furthermore, our inter-joint and inter-sensor experiments suggest good capabilities on previously unseen setups.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Recognizing an activity with a single reference sample using metric learning approaches is a promising research field. The majority of few-shot methods focus on object recognition or face-identification. We propose a metric learning approach to reduce the action recognition problem to a nearest neighbor search in embedding space. We encode signals into images and extract features using a deep residual CNN. Using triplet loss, we learn a feature embedding. The resulting encoder transforms features into an embedding space in which closer distances encode similar actions while higher distances encode different actions. Our approach is based on a signal level formulation and remains flexible across a variety of modalities. It further outperforms the baseline on the large scale NTU RGB+D 120 dataset for the one-shot action recognition protocol by 5.6%. With just 60% of the training data, our approach still outperforms the baseline approach by 3.7%. With 40% of the training data, our approach performs comparably well to the second follow up. Further, we show that our approach generalizes well in experiments on the UTD-MHAD dataset for inertial, skeleton and fused data and the Simitate dataset for motion capturing data. Furthermore, our inter-joint and inter-sensor experiments suggest good capabilities on previously unseen setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Learning to identify unseen classes from a few samples is an active research topic. Metric learning in computer vision research mainly concentrates on one-shot object recognition <ref type="bibr" target="#b0">[1]</ref>, person re-identification <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> or face identification <ref type="bibr" target="#b3">[4]</ref>. Only recently few-shot methods for action recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> have gained popularity. These approaches presented good results for one-shot action recognition but only concentrate on single modalities like image-or skeleton sequences. We propose to use a signal level representation that allows flexible encoding of signals into an image and the fusion of signals from different senor modalities.</p><p>In contrast to classification methods, which predict class labels, metric learning approaches learn an embedding function. Our approach learns a function that embeds signalimages into an embedding space. One-shot action recognition then becomes a nearest neighbor search in embedding space. <ref type="figure" target="#fig_1">Figure 1</ref> gives an application example for one-shot action recognition on skeleton sequences using our approach.</p><p>While it may appear implausible, initially, to encode signals into an image representation for action recognition, it has some benefits. First, it allows generalization across different sensor modalities as long as a sensor originates multivariate signal sequences or higher-level features such as human pose  Each axis in a different color. Our approach encodes an action sequence representation into an embedding vector. Low Euclidean distances on the action embedding represent close similarity, whereas higher distances represent different actions. This approach allows for ine-shot action classification or clustering similar activities. The underlying signal level representation enables multimodal applications.</p><p>estimates. There is no need for modality-specific architectures or pipelines. Further, an image-like representation allows the usage of well-studied and well-performing classification architectures <ref type="bibr" target="#b7">[8]</ref>. Finally, experiments for multimodal or intermodal one-shot action recognition can be conducted flexibly.</p><p>In our study, signals originate from 3D skeleton sequences gathered by an RGB-D camera, inertial, or motion capturing measurements. To fuse multiple modalities, e.g. skeleton se-quences and inertial measurements, the signal matrices are concatenated and represented as an image. Inter-modal experiments are especially interesting, as they allow training on one modality and recognition on another, previously unseen, modality by providing only a single reference. A new sensor can be used for action recognition without any prior training data from that sensor.</p><p>The main contribution of this paper is a novel model for one-shot action recognition on a signal level. A classifier and embedding encoder are jointly optimized using triplet margin loss <ref type="bibr" target="#b8">[9]</ref> in conjunction with a Muti-Similarity Miner <ref type="bibr" target="#b9">[10]</ref>. The nearest neighbor in embedding space defines the most similar action. Our proposed approach lifts the state-of-the-art in oneshot action recognition on skeleton sequences on the NTU RGB+D 120 dataset for the one-shot evaluation protocol by 5.6%. We claim that our approach based on triplet margin loss and a common signal-level representation yields high flexibility for applications in one-shot action recognition. We achieve good results on one-shot action recognition for conventional sensor modalities (skeleton sequences, inertial measurements, motion capturing measurements). Our approach shows good capabilities when being trained on one modality and inferred on a different modality by providing a single reference sample per action class of the unknown modality. This allows e.g. training on skeleton sequences and inference on inertial measurements. We provide the source code for reproduction and validation 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>We give a brief overview of methods related to metric learning and few-show recognition approaches in general. We focus on methods for action embeddings and few-shot action recognition.</p><p>Our approach builds on image representation of sensor sequences. Prior work has already presented representations for action recognition with skeleton sequences. Wang et al. <ref type="bibr" target="#b10">[11]</ref> encode joint trajectory maps into images based on three spatial perspectives. Caetano et al. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> represent a combination of reference joints and a tree-structured skeleton as images. Their approach preserves spatio-temporal relations and joint relevance. Liu et al. <ref type="bibr" target="#b13">[14]</ref> presented a combination of skeleton visualization methods and jointly trained them on multiple streams. In contrast to our approach, their underlying representation enforces custom network architectures and is constrained to skeleton sequences, whereas our approach adds flexibility to other sensor modalities.</p><p>Metric learning has been intensively studied in computer vision. A focus is on metric learning from photos or cropped detection boxes for person re-identification or image-ranking. Schroff et al. <ref type="bibr" target="#b3">[4]</ref> presented a joint face recognition and clustering approach. They trained a network such that the squared L2 distances in the embedding space directly correspond to face similarity <ref type="bibr" target="#b3">[4]</ref>. Triplet loss <ref type="bibr" target="#b8">[9]</ref> is used for training the embedder. The embedding minimizes distances between anchor images 1 https://github.com/raphaelmemmesheimer/sl-dml and positive images (i.e., same person, different viewpoint) and maximizes distances to negative samples (different person). Yi et al. <ref type="bibr" target="#b1">[2]</ref> presented a deep metric learning approach based on a siamese deep neural network for person re-identification. The two sub-nets are combined using a cosine layer. Wojke et al. <ref type="bibr" target="#b2">[3]</ref> propose a deep cosine metric learning approach for the person re-identification task. The Cosine Softmax Classifier pushes class samples towards a defined class mean and therefore allows similarity estimation by a nearest neighbor search.</p><p>A recent action embedding approach by Hahn et al. <ref type="bibr" target="#b14">[15]</ref> takes inspiration from the success of word embeddings in natural language processing. They combine linguistic cues from class labels with spatio-temporal features from sequences. A hierarchical recurrent neural network trains a feature extractor. A joint loss combines classification accuracy and similarity trains a function to encode the input into an embedding. Discriminative embeddings are important for few-shot learning approaches. Jasani et al. <ref type="bibr" target="#b5">[6]</ref> proposed a similar approach for skeleton-based zero-shot action recognition. A Spatio Temporal Graph Convolution Network (ST-GCN) <ref type="bibr" target="#b15">[16]</ref> extracts features, which are encoded in semantic space by a continuous bag of words method.</p><p>One-shot action recognition is in comparison to image ranking, or person re-identification a quite underrepresented research domain. Kliper-Gross et al. <ref type="bibr" target="#b16">[17]</ref> proposed One-Shot-Similarity Metric Learning. A projection matrix that improves the One-Shot-Similarity relation between the example same and not-same training pairs represents a reduced feature space <ref type="bibr" target="#b16">[17]</ref>. Fanello et al. <ref type="bibr" target="#b17">[18]</ref> use Histogram of Flow and Global Histogram of Oriented Gradient descriptors with adaptive sparse coding and are classified using a linear SVM. Careaga et al. <ref type="bibr" target="#b4">[5]</ref> propose a two-stream model for few-show action recognition on image sequences. They aggregate features from optical flow and the image sequences separately by a Long Short Term Memory (LSTM) and fuse them afterward for learning metrics. Rodriguez et al. <ref type="bibr" target="#b18">[19]</ref> presented an one-shot approach based on Simplex Hidden Markov Models (SHMM). Improved dense trajectories are used as base features <ref type="bibr" target="#b19">[20]</ref>. A maximum a posteriori (MAP) adoption and an optimized Expectation Maximisation reduce the feature space. A maximum likelihood classification, in combination with the SHMM, allows oneshot classification. Roy et al. <ref type="bibr" target="#b20">[21]</ref> propose a Siamese network approach for discriminating actions by a contrastive loss on a low dimensional representation gathered factory analysis. Mishra et al. <ref type="bibr" target="#b21">[22]</ref> presented a generative framework for zeroand few-shot action recognition on image sequences. A probability distribution models classes of actions. The parameters are functions for semantic attribute vectors that represent the action classes.</p><p>Liu et al. <ref type="bibr" target="#b6">[7]</ref> presented, along with the NTU RGB+D 120 dataset, an approach for one-shot action recognition. They propose an approach named Action-Part Semantic-Relevance aware (APSR). Features are generated by a ST-LSTM <ref type="bibr" target="#b22">[23]</ref>.  In the example, we transformed skeleton joint axes into images. We use a Resnet18 architecture in conjunction with a triplet loss to train a model that transforms an image into an embedding space. For inference, the trained encoder encodes a set of references and queries. The closest reference in embedding space represents the most similar activities for which we use a nearest-neighbor search.</p><p>Similar semantic relevance for the body parts assigns new action instances. The field of multi-modal few-shot action recognition is entirely unexplored. Somehow related is the work of Al-Naser et al. <ref type="bibr" target="#b23">[24]</ref> who presented a zero-shot action recognition approach by combining gaze guided object recognition with a gesture recognition arm-band. Actions are detected by fusing features of sub-networks per modality and integrating action definitions. Only three actions demonstrate the recognition results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>To cover the action recognition task across a variety of sensor modalities we consider the action recognition problem on a signal level. Signals are encoded in a discriminate image representation. An image-like representation allows direct adaption of already established image classification architectures for extracting features. On the extracted features we train a similarity function yielding an action embedding using triplet loss. The triplet loss minimizes embedding distances between similar action samples while maximizing distances between different actions. Finally to solve the one-shot problem, we apply a nearest neighbor search in the embedding space. An illustration of our approach is given in <ref type="figure" target="#fig_3">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>We consider the one-shot action recognition problem as a metric learning problem. First we encode action sequences on a signal level into an image representation. The input in our case is a signal matrix S ∈ R N ×M where each row vector represents a discrete 1-dimensional signal and each column vector represents a sample of all sensors at one </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Representations</head><p>Our approach builds upon a discriminable image representation. Therefore, we propose a novel, compact signal level representation. Multivariate signal or higher-level feature sequences are reassembled into a 3 channel image. Each row of the resulting image corresponds to one joint and each channel corresponds to one sample in the sequence. The color channels, red, green and blue, represent respectively the signals' x-, y-and z-values. The resulting images are normalized to the range of 0-1. We chose to normalize over the whole image to preserve the relative magnitude of the signals. In contrast to the representations used for multimodal action classification <ref type="bibr" target="#b24">[25]</ref> or skeleton based action recognition <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref> the proposed representation is invertible and more compact. The construction of the representation is depicted in <ref type="figure" target="#fig_4">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Extraction</head><p>Most action recognition approaches based on CNNs present custom architecture designs in their pipelines <ref type="bibr" target="#b13">[14]</ref>. A benefit is the direct control over the number of model parameters that can be specifically engineered for data representations or use cases. Recent advances in architecture design cannot be transferred directly. Searching good hyper-parameters for training is then often an empirical study. Minor architecture changes can result in a completely different set of hyperparameters. He et al. <ref type="bibr" target="#b7">[8]</ref> suggested the use of residual layers during training to tackle the vanishing gradient problem. We take advantage of the recent development in architecture design and decided to use a Resnet18 <ref type="bibr" target="#b7">[8]</ref> architecture. For weight initialization we use a pre-trained model. After the last feature layer we use a two-layer perceptron to transform the features into the embedding size. The embedding is refined by the metric learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Metric Learning</head><p>Metric learning aims to learn a function to transform an image into an embedding space, where the embedding vectors of similar samples are encouraged to be closer, while dissimilar ones are pushed apart from each other <ref type="bibr" target="#b9">[10]</ref>. We use a triplet loss in combination with a Multi-Similarity-Miner <ref type="bibr" target="#b9">[10]</ref> for mining good triplet candidates during training.</p><p>While the triplet loss has been used in image ranking <ref type="bibr" target="#b25">[26]</ref>, face recognition <ref type="bibr" target="#b3">[4]</ref>, person re-identification <ref type="bibr" target="#b26">[27]</ref> it has only rarely been used for inter-and cross-modal ranking to improve action recognition <ref type="bibr" target="#b27">[28]</ref> and for complex event detection <ref type="bibr" target="#b28">[29]</ref>. Given a triplet of an anchor image I • , a positive data sample, representing the same action class image I ↑ and a negative sample, representing a different action class I ↓ the triplet loss can be formulated as:</p><formula xml:id="formula_0">L t (I • , I ↑ , I ↓ ) = max ( f (I • ) − f (I ↑ ) 2 − f (I • ) − f (I ↓ ) 2 + δ, 0 ) ,<label>(1)</label></formula><p>where δ describes an additional distance margin.</p><p>Finding good candidate pairs is crucial. Therefore we use a Multi-Similarity Miner <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref> to mine positive and negative pairs that are assumed to be difficult to push apart in the embedding space. That means positive pairs are constructed by an anchor and positive image pair {I • , I ↑ } and its embedding f (I • ), preferring pairs with a high distance in embedding space with the following condition:</p><formula xml:id="formula_1">f (I • ) − f (I ↑ ) 2 &gt; min y k =yi f (I i ) − f (I k ) 2 − ,<label>(2)</label></formula><p>likewise, negative pairs {I • , I ↓ } are mined by the lowest distance in embedding space:</p><formula xml:id="formula_2">f (I • ) − f (I ↓ ) 2 &lt; max y k =yi f (I i ) − f (I k ) 2 + ,<label>(3)</label></formula><p>where is a given margin. Finally, we yield the total loss by:</p><formula xml:id="formula_3">L = αL t + βL c ,<label>(4)</label></formula><p>such that the influences of the loss can be weighted using the scalars α for the triplet loss L t and β for the classifier loss L c . We utilize a cross entropy loss for L c . Finding an action class by a query and set of references is now reduced to a nearest-neighbor search in the embedding space. The classifier and encoder are jointly optimized. After the last feature layer of the classifier a two-layer perceptron is used to to yield an embedding size of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation</head><p>Our implementation is based on PyTorch <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. We tried to avoid many of the metric learning flaws as pointed out by Musgrave et al. <ref type="bibr" target="#b31">[32]</ref> by using their training setup and hyperparameters where applicable. Key differences are that we use a Resnet18 <ref type="bibr" target="#b7">[8]</ref> architecture and avoid the proposed four-fold cross validation for hyperparameter search in favour of better comparability to the proposed one-shot protocol on the NTU-RGB+D dataset <ref type="bibr" target="#b6">[7]</ref>. Note, we did not perform any optimization of the hyperparameter. A batch size of 32 was used on a single Nvidia GeForce RTX 2080 TI with 11GB GDDR-6 memory. We trained for 100 epochs with initialized weights of a pre-trained Resnet18 <ref type="bibr" target="#b7">[8]</ref>. The classification and metric loss were weighted by 0.5 unless stated otherwise. For the multi similarity miner we used an epsilon of 0.05 while we used a margin of 0.1 for the triplet margin loss. A RMSProp optimizer with a learning rate of 10 −6 was used in all optimizers. The embedding model outputs a 128 dimensional embedding and the classifier yielded a 128 dimensional feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To show the multi-modal one-shot recognition performance we applied our methods to three datasets containing three different modalities. We used skeleton sequences from the NTU RGB+D 120 <ref type="bibr" target="#b6">[7]</ref> dataset for large scale one-shot action recognition. With 100 auxiliary classes and 20 evaluation classes it is the largest dataset that we applied to our approach. To show the multi modal capabilities of our approach we also used the UTD-MHAD <ref type="bibr" target="#b32">[33]</ref> dataset (inertial and skeleton data) and the Simitate <ref type="bibr" target="#b33">[34]</ref> dataset (motion capturing data).</p><p>The datasets are split into an auxiliary set, representing action classes that are used for training, and an evaluation set. In our experiments the evaluation set does contain novel actions or actions from a novel sensor modality. One sample of each test class serves as reference demonstration. This protocol is based on the one proposed by <ref type="bibr" target="#b6">[7]</ref> for the NTU RGB+D 120 dataset. We conducted similar experiments with the remaining two data sets. In depth descriptions are given below in Section IV-A per dataset. Results are discussed after the dataset presentation in Section IV-B. First we trained a model on the auxiliary set. The resulting model estimates embeddings for the reference actions and then for the evaluation actions. We then calculate the nearest neighbour from the evaluation  Attention Network <ref type="bibr" target="#b34">[35]</ref> 41.0 Fully Connected <ref type="bibr" target="#b34">[35]</ref> 42.1 Average Pooling <ref type="bibr" target="#b35">[36]</ref> 42.9 APSR <ref type="bibr" target="#b6">[7]</ref> 45.3 Ours 50.9 embeddings to the reference embeddings. This yields to which action from the reference set the current evaluation sample comes closest.</p><p>A. Datasets a) NTU RGB+D 120: The NTU RGB+D 120 <ref type="bibr" target="#b6">[7]</ref> dataset is a large scale action recognition dataset containing RGB+D image streams and skeleton estimates. The dataset consists of 114,480 sequences containing 120 action classes from 106 subjects in 155 different views. We follow the one-shot protocol as described by the dataset authors. The dataset is split into two parts: an auxiliary set and an evaluation set. The action classes of the two parts are distinct. 100 classes are used for training, 20 classes are used for testing. The unseen classes and reference samples are documented in the accompanied dataset repository 2 . A1, A7, A13, A19, A25, A31, A37, A43, A49, A55, A61, A67, A73, A79, A85, A91, A97, A103, A109, A115 are previously unseen. As reference the demonstration for filenames starting with S001C003P008R001* are used for actions with IDs below 60 and S018C003P008R001* for actions with IDs above 60. One-shot action recognition results are given in <ref type="table" target="#tab_0">Table I</ref>. Like Liu et al. <ref type="bibr" target="#b6">[7]</ref> we also experimented with the effect of the auxiliary set reduction. Results are given in <ref type="figure" target="#fig_5">Fig. 4 (a)</ref> and <ref type="table" target="#tab_0">Table II</ref>. Further we inspect the influence of different loss weighting parameters and compare two miners: Triplet Margin <ref type="bibr" target="#b3">[4]</ref> and the Multi Similarity Miner <ref type="bibr" target="#b9">[10]</ref> in <ref type="table" target="#tab_0">Table III</ref>. b) UTD-MHAD: The UTD-MHAD <ref type="bibr" target="#b32">[33]</ref> contains 27 actions of 8 individuals performing 4 repetitions each. RGB-D camera, skeleton estimates and inertial measurements are included. The RGB-D camera is placed frontal to the demonstrating person. The IMU is either attached at the wrist or the leg during the movements. No one-shot protocol is defined therefore we defined custom splits. We started with 23 auxiliary classes and evaluated with reduced training sets. We evaluated our approach by moving auxiliary instances over to the evaluation set. By this we decreased the training set while increasing the evaluation set. Results for the experiments executed on skeleton, inertial and fused data are given in <ref type="table" target="#tab_0">Table IV</ref>. <ref type="figure" target="#fig_5">Figure 4 (b)</ref> show visually the influence of the auxiliary set. In a third experiment we evaluated the interjoint one-shot learning abilities of our approach. For actions with ids up to 21 the inertial unit was placed on the subjects wrist and for the remaining ids from 22-27 the sensor was placed on the subjects leg. This allows us to inspect the oneshot action recognition transfer to other sensor positions by learning on wrist sequences and recognize on leg sequences with one reference example. We always used the first trial of the first subject as reference sample and the remainder for testing. Results for the inter-joint experiment on inertial  data are given in <ref type="table" target="#tab_4">Table V</ref>. For the fused experiments we concatenated S fused = (S imu |S skl ), where S imu denotes the inertial signal matrix and S skl denotes the skeleton signal matrix. Concatenation is only possible with equal column matrices, therefore we subsampled the modality with the higher signal sample rate. c) Simitate: We further evaluate on the Simitate dataset. The Simitate benchmark focuses on robotic imitation learning tasks. Hand and object data are provided from a motion capturing system in 1932 sequences containing 26 classes of 4 different complexities. The individuals execute tasks of different kinds of activities from drawing motions with their hand over to object interactions and more complex activities like ironing. We consider one action class of each complexity level as unknown. Namely, zickzack from basic motions, mix from motions, close from complex and bring from sequential. Resulting in an auxiliary set of 22 classes and 4 evaluation classes. The corresponding first sequence by filename is used as reference sample. Results are given in <ref type="table" target="#tab_0">Table VII</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>On the NTU RGB+D 120 dataset we compare against the proposed baseline APSR by Liu et al. <ref type="bibr" target="#b6">[7]</ref>. <ref type="table" target="#tab_0">Table I</ref> shows the results with an auxiliary set size of 100 action classes and a evaluation set size of previously unseen 20 action classes. Our proposed approach performs 5.6% better than the  first follow up <ref type="bibr" target="#b6">[7]</ref> and 8% better than the second follow up <ref type="bibr" target="#b35">[36]</ref>. <ref type="figure" target="#fig_5">Figure 4 (a)</ref> and <ref type="table" target="#tab_0">Table II</ref> shows results for an increasing amount of auxiliary classes (100 auxiliary classes and 20 evaluation classes are considered as the standard protocol).</p><p>Overall our approach performs better as the baseline on all conducted auxiliary set experiments. Interestingly to note is the high accuracy with 60 auxiliary classes and that the 20 additional classes added in the 80 classes auxiliary set added confusion. With 60 classes our approach performs 9.8% better than the baseline approach with a same amount of auxiliary classes. Further, our approach performs better, with just 60% of the training data, than the first follow up with the full amount of auxiliary classes. With only 40% of the training data, our approach performs comparably good as the second and third follow up. These experiments strongly highlight the quality of the learned metric. In <ref type="figure" target="#fig_6">Fig. 5</ref> we show UMAP <ref type="bibr" target="#b36">[37]</ref> visualizations that give an insight about the discriminative capabilities. Distances in embedding space capture the amount of identities well. This is the case for the three top clusters containing the actions (grab other person's stuff, take a photo of other person and hugging other person). The two clusters at x-axis around -7.5 correspond to the actions arm circles and throw, suggesting that actions with clear high jointrelevance can also be clustered well. The most bottom cluster corresponds to the class falling and supports this hypothesis. In the left we have a quite sparse cluster reflecting highly noisy skeleton sequences from multiple classes. Mainly sequences with multiple persons, especially with close activity like hugging, resulted in noisy data. <ref type="table" target="#tab_0">Table III</ref> gives an ablation study showing the influence of the loss weighting and the underlying triplet mining approach. A multisimilarity miner <ref type="bibr" target="#b9">[10]</ref> with a metric loss yields the best results fort one-shot action recognition on the NTU 120 dataset. In our ablation study the loss weighting with α1.0, β0.0 yielded the best results for both miners.</p><p>The UTD-MHAD dataset was used to show the generalization capabilities of the proposed approach across different modalities. By considering a signal level action representation we could compare skeleton and inertial results and also perform multimodal, inter-joint and inter-sensor experiments. <ref type="figure" target="#fig_5">Figure 4 (b)</ref> shows the effect on the resulting one-shot accuracy with increasing auxiliary sets. Interesting to note is that in this experiment series we could observe that not necessarily a higher amount of classes used for training will lead to a higher accuracy. This was the case for our experiments on inertial data, where training on only three classes shows a more similar action embedding. This observation could not be transferred to the skeleton experiments on this dataset. The selection of auxiliary classes used for training should be well chosen. Adding more classes does not necessarily mean higher similarity in the embedding but can also add more confusion. Our inter-joint experiments yielded more transferable embeddings by training on data from the wrist and validating on the leg as shown in <ref type="table" target="#tab_4">Table V</ref>. This holds true for our conducted experiments, but we do want not exclude the possibility of finding a subdivision of the wrist auxiliary set that results in a higher transferable embedding. A key-insight among our experiments is that balanced classes for training and testing yielded mostly higher accuracy for lower dimensional modalities like IMU (see <ref type="table" target="#tab_0">Table IV</ref>) and motion capturing (see <ref type="table" target="#tab_0">Table VII</ref>). This is especially visible in our inter-joint experiments as shown in <ref type="table" target="#tab_4">Table V</ref>. In comparison, our experiments applied to skeleton sequences benefited from more auxiliary classes (see <ref type="table" target="#tab_0">Table I</ref>, IV and <ref type="figure" target="#fig_5">Fig. 4 (a,c)</ref>). The conducted fusion experiments, by the the concatenation of skeleton sequences and inertial measurements show good performance in some experiments. The lower performing modality can also negatively impact the performance. This observation suggest to add sensor confidences into the approach as a future research direction. Sensor data fusion on an signal level by a single stream architecture remains an interesting and functional alternative to multistream architectures. In our inter-sensor experiments we used all actions from one modality as auxiliary set and evaluated the other modality with a single reference sample. Results for this experiment are given in <ref type="table" target="#tab_0">Table VI</ref> and <ref type="figure" target="#fig_7">Fig. 6</ref> visualizes the corresponding UMAP embeddings. The resulting one-shot recognition for inertial to skeleton performs by a large margin better (+17.4%) better than the reverse direction using our novel representation. In approximately 40.5% of the time an action trained on a different data modality on the UTD-MHAD dataset could be recognized with just one reference sample. The signal representation <ref type="bibr" target="#b24">[25]</ref> generalize better in this aspect.</p><p>Overall the inter-modal experiments show the flexibility of our proposed approach but are subject for further improvement. We observed that the inertial measurements have a relation to the arm and hand movements of the skeleton, which explains the good transferability across the modalities. Finally, we evaluated our approach on the Simitate dataset with motion capturing data. Results are given in <ref type="table" target="#tab_0">Table VII</ref> and <ref type="figure" target="#fig_5">Fig. 4 (c)</ref>. The amount of classes is comparable to the one from the UTD-MHAD dataset. The effects of the auxiliary set reduction are en par with the experiments conducted on the UTD-MHAD dataset. Therefore, the proposed approach transfers also good to motion capturing data. The classdistances from the motion capturing experiments are higher in embedding space then the ones gathered by the inertial experiments (see <ref type="figure" target="#fig_5">Fig. 4</ref> (b) &amp; (c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We presented a one-shot action recognition approach by employing a signal level representation in conjunction with metric learning using a triplet margin loss. By considering a representation on a signal level, our approach remains flexible across different sensor modalities like skeleton, inertial measurements, motion capturing data or the fusion of multiple modalities. Our approach allows one-shot recognition on all of the modalities we experimented with and further indicated to serve as a flexible framework for inter-joint and even inter-sensor experiments. The novel inter-sensor experiments, by training on one modality and inferring on an unknown modality, can potentially shape future evaluation protocols. We evaluate our approach on three different, publicly available, datasets. Most importantly, we showed an improvement of the current state-of-the-art for one-shot action recognition on the large scale NTU RGB+D 120 dataset. To show the transfer capabilities, we also verified our results using the UTD-MHAD dataset for skeleton and inertial data and the Simitate dataset for motion capturing data. Inter-joint experiments show inertial sensor attached to the wrist and the leg from the UTD-MHAD dataset. On the UTD-MHAD dataset intersensor experiments between the skeleton and inertial data were executed. We found that more classes used during training for lower variate sensor data like IMUs and motion capturing systems do not necessarily improve the one-shot recognition accuracy in our experiments. A good selection of training classes and a balanced training and validation set improved results across all modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Illustrative example. In this example, a skeleton is transformed into an image-like representation. Joint axes are encoded as signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Motivated by word embedding methods, Liu et al. propose to estimate semantic relevance on body parts and the actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Approach overview: We represent actions on a signal level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Examplary representation for a throwing activity of the NTU-RGB+D 120 dataset. specific time step. The matrix is transformed to an RGB image I ∈ {0, . . . , 255} H×W ×3 by normalizing the signal length M to W and the range of the signals to H. The identity of each signal is encoded in the color channel. This results in a dataset D = {(I i , y i )} N i=1 of N training images I 1,...,N with labels y i ∈ {1, . . . , C}. Our goal is to train a feature embedding x = f Θ (I ) with parameters Θ which projects input images I ∈ {0, . . . , 255} H×W ×3 into a feature representation x ∈ X d . The feature representation reflects minimal distances for similar classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Result graphs for the NTU RGB+D 120 dataset (a), the UTD-MHAD dataset (b) and the Simitate dataset (c). Skl denotes skeleton data, IMU denotes inertial data, Fused denotes multimodal data consisting of inertial-and skeleton data s. val denotes a static evaluation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>UMAP embedding visualization for the one-shot experiments using the NTU RGB+D 120 (a) dataset, the UTD-MHAD dataset (IMU) (b) and the Simitate dataset (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>UMAP embedding visualization for inter-modal experiments.Trained on skeleton inertial measurements and validated on skeleton (a). Trained on skeleton sequences and validated on inertial measurements (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="2">: One-shot action recognition results on the NTU RGB+D</cell></row><row><cell>120 dataset.</cell><cell></cell></row><row><cell>Approach</cell><cell>Accuracy [%]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Results for different auxiliary training set sizes for oneshot recognition on the NTU RGB+D 120 dataset.</figDesc><table><row><cell cols="3">#Train Classes APSR [7] [%] Ours (α, β = 0.5) [%]</cell></row><row><cell>20</cell><cell>29.1</cell><cell>36.7</cell></row><row><cell>40</cell><cell>34.8</cell><cell>42.4</cell></row><row><cell>60</cell><cell>39.2</cell><cell>49.0</cell></row><row><cell>80</cell><cell>42.8</cell><cell>46.4</cell></row><row><cell>100</cell><cell>45.3</cell><cell>50.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Ablation study for our proposed one-shot action recognition approach on the NTU RGB+D 120 dataset.</figDesc><table><row><cell>Miner</cell><cell>α</cell><cell cols="2">β Accuracy [%]</cell></row><row><cell>Triplet Margin [4]</cell><cell cols="2">1.0 0.0</cell><cell>50.6</cell></row><row><cell>Triplet Margin [4]</cell><cell cols="2">0.0 1.0</cell><cell>40.4</cell></row><row><cell>Triplet Margin [4]</cell><cell cols="2">0.5 0.5</cell><cell>50.5</cell></row><row><cell cols="3">Multi Similarity [10] 1.0 0.0</cell><cell>52.2</cell></row><row><cell cols="3">Multi Similarity [10] 0.0 1.0</cell><cell>40.4</cell></row><row><cell cols="3">Multi Similarity [10] 0.5 0.5</cell><cell>50.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>one-shot action recognition results on the UTD-MHAD dataset.</figDesc><table><row><cell cols="5">#Train Cl. #Val Cl. Skl. [%] Inertial [%] Fused [%]</cell></row><row><cell>23</cell><cell>4</cell><cell>92.7</cell><cell>81.3</cell><cell>90.2</cell></row><row><cell>19</cell><cell>8</cell><cell>74.8</cell><cell>74.0</cell><cell>76.0</cell></row><row><cell>15</cell><cell>12</cell><cell>81.1</cell><cell>63.5</cell><cell>78.7</cell></row><row><cell>11</cell><cell>16</cell><cell>77.7</cell><cell>43.3</cell><cell>69.4</cell></row><row><cell>7</cell><cell>20</cell><cell>57.2</cell><cell>41.3</cell><cell>65.0</cell></row><row><cell>3</cell><cell>24</cell><cell>59.4</cell><cell>29.2</cell><cell>44.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="4">: Inter-joint one-shot action recognition results on the UTD-</cell></row><row><cell>MHAD dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">#Train Classes #Val Classes Train Joint Accuracy [%]</cell></row><row><cell>21</cell><cell>6</cell><cell>Left wrist</cell><cell>80.4</cell></row><row><cell>6</cell><cell>21</cell><cell>Left leg</cell><cell>28.3</cell></row><row><cell>6</cell><cell>6</cell><cell>Left wrist</cell><cell>18.8</cell></row><row><cell>6</cell><cell>6</cell><cell>Left leg</cell><cell>59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>Inter-sensor one-shot action recognition results on the UTD-MHAD dataset.</figDesc><table><row><cell cols="4">Train Modality Val. Modality Representation Accuracy [%]</cell></row><row><cell>Skeleton</cell><cell>Inertial</cell><cell>Signals [25]</cell><cell>35.5</cell></row><row><cell>Inertial</cell><cell>Skeleton</cell><cell>Signals [25]</cell><cell>40.5</cell></row><row><cell>Skeleton</cell><cell>Inertial</cell><cell>Compact</cell><cell>23.1</cell></row><row><cell>Inertial</cell><cell>Skeleton</cell><cell>Compact</cell><cell>40.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell cols="3">: one-shot action recognition results on the Simitate</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell></row><row><cell cols="3">#Train Classes #Val Classes Accuracy [%]</cell></row><row><cell>22</cell><cell>4</cell><cell>100.0</cell></row><row><cell>18</cell><cell>4</cell><cell>98.8</cell></row><row><cell>14</cell><cell>4</cell><cell>99.4</cell></row><row><cell>10</cell><cell>4</cell><cell>93.1</cell></row><row><cell>18</cell><cell>8</cell><cell>76.7</cell></row><row><cell>14</cell><cell>12</cell><cell>61.2</cell></row><row><cell>10</cell><cell>16</cell><cell>56.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/shahroudy/NTURGB-D</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zero-shot object recognition by semantic manifold distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2635" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 22nd International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep cosine metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Metricbased few-shot learning for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Careaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hodas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Phillips</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09602</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Skeleton based zero shot action recognition in joint pose-language semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazagonwalla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11344</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multisimilarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="43" to="53" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13025</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Skeleton image representation for 3d action recognition based on tree structure and reference joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05704</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Action2vec: A crossmodal embedding approach to action learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00484</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One shot similarity metric learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="31" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">One-shot learning for real-time action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Metta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast simplexhmm for one-shot learning activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Medrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition based on discriminative embedding of actions using siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S R</forename><surname>Murty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3473" to="3477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arulkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical model for zero-shot activity recognition using wearable sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Naser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAART (2)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gimme signals: Discriminative signal encoding for multimodal activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Theisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2003.06156.pdf" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting><address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compact descriptors for sketch-based image retrieval using a triplet loss convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="27" to="37" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cooperative training of deep aggregation networks for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Content-attention representation by factorized action-scene network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1537" to="1547" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pytorch metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<ptr target="https://github.com/KevinMusgrave/pytorch-metric-learning" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08505</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simitate: A hybrid imitation learning benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Seib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1905.06002.pdf" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5243" to="5249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Global contextaware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Skeletonbased human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
