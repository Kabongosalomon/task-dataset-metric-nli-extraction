<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphSAINT: GRAPH SAMPLING BASED INDUCTIVE LEARNING METHOD</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
							<email>zengh@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
							<email>hongkuaz@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
							<email>ajiteshs@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
							<email>rajgopal.kannan.civ@mail.mil</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
							<email>prasanna@usc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Army Research Lab</orgName>
								<orgName type="institution">US</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphSAINT: GRAPH SAMPLING BASED INDUCTIVE LEARNING METHOD</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the "neighbor explosion" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, representation learning on graphs has attracted much attention, since it greatly facilitates tasks such as classification and clustering <ref type="bibr" target="#b24">(Wu et al., 2019;</ref><ref type="bibr" target="#b2">Cai et al., 2017)</ref>. Current works on Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b9">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018b;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 2018;</ref> mostly focus on shallow models (2 layers) on relatively small graphs. Scaling GCNs to larger datasets and deeper layers still requires fast alternate training methods.</p><p>In a GCN, data to be gathered for one output node comes from its neighbors in the previous layer. Each of these neighbors in turn, gathers its output from the previous layer, and so on. Clearly, the deeper we back track, the more multi-hop neighbors to support the computation of the root. The number of support nodes (and thus the training time) potentially grows exponentially with the GCN depth. To mitigate such "neighbor explosion", state-of-the-art methods use various layer sampling techniques. The works by <ref type="bibr" target="#b9">Hamilton et al. (2017)</ref>; <ref type="bibr" target="#b26">Ying et al. (2018a)</ref>;  ensure that only a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. <ref type="bibr" target="#b4">Chen et al. (2018b)</ref> and <ref type="bibr" target="#b14">Huang et al. (2018)</ref> further propose samplers to restrict the neighbor expansion factor to 1, by ensuring a fixed sample size in all layers. While these methods significantly speed up training, they face challenges in scalability, accuracy or computation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Present work</head><p>We present GraphSAINT (Graph SAmpling based INductive learning meThod) to efficiently train deep GCNs. GraphSAINT is developed from a fundamentally different way of minibatch construction. Instead of building a GCN on the full training graph and then sampling across the layers, we sample the training graph first and then build a full GCN on the subgraph. Our method is thus graph sampling based. Naturally, GraphSAINT resolves "neighbor explosion", since every GCN of the minibatches is a small yet complete one. On the other hand, graph sampling based method also brings new challenges in training. Intuitively, nodes of higher influence on each other should have higher probability to form a subgraph. This enables the sampled nodes to "support" each other without going outside the minibatch. Unfortunately, such strategy results in non-identical node sampling probability, and introduces bias in the minibatch estimator. To address this issue, we develop normalization techniques so that the feature learning does not give preference to nodes more frequently sampled. To further improve training quality, we perform variance reduction analysis, and design light-weight sampling algorithms by quantifying "influence" of neighbors. Experiments on GraphSAINT using five large datasets show significant performance gain in both training accuracy and time. We also demonstrate the flexibility of GraphSAINT by integrating our minibatch method with popular GCN architectures such as JK-net <ref type="bibr" target="#b25">(Xu et al., 2018)</ref> and GAT <ref type="bibr" target="#b23">(Veličković et al., 2017)</ref>. The resulting deep models achieve new state-of-the-art F1 scores on PPI (0.995) and Reddit (0.970).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A neural network model that extends convolution operation to the graph domain is first proposed by <ref type="bibr" target="#b1">Bruna et al. (2013)</ref>. Further, <ref type="bibr" target="#b16">Kipf &amp; Welling (2016)</ref>; <ref type="bibr" target="#b6">Defferrard et al. (2016)</ref> speed up graph convolution computation with localized filters based on Chebyshev expansion. They target relatively small datasets and thus the training proceeds in full batch. In order to scale GCNs to large graphs, layer sampling techniques <ref type="bibr" target="#b9">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018b;</ref><ref type="bibr" target="#b26">Ying et al., 2018a;</ref><ref type="bibr" target="#b8">Gao et al., 2018;</ref><ref type="bibr" target="#b14">Huang et al., 2018)</ref> have been proposed for efficient minibatch training. All of them follow the three meta steps: 1. Construct a complete GCN on the full training graph. 2. Sample nodes or edges of each layer to form minibatches. 3. Propagate forward and backward among the sampled GCN. Steps (2) and (3) proceed iteratively to update the weights via stochastic gradient descent. The layer sampling algorithm of GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref> performs uniform node sampling on the previous layer neighbors. It enforces a pre-defined budget on the sample size, so as to bound the minibatch computation complexity. <ref type="bibr" target="#b26">Ying et al. (2018a)</ref> enhances the layer sampler of <ref type="bibr" target="#b9">Hamilton et al. (2017)</ref> by introducing an importance score to each neighbor. The algorithm presumably leads to less information loss due to weighted aggregation. S-GCN  further restricts neighborhood size by requiring only two support nodes in the previous layer. The idea is to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN <ref type="bibr" target="#b4">(Chen et al., 2018b)</ref> performs sampling from another perspective. Instead of tracking down the inter-layer connections, node sampling is performed independently for each layer. It applies importance sampling to reduce variance, and results in constant sample size in all layers. However, the minibatches potentially become too sparse to achieve high accuracy. <ref type="bibr" target="#b14">Huang et al. (2018)</ref> improves FastGCN by an additional sampling neural network. It ensures high accuracy, since sampling is conditioned on the selected nodes in the next layer. Significant overhead may be incurred due to the expensive sampling algorithm and the extra sampler parameters to be learned.</p><p>Instead of sampling layers, the works of <ref type="bibr" target="#b29">Zeng et al. (2018)</ref> and <ref type="bibr" target="#b5">Chiang et al. (2019)</ref> build minibatches from subgraphs. <ref type="bibr" target="#b29">Zeng et al. (2018)</ref> proposes a specific graph sampling algorithm to ensure connectivity among minibatch nodes. They further present techniques to scale such training on shared-memory multi-core platforms. More recently, ClusterGCN <ref type="bibr" target="#b5">(Chiang et al., 2019)</ref> proposes graph clustering based minibatch training. During pre-processing, the training graph is partitioned into densely connected clusters. During training, clusters are randomly selected to form minibatches, and intra-cluster edge connections remain unchanged. Similar to GraphSAINT, the works of <ref type="bibr" target="#b29">Zeng et al. (2018)</ref> and <ref type="bibr" target="#b5">Chiang et al. (2019)</ref> do not sample the layers and thus "neighbor explosion" is avoided. Unlike GraphSAINT, both works are heuristic based, and do not account for bias due to the unequal probability of each node / edge appearing in a minibatch.</p><p>Another line of research focuses on improving model capacity. Applying attention on graphs, the architectures of <ref type="bibr" target="#b23">Veličković et al. (2017)</ref>; ; <ref type="bibr" target="#b21">Lu et al. (2019)</ref> better capture neighbor features by dynamically adjusting edge weights. <ref type="bibr" target="#b17">Klicpera et al. (2018)</ref> combines PageRank with GCNs to enable efficient information propagation from many hops away. To develop deeper models, "skip-connection" is borrowed from CNNs <ref type="bibr" target="#b10">(He et al., 2015;</ref><ref type="bibr" target="#b13">Huang et al., 2017)</ref> into the GCN context. In particular, JK-net <ref type="bibr" target="#b25">Xu et al. (2018)</ref> demonstrates significant accuracy improvement on GCNs with more than two layers. Note, however, that JK-net <ref type="bibr" target="#b25">(Xu et al., 2018)</ref> follows the same sampling strategy as GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref>. Thus, its training cost is high due to neighbor explosion. In addition, high order graph convolutional layers <ref type="bibr" target="#b31">(Zhou, 2017;</ref><ref type="bibr" target="#b18">Lee et al., 2018;</ref><ref type="bibr" target="#b0">Abu-El-Haija et al., 2019)</ref> also help propagate long-distance features. With the numerous architectural variants developed, the question of how to train them efficiently via minibatches still remains to be answered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD: GraphSAINT</head><p>Graph sampling based method is motivated by the challenges in scalability (in terms of model depth and graph size). We analyze the bias (Section 3.2) and variance (Section 3.3) introduced by graph sampling, and thus, propose feasible sampling algorithms (Section 3.4). We show the applicability of GraphSAINT to other architectures, both conceptually (Section 4) and experimentally (Section 5.2).</p><p>In the following, we define the problem of interest and the corresponding notations. A GCN learns representation of an un-directed, attributed graph G (V, E), where each node v ∈ V has a length-f attribute x v . Let A be the adjacency matrix and A be the normalized one (i.e., A = D −1 A, and D is the diagonal degree matrix). Let the dimension of layer-input activation be f <ref type="bibr">( )</ref> </p><formula xml:id="formula_0">. The activation of node v is x ( ) v ∈ R f ( ) , and the weight matrix is W ( ) ∈ R f ( ) ×f ( +1) . Note that x v = x<label>(1)</label></formula><p>v . Propagation rule of a layer is defined as follows:</p><formula xml:id="formula_1">x ( +1) v = σ u∈V A v,u W ( ) T x ( ) u<label>(1)</label></formula><p>where A v,u is a scalar, taking an element of A. And σ (·) is the activation function (e.g., ReLU).</p><p>We use subscript "s" to denote parameterd of the sampled graph (e.g., G s , V s , E s ).</p><p>GCNs can be applied under inductive and transductive settings. While GraphSAINT is applicable to both, in this paper, we focus on inductive learning. It has been shown that inductive learning is especially challenging <ref type="bibr" target="#b9">(Hamilton et al., 2017</ref>) -during training, neither attributes nor connections of the test nodes are present. Thus, an inductive model has to generalize to completely unseen graphs.  GraphSAINT follows the design philosophy of directly sampling the training graph G, rather than the corresponding GCN. Our goals are to 1. extract appropriately connected subgraphs so that little information is lost when propagating within the subgraphs, and 2. combine information of many subgraphs together so that the training process overall learns good representation of the full graph. <ref type="figure" target="#fig_1">Figure 1</ref> and Algorithm 1 illustrate the training algorithm. Before training starts, we perform light-weight pre-processing on G with the given sampler SAMPLE. The pre-processing estimates the probability of a node v ∈ V and an edge e ∈ E being sampled by SAMPLE. Such probability is later used to normalize the subgraph neighbor aggregation and the minibatch loss (Section 3.2). Afterwards, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MINIBATCH BY GRAPH SAMPLING</head><formula xml:id="formula_2">{y v | v ∈ V s } ← Forward propagation of {x v | v ∈ V s }, normalized by α 6:</formula><p>Backward propagation from λ-normalized loss L (y v , y v ). Update weights. 7: end for training proceeds by iterative weight updates via SGD. Each iteration starts with an independently sampled G s (where |V s | |V|). We then build a full GCN on G s to generate embedding and calculate loss for every v ∈ V s . In Algorithm 1, node representation is learned by performing node classification in the supervised setting, and each training node v comes with a ground truth label y v .</p><p>Intuitively, there are two requirements for SAMPLE: 1. Nodes having high influence on each other should be sampled in the same subgraph. 2. Each edge should have non-negligible probability to be sampled. For requirement (1), an ideal SAMPLE would consider the joint information from node connections as well as attributes. However, the resulting algorithm may have high complexity as it would need to infer the relationships between features. For simplicity, we define "influence" from the graph connectivity perspective and design topology based samplers. Requirement (2) leads to better generalization since it enables the neural net to explore the full feature and label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NORMALIZATION</head><p>A sampler that preserves connectivity characteristic of G will almost inevitably introduce bias into minibatch estimation. In the following, we present normalization techniques to eliminate biases.</p><p>Analysis of the complete multi-layer GCN is difficult due to non-linear activations. Thus, we analyze the embedding of each layer independently. This is similar to the treatment of layers independently by prior work <ref type="bibr" target="#b4">(Chen et al., 2018b;</ref><ref type="bibr" target="#b14">Huang et al., 2018)</ref>. Consider a layer-( + 1) node v and a layernode u. If v is sampled (i.e., v ∈ V s ), we can compute the aggregated feature of v as:</p><formula xml:id="formula_3">ζ ( +1) v = u∈V A v,u α u,v W ( ) T x ( ) u 1 u|v = u∈V A v,u α u,vx ( ) u 1 u|v ,<label>(2)</label></formula><p>wherex ( )</p><formula xml:id="formula_4">u = W ( ) T x ( ) u , and 1 u|v ∈ {0, 1} is the indicator function given v is in the subgraph (i.e., 1 u|v = 0 if v ∈ V s ∧ (u, v) ∈ E s ; 1 u|v = 1 if (u, v) ∈ E s ; 1 u|v not defined if v ∈ V s )</formula><p>. We refer to the constant α u,v as aggregator normalization. Define p u,v = p v,u as the probability of an edge (u, v) ∈ E being sampled in a subgraph, and p v as the probability of a node v ∈ V being sampled.</p><formula xml:id="formula_5">Proposition 3.1. ζ ( +1) v is an unbiased estimator of the aggregation of v in the full ( + 1) th GCN layer, if α u,v = pu,v pv . i.e., E ζ ( +1) v = u∈V A v,ux ( ) u .</formula><p>Assuming that each layer independently learns an embedding, we use Proposition 3.1 to normalize feature propagation of each layer of the GCN built by GraphSAINT. Further, let L v be the loss on v in the output layer. The minibatch loss is calculated as L batch = v∈Gs L v /λ v , where λ v is a constant that we term loss normalization. We set λ v = |V| · p v so that:</p><formula xml:id="formula_6">E (L batch ) = 1 |G| Gs∈G v∈Vs L v λ v = 1 |V| v∈V L v .<label>(3)</label></formula><p>Feature propagation within subgraphs thus requires normalization factors α and λ, which are computed based on the edge and node probability p u,v , p v . In the case of random node or random edge samplers, p u,v and p v can be derived analytically. For other samplers in general, closed form expression is hard to obtain. Thus, we perform pre-processing for estimation. Before training starts, we run the sampler repeatedly to obtain a set of N subgraphs G. We setup a counter C v and C u,v for each v ∈ V and (u, v) ∈ E, to count the number of times the node or edge appears in the subgraphs of G. Then we set</p><formula xml:id="formula_7">α u,v = Cu,v Cv = Cv,u</formula><p>Cv and λ v = Cv N . The subgraphs G s ∈ G can all be reused as minibatches during training. Thus, the overhead of pre-processing is small (see Appendix D.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VARIANCE</head><p>We derive samplers for variance reduction. Let e be the edge connecting u, v, and b ( )</p><formula xml:id="formula_8">e = A v,ux ( −1) u + A u,vx ( −1) v . It is desirable that variance of all estimators ζ ( ) v is small. With this objective, we define: ζ = v∈Gs ζ ( ) v p v = v,u A v,u p v α u,vx ( ) u 1 v 1 u|v = e b ( ) e p e 1 ( ) e .<label>(4)</label></formula><p>where</p><formula xml:id="formula_9">1 e = 1 if e ∈ E s ; 1 e = 0 if e ∈ E s . And 1 v = 1 if v ∈ V s ; 1 v = 0 if v ∈ V s . The factor p u in</formula><p>the first equality is present so that ζ is an unbiased estimator of the sum of all node aggregations at all layers:</p><formula xml:id="formula_10">E (ζ) = v∈V E ζ ( ) v</formula><p>. Note that 1 ( ) e = 1 e , ∀ , since once an edge is present in the sampled graph, it is present in all layers of our GCN.</p><p>We define the optimal edge sampler to minimize variance for every dimension of ζ. We restrict ourselves to independent edge sampling. For each e ∈ E, we make independent decision on whether it should be in G s or not. The probability of including e is p e . We further constrain p e = m, so that the expected number of sampled edges equals to m. The budget m is a given sampling parameter. Theorem 3.2. Under independent edge sampling with budget m, the optimal edge probabilities to minimize the sum of variance of each ζ's dimension is given by:</p><formula xml:id="formula_11">p e = m e b ( ) e b ( ) e .</formula><p>To prove Theorem 3.2, we make use of the independence among graph edges, and the dependence among layer edges to obtain the covariance of 1 ( ) e . Then using the fact that sum of p e is a constant, we use the Cauchy-Schwarz inequality to derive the optimal p e . Details are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note that calculating b</head><p>( ) e requires computingx ( −1) v , which increases the complexity of sampling. As a reasonable simplification, we ignorex ( ) v to make the edge probability dependent on the graph topology only. Therefore, we choose</p><formula xml:id="formula_12">p e ∝ A v,u + A u,v = 1 deg(u) + 1 deg(v)</formula><p>. The derived optimal edge sampler agrees with the intuition in Section 3.1. If two nodes u, v are connected and they have few neighbors, then u and v are likely to be influential to each other. In this case, the edge probability p u,v = p v,u should be high. The above analysis on edge samplers also inspires us to design other samplers, which are presented in Section 3.4.</p><p>Remark We can also apply the above edge sampler to perform layer sampling. Under the independent layer sampling assumption of <ref type="bibr" target="#b4">Chen et al. (2018b)</ref></p><formula xml:id="formula_13">, one would sample a connection u ( ) , v ( +1) with probability p ( ) u,v ∝ 1 deg(u) + 1 deg(v) .</formula><p>For simplicity, assume a uniform degree graph (of degree d). Then p ( ) e = p. For an already sampled u ( ) to connect to layer + 1, at least one of its edges has to be selected by the layer + 1 sampler. Clearly, the probability of an input layer node to "survive" the L number of independent sampling process is</p><formula xml:id="formula_14">1 − (1 − p) d L−1</formula><p>. Such layer sampler potentially returns an overly sparse minibatch for L &gt; 1. On the other hand, connectivity within a minibatch of GraphSAINT never drops with GCN depth. If an edge is present in layer , it is present in all layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SAMPLERS</head><p>Based on the above variance analysis, we present several light-weight and efficient samplers that GraphSAINT has integrated. Detailed sampling algorithms are listed in Appendix B.</p><p>Random node sampler We sample |V s | nodes from V randomly, according to a node probability distribution P (u) ∝ A :,u 2 . This sampler is inspired by the layer sampler of <ref type="bibr" target="#b4">Chen et al. (2018b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random edge sampler</head><p>We perform edge sampling as described in Section 3.3.</p><p>Random walk based samplers Another way to analyze graph sampling based multi-layer GCN is to ignore activations. In such case, L layers can be represented as a single layer with edge weights given by B = A L . Following a similar approach as Section 3.3, if it were possible to pick pairs of nodes (whether or not they are directly connected in the original A) independently, then we would</p><formula xml:id="formula_15">set p u,v ∝ B u,v + B v,u , where B u,v</formula><p>can be interpreted as the probability of a random walk to start at u and end at v in L hops (and B v,u vice-versa). Even though it is not possible to sample a subgraph where such pairs of nodes are independently selected, we still consider a random walk sampler with walk length L as a good candidate for L-layer GCNs. There are numerous random walk based samplers proposed in the literature <ref type="bibr" target="#b22">(Ribeiro &amp; Towsley, 2010;</ref><ref type="bibr" target="#b19">Leskovec &amp; Faloutsos, 2006;</ref><ref type="bibr" target="#b12">Hu &amp; Lau, 2013;</ref><ref type="bibr" target="#b20">Li et al., 2015)</ref>. In the experiments, we implement a regular random walk sampler (with r root nodes selected uniformly at random and each walker goes h hops), and also a multi-dimensional random walk sampler defined in <ref type="bibr" target="#b22">Ribeiro &amp; Towsley (2010)</ref>.</p><p>For all the above samplers, we return the subgraph induced from the sampled nodes. The induction step adds more connections into the subgraph, and empirically helps improve convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>Extensions GraphSAINT admits two orthogonal extensions. First, GraphSAINT can seamlessly integrate other graph samplers. Second, the idea of training by graph sampling is applicable to many GCN architecture variants: 1. Jumping knowledge <ref type="bibr" target="#b25">(Xu et al., 2018)</ref>: since our GCNs constructed during training are complete, applying skip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods <ref type="bibr" target="#b4">(Chen et al., 2018b;</ref><ref type="bibr" target="#b14">Huang et al., 2018)</ref>, extra modification to their samplers is required, since the jumping knowledge architecture requires layersamples to be a subset of layer-( − 1) samples * . 2. Attention <ref type="bibr" target="#b23">(Veličković et al., 2017;</ref><ref type="bibr" target="#b7">Fey, 2019;</ref>: while explicit variance reduction is hard due to the dynamically updated attention values, it is reasonable to apply attention within the subgraphs which are considered as representatives of the full graph. Our loss and aggregator normalizations are also applicable † . 3. Others: To support high order layers <ref type="bibr" target="#b31">(Zhou, 2017;</ref><ref type="bibr" target="#b18">Lee et al., 2018;</ref><ref type="bibr" target="#b0">Abu-El-Haija et al., 2019)</ref> or even more complicated networks for the task of graph classification <ref type="bibr" target="#b27">(Ying et al., 2018b)</ref>, we replace the full adjacency matrix A with the (normalized) one for the subgraph A s to perform layer propagation.</p><p>Comparison GraphSAINT enjoys: 1. high scalability and efficiency, 2. high accuracy, and 3. low training complexity. Point <ref type="formula" target="#formula_0">(1)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Setup Experiments are under the inductive, supervised learning setting. We evaluate GraphSAINT on the following tasks: 1. classifying protein functions based on the interactions of human tissue proteins (PPI), 2. categorizing types of images based on the descriptions and common properties of online images (Flickr), 3. predicting communities of online posts based on user comments (Reddit), 4. categorizing types of businesses based on customer reviewers and friendship (Yelp), and 5. classifying product categories based on buyer reviewers and interactions (Amazon). For PPI, we use the small version for the two layer convergence comparison (Table 2 and <ref type="figure" target="#fig_3">Figure 2</ref>), since <ref type="bibr" target="#b9">Hamilton et al. (2017)</ref> and  report accuracy for this version in their original papers. We use the large version for additional comparison with <ref type="bibr" target="#b5">Chiang et al. (2019)</ref> to be consistent with its reported accuracy. All datasets follow "fixed-partition" splits. Appendix C.2 includes further details. * The skip-connection design proposed by <ref type="bibr" target="#b14">Huang et al. (2018)</ref> does not have such "subset" requirement, and thus is compatible with both graph sampling and layer sampling based methods.</p><p>† When applying GraphSAINT to GAT <ref type="bibr" target="#b23">(Veličković et al., 2017)</ref>, we remove the softmax step which normalizes attention values within the same neighborhood, as suggested by <ref type="bibr" target="#b14">Huang et al. (2018)</ref>. See Appendix C.3.  <ref type="bibr" target="#b14">(Huang et al., 2018)</ref>, and 6. ClusterGCN <ref type="bibr" target="#b5">(Chiang et al., 2019)</ref>. All baselines are executed with their officially released code (see Appendix C.3 for downloadable URLs and commit numbers). Baselines and GraphSAINT are all implemented in Tensorflow with Python3.</p><p>We run experiments on a NVIDIA Tesla P100 GPU (see Appendix C.1 for hardware specification). <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_3">Figure 2</ref> show the accuracy and convergence comparison of various methods. All results correspond to two-layer GCN models (for GraphSAGE, we use its mean aggregator). For a given dataset, we keep hidden dimension the same across all methods. We describe the detailed architecture and hyperparameter search procedure in Appendix C.3. The mean and confidence interval of the accuracy values in <ref type="table" target="#tab_2">Table 2</ref> are measured by three runs under the same hyperparameters. The training time of <ref type="figure" target="#fig_3">Figure 2</ref> excludes the time for data loading, pre-processing, validation set evaluation and model saving. Our pre-processing incurs little overhead in training time. See Appendix D.2 for cost of graph sampling. For GraphSAINT, we implement the graph samplers described in Section 3.4. In <ref type="table" target="#tab_2">Table 2</ref>, "Node" stands for random node sampler; "Edge" stands for random edge sampler; "RW" stands for random walk sampler; "MRW" stands for multi-dimensional random walk sampler. GraphSAINT-Node 0.960±0.001 0.507±0.001 0.962±0.001 0.641±0.000 0.782±0.004 GraphSAINT-Edge 0.981±0.007 0.510±0.002 0.966±0.001 0.653±0.003 0.807±0.001 GraphSAINT-RW 0.981±0.004 0.511±0.001 0.966±0.001 0.653±0.003 0.815±0.001 GraphSAINT-MRW 0.980±0.006 0.510±0.001 0.964±0.000 0.652±0.001 0.809±0.001  Clearly, with appropriate graph samplers, GraphSAINT achieves significantly higher accuracy on all datasets. For GraphSAINT-Node, we use the same node probability as FastGCN. Thus, the accuracy improvement is mainly due to the switching from layer sampling to graph sampling (see "Remark" in Section 3.3). Compared with AS-GCN, GraphSAINT is significantly faster. The sampler of AS-GCN is expensive to execute, making its overall training time even longer than vanilla GCN. We provide detailed computation complexity analysis on the sampler in Appendix D.2. For S-GCN on Reddit, it achieves similar accuracy as GraphSAINT, at the cost of over 9× longer training time. The released code of FastGCN only supports CPU execution, so its convergence curve is dashed. <ref type="table" target="#tab_3">Table 3</ref> presents additional comparison with ClusterGCN. We use L × f to specify the architecture, where L and f denote GCN depth and hidden dimension, respectively. The four architectures are the ones used in the original paper <ref type="bibr" target="#b5">(Chiang et al., 2019)</ref>. Again, GraphSAINT achieves significant accuracy improvement. To train models with L &gt; 2 often requires additional architectural tweaks. ClusterGCN uses its diagonal enhancement technique for the 5-layer PPI and 4-layer Reddit models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">COMPARISON WITH STATE-OF-THE-ART</head><p>GraphSAINT uses jumping knowledge connection <ref type="bibr" target="#b25">(Xu et al., 2018)</ref> for 4-layer Reddit.</p><p>Evaluation on graph samplers From <ref type="table" target="#tab_2">Table 2</ref>, random edge and random walk based samplers achieve higher accuracy than the random node sampler. <ref type="figure">Figure 3</ref> presents sensitivity analysis on parameters of "RW". We use the same hyperparameters (except the sampling parameters) and network architecture as those of the "RW" entries in <ref type="table" target="#tab_2">Table 2</ref>. We fix the length of each walker to 2 (i.e., GCN depth), and vary the number of roots r from 250 to 2250. For PPI, increasing r from 250 to 750 significantly improves accuracy. Overall, for all datasets, accuracy stabilizes beyond r = 750.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GraphSAINT ON ARCHITECTURE VARIANTS AND DEEP MODELS</head><p>In <ref type="figure" target="#fig_4">Figure 4</ref>, we train a 2-layer and a 4-layer model of GAT <ref type="bibr" target="#b23">(Veličković et al., 2017)</ref> and JK-net <ref type="bibr" target="#b25">(Xu et al., 2018)</ref>, by using minibatches of GraphSAGE and GraphSAINT respectively. On the two 4-layer architectures, GraphSAINT achieves two orders of magnitude speedup than GraphSAGE, indicating much better scalability on deep models. From accuracy perspective, 4-layer GAT-SAGE and JK-SAGE do not outperform the corresponding 2-layer versions, potentially due to the smoothening effect caused by the massive neighborhood size. On the other hand, with minibatches returned by our edge sampler, increasing model depth of JK-SAINT leads to noticeable accuracy improvement (from 0.966 of 2-layer to 0.970 of 4-layer). Appendix D.1 contains additional scalability results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have presented GraphSAINT, a graph sampling based training method for deep GCNs on large graphs. We have analyzed bias and variance of the minibatches defined on subgraphs, and proposed An interesting future direction is to develop distributed training algorithms using graph sampling based minibatches. After partitioning the training graph in distributed memory, sampling can be performed independently on each processor. Afterwards, training on the self-supportive subgraphs can significantly reduce the system-level communication cost. To ensure the overall convergence quality, data shuffling strategy for the graph nodes and edges can be developed together with each specific graph sampler. Another direction is to perform algorithm-system co-optimization to accelerate the training of GraphSAINT on heterogeneous computing platforms <ref type="bibr" target="#b29">(Zeng et al., 2018;</ref><ref type="bibr" target="#b28">Zeng &amp; Prasanna, 2019)</ref>. The resolution of "neighbor explosion" by GraphSAINT not only reduces the training computation complexity, but also improves hardware utilization by significantly less data traffic to the slow memory. In addition, task-level parallelization is easy since the light-weight graph sampling is completely decoupled from the GCN layer propagation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS</head><p>Proof of Proposition 3.1. Under the condition that v is sampled in a subgraph:</p><formula xml:id="formula_16">E ζ ( +1) v =E u∈V A v,u α u,vx ( ) u 1 u|v = u∈V A v,u α u,vx ( ) u E 1 u|v = u∈V A v,u α u,vx ( ) u P ((u, v) sampled|v sampled) = u∈V A v,u α u,vx ( ) u P ((u, v) sampled) P (v sampled) = u∈V A v,u α u,vx ( ) u p u,v p v (5)</formula><p>where the second equality is due to linearity of expectation, and the third equality (conditional edge probability) is due to the initial condition that v is sampled in a subgraph.</p><p>It directly follows that, when α u,v = pu,v pv ,</p><formula xml:id="formula_17">E ζ ( +1) v = u∈V A v,ux ( ) u</formula><p>Proof of Theorem 3.2. Below, we use Cov (·) to denote covariance and Var (·) to denote variance.</p><p>For independent edge sampling as defined in Section 3.3, Cov 1 ( 1) e1 , 1 ( 2) e2 = 0, ∀e 1 = e 2 . And for a full GCN on the subgraph, Cov 1 ( 1) e , 1 ( 2) e = p e − p 2 e . To start the proof, we first assume that the b ( ) e is one dimensional (i.e., a scalar) and denote it by b ( ) e . Now, It directly follows that:</p><formula xml:id="formula_18">Var (ζ) = e, b ( ) e p e 2 Var 1 ( ) e + 2 e, 1 &lt; 2 b ( 1) e b ( 2 ) e p 2 e Cov 1 ( 1) e , 1 ( 2 ) e = e, b ( ) e 2 p e − e, b ( ) e 2 + 2 e, 1 &lt; 2 b ( 1) e b<label>(</label></formula><formula xml:id="formula_19">p e = m e b ( ) e b ( ) e</formula><p>For the multi-dimensional case of b ( ) e , following similar steps as above, it is easy to show that the optimal edge probability to minimize i Var (ζ i ) (where i is the index for ζ's dimensions) is:</p><formula xml:id="formula_20">p e = m e b ( ) e b ( ) e B SAMPLING ALGORITHM</formula><p>Algorithm 2 lists the four graph samplers we have integrated into GraphSAINT. The naming of the samplers follows that of <ref type="table" target="#tab_2">Table 2</ref>. Note that the sampling parameters n and m specify a budget rather than the actual number of nodes and edges in the subgraph G s . Since certain nodes or edges in the training graph G may be repeatedly sampled under a single invocation of the sampler, we often have |V s | &lt; n for node and MRW samplers, |V s | &lt; 2m for edge sampler, and |V s | &lt; r · h for RW sampler.</p><p>Also note that the edge sampler presented in Algorithm 2 is an approximate version of the independent edge sampler defined in Section 3.4. Complexity (excluding the subgraph induction step) of the original version in Section 3.4 is O (|E|), while complexity of the approximate one is O (m). When m |E|, the approximate version leads to identical accuracy as the original one, for a given m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DETAILED EXPERIMENTAL SETUP C.1 HARDWARE SPECIFICATION AND ENVIRONMENT</head><p>We run our experiments on a single machine with Dual Intel Xeon CPUs (E5-2698 v4 @ 2.2Ghz), one NVIDIA Tesla P100 GPU (16GB of HBM2 memory) and 512GB DDR4 memory. The code is written in Python 3.6.8 (where the sampling part is written with Cython 0.29.2). We use Tensorflow 1.12.0 on CUDA 9.2 with CUDNN 7.2.1 to train the model on GPU. Since the subgraphs are sampled independently, we run the sampler in parallel on 40 CPU cores. The Flickr dataset originates from NUS-wide § . The SNAP website ¶ collected Flickr data from four different sources including NUS-wide, and generated an un-directed graph. One node in the graph represents one image uploaded to Flickr. If two images share some common properties (e.g., same geographic location, same gallery, comments by the same user, etc.), there is an edge between the nodes of these two images. We use as the node features the 500-dimensional bag-of-word representation of the images provided by NUS-wide. For labels, we scan over the 81 tags of each image and manually merged them to 7 classes. Each image belongs to one of the 7 classes.</p><p>The Yelp dataset is prepared from the raw json data of businesses, users and reviews provided in the open challenge website . For nodes and edges, we scan the friend list of each user in the raw json file of users. If two users are friends, we create an edge between them. We then filter out all the reviews by each user and separate the reviews into words. Each review word is converted to a 300-dimensional vector using the Word2Vec model pre-trained on GoogleNews * * . The word vectors of each node are added and normalized to serve as the node feature (i.e., x v ). As for the node labels, we scan the raw json file of businesses, and use the categories of the businesses reviewed by a user v as the multi-class label of v.</p><p>For the Amazon dataset, a node is a product on the Amazon website and an edge (u, v) is created if products u and v are bought by the same customer. Each product contains text reviews (converted to 4-gram) from the buyer. We use SVD to reduce the dimensionality of the 4-gram representation to 200, and use the obtained vectors as the node feature. The labels represent the product categories (e.g., books, movies, shoes). <ref type="figure">Figure 5</ref> shows the degree distribution of the five graphs. A point (k, p) in the plot means the probability of a node having degree at least k is p.   The hidden dimensions used for Table 2, <ref type="figure" target="#fig_3">Figure 2</ref>, <ref type="figure">Figure 3</ref> and <ref type="figure" target="#fig_4">Figure 4</ref> are: 512 for PPI, 256 for Flickr, 128 for Reddit, 512 for Yelp and 512 for Amazon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 ADDITIONAL DETAILS IN EXPERIMENTAL CONFIGURATION</head><p>All methods terminate after a fixed number of epochs based on convergence. We save the model producing the highest validation set F1-micro score, and reload it to evaluate the test set accuracy.</p><p>For vanilla GCN and AS-GCN, we set the batch size to their default value 512. For GraphSAGE, we use the mean aggregator with the default batch size 512. For S-GCN, we set the flag -cv -cvd (which stand for "control variate" and "control variate dropout") with pre-computation of the first layer aggregation. According to the paper , such pre-computation significantly reduces training time without affecting accuracy. For S-GCN, we use the default batch size 1000, and for FastGCN, we use the default value 400. For ClusterGCN, its batch size is determined by two parameters: the cluster size and the number of clusters per batch. We sweep the cluster size from 500 to 10000 with step 500, and the number of clusters per batch from {1, 10, 20, 40} to determine the optimal configuration for each dataset / architecture. Considering that for ClusterGCN, the cluster structure may be sensitive to the cluster size, and for FastGCN, the minibatch connectivity may increase with the sample size, we present additional experimental results to reveal the relation between accuracy and batch size in Appendix D.3.  Configuration of GraphSAINT to reproduce <ref type="table" target="#tab_2">Table 2</ref> results is shown in <ref type="table" target="#tab_6">Table 5</ref>. Configuration of GraphSAINT to reproduce <ref type="table" target="#tab_3">Table 3</ref> results is shown in <ref type="table" target="#tab_7">Table 6</ref>.</p><p>Below we describe the configuration for <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>The major difference between a normal GCN and a JK-net <ref type="bibr" target="#b25">(Xu et al., 2018)</ref> is that JK-net has an additional final layer that aggregates all the output hidden features of graph convolutional layers 1 to L. Mathematically, the additional aggregation layer outputs the final embedding x JK as follows:</p><formula xml:id="formula_21">x JK = σ W T JK · L =1 x ( ) v<label>(7)</label></formula><p>where based on <ref type="bibr" target="#b25">Xu et al. (2018)</ref>, is the vector aggregation operator: max-pooling, concatenation or LSTM <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997)</ref> based aggregation.</p><p>The graph attention of GAT <ref type="bibr" target="#b23">(Veličković et al., 2017)</ref> calculates the edge weights for neighbor aggregation by an additional neural network. With multi-head (K) attention, the layer-( − 1) features propagate to layer-( ) as follows:</p><formula xml:id="formula_22">x ( ) v = K k=1 σ   u∈neighbor(v) α k u,v W k x ( −1) v  <label>(8)</label></formula><p>where is the vector concatenation operation, and the coefficient α is calculated with the attention weights a k by:</p><formula xml:id="formula_23">α k u,v = LeakyReLU a k T W k x u W k x v<label>(9)</label></formula><p>Note that the α calculation is slightly different from the original equation in <ref type="bibr" target="#b23">Veličković et al. (2017)</ref>. Namely, GAT-SAINT does not normalize α by softmax across all neighbors of v. We make such modification since under the minibatch setting, node v does not see all its neighbors in the training graph. The removal of softmax is also seen in the attention design of <ref type="bibr" target="#b14">Huang et al. (2018)</ref>. Note that during the minibatch training, GAT-SAINT further applies another edge coefficient on top of attention for aggregator normalization. <ref type="table" target="#tab_8">Table 7</ref> shows the configuration of the GAT-SAINT and JK-SAINT curves in <ref type="figure" target="#fig_4">Figure 4</ref>. We evaluate the training efficiency for deeper GCNs. We only compare with S-GCN, since implementations for other layer sampling based methods have not yet supported arbitrary model depth. The batch size and hidden dimension are the same as <ref type="table" target="#tab_2">Table 2</ref>. On the two large graphs (Reddit and Yelp), we increase the number of layers and measure the average time per minibatch execution. In <ref type="figure">Figure  6</ref>, training cost of GraphSAINT is approximately linear with GCN depth. Training cost of S-GCN grows dramatically when increasing the depth. This reflects the "neighbor explosion" phenomenon (even though the expansion factor of S-GCN is just 2). On Yelp, S-GCN gives "out-of-memory" error for models beyond 5 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 COST OF SAMPLING AND PRE-PROCESSING</head><p>Cost of graph samplers of GraphSAINT Graph sampling introduces little training overhead. Let t s be the average time to sample one subgraph on a multi-core machine. Let t t be the average time to perform the forward and backward propagation on one minibatch on GPU. <ref type="figure" target="#fig_8">Figure 7</ref> shows the ratio t s /t t for various datasets. The parameters of the samplers are the same as <ref type="table" target="#tab_2">Table 2</ref>. For Node, Edge and RW samplers, we observe that time to sample one subgraph is in most cases less than 25% of the training time. The MRW sampler is more expensive to execute. Regarding the complete pre-processing procedure, we repeatedly run the sampler for N = 50 · |V| /|V s | times before training, to estimate the node and edge probability as discussed in Section 3.2 (where |V s | is the average subgraph size). These sampled subgraphs are reused as training minibatches. Thus, if training runs for more than N iterations, the pre-processing is nearly zero-cost. Under the setting of <ref type="table" target="#tab_2">Table 2</ref>, pre-processing on PPI and Yelp and Amazon does not incur any overhead in training time. Pre-processing on Flickr and Reddit (with RW sampler) takes less than 40% and 15% of their corresponding total training time.</p><p>Cost of layers sampler of AS-GCN AS-GCN uses an additional neural network to estimate the conditional sampling probability for the previous layer. For a node v already sampled in layer , features of layer-( − 1) corresponding to all v's neighbors need to be fed to the sampling neural network to obtain the node probability. For sake of analysis, assume the sampling network is a single layer MLP, whose weight W MLP has the same shape as the GCN weights W ( ) . Then we can show, for a L-layer GCN on a degree-d graph, per epoch training complexity of AS-GCN is approximately γ = (d · L) / L−1 =0 d times that of vanilla GCN. For L = 2, we have γ ≈ 2. This explains the observation that AS-GCN is slower than vanilla GCN in <ref type="figure" target="#fig_3">Figure 2</ref>. Additional, <ref type="table" target="#tab_9">Table 8</ref> shows the training time breakdown for AS-GCN. Clearly, its sampler is much more expensive than the graph sampler of GraphSAINT. Cost of clustering of ClusterGCN ClusterGCN uses the highly optimized METIS software † † to perform clustering. <ref type="table" target="#tab_10">Table 9</ref> summarizes the time to obtain the clusters for the five graphs. On the large and dense Amazon graph, the cost of clustering increase dramatically. The pre-processing time of ClusterGCN on Amazon is more than 4× of the total training time. On the other hand, the sampling cost of GraphSAINT does not increase significantly for large graphs (see <ref type="figure" target="#fig_8">Figure 7</ref>). Taking into account the pre-processing time, sampling time and training time altogether, we summarize the total convergence time of GraphSAINT and ClusterGCN in <ref type="table" target="#tab_1">Table 10 (corresponding to  Table 2</ref> configuration). On graphs that are large and dense (e.g., Amazon), GraphSAINT achieves significantly faster convergence. Note that both the sampling of GraphSAINT and clustering of ClusterGCN can be performed offline.  <ref type="table" target="#tab_1">Table 11</ref> shows the change of test set accuracy with batch sizes. For each row of <ref type="table" target="#tab_1">Table 11</ref>, we fix the batch size, tune the other hyperparameters according to Appendix C.3, and report the highest test set accuracy achieved. For GraphSAGE, S-GCN and AS-GCN, their default batch sizes (512,1000 and 512, respectively) lead to the highest accuracy on all datasets. For FastGCN, increasing the default batch size (from 400 to 4000) leads to noticeable accuracy improvement. For ClusterGCN, different datasets correspond to different optimal batch sizes. Note that the accuracy in Section 5.1 is already tuned by identifying the optimal batch size on a per graph basis.</p><p>For FastGCN, intuitively, increasing batch size may help with accuracy improvement since the minibatches may become better connected. Such intuition is verified by the rows of 400 and 2000. However, increasing the batch size from 2000 to 4000 does not further improve accuracy significantly. For ClusterGCN, the optimal batch size depends on the cluster structure of the training graph. For PPI, small batches are better, while for Amazon, batch size does not have significant impact on accuracy. For GraphSAGE, overly large batches may have negative impact on accuracy due to neighbor explosion. Approximately, GraphSAGE expand 10× more neighbors per layer. For a 2-layer GCN, a size 2 × 10 3 minibatch would then require the support of 2 × 10 5 nodes from the † † http://glaros.dtc.umn.edu/gkhome/metis/metis/download * Default batch size ¶ The training does not converge. ‡ The codes throw runtime error on the large datasets (Yelp or Amazon). input layer. Note that the full training graph size of Reddit is just around 1.5 × 10 5 . Thus, no matter which nodes are sampled in the output layer, GraphSAGE would almost always propagate features within the full training graph for initial layers. We suspect this would lead to difficulties in learning. For S-GCN, with batch size of 500, it fails to learn properly on Reddit and Yelp. The accuracy fluctuates in a region of very low value, even after appropriate hyperparameter tuning. For AS-GCN, its accuracy is not sensitive to the batch size, since AS-GCN addresses neighbor explosion and also ensures good inter-layer connectivity within the minibatch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>GraphSAINT training algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is due to the significantly reduced neighborhood size compared with Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a). Point(2)is due to the better interlayer connectivity compared with<ref type="bibr" target="#b4">Chen et al. (2018b)</ref>, and unbiased minibatch estimator compared with<ref type="bibr" target="#b5">Chiang et al. (2019)</ref>. Point (3) is due to the simple and trivially parallelizable pre-processing compared with the sampling of<ref type="bibr" target="#b14">Huang et al. (2018)</ref> and clustering of<ref type="bibr" target="#b5">Chiang et al. (2019)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Convergence curves of 2-layer models on GraphSAINT and baselines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>GraphSAINT with JK-net and GAT (Reddit) normalization techniques and sampling algorithms to improve training quality. We have conducted extensive experiments to demonstrate the advantage of GraphSAINT in accuracy and training time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>ACKNOWLEDGEMENT</head><label></label><figDesc>This material is based on work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract Number FA8750-17-C-0086 and National Science Foundation (NSF) under Contract Numbers CCF-1919289 and OAC-1911229. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or NSF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>constant m = e p e be the expected number of sampled edges. By Cauchy-Schwarz ine . i.e., variance is minimized when p e ∝ b ( ) e .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 2</head><label>2</label><figDesc>Graph sampling algorithms by GraphSAINT Input: Training graph G (V, E); Sampling parameters: node budget n; edge budget m; number of roots r; random walk length h Output: Sampled graph G s (V s , E s ) 1: function NODE(Gs ← n nodes randomly sampled (with replacement) from V according to P 4:G s ← Node induced subgraph of G from V s 5: end function 6: function EDGE(G,m)Edge sampler (approximate version) 7:P ((u, v)) := 1 deg(u) + 1 deg(v) / (u ,v )∈E 1 deg(u ) + 1 deg(v ) 8: E s ← medges randomly sampled (with replacement) from E according to P 9: V s ← Set of nodes that are end-points of edges in E s 10: G s ← Node induced subgraph of G from V s 11: end function 12: function RW(G,r,h) Random walk sampler 13: V root ← r root nodes sampled uniformly at random (with replacement) from V Node induced subgraph of G from V s 23: end function 24: function MRW(G,n,r) Multi-dimensional random walk sampler 25: V FS ← r root nodes sampled uniformly at random (with replacement) from V 26: V s ← V FS 27:for i = r + 1 to n do 28:Select u ∈ V FS with probability deg(u)/ v∈VFS deg(v) Node induced subgraph of G from V s 34: end function 10 0 10 1 10 2 10 3 10 4 10 5 DATASET DETAILS Here we present the detailed procedures to prepare the Flickr, Yelp and Amazon datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Fraction of training time on sampling D ADDITIONAL EXPERIMENTS D.1 TRAINING EFFICIENCY ON DEEP MODELS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics ("m" stands for multi-class classification, and "s" for single-class.)</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="4">Degree Feature Classes Train / Val / Test</cell></row><row><cell>PPI</cell><cell>14,755</cell><cell>225,270</cell><cell>15</cell><cell>50</cell><cell cols="2">121 (m) 0.66 / 0.12 / 0.22</cell></row><row><cell>Flickr</cell><cell>89,250</cell><cell>899,756</cell><cell>10</cell><cell>500</cell><cell>7 (s)</cell><cell>0.50 / 0.25 / 0.25</cell></row><row><cell>Reddit</cell><cell>232,965</cell><cell>11,606,919</cell><cell>50</cell><cell>602</cell><cell>41 (s)</cell><cell>0.66 / 0.10 / 0.24</cell></row><row><cell>Yelp</cell><cell>716,847</cell><cell>6,977,410</cell><cell>10</cell><cell>300</cell><cell cols="2">100 (m) 0.75 / 0.10 / 0.15</cell></row><row><cell cols="3">Amazon 1,598,960 132,169,734</cell><cell>83</cell><cell>200</cell><cell cols="2">107 (m) 0.85 / 0.05 / 0.10</cell></row><row><cell>PPI (large version)</cell><cell>56,944</cell><cell>818,716</cell><cell>14</cell><cell>50</cell><cell cols="2">121 (m) 0.79 / 0.11 / 0.10</cell></row><row><cell cols="7">We open source GraphSAINT  ‡ . We compare with six baselines: 1. vanilla GCN (Kipf &amp; Welling,</cell></row><row><cell cols="7">2016), 2. GraphSAGE (Hamilton et al., 2017), 3. FastGCN (Chen et al., 2018b), 4. S-GCN (Chen</cell></row><row><cell cols="2">et al., 2018a), 5. AS-GCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of test set F1-micro score with state-of-the-art methods 515±0.006 0.492±0.003 0.933±0.000 0.378±0.001 0.281±0.005 GraphSAGE 0.637±0.006 0.501±0.013 0.953±0.001 0.634±0.006 0.758±0.002FastGCN 0.513±0.032 0.504±0.001 0.924±0.001 0.265±0.053 0.174±0.021 S-GCN 0.963±0.010 0.482±0.003 0.964±0.001 0.640±0.002 - ‡ AS-GCN 0.687±0.012 0.504±0.002 0.958±0.001 - ClusterGCN 0.875±0.004 0.481±0.005 0.954±0.001 0.609±0.005 0.759±0.008</figDesc><table><row><cell>Method</cell><cell>PPI</cell><cell>Flickr</cell><cell>Reddit</cell><cell>Yelp</cell><cell>Amazon</cell></row><row><cell>GCN 0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>‡ -‡</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Additional comparison with ClusterGCN (test set F1-micro score)</figDesc><table><row><cell cols="2">PPI (large version)</cell><cell>Reddit</cell><cell></cell></row><row><cell>2 × 512</cell><cell>5 × 2048</cell><cell>2 × 128</cell><cell>4 × 128</cell></row></table><note>ClusterGCN 0.903±0.002 0.994±0.000 0.954±0.001 0.966±0.001 GraphSAINT 0.941±0.003 0.995±0.000 0.966±0.001 0.970±0.001 ‡ Open sourced code: https://github.com/GraphSAINT/GraphSAINT‡ The codes throw runtime error on the large datasets (Yelp or Amazon).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>summarizes the URLs to download the baseline codes. The optimizer for GraphSAINT and all baselines is Adam (Kingma &amp; Ba, 2014). For all baselines and datasets, we perform grid search on the hyperparameter space defined by: • Hidden dimension: {128, 256, 512} § http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm ¶ https://snap.stanford.edu/data/web-flickr.html https://www.yelp.com/dataset</figDesc><table /><note>* * https://code.google.com/archive/p/word2vec/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>URLs and commit number to run baseline codes</figDesc><table><row><cell>Baseline</cell><cell>URL</cell><cell>Commit</cell></row><row><cell>Vanilla GCN</cell><cell>github.com/williamleif/GraphSAGE</cell><cell>a0fdef</cell></row><row><cell>GraphSAGE</cell><cell>github.com/williamleif/GraphSAGE</cell><cell>a0fdef</cell></row><row><cell>FastGCN</cell><cell>github.com/matenure/FastGCN</cell><cell>b8e6e6</cell></row><row><cell>S-GCN</cell><cell>github.com/thu-ml/stochastic_gcn</cell><cell>da7b78</cell></row><row><cell>AS-GCN</cell><cell>github.com/huangwb/AS-GCN</cell><cell>5436ec</cell></row><row><cell cols="3">ClusterGCN github.com/google-research/google-research/tree/master/cluster_gcn 99021e</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Training configuration of GraphSAINT forTable 2</figDesc><table><row><cell cols="2">Sampler Dataset</cell><cell>Training</cell><cell></cell><cell></cell><cell>Sampling</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">Learning rate Dropout Node budget Edge budget Roots Walk length</cell></row><row><cell></cell><cell>PPI</cell><cell>0.01</cell><cell>0.0</cell><cell>6000</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Node</cell><cell>Flickr Reddit</cell><cell>0.01 0.01</cell><cell>0.2 0.1</cell><cell>8000 8000</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Yelp</cell><cell>0.01</cell><cell>0.1</cell><cell>5000</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Amazon</cell><cell>0.01</cell><cell>0.1</cell><cell>4500</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PPI</cell><cell>0.01</cell><cell>0.1</cell><cell>-</cell><cell>4000</cell><cell>-</cell><cell>-</cell></row><row><cell>Edge</cell><cell>Flickr Reddit</cell><cell>0.01 0.01</cell><cell>0.2 0.1</cell><cell>--</cell><cell>6000 6000</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Yelp</cell><cell>0.01</cell><cell>0.1</cell><cell>-</cell><cell>2500</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Amazon</cell><cell>0.01</cell><cell>0.1</cell><cell>-</cell><cell>2000</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PPI</cell><cell>0.01</cell><cell>0.1</cell><cell>-</cell><cell>-</cell><cell>3000</cell><cell>2</cell></row><row><cell>RW</cell><cell>Flickr Reddit</cell><cell>0.01 0.01</cell><cell>0.2 0.1</cell><cell>--</cell><cell>--</cell><cell>6000 2000</cell><cell>2 4</cell></row><row><cell></cell><cell>Yelp</cell><cell>0.01</cell><cell>0.1</cell><cell>-</cell><cell>-</cell><cell>1250</cell><cell>2</cell></row><row><cell></cell><cell>Amazon</cell><cell>0.01</cell><cell>0.1</cell><cell>-</cell><cell>-</cell><cell>1500</cell><cell>2</cell></row><row><cell></cell><cell>PPI</cell><cell>0.01</cell><cell>0.1</cell><cell>8000</cell><cell>-</cell><cell>2500</cell><cell>-</cell></row><row><cell>MRW</cell><cell>Flickr Reddit</cell><cell>0.01 0.01</cell><cell>0.2 0.1</cell><cell>12000 8000</cell><cell>--</cell><cell>3000 1000</cell><cell>--</cell></row><row><cell></cell><cell>Yelp</cell><cell>0.01</cell><cell>0.1</cell><cell>2500</cell><cell>-</cell><cell>1000</cell><cell>-</cell></row><row><cell></cell><cell>Amazon</cell><cell>0.01</cell><cell>0.1</cell><cell>4500</cell><cell>-</cell><cell>1500</cell><cell>-</cell></row><row><cell cols="3">• Dropout: {0.0, 0.1, 0.2, 0.3}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">• Learning rate: {0.1, 0.01, 0.001, 0.0001}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Training configuration of GraphSAINT for Table 3</figDesc><table><row><cell>Arch.</cell><cell>Sampler</cell><cell>Dataset</cell><cell>Training</cell><cell></cell><cell></cell><cell>Sampling</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Learning rate Dropout Node budget Edge budget Roots Walk length</cell></row><row><cell>2 × 512</cell><cell>MRW</cell><cell>PPI (large)</cell><cell>0.01</cell><cell>0.1</cell><cell>1500</cell><cell>-</cell><cell>300</cell><cell>-</cell></row><row><cell>5 × 2048</cell><cell>RW</cell><cell>PPI (large)</cell><cell>0.01</cell><cell>0.1</cell><cell>-</cell><cell>-</cell><cell>3000</cell><cell>2</cell></row><row><cell>2 × 128</cell><cell>Edge</cell><cell>Reddit</cell><cell>0.01</cell><cell>0.1</cell><cell>-</cell><cell>6000</cell><cell>-</cell><cell>-</cell></row><row><cell>4 × 128</cell><cell>Edge</cell><cell>Reddit</cell><cell>0.01</cell><cell>0.2</cell><cell>-</cell><cell>11000</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: Training configuration of GraphSAINT for Figure 4 (Reddit)</cell></row><row><cell></cell><cell>2-layer GAT-SAINT</cell><cell>4-layer GAT-SAINT</cell><cell cols="2">2-layer JK-SAINT 4-layer JK-SAINT</cell></row><row><cell>Hidden dimension</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>Attention K</cell><cell>8</cell><cell>8</cell><cell>-</cell><cell>-</cell></row><row><cell>Aggregation</cell><cell>-</cell><cell>-</cell><cell>Concat.</cell><cell>Concat.</cell></row><row><cell>Sampler</cell><cell cols="2">RW (root: 3000; length: 2) (root: 2000; length: 4) RW</cell><cell>Edge (budget: 6000)</cell><cell>Edge (budget: 11000)</cell></row><row><cell>Learning rate</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Dropout</cell><cell>0.2</cell><cell>0.2</cell><cell>0.1</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Per epoch training time breakdown for AS-GCN</figDesc><table><row><cell cols="2">Dataset Sampling time (sec)</cell><cell>Forward / Backward propagation time (sec)</cell></row><row><cell>PPI</cell><cell>1.1</cell><cell>0.2</cell></row><row><cell>Flickr</cell><cell>5.3</cell><cell>1.1</cell></row><row><cell>Reddit</cell><cell>20.7</cell><cell>3.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="4">: Clustering time of ClusterGCN</cell><cell></cell></row><row><cell cols="5">PPI Flickr Reddit Yelp Amazon</cell></row><row><cell>Time (sec) 2.2</cell><cell>11.6</cell><cell>40.0</cell><cell>106.7</cell><cell>2254.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison of total convergence time (pre-processing + sampling + training, unit: second)</figDesc><table><row><cell>PPI</cell><cell cols="4">Flickr Reddit Yelp Amazon</cell></row><row><cell>GraphSAINT-Edge 91.0</cell><cell>7.0</cell><cell>16.6</cell><cell>273.9</cell><cell>401.0</cell></row><row><cell>GraphSAINT-RW 103.6</cell><cell>7.5</cell><cell>17.2</cell><cell>310.1</cell><cell>425.6</cell></row><row><cell>ClusterGCN 163.2</cell><cell>12.9</cell><cell>55.3</cell><cell>256.0</cell><cell>2804.8</cell></row><row><cell>D.3 EFFECT OF BATCH SIZE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Test set F1-micro for the baselines under various batch sizes</figDesc><table><row><cell cols="2">Method Batch size</cell><cell>PPI</cell><cell cols="3">Flickr Reddit Yelp Amazon</cell></row><row><cell></cell><cell>256</cell><cell cols="4">0.600 0.474 0.934 0.563</cell><cell>0.428</cell></row><row><cell>GraphSAGE</cell><cell>512  *  1024</cell><cell cols="4">0.637 0.501 0.953 0.634 0.610 0.482 0.935 0.632</cell><cell>0.758 0.705</cell></row><row><cell></cell><cell>2048</cell><cell cols="4">0.625 0.374 0.936 0.563</cell><cell>0.447</cell></row><row><cell></cell><cell>400  *</cell><cell cols="4">0.513 0.504 0.924 0.265</cell><cell>0.174</cell></row><row><cell>FastGCN</cell><cell>2000</cell><cell cols="4">0.561 0.506 0.934 0.255</cell><cell>0.196</cell></row><row><cell></cell><cell>4000</cell><cell cols="4">0.564 0.507 0.934 0.260</cell><cell>0.195</cell></row><row><cell></cell><cell>500</cell><cell cols="2">0.519 0.462</cell><cell>- ¶</cell><cell>- ¶</cell><cell>- ‡</cell></row><row><cell></cell><cell>1000  *</cell><cell cols="4">0.963 0.482 0.964 0.640</cell><cell>- ‡</cell></row><row><cell>S-GCN</cell><cell>2000</cell><cell cols="4">0.646 0.482 0.949 0.614</cell><cell>- ‡</cell></row><row><cell></cell><cell>4000</cell><cell cols="4">0.804 0.482 0.949 0.594</cell><cell>- ‡</cell></row><row><cell></cell><cell>8000</cell><cell cols="4">0.694 0.481 0.950 0.613</cell><cell>- ‡</cell></row><row><cell></cell><cell>256</cell><cell cols="3">0.682 0.504 0.950</cell><cell>- ‡</cell><cell>- ‡</cell></row><row><cell>AS-GCN</cell><cell>512  *  1024</cell><cell cols="3">0.687 0.504 0.958 0.687 0.502 0.951</cell><cell>- ‡ - ‡</cell><cell>- ‡ - ‡</cell></row><row><cell></cell><cell>2048</cell><cell cols="3">0.670 0.502 0.952</cell><cell>- ‡</cell><cell>- ‡</cell></row><row><cell></cell><cell>500</cell><cell cols="4">0.875 0.481 0.942 0.604</cell><cell>0.752</cell></row><row><cell></cell><cell>1000</cell><cell cols="4">0.831 0.478 0.947 0.602</cell><cell>0.756</cell></row><row><cell></cell><cell>1500</cell><cell cols="4">0.865 0.480 0.954 0.602</cell><cell>0.752</cell></row><row><cell>ClusterGCN</cell><cell>2000 2500</cell><cell cols="4">0.828 0.469 0.954 0.609 0.849 0.476 0.954 0.598</cell><cell>0.759 0.745</cell></row><row><cell></cell><cell>3000</cell><cell cols="4">0.840 0.473 0.954 0.607</cell><cell>0.754</cell></row><row><cell></cell><cell>3500</cell><cell cols="4">0.846 0.473 0.952 0.602</cell><cell>0.754</cell></row><row><cell></cell><cell>4000</cell><cell cols="4">0.853 0.472 0.949 0.605</cell><cell>0.756</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00067</idno>
		<title level="m">Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolution architectures via sparsified neighborhood mixing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6203</idno>
		<ptr target="http://arxiv.org/abs/1312.6203" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A comprehensive survey of graph embedding: Problems, techniques and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin Chen-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1709.07604</idno>
		<ptr target="http://arxiv.org/abs/1709.07604" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno>abs/1905.07953</idno>
		<ptr target="http://arxiv.org/abs/1905.07953" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Just jump: Dynamic neighborhood aggregation in graph neural networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<idno>abs/1904.04849</idno>
		<ptr target="http://arxiv.org/abs/1904.04849" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno>978-1-4503-5552-0</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;18</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition. CoRR, abs/1512.03385</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512.03385" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pili</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wing Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.5865</idno>
		<title level="m">A survey and taxonomy of graph sampling</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<ptr target="http://arxiv.org/abs/1609.02907" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Personalized embedding propagation: Combining neural networks on graphs with personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>abs/1810.05997</idno>
		<ptr target="http://arxiv.org/abs/1810.05997" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Higherorder graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Boaz</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<idno>abs/1809.07697</idno>
		<ptr target="http://arxiv.org/abs/1809.07697" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sampling from large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="631" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On random walk based graph sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE.2015.7113345</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 31st International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="927" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Graph star net for generalized multi-task learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyan</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.12330" />
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimating and sampling graphs with multidimensional random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGCOMM conference on Internet measurement</title>
		<meeting>the 10th ACM SIGCOMM conference on Internet measurement</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="390" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1901.00596</idno>
		<ptr target="http://arxiv.org/abs/1901.00596" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>978-1-4503-5552-0</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;18</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3327345.3327389" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02498</idno>
		<title level="m">GraphACT: Accelerating GCN training on CPU-FPGA heterogeneous platforms</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Accurate, efficient and scalable graph embedding. CoRR, abs/1810.11899</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1810.11899" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph convolutional networks for molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpeng</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1706.09916</idno>
		<ptr target="http://arxiv.org/abs/1706.09916" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
