<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizable Pedestrian Detection: The Elephant In The Room</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irtiza</forename><surname>Hasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI) 1</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI) 1</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI) 1</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saad</forename><surname>Ullah Akram</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI) 1</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@inceptioniai.org</email>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI) 1</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalizable Pedestrian Detection: The Elephant In The Room</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pedestrian detection is used in many vision based applications ranging from video surveillance to autonomous driving. Despite achieving high performance, it is still largely unknown how well existing detectors generalize to unseen data. This is important because a practical detector should be ready to use in various scenarios in applications. To this end, we conduct a comprehensive study in this paper, using a general principle of direct crossdataset evaluation. Through this study, we find that existing state-of-the-art pedestrian detectors, though perform quite well when trained and tested on the same dataset, generalize poorly in cross dataset evaluation. We demonstrate that there are two reasons for this trend. Firstly, their designs (e.g. anchor settings) may be biased towards popular benchmarks in the traditional single-dataset training and test pipeline, but as a result largely limit their generalization capability. Secondly, the training source is generally not dense in pedestrians and diverse in scenarios. Under direct cross-dataset evaluation, surprisingly, we find that a general purpose object detector, without pedestriantailored adaptation in design, generalizes much better compared to existing state-of-the-art pedestrian detectors. Furthermore, we illustrate that diverse and dense datasets, collected by crawling the web, serve to be an efficient source of pre-training for pedestrian detection. Accordingly, we propose a progressive training pipeline and find that it works well for autonomous-driving oriented pedestrian detection. Consequently, the study conducted in this paper suggests that more emphasis should be put on cross-dataset evaluation for the future design of generalizable pedestrian detectors. Code and models can be accessed at https: //github.com/hasanirtiza/Pedestron.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian detection is one of the longest standing prob- â€  Corresponding author. lems in computer vision. Numerous real-world applications, such as, autonomous driving <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b15">17]</ref>, video surveillance <ref type="bibr" target="#b14">[16]</ref>, action recognition <ref type="bibr" target="#b45">[45]</ref> and tracking <ref type="bibr" target="#b19">[21]</ref> rely on accurate pedestrian/person detection. Recently, convolutional neural network (CNNs) based approaches have shown considerable progress in the field of pedestrian detection, where on certain benchmarks, the progress is within striking distance of a human baseline as shown in <ref type="figure">Fig. 1 left.</ref> However, some current pedestrian detection methods show signs of over-fitting to source datasets, especially in the case of autonomous driving. As shown in <ref type="figure">Fig. 1</ref> right, current pedestrian detectors, do not generalize well to other (target) pedestrian detection datasets, even when trained on a relatively large scale dataset which is reasonably closer to the target domain. This problem prevents pedestrian detection from scaling up to real-world applications.</p><p>Despite being a key problem, generalizable pedestrian detection has not received much attention in the past. More importantly, reasons behind poor performances of pedestrian detectors in cross-dataset evaluation has not been properly investigated or discussed. In this paper, we argue that this is mainly due to the fact that the current state-of-the-art pedestrian detectors are tailored for target datasets and their overall design is biased towards target datasets, thus reducing their generalization. Secondly, the training source is generally not dense in pedestrians and diverse in scenarios. Since current state-of-the-art methods are based on deep learning, their performance depend heavily on the quantity and quality of data and there is some evidence that the performance on some computer vision tasks (e.g. image classification) keeps improving at least up-to billions of samples <ref type="bibr" target="#b27">[28]</ref>.</p><p>At present, all autonomous driving related datasets have at least three main limitations, 1) limited number of unique pedestrians, 2) low pedestrian density, i.e. the challenging occlusion samples are relatively rare, and 3) limited diversity as the datasets are captured by a small team primarily for dataset creation instead of curating them from more diverse sources (e.g. youtube, facebook, etc.).</p><p>In last couple of years, few large and diverse datasets, CrowdHuman <ref type="bibr" target="#b33">[34]</ref>, WiderPerson <ref type="bibr" target="#b48">[48]</ref> and Wider Pedestrian Figure 1: Left: Pedestrian detection performance over the years for Caltech, CityPersons and EuroCityPersons on the reasonable subset. EuroCityPersons was released in 2018 but we include results of few older models on it as well. Dotted line marks the human performance on Caltech. Right: We show comparison between traditional single-dataset train and test evaluation on Caltech <ref type="bibr" target="#b10">[12]</ref> vs. cross-dataset evaluation for three pedestrian detectors and one general object detector (Cascade R-CNN). Methods enclosed with bounding boxes are trained on CityPersons <ref type="bibr" target="#b47">[47]</ref> and evaluated on Caltech <ref type="bibr" target="#b10">[12]</ref>, while others are trained on Caltech.</p><p>[1], have been collected by crawling the web and through surveillance cameras. These datasets address the above mentioned limitations but as they are from a much broader domain, they do not sufficiently cover autonomous driving scenarios. Nevertheless, they can still be very valuable for learning a more general and robust model of pedestrians. As these datasets contain more person per image, they are likely to contain more human poses, appearances and occlusion scenarios, which is beneficial for autonomous driving scenarios, provided current pedestrian detectors have the innate ability to digest large-scale data.</p><p>In this paper, we demonstrate that the existing pedestrian detection methods fare poorly compared to general object detectors when provided with larger and more diverse datasets, and that the state-of-the-art general detectors when carefully trained can significantly out-perform pedestrian-specific detection methods on pedestrian detection task, without any pedestrian-specific adaptation on the target data (see <ref type="figure">Fig. 1 right)</ref>. We also propose a progressive training pipeline for better utilization of general pedestrian datasets for improving the pedestrian detection performance in case of autonomous driving. We show that by progressively fine-tuning the models from the largest (but farthest away from the target domain) to smallest (but closest to the target domain) dataset, we can achieve large gains in performance in terms of M R âˆ’2 on reasonable subset of Caltech (3.7%) and CityPerson (1.5%) without fine-tuning on target domain. These improvement hold true for models from all pedestrian detection families that we tested such as Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>, Faster RCNN <ref type="bibr" target="#b32">[33]</ref> and embedded vision based backbones such as MobileNet <ref type="bibr" target="#b18">[20]</ref> The rest of the paper is organized as follows. Section 2 reviews the relevant literature. We introduce datasets and evaluation protocol in Sec. 3. We benchmark our baseline in Sec. 4. We test the generalization capabilities of the pedestrian specific and general object detectors in Sec. 5. Finally, conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pedestrian detection. Before the emergence of CNNs, a common way to address this problem was to exhaustively operate in a sliding window manner over all possible locations and scales, inspired from Viola and Jones <ref type="bibr" target="#b37">[38]</ref>. Dalal and Triggs in their landmark pedestrian detection work <ref type="bibr" target="#b8">[10]</ref> proposed Histogram of Oriented Gradients (HOG) feature descriptor for representing pedestrians. Dollar et al <ref type="bibr" target="#b9">[11]</ref>, proposed ACF, where the key idea was to use features across multiple channels. Similarly, <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b30">31]</ref>, used filtered channel features and low-level visual features along with spatial pooling respectively for pedestrian detection. However, the use of engineered features meant very limited generalization ability and limited performance.</p><p>In recent years, Convolutional Neural Networks (CNNs) have become the dominant paradigm in generic object detection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">24]</ref>. The same trend is also true for the pedestrian detection <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b5">7]</ref>. Some of the pioneer works for CNN based pedestrian detection <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b46">46]</ref> used R-CNN framework <ref type="bibr" target="#b13">[15]</ref>, which is still the most popular framework. RPN+BF <ref type="bibr" target="#b43">[44]</ref> was the first work to use Region Proposal Network (RPN); it used boosted forest for improving pedestrian detection performance. This work also pointed out some problems in the underlying classification branch of Faster RCNN <ref type="bibr" target="#b32">[33]</ref>, namely that the resolution of the feature maps and class-imbalance. However, RPN+BF despite achieving good performances had a shortcoming of not being optimized end-to-end. After the initial works, Faster RCNN <ref type="bibr" target="#b32">[33]</ref> became most popular framework with wide range of literature deploying it for pedestrian detection <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Some of the recent state-of-the-art pedestrian detectors include ALF <ref type="bibr" target="#b24">[26]</ref>, CSP <ref type="bibr" target="#b26">[27]</ref> and MGAN <ref type="bibr" target="#b31">[32]</ref>. ALF <ref type="bibr" target="#b24">[26]</ref> is based on Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b22">[24]</ref>, it stacks together multiple predictors to learn a better detection from default anchor boxes. MGAN <ref type="bibr" target="#b31">[32]</ref> uses the segmentation mask of the visible region of a pedestrian to guide the network attention and improve performance on occluded pedestrians. CSP is an anchor-less fully convolutional detector, which utilizes concatenated feature maps for predicting pedestrians.</p><p>Pedestrian detection benchmarks. Over the years, several datasets for pedestrian detection have been created such as Town Center <ref type="bibr" target="#b1">[3]</ref>, USC <ref type="bibr" target="#b41">[42]</ref>, Daimler-DB <ref type="bibr" target="#b29">[30]</ref>, INRIA <ref type="bibr" target="#b8">[10]</ref>, ETH <ref type="bibr" target="#b11">[13]</ref>, and TUDBrussels <ref type="bibr" target="#b40">[41]</ref>. All of the aforementioned datasets were typically collected for surveillance application. None of these datasets were created with the aim of providing large-scale images for the autonomous driving systems. However, in the last decade several datasets have been proposed from the context of autonomous driving such as KITTI <ref type="bibr" target="#b12">[14]</ref>, Caltech <ref type="bibr" target="#b10">[12]</ref>, CityPersons <ref type="bibr" target="#b47">[47]</ref> and ECP <ref type="bibr" target="#b2">[4]</ref>. Typically these datasets are captured by a vehicle-mounted camera navigating through crowded scenarios. These datsets have been used by several methods with Caltech <ref type="bibr" target="#b10">[12]</ref> and CityPersons <ref type="bibr" target="#b47">[47]</ref> being the most established benchmarks in this domain. However, Caltech <ref type="bibr" target="#b10">[12]</ref> and CityPersons <ref type="bibr" target="#b47">[47]</ref> datasets are monotonous in nature and they lack diverse scenarios (contain only street view images). Recently, ECP <ref type="bibr" target="#b2">[4]</ref> dataset which is an order of magnitude larger than CityPersons <ref type="bibr" target="#b47">[47]</ref> has been porposed. ECP <ref type="bibr" target="#b2">[4]</ref> is much bigger and diverse as it contains images from all seasons, under both day and night times, in several different countries. However, despite its large scale, ECP <ref type="bibr" target="#b2">[4]</ref> provides a limited diversity (in terms of scene and background) and density (number of people per frame is less than 10). Therefor, in this paper we argue that despite some recent large scale datasets, the ability of pedestrian detectors to generalize has been constrained by lack of diversity and density. Moreover, benchmarks such as WiderPerson <ref type="bibr" target="#b48">[48]</ref> Wider Pedestrian [1] and CrowdHuman <ref type="bibr" target="#b33">[34]</ref>, which con-tain web crawled images provide a much larger diversity and density. This enables detectors to learn a more robust representation of pedestrians with increased generalization ability. Cross-dataset evaluation. Previously, other works <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">47]</ref> have investigated the role of diverse and dense datasets in the performance of pedestrian detectors. Broadly, these works focused on the aspect that how much pre-training on a large-scale dataset helps in the performance of a pedestrian detector and used cross-dataset evaluation for this task. However, in this work we adopt cross-dataset evaluation to test the generalization abilities of several state-of-theart pedestrian detectors. Under this principal, we illustrate that current state-of-the-art detectors lack in generalization, whereas a general object detector generalizes performs and through a progressive training pipeline significantly surpasses current pedestrian detectors. Moreover, we include more recent pedestrian detection benchmarks in our evaluation setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Settings</head><p>Datasets. We thoroughly evaluate and compare against state-of-the-art on three large-scale pedestrian detection benchmarks. These benchmarks are recorded from the context of autonomous driving, we refer to them as autonomous driving datasets. The Caltech <ref type="bibr" target="#b10">[12]</ref> dataset has around 13K persons extracted from 10 hours of video recorded by a vehicle in Los Angeles, USA. All experiments on Caltech <ref type="bibr" target="#b10">[12]</ref> are conducted using new annotations provided by <ref type="bibr" target="#b46">[46]</ref>. CityPersons <ref type="bibr" target="#b47">[47]</ref> is a more diverse dataset compared to Caltech as it is recorded in 27 different cities of Germany and neighboring countries. CityPersons dataset has roughly 31k annotated bounding boxes and its training, validation and testing sets contain 2,975, 500, 1,575 images, respectively. Finally, EuroCity Persons (ECP) <ref type="bibr" target="#b2">[4]</ref> is a new pedestrian detection dataset, which surpasses Caltech and CityPersons in terms of diversity and difficulty. It is recorded in 31 different cities across 12 countries in Europe. It has images for both day and night-time (thus referred to as ECP day-time and ECP night-time). Total annotated bounding-boxes are over 200K. As mentioned in ECP <ref type="bibr" target="#b2">[4]</ref>, for the sake of comparison with other approaches, all experiment and comparisons are done on the day-time ECP. We report results on the validation set of ECP <ref type="bibr" target="#b2">[4]</ref> unless stated otherwise. Evaluation server is available for the test set and frequency submissions are limited. Finally, in our experiments we also include two non-traffic related recent datasets namely, CrowdHuman <ref type="bibr" target="#b33">[34]</ref> and Wider Pedestrian 1 [1]. Collectively we refer to Caltech,   <ref type="table" target="#tab_0">Table 1</ref>. Evaluation protocol. Following the widely accepted protocol of Caltech <ref type="bibr" target="#b10">[12]</ref>, CityPersons <ref type="bibr" target="#b47">[47]</ref> and ECP <ref type="bibr" target="#b2">[4]</ref>, the detection performance is evaluated using log average miss rate over False Positive Per Image (FPPI) over range [10 âˆ’2 , 10 0 ] denoted by (M R âˆ’2 ). We evaluate and compare all methods using similar evaluation settings. We report numbers for different occlusion levels namely, Reasonable, Small, Heavy, Heavy* 2 and All unless stated otherwise, definition of each split is given in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Cross-dataset evaluation. In cross-dataset evaluation, when written Aâ†’B, we train a model only on the training set of A and test it on the testing/validation set of B, this training and testing routine is consistent across all experiments.</p><p>Baseline. Since most of the top ranked methods on Caltech, CityPersons and ECP are direct extension of Faster/Mask R-CNN <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b16">18]</ref> family, we also select recent Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref> (an extension of R-CNN family) as our baseline. In text, we interchangeably use baseline and Cascade RCNN, they both refer to exactly the same method Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>. Cascade R-CNN contains multiple detection heads in a sequence, which progressively try to filter out harder and harder false positives. We tested several backbones with our baseline detector as shown in <ref type="table" target="#tab_2">Table 3</ref>. HRNet <ref type="bibr" target="#b38">[39]</ref> and ResNeXt <ref type="bibr" target="#b42">[43]</ref> are two top performing backbones. We choose HRNet <ref type="bibr" target="#b38">[39]</ref> as our backbone network. Better performance of HRNet <ref type="bibr" target="#b38">[39]</ref> challenge. Data can be accessed at : https://competitions. codalab.org/competitions/20132 2 In the case of CityPersons, under Heavy* occlusion the visibility level is [0.0,0.65], for the sake of comparison with previous approaches, we used the same visibility level.</p><p>can be attributed to the fact that it retains feature maps at higher resolution, reducing the likelihood of important information being lost in repeated down-sampling and up-sampling, which is especially beneficial for pedestrian detection where the most difficult samples are very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Benchmarking</head><p>First, we present the benchmarking of our Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref> on three autonomous driving datasets. <ref type="table">Table 4</ref> presents benchmarking on Caltech <ref type="bibr" target="#b10">[12]</ref> dataset, CityPersons <ref type="bibr" target="#b47">[47]</ref> and on ECP <ref type="bibr" target="#b2">[4]</ref> respectively. In the case of Caltech and CityPersons, our baseline (Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>) without "bells and whistles" performs comparable to the existing state-of-the-art, which are tailored for pedestrian detection tasks. Its performance has a greater improvement compared to other methods with increasing dataset size. Its relative performance is the worst on the smallest dataset (Caltech) and the best on the largest dataset (EuroCityPersons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Generalization Capabilities</head><p>As discussed in the previous sections, traditionally, pedestrian detectors have been evaluated using the classical within-dataset evaluation, i.e., they are trained and tested on the same dataset. We find that existing methods may over-fit on a single dataset, and so we suggest to put more emphasis on cross-dataset evaluation for a new way of benchmarking. Cross-dataset evaluation is an effective way of testing how well a given method adapts to unseen domain. Therefore, in this section we evaluated the robustness of each method using cross-dataset evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Cross Dataset Evaluation of Existing State-ofthe-Art</head><p>In this section we demonstrate that existing state-of-the art pedestrian detectors generalize worse than general object detector. We show that this is mainly due to the biases in the design of methods for the target set, even when other factors, such as backbone, are kept consistent.</p><p>To see how well state-of-the-art pedestrian detectors generalize to different datasets, we performed cross dataset evaluation of five state-of-the-art pedestrian detectors and our baseline (Cascade RCNN) on CityPersons <ref type="bibr" target="#b47">[47]</ref> and Caltech <ref type="bibr" target="#b10">[12]</ref> datasets. We evaluated recently proposed BGC-Net <ref type="bibr" target="#b20">[22]</ref>, CSP <ref type="bibr" target="#b26">[27]</ref>, PRNet <ref type="bibr" target="#b35">[36]</ref>, ALFNet <ref type="bibr" target="#b24">[26]</ref> and FR-CNN <ref type="bibr" target="#b47">[47]</ref>(tailored for pedestrian detection). Furthermore, we added along with baseline, Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>, without "bells and whistles", but with a more recent backbone ResNext-101 <ref type="bibr" target="#b42">[43]</ref> with FPN <ref type="bibr" target="#b21">[23]</ref>. Moreover, we implemented a vanilla FRCNN <ref type="bibr" target="#b47">[47]</ref> with VGG-16 <ref type="bibr" target="#b34">[35]</ref> as a backbone and with no pedestrian specific adaptations proposed in <ref type="bibr" target="#b47">[47]</ref> (namely quantized anchors, input scaling, finer feature stride, adam solver, ignore region handling, etc). We present results for Caltech and CityPersons in <ref type="table" target="#tab_3">Table  5</ref>, respectively. We also report results when training is done on target dataset for readability purpose. For our results presented in <ref type="table" target="#tab_3">Table 5</ref> (Fourth column, CityPersonsâ†’Caltech), we trained each detector on CityPersons and tested on Caltech. Similarly, in the last column of the <ref type="table" target="#tab_3">Table 5</ref>, all detectors were trained on the Caltech and evaluated on CityPersons benchmark. As expected, all methods suffer a performance drop when trained on CityPersons and tested on Caltech. Particularly, BCGNet <ref type="bibr" target="#b20">[22]</ref>, CSP <ref type="bibr" target="#b26">[27]</ref>, ALFNet <ref type="bibr" target="#b24">[26]</ref> and FRCNN <ref type="bibr" target="#b47">[47]</ref> degraded by more than 100 % (in comparison with fifth column, Caltechâ†’Caltech). Whereas in the case of Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>, performance remained comparable to the model trained and tested on target set. Since, CityPersons is a relatively diverse and dense dataset in comparison with Caltech, this performance deterioration cannot be linked to dataset scale and crowd density. This illustrates better generalization ability of general object detectors over state-of-the-art pedestrian detectors. Moreover, it is note-worthy that BGCNet <ref type="bibr" target="#b20">[22]</ref> like the Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>, also uses HRNet <ref type="bibr" target="#b38">[39]</ref> as a backbone, making it directly comparably to the Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>.</p><p>Importantly, pedestrian specific FRCNN <ref type="bibr" target="#b47">[47]</ref> performs worse in cross dataset (fourth column only), compared with its direct variant vanilla FRCNN. The only difference between between the two being pedestrian specific adaptations for the target set, highlighting the bias in the design of tailored pedestrian detectors.</p><p>Similarly, standard Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>, though performs worse than FRCNN <ref type="bibr" target="#b47">[47]</ref> when trained and tested on the target dataset, it performs better than FRCNN <ref type="bibr" target="#b47">[47]</ref> when it is evaluated on Caltech without any training on Caltech.</p><p>It is noteworthy that Faster R-CNN <ref type="bibr" target="#b32">[33]</ref> outperforms state-of-the-art pedestrian detectors (except for BGCNet <ref type="bibr" target="#b20">[22]</ref>) as well in cross dataset evaluation, presented in <ref type="table" target="#tab_3">Table 5</ref>. We again attribute this to the bias present in the design of current state-of-the-art pedestrian detectors, which are tailored for specific datasets and therefore limit their generalization ability. Moreover, a significant performance drop for all methods (though ranking is preserved except for vanilla FRCNN), including Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>, can be seen in <ref type="table" target="#tab_3">Table 5</ref>, last column. However, this performance drop is attributed to lack of diversity and density of the Caltech dataset. Caltech dataset has less annotations than CityPersons and number of people per frame is less than 1 as reported in <ref type="table" target="#tab_0">Table 1</ref>. However, still it is important to highlight, even when trained on a limited dataset, usually general object detectors are better at generalization than state-of-the-art pedestrian detectors. Interestingly, Faster R-CNN's <ref type="bibr" target="#b32">[33]</ref> error is nearly twice as high as that of BGCNet <ref type="bibr" target="#b20">[22]</ref> in within-dataset evaluation, whereas it outperforms in BGCNet <ref type="bibr" target="#b20">[22]</ref> in cross-dataset evaluation.</p><p>As discussed previously, most pedestrian detection methods are extensions of general object detectors (FR-CNN, SSD, etc.). However, they adapt to the task of pedestrian detection. These adaptations are often too specific to the dataset or detector/backbones (e.g. anchor settings <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b24">26]</ref>, finer stride <ref type="bibr" target="#b47">[47]</ref>, additional annotations <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b31">32]</ref>, constraining aspect-ratios and fixed body-line annotation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">22]</ref> etc.). These adaptations usually limit the generalization as shown in <ref type="table" target="#tab_3">Table 5</ref>, also discussed, task specific configurations of anchors limits generalization as discussed in <ref type="bibr" target="#b23">[25]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Autonomous Driving Datasets for Generalization</head><p>We illustrate that even when training dataset is as large as ECP and testing set is as small as Caltech, general object detection methods are better at learning a generic representation for pedestrians compared to existing pedestrian detectors (such as CSP <ref type="bibr" target="#b26">[27]</ref>). Moreover, large scale dense autonomous driving datasets provide better generalization abilities.</p><p>As illustrated in Section 5.1, cross dataset evaluation provides insights on the generalization abilities of different methods. However, another vital factor in generalization is dataset itself. A diverse dataset should capture the true essence of real world without bias <ref type="bibr" target="#b2">[4]</ref>, detector trained on such dataset should be able to learn a generic representation that should handle subtle shifts in domain robustly. Deviating from previous studies <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">48]</ref> on the role of dataset in generalization, we perform a line by line comparison between state-of-the-art pedestrian detector and a general object detector when trained and tested on different datasets. In order to provide level playing field, we replace ResNet-50 in CSP <ref type="bibr" target="#b26">[27]</ref> with a more powerful and recent backbone HRNet <ref type="bibr" target="#b38">[39]</ref>. HRNet's effectiveness can be observed in <ref type="table" target="#tab_4">Table 6</ref>, second row, where an improvement of 1.6% (11.0 vs. 9.4 ) in M R âˆ’2 an be seen.</p><p>We begin by using the largest dataset in terms of diversity (more countries and cities included) and pedestrian density from the context of autonomous driving, ECP, for training and evaluate both Cascade RCNN and CSP on CityPersons ( <ref type="table" target="#tab_4">Table 6</ref> third and fourth row respectively). It can be seen that Cascade RCNN adapts better on CityPersons, compared to CSP (Reasonable setting), provided the same backbone. ECP is large scale dataset and intuitively one would expect CSP to outperform Cascade RCNN, since in within-dataset evaluation, CSP is better by significant margin (nearly 2% M R âˆ’2 points).</p><p>Furthermore, we swapped our training and testing set, and evaluated on ECP <ref type="bibr" target="#b2">[4]</ref>. Cascade RCNN adapted better than CSP, even when the training source is not diverse. Besides Reasonable setting, the difference between the performances are at least 5 % M R âˆ’2 points (across small scale pedestrians, its 10.5 % M R âˆ’2 ). Lastly, we fixed the smallest dataset Caltech as our testing set and used both ECP and CityPersons as our training source. Last four rows of Table 6, illustrates the robustness of a Cascade RCNN across all settings. Importantly, when trained on a dense and diverse dataset ECP Cascade RCNN has more ability to learn a better representation than CSP across all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Diverse General Person Detection Datasets for Generalization</head><p>In this section, we investigated how well diverse and dense datasets improve generalization. We conclude, In the case of small autonomous driving datasets, such as Caltech <ref type="bibr" target="#b10">[12]</ref>, training on diverse and dense sources, which may be further away from the target domain can also benefit. However, in the case of large scale target sets, training on sources close to target domains are more effective. General object detection methods, such as cascade RCNN tend to benefit more from diverse and dense datasets than a pedestrian detector such as CSP. <ref type="table" target="#tab_5">Table 7</ref>, presents results of pre-training of Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref> and CSP <ref type="bibr" target="#b26">[27]</ref> (HRNet <ref type="bibr" target="#b38">[39]</ref> as a backbone) on CrowdHuman <ref type="bibr" target="#b33">[34]</ref> and Wider Pedestrian [1] datasets. These two datasets are different from autonomous driving datasets, as CrowdHuman <ref type="bibr" target="#b33">[34]</ref> contain web-crawled images of persons in different scenarios and Wider Pedestrian [1] contains images from surveillance cameras and street view images (not just street view images, making them both diverse and dense). Since the autonomous driving datasets (Caltech <ref type="bibr" target="#b10">[12]</ref>, CityPersons <ref type="bibr" target="#b47">[47]</ref> and ECP <ref type="bibr" target="#b2">[4]</ref>) lack in density and diversity [1], CrowdHuman <ref type="bibr" target="#b33">[34]</ref> and Wider Pedestrian [1] are a suitable choice for pre-training, since average person per frame and crowd density is much larger in CrowdHuman <ref type="bibr" target="#b33">[34]</ref> and Wider Pedestrian [1] combines street view images and surveillance cameras based images, adding a different form of diversity. In <ref type="table" target="#tab_5">Table 7</ref>, it can be observed that training on CrowdHuman <ref type="bibr" target="#b33">[34]</ref> and Wider Pedestrian [1] can reduce nearly half of the error on Caltech dataset for Cascade RCNN, outperforming previous stateof-the-art, that are trained only on Caltech. Performance improvement is also consistent in CSP <ref type="bibr" target="#b26">[27]</ref>, though the margin of improvement is less than that of a general object de-  tector. On CityPersons <ref type="bibr" target="#b47">[47]</ref>, training on CrowdHuman <ref type="bibr" target="#b33">[34]</ref> does not improve the performance for CSP <ref type="bibr" target="#b26">[27]</ref> or Cascade RCNN, since, CityPersons <ref type="bibr" target="#b47">[47]</ref> is a relatively challenging dataset compared to Caltech <ref type="bibr" target="#b10">[12]</ref> (in terms of density and diversity), and requires training on sources closer to the domain. This trend can also be seen in the case of ECP <ref type="bibr" target="#b2">[4]</ref>, where for Cascade RCNN and CSP <ref type="bibr" target="#b26">[27]</ref>, the performance is lower when trained on CrowdHuman <ref type="bibr" target="#b33">[34]</ref>, compared to training on CityPersons <ref type="bibr" target="#b47">[47]</ref> as in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Progressive Training Pipeline</head><p>We conducted experiments to show that performance can be significantly improved through progressive fine-tuning, where starting from a general diverse dataset (farther from target domain), and subsequently fine-tuning on dataset closer to the target domain.</p><p>With the study presented in above sections, we conduct additional experiments on the importance of progressive training. To be consistent, we do not fine-tune on the target set and for training we only use the training subset of each respective dataset. A â†’ B refers to pre-training on dataset A and fine-tuning on B. Whereas, A + B refers to simply merging the two datasets together and training the model on merged larger set. For our results presented in <ref type="table" target="#tab_7">Table 8</ref>, we used CityPersons <ref type="bibr" target="#b47">[47]</ref> and Caltech <ref type="bibr" target="#b10">[12]</ref> as our testing sets. It can be seen, in <ref type="table" target="#tab_7">Table 8</ref>, first two rows, that progressive training pipeline significantly improves the performance of Cascade RCNN. Particularly, pre-training on  <ref type="table" target="#tab_7">Table 8</ref>, third and fourth row), leads to improvement in performance, but it is still slightly worse than the progressive training that we have used, where we fine-tune on the autonomous driving benchmark. The results illustrate that this strategy enables us to significantly improve the performances of state-ofthe-art without fine-tuning on the actual target set. This illustrates the generalization capability of the proposed approaches can be enhanced by progressive training strategy, without exposure to the target set, Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref> is on par with top performer on CityPersons and best performing on Caltech <ref type="bibr" target="#b10">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Application Oriented Models</head><p>For real-world applications, we show that even with a light-weight backbone architecture, by pre-training on diverse and dense datasets, MobileNet <ref type="bibr" target="#b18">[20]</ref> performs competitively to state-of-the-art method on CityPersons <ref type="bibr" target="#b47">[47]</ref>, i.e. CSP <ref type="bibr" target="#b26">[27]</ref>.</p><p>In many pedestrian detection applications, such as autonomous driving and cameras mounted on drones to localize persons, the size and computational cost of models is constrained. We experiment with a small and light-weight model MobileNet <ref type="bibr" target="#b18">[20]</ref> v2, which is designed for mobile and embedded vision applications, to investigate if with progressive training pipeline, with a light backbone the performance improvements hold true. <ref type="table" target="#tab_8">Table 9</ref> shows results on CityPersons <ref type="bibr" target="#b47">[47]</ref> using Mo-bileNet <ref type="bibr" target="#b18">[20]</ref> as a backbone network architecture into Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref>. First row of <ref type="table" target="#tab_8">Table 9</ref>, is for reference when, MobileNet <ref type="bibr" target="#b18">[20]</ref> trained and evaluated on CityPersons <ref type="bibr" target="#b47">[47]</ref>. Intuitively, MobileNet <ref type="bibr" target="#b18">[20]</ref> performs worse than the HRNet <ref type="bibr" target="#b38">[39]</ref>. However, in the case of MobileNet <ref type="bibr" target="#b18">[20]</ref> as well, we  <ref type="bibr" target="#b2">[4]</ref> a performance gain of 0.6% M R âˆ’2 an be seen. This is consistent with our previous finding reported in <ref type="table" target="#tab_5">Table 7</ref>, Wider Pedestrian [1] is a better source of pre-training than CrowdHuman <ref type="bibr" target="#b33">[34]</ref>, since it has images of autonomous driving scenes as well making it more closer to the target domain than CrowdHuman <ref type="bibr" target="#b33">[34]</ref>. Interestingly, in the case of CrowdHuman <ref type="bibr" target="#b33">[34]</ref> and Wider Pedestrian [1], even with a light-weight architecture, Cascade R-CNN <ref type="bibr" target="#b6">[8]</ref> is comparable state-of-the-art pedestrian detector CSP <ref type="bibr" target="#b26">[27]</ref> (ResNet-50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Encouraged by the recent progress of pedestrian detectors on existing benchmarks from the context of autonomous driving, we assessed real-world performance of several state-of-the-art pedestrian detectors using standard cross-dataset evaluation. We came to the conclusion that current state-of-the-art pedestrian detectors, despite achieving impressive performances on several benchmarks, poorly handle even small domain shifts. This is due to the fact that the current state-of-the-art pedestrian detectors are tailored for target datasets and their overall design contains biasness towards target datasets, thus reducing their generalization. In contrast, general object detectors are more robust and generalize better to new datasets. We thoroughly investigated and verified that general object detectors due to generic design can benefit more from large-scale datasets diverse in scenes and dense in pedestrians. Besides, a progressive training pipeline proposed in this paper works well for autonomous-driving oriented pedestrian detection. In summary, our findings in this paper can serve as a stepping stone in developing new generalizable pedestrian detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics. â€¡ Fixed aspect-ratio for bounding boxes.</figDesc><table><row><cell></cell><cell cols="2">Caltech  â€¡ CityPersons  â€¡</cell><cell>ECP</cell><cell cols="2">CrowdHuman Wider Pedestrian</cell></row><row><cell>images</cell><cell>42,782</cell><cell>2,975</cell><cell>21,795</cell><cell>15,000</cell><cell>90,000</cell></row><row><cell>persons</cell><cell>13,674</cell><cell>19,238</cell><cell>201,323</cell><cell>339,565</cell><cell>287,131</cell></row><row><cell>persons/image</cell><cell>0.32</cell><cell>6.47</cell><cell>9.2</cell><cell>22.64</cell><cell>3.2</cell></row><row><cell>unique persons</cell><cell>1,273</cell><cell>19,238</cell><cell>201,323</cell><cell>339,565</cell><cell>287,131</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental settings.</figDesc><table><row><cell>Setting</cell><cell>Height</cell><cell>Visibility</cell></row><row><cell cols="3">Reasonable [50, inf] [0.65, inf]</cell></row><row><cell>Small</cell><cell cols="2">[50, 75] [0.65, inf]</cell></row><row><cell>Heavy</cell><cell cols="2">[50, inf] [0.2, 0.65]</cell></row><row><cell>Heavy*</cell><cell cols="2">[50, inf] [0.0, 0.65]</cell></row><row><cell>All</cell><cell>[20, inf]</cell><cell>[0.2, inf]</cell></row></table><note>CityPersons and ECP as autonomous driving datasets and CrowdHuman, Wider Pedestrian as web-crawled datasets. Details of the datasets are presented in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluating generalization abilities of different backbones using our baseline detector.</figDesc><table><row><cell></cell><cell cols="2">Backbone</cell><cell></cell><cell>Training</cell><cell>Testing</cell><cell>Reasonable</cell></row><row><cell></cell><cell>HRNet</cell><cell cols="4">WiderPedestrian + CrowdHuman CityPersons</cell><cell>12.8</cell></row><row><cell></cell><cell>ResNeXt</cell><cell cols="4">WiderPedestrian + CrowdHuman CityPersons</cell><cell>12.9</cell></row><row><cell></cell><cell cols="5">Resnet-101 WiderPedestrian + CrowdHuman CityPersons</cell><cell>15.8</cell></row><row><cell></cell><cell cols="5">ResNet-50 WiderPedestrian + CrowdHuman CityPersons</cell><cell>16.0</cell></row><row><cell cols="5">Table 4: Benchmarking on autonomous driving datasets.</cell></row><row><cell>Method</cell><cell>Testing</cell><cell cols="3">Reasonable Small Heavy</cell></row><row><cell>ALFNet [26]</cell><cell>Caltech</cell><cell>6.1</cell><cell>7.9</cell><cell>51.0</cell></row><row><cell>Rep Loss [40]</cell><cell>Caltech</cell><cell>5.0</cell><cell>5.2</cell><cell>47.9</cell></row><row><cell>CSP [27]</cell><cell>Caltech</cell><cell>5.0</cell><cell>6.8</cell><cell>46.6</cell></row><row><cell>Cascade R-CNN [8]</cell><cell>Caltech</cell><cell>6.2</cell><cell>7.4</cell><cell>55.3</cell></row><row><cell>RepLoss [40]</cell><cell>CityPersons</cell><cell>13.2</cell><cell>-</cell><cell>-</cell></row><row><cell>ALFNet [26]</cell><cell>CityPersons</cell><cell>12.0</cell><cell>19.0</cell><cell>48.1</cell></row><row><cell>CSP [27]</cell><cell>CityPersons</cell><cell>11.0</cell><cell>16.0</cell><cell>39.4</cell></row><row><cell cols="2">Cascade R-CNN [8] CityPersons</cell><cell>11.2</cell><cell>14.0</cell><cell>37.1</cell></row><row><cell>Faster R-CNN [4]</cell><cell>ECP</cell><cell>7.3</cell><cell>16.6</cell><cell>52.0</cell></row><row><cell>YOLOv3 [4]</cell><cell>ECP</cell><cell>8.5</cell><cell>17.8</cell><cell>37.0</cell></row><row><cell>SSD [4]</cell><cell>ECP</cell><cell>10.5</cell><cell>20.5</cell><cell>42.0</cell></row><row><cell>Cascade R-CNN [8]</cell><cell>ECP</cell><cell>6.6</cell><cell>13.6</cell><cell>33.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Cross dataset evaluation on Caltech and CityPersons. Aâ†’B refers to training on A and testing on B.</figDesc><table><row><cell>Method</cell><cell>Bakcbone</cell><cell cols="4">CityPersonsâ†’CityPersons CityPersonsâ†’Caltech Caltechâ†’Caltech Caltechâ†’CityPersons</cell></row><row><cell>FRCNN [47]</cell><cell>VGG-16</cell><cell>15.4</cell><cell>21.1</cell><cell>8.7</cell><cell>46.9</cell></row><row><cell>Vanilla FRCNN [47]</cell><cell>VGG-16</cell><cell>24.1</cell><cell>17.6</cell><cell>12.2</cell><cell>52.4</cell></row><row><cell>ALFNET [26]</cell><cell>ResNet-50</cell><cell>12.0</cell><cell>17.8</cell><cell>6.1</cell><cell>47.3</cell></row><row><cell>CSP [27]</cell><cell>ResNet-50</cell><cell>11.0</cell><cell>12.1</cell><cell>5.0</cell><cell>43.7</cell></row><row><cell>PRNet [36]</cell><cell>ResNet-50</cell><cell>10.8</cell><cell>10.7</cell><cell>-</cell><cell>-</cell></row><row><cell>BGCNet [22]</cell><cell>HRNet</cell><cell>8.8</cell><cell>10.2</cell><cell>4.1</cell><cell>41.4</cell></row><row><cell>Faster R-CNN [33]</cell><cell>ResNext-101</cell><cell>16.4</cell><cell>11.8</cell><cell>9.7</cell><cell>40.8</cell></row><row><cell>Cascade R-CNN [8]</cell><cell>HRNet</cell><cell>11.2</cell><cell>8.8</cell><cell>6.2</cell><cell>36.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Cross dataset evaluation of (Casc. R-CNN and CSP) on Autonomous driving benchmarks. Both detectors are trained with HRNet as a backbone.</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>Testing</cell><cell cols="3">Reasonable Small Heavy</cell></row><row><cell cols="3">Casc. RCNN CityPersons CityPersons</cell><cell>11.2</cell><cell>14.0</cell><cell>37.0</cell></row><row><cell>CSP</cell><cell cols="2">CityPersons CityPersons</cell><cell>9.4</cell><cell>11.4</cell><cell>36.7</cell></row><row><cell>Casc. RCNN</cell><cell>ECP</cell><cell>CityPersons</cell><cell>10.9</cell><cell>11.4</cell><cell>40.9</cell></row><row><cell>CSP</cell><cell>ECP</cell><cell>CityPersons</cell><cell>11.5</cell><cell>16.6</cell><cell>38.2</cell></row><row><cell>Casc. RCNN</cell><cell>ECP</cell><cell>ECP</cell><cell>6.9</cell><cell>12.6</cell><cell>33.1</cell></row><row><cell>CSP</cell><cell>ECP</cell><cell>ECP</cell><cell>19.4</cell><cell>50.4</cell><cell>57.3</cell></row><row><cell cols="2">Casc. RCNN CityPersons</cell><cell>ECP</cell><cell>17.4</cell><cell>40.5</cell><cell>49.3</cell></row><row><cell>CSP</cell><cell>CityPersons</cell><cell>ECP</cell><cell>19.6</cell><cell>51.0</cell><cell>56.4</cell></row><row><cell cols="2">Casc. RCNN CityPersons</cell><cell>Caltech</cell><cell>8.8</cell><cell>9.8</cell><cell>28.8</cell></row><row><cell>CSP</cell><cell>CityPersons</cell><cell>Caltech</cell><cell>10.1</cell><cell>13.3</cell><cell>34.4</cell></row><row><cell>Casc. RCNN</cell><cell>ECP</cell><cell>Caltech</cell><cell>8.1</cell><cell>9.6</cell><cell>29.9</cell></row><row><cell>CSP</cell><cell>ECP</cell><cell>Caltech</cell><cell>10.4</cell><cell>13.7</cell><cell>31.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Benchmarking with CrowdHuman and Wider Pedestrian dataset.</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell>Testing</cell><cell cols="3">Reasonable Small Heavy</cell></row><row><cell>Casc. RCNN</cell><cell>CrowdHuman</cell><cell>Caltech</cell><cell>3.4</cell><cell>11.2</cell><cell>32.3</cell></row><row><cell>CSP</cell><cell>CrowdHuman</cell><cell>Caltech</cell><cell>4.8</cell><cell>5.7</cell><cell>31.9</cell></row><row><cell>Casc. RCNN</cell><cell>CrowdHuman</cell><cell>CityPersons</cell><cell>15.1</cell><cell>21.4</cell><cell>49.8</cell></row><row><cell>CSP</cell><cell>CrowdHuman</cell><cell>CityPersons</cell><cell>11.8</cell><cell>18.3</cell><cell>44.8</cell></row><row><cell>Casc. RCNN</cell><cell>CrowdHuman</cell><cell>ECP</cell><cell>17.9</cell><cell>36.5</cell><cell>56.9</cell></row><row><cell>CSP</cell><cell>CrowdHuman</cell><cell>ECP</cell><cell>19.8</cell><cell>48.9</cell><cell>60.1</cell></row><row><cell cols="2">Casc. RCNN Wider Pedestrian</cell><cell>Caltech</cell><cell>3.2</cell><cell>10.8</cell><cell>31.7</cell></row><row><cell>CSP</cell><cell>Wider Pedestrian</cell><cell>Caltech</cell><cell>3.4</cell><cell>3.0</cell><cell>29.5</cell></row><row><cell cols="3">Casc. RCNN Wider Pedestrian CityPersons</cell><cell>16.0</cell><cell>21.6</cell><cell>57.4</cell></row><row><cell>CSP</cell><cell cols="2">Wider Pedestrian CityPersons</cell><cell>17.0</cell><cell>22.4</cell><cell>58.2</cell></row><row><cell cols="2">Casc. RCNN Wider Pedestrian</cell><cell>ECP</cell><cell>16.1</cell><cell>32.8</cell><cell>58.0</cell></row><row><cell>CSP</cell><cell>Wider Pedestrian</cell><cell>ECP</cell><cell>24.1</cell><cell>62.6</cell><cell>76.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>. Interestingly,</cell></row><row><cell>in the case of Wider Pedestrian [1] (bottom half Table 7),</cell></row><row><cell>besides CityPersons [47], the relative improvements in the</cell></row><row><cell>case of Wider Pedestrian [1] is relatively larger for general</cell></row><row><cell>object detector. The potential reason is that compared with</cell></row><row><cell>CrowdHuman [34], Wider Pedestrian [1] is large scale and</cell></row><row><cell>closer to the target domain. Since it contains images essen-</cell></row><row><cell>tially for two views (street view and surveillance), where</cell></row><row><cell>as CrowdHuman [34] contains web-crawled person images</cell></row><row><cell>appearing in different poses and scenes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Investigating the effect on performance when CrowdHuman, Wider Pedestrian and ECP are merged and Cascade R-CNN<ref type="bibr" target="#b6">[8]</ref> is trained only on the merged dataset.</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell cols="4">Testing Reasonable Small Heavy</cell></row><row><cell>Casc. RCNN</cell><cell>CrowdHuman â†’ ECP</cell><cell>CP</cell><cell>10.3</cell><cell>12.6</cell><cell>40.7</cell></row><row><cell>Casc. RCNN</cell><cell>Wider Pedestrian â†’ ECP</cell><cell>CP</cell><cell>9.7</cell><cell>11.8</cell><cell>37.7</cell></row><row><cell cols="2">Casc. RCNN Wider Pedestrian + CrowdHuman + ECP</cell><cell>CP</cell><cell>10.9</cell><cell>12.7</cell><cell>43.1</cell></row><row><cell cols="2">Casc. RCNN Wider Pedestrian + CrowdHuman â†’ ECP</cell><cell>CP</cell><cell>9.7</cell><cell>12.1</cell><cell>39.8</cell></row><row><cell>Casc. RCNN</cell><cell>CrowdHuman â†’ ECP</cell><cell>Caltech</cell><cell>2.9</cell><cell>11.4</cell><cell>30.8</cell></row><row><cell>Casc. RCNN</cell><cell>Wider Pedestrian â†’ ECP</cell><cell>Caltech</cell><cell>2.5</cell><cell>9.9</cell><cell>31.0</cell></row><row><cell cols="2">Wider Pedestrian [1] and fine-tuning on ECP [4] brings Cas-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">cade RCNN on par with other state-of-the-art approaches on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CityPersons [47], without training on the CityPersons [47].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Similarly, in the case of Caltech [12] as well, progressive</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">training, outperformed previously established state-of-the-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">art on Caltech [12] dataset. Noteworthy is the fact that</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">performance on Caltech [12] is within a close vicinity of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a human-baseline (0.88).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Finally, concatenating all datasets (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Investigating the performance of embedded vision model, when pre-trained on diverse and dense datasets. see pre-training on CrowdHuman<ref type="bibr" target="#b33">[34]</ref> and fine-tuning on ECP<ref type="bibr" target="#b2">[4]</ref> improves the performance of the MobileNet<ref type="bibr" target="#b18">[20]</ref>. Furthermore, we replaced CrowdHuman<ref type="bibr" target="#b33">[34]</ref> with Wider Pedestrian [1] as the initial source of pre-training. Improvement over the Cascade R-CNN<ref type="bibr" target="#b6">[8]</ref> (1st row) can be observed (3rd row), where with Wider Pedestrian [1] and finetuning on ECP</figDesc><table><row><cell>Training</cell><cell cols="4">Testing Reasonable Small Heavy</cell></row><row><cell>CP</cell><cell>CP</cell><cell>12.0</cell><cell>15.3</cell><cell>47.8</cell></row><row><cell>ECP</cell><cell>CP</cell><cell>19.1</cell><cell>19.3</cell><cell>51.3</cell></row><row><cell>CrowdHumanâ†’ECP</cell><cell>CP</cell><cell>11.9</cell><cell>15.7</cell><cell>48.9</cell></row><row><cell>Wider Pedestrianâ†’ECP</cell><cell>CP</cell><cell>11.4</cell><cell>14.6</cell><cell>43.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Wider Pedestrian has images from surveillance and autonomous driving scenarios. In our experiments, we used the data provided in 2019</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Real-time pedestrian detection with deep network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Ogale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Ferguson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stable multi-target tracking in real-time surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Benfold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3457" to="3464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eurocity persons: A novel benchmark for person detection in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dariu M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1844" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Illuminating pedestrians via simultaneous detection &amp; segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4950" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rogerio S Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3361" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gpu-based pedestrian detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Campmany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Moure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>VÃ¡zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>LÃ³pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2377" to="2381" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international Conference on computer vision &amp; Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Serge Belongie, and Pietro Perona. Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth and appearance for mobile scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning scene-specific pedestrian detectors without real data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hironori</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3819" to="3827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pedestrian detection for autonomous driving within cooperative communication system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><surname>Hbaieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihene</forename><surname>Rezgui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamia</forename><surname>Chaari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Wireless Communications and Networking Conference (WCNC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr DollÃ¡r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4073" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bridging the gap between detection and tracking: A unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3999" to="4009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Box guided convolution for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangzhi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Center and scale prediction: A box-free approach for pedestrian and face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irtiza</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02948</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning efficient single-stage pedestrian detectors by asymptotic localization fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="618" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-level semantic feature detection: A new perspective for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3127" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An experimental study on pedestrian classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Munder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavrila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1863" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakrapee</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="546" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mask-guided attention network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Progressive refinement network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Sheng Chu Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fishnet: A versatile backbone for image, region, and pixel level prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="760" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07919</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Repulsion loss: detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7774" to="7783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-cue onboard pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="794" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cluster boosted tree classifier for multi-view, multi-pose object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1259" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Widerperson: A diverse dataset for dense pedestrian detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansheng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bi-box regression for pedestrian detection and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
