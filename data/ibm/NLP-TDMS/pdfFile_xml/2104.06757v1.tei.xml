<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharif</forename><forename type="middle">Amit</forename><surname>Kamran</surname></persName>
							<email>skamran@nevada.unr.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariha</forename><surname>Khondker</surname></persName>
							<email>khondkerfarihah@nevada.unr.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hossain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Tavakkoli</surname></persName>
							<email>tavakkol@unr.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuckerbrod</forename><surname>Houston</surname></persName>
							<email>szuckerbrod@houstoneye.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eye</forename><surname>Associates</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><forename type="middle">M</forename><surname>Sanders</surname></persName>
							<email>ksanders@med.unr.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
							<email>sabubaker@med.unr.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Fluorescein Angiography (FA), an exogenous dye is injected in the bloodstream to image the vascular structure of the retina. The injected dye can cause adverse reactions such as nausea, vomiting, anaphylactic shock, and even death. In contrast, color fundus imaging is a non-invasive technique used for photographing the retina but does not have sufficient fidelity for capturing its vascular structure. The only non-invasive method for capturing retinal vasculature is optical coherence tomography-angiography (OCTA). However, OCTA equipment is quite expensive, and stable imaging is limited to small areas on the retina. In this paper, we propose a novel conditional generative adversarial network (GAN) capable of simultaneously synthesizing FA images from fundus photographs while predicting retinal degeneration. The proposed system has the benefit of addressing the problem of imaging retinal vasculature in a non-invasive manner as well as predicting the existence of retinal abnormalities. We use a semi-supervised approach to train our GAN using multiple weighted losses on different modalities of data. Our experiments validate that the proposed architecture exceeds recent state-of-the-art generative networks for fundus-to-angiography synthesis. Moreover, our vision transformer-based discriminators generalize quite well on out-of-distribution data sets for retinal disease prediction. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fluorescein Angiography (FA) combined with retinal funduscopy is a standard tool for diagnosing various retinal vascular abnormalities and degenerative conditions <ref type="bibr" target="#b39">[40]</ref>. In Fluorescein Angiography, a fluorescent fluid is injected into <ref type="bibr" target="#b0">1</ref> The code repository can be found in the following Link.</p><p>the blood stream and becomes visible 8 to 10 minutes after insertion. The dosage depends on the viscosity of the dye, age of the patient, and the retina vascular structure <ref type="bibr" target="#b38">[39]</ref>. In general, the procedure is safe, but there have been reported cases of allergic reactions, ranging from nausea and vomiting to anaphylactic shock and death <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. The risk associated with FA signifies the need for non-invasive mechanisms to assess retinal vascular structure.</p><p>Various automated systems incorporating image processing and machine learning techniques have been proposed for diagnosing retinal abnormalities and degenerative diseases from fundus photographs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b37">38]</ref>. Currently, there are no inexpensive imaging alternative for imaging the vascular structure of the retina. The only alternative for scanning retinal vasculature is via Optical coherence Tomography-Angiography (OCTA) <ref type="bibr" target="#b21">[22]</ref>. Although OCTA is capable of mapping the retinal subspace in 3D, the equipment is expensive and its highest resolutions are only achievable on very small areas. Consequently, it is imperative to develop non-invasive and inexpensive techniques for measuring retinal vascular structure to circumvent the inaccessibility and potential risks associated with the existing invasive procedures. The first contribution of this paper is a novel deep learning architecture for producing retinal vascular images (i.e., FA images) from non-invasive fundus photographs.</p><p>A major obstacle in using machine learning architectures for ophthalmic applications is the lack of publicly available data -in our work Fluorescein Angiography data <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Only a handful of machine learning systems have been proposed to predict disease using FA images. However, these systems are trained and tested on privately held data-sets. Pan et al. <ref type="bibr" target="#b42">[43]</ref> utilized three such pre-trained architectures on 4067 privately curated images for Abnormal and Normal FA prediction. Similarly, Li et al. <ref type="bibr" target="#b35">[36]</ref> incorporated the encoder of U-Net <ref type="bibr" target="#b45">[46]</ref> for classifying different levels of degradation for 3935 privately held Ultra-widefield FA images. To our knowledge, the only two available public data-sets have only 59 and 70 FA images in total <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. The second contribution of this paper is a semi-supervised approach in conjunction with a vision transformer architecture to address the challenges in training models that learn pathologies from such small amounts of data.</p><p>In particular, we introduce VTGAN, a semi-supervised conditional GAN that can simultaneously produce the retinal vascular structure (i.e. FA images) from fundus photographs, while differentiating between healthy and abnormal retina. The proposed architecture incorporates a novel discriminator based on vision transformers for both patchlevel adversarial image detection and image-level disease classification. For qualitative assessment, we compare our proposed architecture with recent state-of-the-art fundus-toangiogram synthesis architectures <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9]</ref>. For quantitative evaluation, we use Frechet inception Distance (FID) <ref type="bibr" target="#b20">[21]</ref> and Kernel Inception Distance (KID) <ref type="bibr" target="#b0">[1]</ref> for quantifying image features and measuring structural similarity. To validate the discriminator's robustness, we use standard metrics for out-of-distribution classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative adversarial networks (GAN) have become a staple for image-to-image synthesis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47]</ref>, inpainting <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b10">11]</ref> and style transfer <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>. By a combination of multi-scale architectures, these networks can detect and learn fine and coarse features from images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>. This approach, in turn, can be employed for both conditional <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12]</ref> and unconditional variants of GAN architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b56">57]</ref>. Generative networks have also seen success in computer tomography (CT), magnetic resonance imaging (MRI), and X-ray for image segmentation, augmentation and cross-domain information fusion tasks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Recently, GAN models are utilized for synthesizing FA images from fundus photographs <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b25">26]</ref>. These models employ two generators for coarse and fine image generation. The generators were trained on randomly cropped patches of different scales, while multi-scale discriminators are used to discern local and global features from images. Usually, a discriminator incorporates a linear output for classifying among real or fake (generated) images. In addition, these models utilize the PatchGAN architecture <ref type="bibr" target="#b34">[35]</ref> as a discriminator for adversarial and real image classification, by producing the output as a feature map of size N × N . Incorporating these ideas, the state-of-the-art models employ an architecture that can retain global information like the shape of optic-disc, contrast, and local features like venular structures, arteries, and microaneurysm <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b25">26]</ref>. However, the problem with this approach is that the discriminator works on patch-level information. As a result, the cohesive relationship between global and local features is lost while generating FA from crops of fundus images.</p><p>The advent of Vision Transformers (ViT) have improved the performance of state-of-the-art architectures in image classification tasks. The premise of ViT is that image pixels contain inherent spatial coherencies. Therefore, utilizing an image as a sequence of non-overlapping patches and incorporating them into a transformer can better extract intrinsic and spatial features <ref type="bibr" target="#b51">[52]</ref>. Unlike image generative pretrained models such as iGPT <ref type="bibr" target="#b3">[4]</ref>, which apply transformers to obtain image-level features, ViT works on patch-level features and incorporates their positions by utilizing embedding layers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Vision Transformers retain the cohesiveness of coarse and fine features by utilizing the position of each patch. By mapping this information for N × N patches into an N × N feature-map, we propose a novel architecture based on vision transformers, called VTGAN and illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Moreover, we extend VTGAN's capability by adding a multi layer perceptron (MLP) head for classifying Abnormal and Normal FA images in a semi-supervised manner. This architecture is a significant contribution that addresses training models for FA image classification task based on limited amounts of publicly available data. Qualitative and quantitative evaluations demonstrate that VTGAN surpasses other state-of-the-architectures both in terms of synthesis and classification tasks. Additionally, we demonstrate that the produce FA images by VTGAN are such high quality that expert ophthalmologists cannot reliably identify fake image from real ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methodology</head><p>This paper proposes a vision-transformer-based generative adversarial network (GAN) consisting of residual, spatial feature aggregation, upsampling and downsampling blocks for generators and transformer encoder blocks for discriminators. For training, perceptual, feature matching, and reconstruction loss is incorporated for generating realistic angiograms from retinal fundus images. First, we discuss coarse and fine generators in section 3.1. We elaborate each distinct blocks in sections 3.2, 3.3, and 3.4. We then elaborate on our newly proposed vision-transformer-based discriminators and their interconnection with the generators to establish the end-to-end generative network in section 3.5. Finally, in section 3.6 we discuss the associated loss functions and their weight multipliers for each distinct architecture that forms the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-scale Generators</head><p>In order to capture large and fine-scale features to produces realistic vascular images we combine multi-scale coarse and fine generators. We adopt two generators (G f ine and G coarse ), as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> in our architecture. G f ine synthesizes local features such as arteries and venules. Conversely, G coarse translates global features such as large blood vessels, optic disc, and overall contrast and illumination. The generators consist of multiple downsampling, upsampling, spatial-aggregation, residual blocks, and a feature summation block between the G f ine and G coarse . The input and output dimension of G f ine are 512 × 512, while G coarse has half the dimension of G f ine (256 × 256). Furthermore, the G coarse outputs a feature vector of size 256×256×64, which is element-wise added with one of the intermediate layers of the G f ine generator. Both G coarse and G f ine take fundus images and synthesize FA images. The detailed arrangement is visualized in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Downsampling and Upsampling Blocks</head><p>We use, as generators, auto-encoders comprising of multiple downsampling and upsampling blocks for feature extraction. A single downsampling block contains a convolution layer, a batch-norm layer <ref type="bibr" target="#b23">[24]</ref> and a Leaky-ReLU activation function successively and is given in <ref type="figure">Fig. 2</ref>(i). In contrast, an upsampling block consists of a transposed convolution layer, batch-norm <ref type="bibr" target="#b23">[24]</ref>, and Leaky-ReLU activation layer consecutively and is illustrated in <ref type="figure">Fig. 2</ref>(ii). We use the downsampling block twice in G coarse , followed by nine successive residual identity blocks. Finally, the upsampling blocks are used 2× again to make the spatial output the same as the input. For G f ine , we utilize the downsampling once, and after three consecutive residual blocks, a single upsampling block is employed to get the same spatial output as the input. We use kernel size, k = 3, and stride, s = 2 for both of our convolutions and transposed convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generator's Residual Blocks</head><p>For spatial and depth feature extraction, residual blocks have become the fundamental building blocks for imageto-image translation, inpainting, and style transfer tasks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44]</ref>. The original residual block consisted of two consecutive convolution layers and a skip connection between the input with the output <ref type="bibr" target="#b18">[19]</ref>. Regular convolution operations are computationally inefficient and fail to retain accurate spatial and depth information, unlike separable convolution <ref type="bibr" target="#b9">[10]</ref>. Separable convolution incorporates a depth-wise convolution layer followed by a point-wise convolution. Consequently, it obtains and retains depth and spatial features better. We use residual blocks for our generators, as illustrated in <ref type="figure">Fig. 2</ref>(iv). The residual block consists of Reflection padding, Separable Convolution, Batch-Norm, and Leaky-ReLU layers followed by two branches of the same repetitive layers. The main difference is, one branch consists of a dilation rate of d = 1 and the other <ref type="figure">Figure 2</ref>. Individual blocks of our proposed GAN architecture consisting of (i) Downsampling, (ii) Upsampling, (c) SFA block, (d) Residual Block for Generator and (e) Transformer Encoder where K stands for kernel size, S is for stride and D is for Dilation rate with a dilation rate, d = 2 in the separable convolution layers. We use, kernel size, k = 3 and stride, s = 1. The skip connection and output of the two branches are all elementwise summed in the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Spatial Feature Aggregation Block</head><p>In this section, we elaborate on the spatial feature aggregation (SFA) block, as illustrated in <ref type="figure">Fig. 2</ref>(iii). The block consists of two residual units with Convolution, Batch-Norm, Leaky-ReLU layer successively. The convolution layer have kernel, k = 3, and stride, s = 1. Additionally, there are two skip connections, one going from the input and element-wise summed to the first residual unit's output. The next one is coming from the input and being added with the last residual unit's output. We use spatial feature aggregation block for combining spatial features from the bottom layers with the top layers of the network, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. G coarse consists of two SFA blocks that connect each of the two downsampling blocks with the two upsampling blocks successively. In contrast, G f ine has only one SFA block between the single downsampling and upsampling block. The reason behind incorporating the SFA block is to extract and retain spatial information that is otherwise lost due to consecutive downsampling and upsampling. As a result, we can combine these features with the learned features of the later layers of the network to get an accurate approximation, as seen in similar GAN architectures <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Vision-transformer as Discriminators</head><p>GAN discriminators require adapting to local and global information changes for distinguishing between real and fake images. To alleviate this underlying issue, we need a dense network with a large number of computable parameters. Alternatively, convolution with a large receptive field can be employed for obtaining spatial features but can easily lead to overfitting while training the model. To address this issue, we propose a new Vision Transformer-based Markovian discriminator, in the same vein as PatchGAN <ref type="bibr" target="#b34">[35]</ref>. We use eight vision transformer encoders consisting of a multiheaded attention layer and multi-layer perceptron (MLP) block. The Layer Normalization layer precedes each block, and residual skip connection is added to the output from the input. The block is visualized in <ref type="figure">Fig. 2(v)</ref>. To handle 2D images of 512×512, we reshape the images into a sequence of flattened 2D patches with resolution 64 × 64. By doing so, we end up having 64 patches in total. The Transformer uses a constant latent vector size of D = 64 through all of its layers, so we flatten the patches and map to D dimensions with a trainable linear projection. The output of this projection is called the patch embeddings as mentioned in <ref type="bibr" target="#b12">[13]</ref>. Position embeddings are added to the patch embeddings to preserve positional information. We use regular learnable 1D position embeddings in a similar manner to Dosovitskiy et al. <ref type="bibr" target="#b12">[13]</ref>. For multi-headed attention, we use n = 4 heads. For MLP blocks, we use two dense layers with features size h = [128, 64], each succeeded by a GeLU activation <ref type="bibr" target="#b19">[20]</ref> and a dropout of 0.1. Contrarily, our vision transformer has two outputs, an MLP head, and a Convolutional layer, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The MLP head has two output hidden units for FA image classification (Abnormal and Normal). In contrast, the convolution layer outputs a feature map of 64 × 64 for classifying each patch in the original image as Real or Fake.</p><p>We use two vision transformer-based discriminators that incorporate identical structures but operate at two different scales. We term the two discriminators as, V T f ine and V T coarse as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. We resize the coarse angiograms and fundus with size 256×256 by a factor of 2 using the Lanczos filter <ref type="bibr" target="#b13">[14]</ref>. Both discriminators have identical transformer encoder and output layers (in <ref type="figure">Fig. 2(v)</ref>).</p><p>V T coarse tries to convince the coarse generator, G coarse to retain more global features such as the macula, optic disc, contrast, and illumination. On the other hand, the V T f ine steers the fine generator, G f ine to synthesize more accurate local features such as small vasculature, venules, exudates, arteries. Consequently, we fuse learnable elements from both generators while training them separately with their paired vision transformer-based discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Weighted Cost Functions and Adversarial Loss</head><p>With the given discriminators and generators, our whole network's objective function can be formulated as Eq. 1. It's a multi-objective problem of maximizing the discriminators' loss while minimizing the generator's loss.</p><formula xml:id="formula_0">min G f ,Gc max D f ,Dc L adv (G f , G c , D f , D c )<label>(1)</label></formula><p>For adversarial training, we use Hinge-Loss <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b36">37]</ref> as illustrated in Eq. 2 and Eq. 3. All the fundus images and their corresponding angiogram pairs are normalized to [−1, 1], and we employ tanh as the output activation for the adversarial feature-map. In Eq. 4 we add them and use λ adv as weight multiplier with the L adv (G). For classification we use sof tmax activation after the MLP head output. In Eq. 5 the categorical cross-entropy loss is given where y is the real class andŷ is the predicted class. </p><formula xml:id="formula_1">L cce (D) = E y,ŷ − k i=1 y i logŷ i<label>(4)</label></formula><p>Here, In Eq. 2 and Eq. 3 the discriminators are first trained on the real fundus, x and real angiogram, y, and then trained on the real fundus, x and synthesized angiogram, G(x). We begin by batch-wise training the discriminators D f and D c for a couple of iterations on randomly sampled data. After that, we train the G c while keeping the weights of the discriminators frozen. Similarly, we train the G f on a batch of random images while keeping the weights of all the discriminators frozen. The generators also incorporate the reconstruction and perceptual loss <ref type="bibr" target="#b24">[25]</ref> as shown in Eq. 6 and Eq. 7. We ensure the synthesized images retain more realistic color, contrast, and vascular structure by employing these losses. We also incorporate feature matching loss <ref type="bibr" target="#b53">[54]</ref> with all our discriminators and is given in Eq. 8.</p><formula xml:id="formula_3">L rec (G) = E x,y G(x) − y 2 (6) L perc (G) = E x,y k i=1 1 M F i vgg (y) − F i vgg (G(x)) (7) L f m (G, D n ) = E x,y k i=1 1 N D i n (x, y) − D i n (x, G(x))<label>(8)</label></formula><p>For Eq. 6, L rec is the reconstruction loss for the real angiogram, y, given a generated angiogram, G(x). We use this loss for both G f and G c so that the model can generate high-quality angiograms of different scales. In Eq. 7, L perc calculates the difference between real and fake angiogram features extracted by pushing both of them successively in VGG19 architecture <ref type="bibr" target="#b49">[50]</ref>. Lastly, Eq. 8 is calculated by taking the features from intermediate layers of the discriminator by first inserting the real and fake angiograms consecutively. Here, M and N stand for the number of feature layers extracted from VGG19 and the discriminators consecutively.</p><p>By incorporating Eq. 4, 5, 6, 7 and 8 we can formulate our final objective function as given in Eq. 9.</p><formula xml:id="formula_4">min G f ,Gc max D f ,Dc (L adv (G f , G c , D f , D c ))+ λ rec L rec (G f , G c ) + λ f m L f m (G f , G c , D f , D c ) + λ perc L perc (G f , G c ) + λ cce L cce (D f , D c )<label>(9)</label></formula><p>Here, λ adv , λ rec , λ perc , λ f m and λ ccse signify loss weights, and dictate which networks to prioritize while training. For our architecture, more weight is given to the L adv (G), L rec , L perc , L cce , and thus we select bigger λ values for those.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The following section describes our model's experiments and evaluates our architecture based on qualitative and quantitative metrics. First, we discuss the pre-processing pipeline for our dataset in Sec. 4.1. Next, we detail our hyper-parameter selection and tuning in Sec. 4.2. Also, we compare our architecture with existing state-of-the-art generative models based on some qualitative evaluation metrics in Sec. 4.3. Lastly, in Sec. 4.4 and Sec. 4.5, we analyze the quantification done by experts and the performance on the out-of-distribution dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We use the fundus and angiography dataset provided in <ref type="bibr" target="#b16">[17]</ref>. The dataset contains thirty images and twenty-nine pairs of the healthy and unhealthy fundus and angiogram images. Each image pair is collected from individual patients. After close observation, we found seventeen images based on one-to-one alignment between the fundus and angiogram pairs. These image pairs are either accurately aligned or almost aligned. The original image size is 576 × 720, but we take 50 overlapping crops of 512 × 512 sized samples from each. For the image synthesis task, we end up having 850 images in total for training. The fundus images are in RGB format, and angiograms are in a Gray-scale format. As the dataset is categorized into Abnormal and Normal classes, we use this annotation for our supervised classification training. Out of seventeen images, ten are Abnormal, and seven are Normal patients. Due to cropping, we end up having 500 for abnormal and 350 for normal images. We use data augmentation to increase the sample size the same as the abnormal one. For testing, we take fourteen image pairs and crop four overlapping quadrants of the image to generate a set of fifty-six images and use their associated labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hyper-parameter tuning</head><p>For adversarial training, we used hinge loss <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b36">37]</ref>. We picked λ adv = 10 (Eq. 4) and λ rec = 10, λ perc = 10, λ f m = 1, λ cce = 10 (Eq. 9). We use Adam optimizer <ref type="bibr" target="#b31">[32]</ref>, with learning rate α = 0.0002, β 1 = 0.5 and β 2 = 0.999. We train with mini-batches with batch size, b = 2 for 200 epochs for 50 hours with NVIDIA P100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Evaluation</head><p>For evaluating our architecture's performance, we used 14 test samples and cropped four quadrants of the image with a resolution of 512 × 512. We conducted two experiments for estimating the accurate visual representation i) without transformation and ii) with spatial and radial transformations. By doing so, we measured the network's ability to adjust to structural changes due to imaging error and pa-  <ref type="bibr" target="#b25">[26]</ref> 0.00087 0.05045 0.00235 0.05162 0.05390 0.04575 A2AGAN <ref type="bibr" target="#b25">[26]</ref> 0.00392 0.05390 0.00505 0.05301 0.05657 0.05341 StarGAN-v2 <ref type="bibr" target="#b8">[9]</ref> 0.00118 0.05274 0.00235 0.05331 0.05539 0.05271 U-GAT-IT <ref type="bibr" target="#b30">[31]</ref> 0.00131 0.05610 0.00278 0.05533 0.05815 0.05719 Fundus2Angio <ref type="bibr" target="#b26">[27]</ref> 0.00184 0.05328 0.00272 0.05267 0.05278 0.04985 Pix2PixHD <ref type="bibr" target="#b53">[54]</ref> 0.00258 0.05613 0.00254 0.05788 0.06029 0.05838 1 PL = Perceptual Loss 2 FID: Lower is better; KID: Lower is better tient eye movements.</p><p>For our first experiment, we compare our network with other state-of-the-art image-to-image translation architectures. A side-by-side comparison of the results is visualized in <ref type="figure" target="#fig_2">Fig. 3</ref>. Column, A &amp; C in <ref type="figure" target="#fig_2">Fig. 3</ref>, display the global feature differences while column B &amp; D are zoomed-in local vasculature changes. By its looks, our model produces vivid and convincing results compared to other architectures. At-tention2Angio (A2AGAN) and U-GAT-IT also yield impressive results. However, if observed for the closed-up versions in columns B &amp; D, we can witness that the optic disc contains fewer blood vessels. StarGAN-v2 and Pix2PixHD also fail to generate rich arteries, exudates, and vasculature.</p><p>In the second set of experiments, we applied three transformations and two distortions on the fundus images: 1) Blur to represent severe cataracts, 2) Sharpening to represent dilated pupils, 3) Noise to sensor impedance during fundoscopy, 4) Pinch to visualize squinting as a vascular change, and, 5) Whirl, for distortions caused by increased intraocular pressure (IOP). We can see in <ref type="figure" target="#fig_3">Fig. 4</ref> a sideby-side comparison of different architecture predictions on these transformed images. As observed from the results, VTGAN synthesizes images very similar to the groundtruth (GT) and performs robustly to preserve the vascular structures.</p><p>For Blurred fundus images, VTGAN is less impacted by the transformation compared to other state-of-the-art architecture, as seen in (row 4 to 9 of column 1) of <ref type="figure" target="#fig_3">Fig. 4</ref>. The vascular structures are better retained as opposed to U-GAT-IT and Fundus2Angio. For Sharpened fundus, the angiogram generated by StarGAN-v2 and A2AGAN (row 4 to 6 of column 2) exhibits small artifacts around the blood vessels not present in our case. For Noisy images, our result is unaffected by this pixel-level alteration. However, all other state-of-the-art models (row 4 to 9 of column 3) fail to synthesize tiny and slim venular branches.</p><p>For Pinch and Whirl, our experimental result shows the versatility and reproducibility of VTGAN for retaining vascular structure and is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref> (row 3 of column 4 and 5). Compared to ours, only A2AGAN and U-GAT-IT reserves the flattening condition and manifestation of vascular changes but loses the overall smoothness in the process (row 5 to 7 of columns 4 and 5). In <ref type="figure" target="#fig_3">Fig. 4</ref> VTGAN encodes the vascular feature information and is much less affected by both kinds of warping. The other architectures failed to generate microvessels due to IOP or vitreous variations, as can be seen in <ref type="figure" target="#fig_3">Fig. 4</ref>. Consequently, For all kinds of transformation and contortion, VTGAN surpasses existing state-of-the-art image-to-image translation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative Evaluations</head><p>For quantitative evaluation, we performed two experiments. In the first experiment, we use the Fréchet inception distance (FID) <ref type="bibr" target="#b20">[21]</ref>, and Kernel Inception distance (KID) <ref type="bibr" target="#b0">[1]</ref> which has been previously employed for measuring similar image-to-image translation GANs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>. We computed the FID and KID scores for different architectures on the generated FA image and original angiogram, including the five spatial and radial transformations. The results are given in <ref type="table">Table.</ref> 4.2. A lower FID and KID score means the synthesized images are more close to the real angiogram.</p><p>From <ref type="table">Table.</ref> 4.2, out of our three networks, the best KID In the second experiment, we assess the generated angiogram's quality by asking two expert ophthalmologists to identify them. We use a balanced set of 50 images, 25 real and 25 fake. We then shuffle the data before the expert evaluation. For this experiment, experts did not know the exact number of fake and real images. By not disclosing this, we tried to estimate the following criterion: 1) Correct fake and real angiograms found by the experts, where lower equates to better, 2) Incorrect fake and real angiograms missed by the experts, where higher equates to better, and 3) The average precision of the experts for identifying fake angiograms, where lower equates to better. <ref type="table" target="#tab_1">Table 2</ref> illustrates the detailed result.</p><p>As it can be seen from <ref type="table" target="#tab_1">Table 2</ref>, experts assigned 94% of the fake angiograms as real, for images synthesized by our models. The result also shows that experts had trouble identifying fake images, while they easily identified real angiograms with 80% certainty. On average, the experts misclassified 57% of all images produced by VTGAN. The average precision diagnosis of the experts are 45.9%. Consequently, our model successfully fools the experts to identify fake angios as real.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Disease Classification</head><p>For our subsequent experimentation, we test our visual transformer's accuracy on in-distribution and out-ofdistribution classification tasks. We use data provided by Hajeb et al. <ref type="bibr" target="#b16">[17]</ref>. We crop four quadrants from the 14 Fundus and Angiogram pairs because of the shortage of data. Out of 56 images, 20 are for Abnormal, and 36 are for Normal classes. We name this test set as in-distribution and measure the performance using three standard metrics: Accuracy, Sensitivity, and Specificity. The result is provided in <ref type="table">Table.</ref> 3, and our vision transformer-based model scores <ref type="table">Table 3</ref>. Test Accuracy on in-distribution Abnormal/Normal Angiograms Accuracy Sensitivity Specificity 85.7 83.3 90.0 We use spatial and radial transformation on test images for out-of-distribution evaluation. The model's performance is illustrated in <ref type="table">Table.</ref> 4. As can be seen from the table, for Blur, Noise, Pinch, and Whirl transformations, the Accuracy and Sensitivity decreased to 78.6%, 72.2%, consecutively. Compared to the in-distribution data, the decrease is 7.1%, 11.1% for Accuracy and Sensitivity. For Sharp transformation, the accuracy and sensitivity are worse than the other distortions. They are 76.7%, 69.4% successively. An interesting revelation is the specificity has no effect due to these distortions. The specificity is firm 90.0% for all five distortions. A higher specificity signifies our model accurately predicts Abnormal classes better compared to Normal ones. This is significant, as we want to identify patients with degenerative conditions compared to predicting false positives for healthy patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new fundus-to-angiogram translation architecture called VTGAN. The architecture generates realistic angiograms from fundus images without any expert intervention. Additionally, we provided results for its robustness and adaptability conditioned upon radial and spatial transformations, which imitate biological markers seen in real fundus images. We believe the proposed network can be incorporated in the wild to generate precise FA images of patients developing disease overtime. It can be a complimentary disease progression monitoring system for predicting the development of diseases in vivo. We hope to extend this work to other areas of ophthalmological image modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>VTGAN consists of Coarse and Fine generators G f , Gc and Vision Transformers as discriminators V T f , V Tc. The generators take fundus as input and synthesize angiograms. Whereas the vision transformers take patches of the concatenated fundus and angiograms as input and outputs, i) feature-map for adversarial example detection, and ii) image-level classification for Fluorescein angiograms. The generators consist of Downsampling, Upsampling, Spatial Feature Aggregation, and Residual blocks. On the other hand, the Vision transformers consist of Transformer Encoder, Multi-layer perceptron (MLP) head, and adversarial feature output block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>L</head><label></label><figDesc>adv (D) = −E x,y min(0, −1 + D(x, y)) − E x min(0, −1 − D(x, G(x))) (2) L adv (G) = −E x,y (D(G(x), y)) (3) L adv (G, D) = L adv (D) + λ adv (L adv (G))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparativie results of different Angiograms generated using different state-of-the-art architectures. Column (A) and (C) represents two samples of real fundus, real angio and predicted angio images. Whereas column (B) and (D) represents the red rectangle box to show zoomed in local venular structures corresponding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Angiogram generated from transformed and distorted Fundus images with natural changes, imaging errors and biological markers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Test results for different architectures Fréchet Inception Distance (FID) (1.4↑) 24.1 (6.8↑) 24.5 (7.2↑) 28.4 (11.1↑) 22.3 (5.0↑) A2AGAN w/ PL 1 [26] 24.6 21.6 (3.0↓) 30.0 (5.4↑) 25.6 (1.0↑) 40.0 (15.4↑) 24.9 (0.3↑) A2AGAN [26] 20.7 20.8 (0.1↑) 23.5 (2.8↑) 24.9 (4.2↑) 27.8 (7.1↑) 19.5 (1.</figDesc><table><row><cell>Architecture</cell><cell>Orig.</cell><cell>Noise</cell><cell>Blur</cell><cell>Sharp</cell><cell>Whirl</cell><cell>Pinch</cell></row><row><cell>VTGAN</cell><cell>17.3</cell><cell cols="5">18.7 2↓)</cell></row><row><cell>StarGAN-v2 [9]</cell><cell>27.7</cell><cell cols="5">35.1 (7.4↑) 32.6 (4.9↑) 27.4 (0.3↓) 32.7 (5.0↑) 26.7 (1.0↓)</cell></row><row><cell>U-GAT-IT [31]</cell><cell>24.5</cell><cell cols="5">26.0 (1.5↑) 30.4 (5.9↑) 26.8 (2.3↑) 33.0 (9.5↑) 29.1 (4.6↑)</cell></row><row><cell>Fundus2Angio [27]</cell><cell>30.3</cell><cell cols="5">41.5 (11.2↑) 32.3 (2.0↑) 34.3 (4.0↑) 38.2 (7.9↑) 33.1 (2.8↑)</cell></row><row><cell>Pix2PixHD [54]</cell><cell>42.8</cell><cell cols="5">53.0 (10.2↑) 43.7 (1.1↑) 47.5 (4.7↑) 45.9 (3.1↑) 39.2 (3.6↓)</cell></row><row><cell></cell><cell></cell><cell cols="3">Kernel Inception Distance (KID)</cell><cell></cell><cell></cell></row><row><cell>Architecture</cell><cell>Orig.</cell><cell>Noise</cell><cell>Blur</cell><cell>Sharp</cell><cell>Whirl</cell><cell>Pinch</cell></row><row><cell>VTGAN</cell><cell>0.00053</cell><cell>0.04953</cell><cell>0.00205</cell><cell>0.04927</cell><cell>0.04815</cell><cell>0.04542</cell></row><row><cell>A2AGAN w/ PL 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of Qualitative with Undisclosed Portion of Fake/Real Experiment</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Results</cell><cell></cell><cell>Average</cell><cell></cell></row><row><cell>Architecture</cell><cell></cell><cell cols="5">Correct Incorrect Missed 1 Found 1 Precision 2</cell></row><row><cell>VTGAN</cell><cell>Fake Real</cell><cell>6% 80%</cell><cell>94% 20%</cell><cell>57%</cell><cell>43%</cell><cell>45.9%</cell></row><row><cell cols="5">1 Missed higher is better; Found lower is better</cell><cell></cell><cell></cell></row><row><cell cols="3">2 Precision Lower is better</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">is achieved for VTGAN. And it reaches the lowest scores</cell></row><row><cell cols="7">out of all other architecture, for both with and without dis-</cell></row><row><cell cols="7">tortions. For FID, our model achieves the lowest score</cell></row><row><cell cols="7">for three out of five types of distortions. Attention2Angio</cell></row><row><cell cols="7">scores lower FID for Blur and Whirl transformation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Test Accuracy on out-of-distribution Abnormal/Normal Angiograms</figDesc><table><row><cell>Distortion</cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Specificity</cell></row><row><cell>Blur</cell><cell cols="2">78.6 (7.1 ↓) 72.2 (11.1 ↓)</cell><cell>90.0 (-)</cell></row><row><cell>Sharp</cell><cell cols="2">76.7 (8.0 ↓) 69.4 (13.9 ↓)</cell><cell>90.0 (-)</cell></row><row><cell>Noise</cell><cell cols="2">78.6 (7.1 ↓) 72.2 (11.1 ↓)</cell><cell>90.0 (-)</cell></row><row><cell>Pinch</cell><cell cols="2">78.6 (7.1 ↓) 72.2 (11.1 ↓)</cell><cell>90.0 (-)</cell></row><row><cell>Whirl</cell><cell cols="2">78.6 (7.1 ↓) 72.2 (11.1 ↓)</cell><cell>90.0 (-)</cell></row><row><cell cols="4">85.7%, 83.3%, 90% for accuracy, sensitivity, and specificity</cell></row><row><cell>successively.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikołaj</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Demystifying mmd gans. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognising panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1218</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sketchygan: Towards diverse and realistic sketch to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9416" to="9425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention-gan for object transfiguration in wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="164" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse, smart contours to represent and edit images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3511" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lanczos filtering in one and two dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied meteorology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Disc-aware ensemble network for glaucoma screening from fundus image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon Wing Kee</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2493" to="2501" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine learning identification of diabetic retinopathy from fundus images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Gurudath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><surname>Celenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Bryan</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing in Medicine and Biology Symposium (SPMB)</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Diabetic retinopathy grading by digital curvelet transform. Computational and mathematical methods in medicine</title>
		<editor>Shirin Hajeb Mohammad Alipour, Hossein Rabbani, and Mohammad Reza Akhlaghi</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A new combined method based on curvelet transform and morphological operators for automatic detection of foveal avascular zone. Signal, Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Shirin Hajeb Mohammadalipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akhlaghi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="205" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Schuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Stinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Flotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><forename type="middle">A</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Puliafito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical coherence tomography. science</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="issue">5035</biblScope>
			<biblScope unit="page" from="1178" to="1181" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5077" to="5086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khondker</forename><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fariha Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><forename type="middle">Lee</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuckerbrod</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09191</idno>
		<title level="m">Atten-tion2angiogan: Synthesizing fluorescein angiography from retinal fundus images using generative adversarial networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khondker</forename><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fariha Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><forename type="middle">Lee</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuckerbrod</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05267</idno>
		<title level="m">Fundus2angio: A novel conditional gan architecture for generating fluorescein angiography images from retinal fundus photography</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optic-net: A novel convolutional neural network for diagnosis of retinal diseases from optical tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourajit</forename><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Shihab</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Sabbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tavakkoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th IEEE International Conference On Machine Learning And Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="964" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving robustness using joint attention network for detecting retinal degeneration from optical coherence tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><forename type="middle">Lee</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuckerbrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference On Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2476" to="2480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">U-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghee</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10830</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fluorescein angiography and adverse drug reactions revisited: the lions eye experience. Clinical &amp; experimental ophthalmology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Anthony Sl Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Constable</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Frequency of adverse systemic reactions after fluorescein angiography: results of a prospective study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maureen</forename><forename type="middle">G</forename><surname>Kwiterovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schachat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">B</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart L</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1139" to="1142" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automated quality assessment and image selection of ultra-widefield fluorescein angiography images through deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duriye</forename><forename type="middle">Damla</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sevgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><forename type="middle">M</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">L</forename><surname>Vasanji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justis P</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ehlers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Translational Vision Science &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="52" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adverse reactions of fluorescein angiography: a prospective study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo Pessoa Cavalcanti</forename><surname>Lira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleriston</forename><surname>Lucena De Andrade Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Virgínia Ribeiro Brito Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaine</forename><forename type="middle">Rocha</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristiano</forename><surname>De Carvalho Pessoa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arquivos brasileiros de oftalmologia</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="615" to="618" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Naresh Mandava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elias Reichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Mosby</publisher>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="800" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Retinal fundus image analysis for diagnosis of glaucoma: a comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><forename type="middle">Blessing</forename><surname>Viola Stella Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajsingh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganesh R Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Medical image synthesis with context-aware generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Trullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Medical image synthesis with deep convolutional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Trullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2720" to="2730" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-label classification of retinal lesions in diabetic retinopathy for automatic analysis of fundus fluorescein angiography based on deep learning. Graefe&apos;s Archive for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangji</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiekai</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical and Experimental Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">258</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="779" to="785" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Avinash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katy</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lily</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">158</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scribbler: Controlling deep image synthesis with sketch and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4570" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Medical image synthesis for data augmentation and anonymization using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoo-Chang</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jameson</forename><forename type="middle">K</forename><surname>Tenenholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Senjem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">P</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Andriole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on simulation and synthesis in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A novel deep learning conditional generative adversarial network for producing angiography images from retinal fundus photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khondker</forename><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><forename type="middle">Lee</forename><surname>Fariha Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuckerbrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Covidgan: data augmentation using auxiliary classifier gan for improved covid-19 detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><surname>Waheed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muskan</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadi</forename><surname>Al-Turjman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plácido Rogerio</forename><surname>Pinheiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Ieee Access</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="91916" to="91923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Texturegan: Controlling deep image synthesis with texture patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8456" to="8465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
