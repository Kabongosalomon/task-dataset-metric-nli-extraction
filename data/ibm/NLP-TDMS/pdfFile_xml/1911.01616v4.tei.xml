<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<email>luo.si@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Target-based sentiment analysis or aspect-based sentiment analysis (ABSA) refers to addressing various sentiment analysis tasks at a fine-grained level, which includes but is not limited to aspect extraction, aspect sentiment classification, and opinion extraction. There exist many solvers of the above individual subtasks or a combination of two subtasks, and they can work together to tell a complete story, i.e. the discussed aspect, the sentiment on it, and the cause of the sentiment. However, no previous ABSA research tried to provide a complete solution in one shot. In this paper, we introduce a new subtask under ABSA, named aspect sentiment triplet extraction (ASTE). Particularly, a solver of this task needs to extract triplets (What, How, Why) from the inputs, which show WHAT the targeted aspects are, HOW their sentiment polarities are and WHY they have such polarities (i.e. opinion reasons). For instance, one triplet from "Waiters are very friendly and the pasta is simply average" could be ('Waiters', positive, 'friendly'). We propose a two-stage framework to address this task. The first stage predicts what, how and why in a unified model, and then the second stage pairs up the predicted what (how) and why from the first stage to output triplets. In the experiments, our framework has set a benchmark performance in this novel triplet extraction task. Meanwhile, it outperforms a few strong baselines adapted from state-of-the-art related methods. * Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Target-based sentiment analysis (TBSA) or aspect-based sentiment analysis (ABSA 1 ) refers to addressing various sentiment analysis tasks at a fine-grained level <ref type="bibr" target="#b14">(Liu 2012;</ref><ref type="bibr">Pontiki 2014)</ref>, which includes but is not limited to aspect/target term extraction (ATE), opinion term extraction (OTE), aspect/target term sentiment classification (ATC), etc. Given an example sentence such as 'Waiters are very friendly and the pasta is simply average', the ATE is to extract 'Waiters' and 'pasta', and the ATC is to classify them to positive and negative sentiment, respectively. The OTE <ref type="figure">Figure 1</ref>: The road map to aspect-based sentiment analysis tasks. The bottom blue-filled circle anchors our task. is to extract 'friendly' and 'average'. Although these tasks seem to be intersecting and confusing at the first glance, they distinguish from each other black and white when fulfilling the three goals in ABSA. As shown in <ref type="figure">Fig 1,</ref> the top three squares represent ultimate goals for ABSA, where the aspect term represents an explicit mention of discussed target, such as 'Waiters' in the example. The opinion term represents the opinionated comment terms/phrases, like 'friendly'. The aspect category refers to certain predefined categories, such as SERVICE and FOOD in the previous example <ref type="bibr" target="#b29">(Wang et al. 2019;</ref><ref type="bibr">Pontiki 2015)</ref>.</p><p>Each circle in the middle layer denotes a direct subtask to realize the goal. The 'Sentiment' circle linked to aspect terms refers to ATC which attracts a heated research popularity <ref type="bibr" target="#b1">(Dong et al. 2014;</ref><ref type="bibr" target="#b23">Tang, Qin, and Liu 2016;</ref><ref type="bibr" target="#b17">Nguyen and Shirai 2015;</ref><ref type="bibr" target="#b26">Wang et al. 2016b;</ref><ref type="bibr" target="#b15">Ma et al. 2017;</ref><ref type="bibr" target="#b24">Tay, Luu, and Hui 2017;</ref><ref type="bibr" target="#b16">Ma, Peng, and Cambria 2018;</ref><ref type="bibr">Hazarika et al. 2018;</ref><ref type="bibr" target="#b7">Li et al. 2018a;</ref><ref type="bibr" target="#b28">Wang et al. 2018;</ref><ref type="bibr" target="#b31">Xue and Li 2018;</ref><ref type="bibr" target="#b2">He et al. 2018;</ref><ref type="bibr" target="#b0">Bailin and Lu 2018;</ref><ref type="bibr" target="#b10">Li et al. 2019b</ref>). The 'Extract' circle linked to aspect term denotes ATE, such as <ref type="bibr" target="#b22">(Qiu et al. 2011;</ref><ref type="bibr" target="#b12">Liu, Xu, and Zhao 2013;</ref><ref type="bibr" target="#b13">Liu, Xu, and Zhao 2014;</ref><ref type="bibr" target="#b11">Liu, Joty, and Meng 2015;</ref><ref type="bibr" target="#b32">Yin et al. 2016;</ref><ref type="bibr" target="#b25">Wang et al. 2016a;</ref><ref type="bibr">He et al. 2017;</ref><ref type="bibr" target="#b5">Li and Lam 2017;</ref><ref type="bibr"></ref>   <ref type="bibr" target="#b8">Li et al. 2018b;</ref><ref type="bibr" target="#b30">Xu et al. 2018)</ref>. The same also applies to other circles in the middle layer. Researchers also realized that solving these subtasks individually is insufficient so they proposed to couple two subtasks as a compound task, such as aspect term extraction and sentiment classification <ref type="bibr" target="#b9">(Li et al. 2019a;</ref><ref type="bibr" target="#b17">Mitchell et al. 2013;</ref><ref type="bibr" target="#b33">Zhang, Zhang, and Vo 2015;</ref><ref type="bibr" target="#b6">Li and Lu 2017;</ref><ref type="bibr" target="#b3">He et al. 2019)</ref>, aspect term and opinion term co-extraction <ref type="bibr" target="#b0">Dai and Song 2019)</ref>, aspect category and sentiment classification <ref type="bibr" target="#b4">(Hu et al. 2018)</ref>, as circles illustrated at the bottom. Nevertheless, the above compound tasks are still not enough to get a complete picture regarding sentiment. For instance, in the previous example, knowing a positive sentiment towards aspect term 'waiters' does not give a clue of why it is positive. Only by knowing 'friendly' will people understand the cause of sentiment. <ref type="bibr" target="#b1">Fan et al. (2019)</ref> aim to extract the opinion terms for a given target, thus the extraction can be regarded as the cause for certain sentiment on the target, through sentiment prediction is not in the scope of their paper. Note that <ref type="bibr" target="#b1">Fan et al. (2019)</ref> assume the targets are given in advance. On the other hand, the co-extraction methods fail to tackle pairing of multiple aspects and opinion expressions in a single sentence <ref type="bibr" target="#b0">Dai and Song 2019)</ref>. <ref type="bibr" target="#b9">Li et al. (2019a)</ref> couple the tasks of aspect extraction and sentiment classification with the unified tags (e.g. "B-POS" standing for the beginning of a positive aspect) but they do not extract the opinion terms for the extracted aspects, leaving blank the sentiment cause. So did the modular architectures presented by <ref type="bibr" target="#b32">Zhang and Goldwasser (2019)</ref>. In summary, no previous ABSA research try to handle such a requirement in one shot, namely knowing What target is being discussed (e.g. 'waiters'), How is the sentiment (e.g. 'positive') and Why is this sentiment (e.g. 'friendly'). Moreover, the mutual influence among the three questions lacks study either. To this end, we introduce an aspect sentiment triplet extraction task (ASTE), shown in the blue-filled circle at the bottom in <ref type="figure">Fig 1.</ref> We propose a two-stage framework to address this task. In the first stage, we aim to extract potential aspect terms, together with their sentiment, and extract potential opinion terms. The task is formulated as a labeling problem with two label sequences <ref type="bibr" target="#b17">(Mitchell et al. 2013;</ref>. Specifically, we couple a unified tagging system by following <ref type="bibr" target="#b9">(Li et al. 2019a)</ref> for aspect extraction and sentiment classification, and a BIO-like tagging system for opinion extraction, as shown in <ref type="table" target="#tab_0">Table 1</ref>. For the unified tagging system, it builds on top of two stacked Bidirectional Long Short Term Memory (BLSTM) networks. The upper one produces the aspect term and sentiment tagging results based on the unified tagging schema. The lower performs an auxiliary prediction of aspect boundaries with the aim for guiding the upper BLSTM. Gate mechanism is explicitly designed to maintain the sentiment consistency within each multi-word aspect. For the opinion term tagging system, it builds on top of a BLSTM layer and a Graph Convolutional Network (GCN) to make full use of semantic and syntactic information in a sentence. According to the task definition <ref type="bibr">(Pontiki 2014;</ref><ref type="bibr">Pontiki 2015;</ref><ref type="bibr">Pontiki 2016;</ref><ref type="bibr" target="#b8">Li et al. 2018b;</ref><ref type="bibr" target="#b9">Li et al. 2019a</ref>), for a term/phrase being regarded as an aspect, it should co-occur with some "opinion terms" that indicate a sentiment polarity on it. Therefore, aspect information is beneficial to extracting opinion terms, as already demonstrated in <ref type="bibr" target="#b0">Dai and Song 2019)</ref>. We specifically design a target-guiding module to transfer aspect information for opinion term extraction.</p><p>After the first stage, we have obtained a bunch of aspects with sentiment polarities and a bunch of opinion expressions. In the second stage, the goal is to pair up aspects with the corresponding opinion expressions. As we observed, for sentences with multiple aspects and opinions, word distance is very indicative for correctly pairing up an aspect and its opinion as shown in the bottom section of <ref type="table" target="#tab_0">Table 1</ref>. Thus, we design distance embeddings to capture the distance between aspects and opinion expressions that are predicted from the stage one. With a BLSTM encoder, we encode sentence-level contexts into aspect and opinion terms for the final classification of candidate pairs. In the experiments, our framework has set a benchmark performance in this novel sentiment triplet extraction task. Meanwhile, our framework outperforms the state-of-the-art methods (with modification to fit in our task) and the strongest sequence taggers on several benchmark datasets. We also conduct extensive ablation tests to validate the rationality of our framework design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Framework Problem Formulation</head><p>For a given input sentence X = {x 1 , . . . , x T } with length T , the ASTE task is to extract sentiment triplets (What, How, Why), consisting of the aspects/targets (i.e. 'What'), the sentiment polarity on them (i.e. 'How'), and the opinions causing such a sentiment (i.e. 'Why'). 2 Here, we formulate the task in two stages. In the stage one, the task includes two sequence labeling (SL) subtasks, the unified tag SL and the opinion tag SL. The unified tag schema is Y T S = {B-POS, I-POS, E-POS, S-POS, B-NEG, I-NEG,  </p><formula xml:id="formula_0">BG SC !"#$% &amp; !"#$% ' GCN !"#$% ()</formula><formula xml:id="formula_1">= {y T S 1 , . . . , y T S T }, where y T S i ∈ Y T S , while the opinion SL predicts a sequence Y OPT = {y OPT 1 , . . . , y OPT T }, where y OPT i ∈ Y OPT .</formula><p>In the stage two, given the sets of aspects {T 1 , T 2 , ..., T n } and opinion expressions {O 1 , O 2 , ..., O m } labeled from the same sentence in the stage one, where there are n aspects 3 and m opinion expressions, a candidate pair pool is constructed by coupling the elements from the two sets as</p><formula xml:id="formula_2">{(T 1 , O 1 ), (T 1 , O 2 ), ..., (T n , O m )}.</formula><p>The goal of this stage is to identify the legitimate ones from the candidate pool, and outputs them as the final results. Note that the 'How' is embedded in T i with the unified tags. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the overview of our two-stage framework. Recall that the stage one predicts two kinds of labels, i.e. Y T S and Y OPT . For predicting Y T S , the left half of stage one model resembles the state-of-the-art work <ref type="bibr" target="#b9">(Li et al. 2019a)</ref> for unified tag schema, and adapts one of its original component as a shared one (i.e. TG) with the right part for predicting Y OPT . Specifically, the left side contains two stacked BLSTM. The lower one BLSTM T performs an auxiliary prediction of target boundaries (i.e. BIO) for producing signals for the upper BLSTM, the boundary guidance (BG), and the target guidance (TG). The hidden states from the upper BLSTM S are first manipulated by the sentiment consistency (SC), and then used as the major signal to predict the unified tags by the BG component, which also transforms the pure target boundary tag prediction to guide unified tag prediction. Our design distinguishes from <ref type="bibr" target="#b9">Li et al. (2019a)</ref> in the specific injection of opinion information for predicting unified tag (i.e. h OP T is used by BG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Overview</head><p>The right part of the stage one is for the opinion term prediction, i.e. Y OPT . The sentence is fed into a GCN to learn the mutual influence of target and opinion terms via dependency relations 4 . Afterwards, this signal will be sent to two different modules, TG and BLST M OP T . The TG component in the middle is the concatenation of pure target boundary information and the GCN output, which leverages the target information for opinion term extraction. Unlike Li et al. (2019a) whose opinion information is weak supervision from sentiment lexicon lookup, our design specifically constructs a component sharing both target and opinion information. This component is strongly supervised by opinion term extraction, therefore, both BLST M T and GCN can benefit from its backpropagation. Meanwhile, the output from BLST M OP T will carry the sentence context on top of the GCN output. It will be sent for opinion term extraction, as well as for guiding unified tag prediction. The stage two model firstly uses the aspects and opinion expressions predicted from stage one to generate all possible pairs in each sentence. Based on the distance between target and opinion expression in each pair, a position embedding is applied for each target and opinion terms. Non-target/nonopinion term will have the same position embedding, which is zero in our experiments. After a BLSTM encoder, the hidden states from the aspect and opinion expressions will be concatenated for binary classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage One</head><p>Unified aspect boundary and sentiment labeling. As demonstrated by <ref type="bibr" target="#b9">Li et al. (2019a)</ref>, a target boundary tag is beneficial to unified tag prediction. We implement a similar structure for unified aspect and sentiment tag labeling. In order to learn the target boundary labeling, we employ a BLSTM T layer on top of sentence word embeddings. The sequence output of this BLSTM</p><formula xml:id="formula_3">T , h T = [ − −−− → LSTM T (x); ← −−− − LSTM T (x)]</formula><p>, will be fed into a softmax classifier to predict the target sequence tag without sentiment, whose tag set Y T is {B, I, E, S, O}. With this supervision learning, h T is expected to carry target boundary information. Thus we input it to the second BLSTM S layer to ac-cumulate sentiment information. Specifically, the sequence</p><formula xml:id="formula_4">output of this BLSTM S is h S = [ − −−− → LSTM S (x); ← −−− − LSTM S (x)]</formula><p>. The expected tag set for each time step in this sequence is Y S = {B-POS, I-POS, E-POS, S-POS, B-NEG, I-NEG, E-NEG, S-NEG, B-NEU, I-NEU, E-NEU, S-NEU}, which appends each tag in Y T with three sentiment polarities.</p><p>As illustrated in <ref type="table" target="#tab_0">Table 1</ref>, some aspects may contain more than one term. In our task formulation, however, we predict unified sequence tag term by term. It is possible, although contradictory, to have 'fugu' labeled as positive but 'sashimi' labeled as negative. To avoid such situation, a Sentiment Consistency (SC) module <ref type="bibr" target="#b9">(Li et al. 2019a</ref>) was designed with a gate mechanism:</p><formula xml:id="formula_5">g t = σ(W g h S t + b g ) h S t = g t h S t + (1 − g t ) h S t−1<label>(1)</label></formula><p>where W g and b g are model parameters of the SC module, and is the element-wise multiplication. σ is a sigmoid function. With this gate mechanism, current time step prediction will also inherit features from the previous time step, reducing the risk of drastic sentiment label change.</p><p>Given an aspect boundary tag, it can only be transformed into one of its three legitimate sentiment-appended unified tags. The Boundary Guidance (BG) module consolidates this observation into a constraint matrix transformation W tr ∈ R |Y T |×|Y S | . This is a probability transformation matrix in which W tr i,j indicates the probability of tag Y Ti transforming to tag Y Sj . For instance, if Y Ti is I and Y Sj is B-POS, then the W tr I,B-POS will be zero because I cannot transform to B-POS. With this transformation matrix, the aspect boundary probability distribution can now be transformed into unified probability distribution as:</p><formula xml:id="formula_6">z T t = p(y T t |x t ) = Softmax(W T h T t ) z S t = (W tr ) z T t<label>(2)</label></formula><p>W T is the model parameter and z S t is the obtained unified tag probability distribution.</p><p>Up to this moment, for the unified tag prediction, we have not directly utilized the opinion term information, which should apparently affect the detection of aspect. To this end, we integrate the opinion term information h OPT (which will be introduced in the next subsection) withh S by concatenating them together to form a reinforced representation h U for unified tag prediction with a softmax classifier:</p><formula xml:id="formula_7">z S t = p(y S t |x t ) = Softmax(W S h U t )<label>(3)</label></formula><p>where the Y S is the probability distribution and W S is the model parameter. z S t is the obtained unified tag probability distribution. Note that the integration of h OPT for unified tag prediction was not used in <ref type="bibr" target="#b9">Li et al. (2019a)</ref>.</p><p>Next, we design a fusion mechanism to merge this reinforced unified tag probability with the previous transformed unified tag probability. We calculate a fusion weight score α t ∈ R with the concentration score c t from the target boundary tagger, defined as below:</p><formula xml:id="formula_8">c t = (z T t ) z T t α t = c t<label>(4)</label></formula><p>where the concentration score c t , with a maximum value of 1, represents how confident the target boundary tagger predicts. The higher the score, the more confident is the target boundary tagger. The hyper-parameter (we empirically set as 0.5) controls the proportional weight that transformed unified tag probability contributes in the final decision. Then the final fused score between transformed and reinforced unified tag probability is given as:</p><formula xml:id="formula_9">z T S t = α t z S t + (1 − α t )z S t .<label>(5)</label></formula><p>Opinion term extraction. Previous studies <ref type="bibr" target="#b0">Dai and Song 2019)</ref> suggest that aspect extraction and opinion extraction are mutually beneficial. We also observe that aspects are usually co-occur with opinion terms and especially so on our datasets (see <ref type="table" target="#tab_3">Table 2</ref>). This drives us to utilize the target information to guide opinion term extraction. Particularly, we feed the sentence embedding to a GCN module to learn the mutual dependency relations between different words. The adjacency matrix for GCN is constructed based on the dependency parsing of the sentence, namely W GCN ∈ R |L|×|L| , where L is the length of the sentence. If the ith word has dependency relation with the jth word, W GCN i,j and W GCN j,i will both have value 1, otherwise, value 0. This operation is designed to capture the relation between aspects and opinion terms, as they are constructed as syntactic modifying pairs.</p><p>To utilize the target information for opinion term extraction, we design an auxiliary task to integrate the target boundary information with the output from GCN with a Target Guidance (TG) module. If a sentence contains an aspect-opinion pair, the opinion expression should modify its aspect following syntactic rules. Thus, given a target signal from BLST M T , it is intuitive to use it to guide opinion term extraction. We have tried various implementations of TG, and in the end a simple concatenation achieved the best performance. The concatenation will be fed into a softmax classifier for opinion tag classification in the tag space of Y T G = {B, I, E, S} ∪ {O}:</p><formula xml:id="formula_10">z T G t = p(y OPT t |x t ) = Softmax(W T G [h T t ; h O t ])</formula><p>. (6) Next, the sequence of hidden states from GCN (h O ) is sent to a BLSTM OPT for sequence learning, namely to encode the contextual information within the sentence, and the output, h OP T , will be sent to both the BG component to assist unified tag prediction (Eq.3) and a softmax classifier to predict opinion prediction:</p><formula xml:id="formula_11">z OPT t = p(y OPT t |x t ) = Softmax(W OPT h OPT t ). (7)</formula><p>Stage one training. Stage one is trained with stochastic gradient descent optimizer. The loss of each output signal is computed using crossentropy as:</p><formula xml:id="formula_12">L I = − 1 T T t=1 I(y I,g t ) • log(z I t )<label>(8)</label></formula><p>where I is the symbol of task indicator and its possible values are T , T S, T G and OPT . I(y) represents the one-hot vector with the y-th component being 1 and y I,g t is the gold standard tag for the task I at the time step t. The total training objective of stage one is to minimize the sum of individual loss from each output signal, J (θ):</p><formula xml:id="formula_13">J (θ) = L T + L T S + L T G + L OPT .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage Two</head><p>After stage one, for each sentence, we output two sets of text segments, i.e., aspect terms and opinion expressions, denoted as {T 1 , T 2 , ..., T n } and {O 1 , O 2 , ..., O m } respectively, where there are n aspects and m opinion expressions. Then, we generate a candidate pair pool as {(T 1 , O 1 ), (T 1 , O 2 ), ..., (T n , O m )} by enumerating all possible aspect-opinion pairs. Stage two is to classify whether each of these pair is valid or not.</p><p>Position embeddings. In order to utilize the position relation between an aspect and an opinion expression, we calculate the word-length distance between the center of the aspect and that of the opinion expression by counting how many words appear in the middle. The absolute distance will be treated as relative position information that encodes the position relation between them. For the ease of training, we create position embeddings by treating the distance as position index for aspects and opinions, and zero to non-aspect and non-opinion words. For instance, position indexes of a true pair ('Waiter','friendly') and a fake pair ('fugu sashimi','friendly') are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Pair encoder and classification. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we concatenate the pretrained GloVe word embeddings (Pennington, Socher, and Manning 2014) with our position embeddings to form word representation. The position embedding is randomly initialized and kept trainable in the training step. We then feed the sentence to a BLSTM layer to encode sentence contextual information into aspects and opinion expressions. Based on the sentence term index, we average the hidden states output from BLSTM for both aspect and opinion expression respectively as their features. Next, we concatenate the two features and send it to softmax layer for binary classification. For the training of classifier, we used the gold pairs annotated in the training set of our experimental datasets. During testing stage, we freeze the classifier parameters tuned against the validation sets, and directly test on the pairs generated in the candidate pool.  The example consists of two target and opinion pairs, the first pair is 'price' and 'best', the second pair is 'feature' and 'newer'. Note that 'TT-POS' is only used for indicating the pairing relation with 'SS', for model training, the used tags are 'T-POS' and 'S'. We also correct a small number of samples whose targets and opinions are overlapped. The validation set is randomly selected 20% of data from training set. <ref type="table" target="#tab_3">Table 2</ref> shows the detailed statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setting</head><p>Our framework is evaluated on a two-stage setting due to our framework design. Since the output of our stage one contains both aspects and opinion terms, we compared with other aspect and opinion co-extraction methods in the first stage. The compared methods are as follows. RINANTE (Dai and Song 2019): It is an aspect and opinion co-extraction method that mines aspect and opinion term extraction rules based on the dependency relations of words in a sentence. CMLA ): A co-extraction model that leverages attention mechanism to utilize the direct and direction dependency relations. Note that both CMLA and RINANTE use BIO tags for aspect and opinion extraction. For comparison, we train them with unified tags for aspect extraction, and BIO tags for opinion extraction.   have the mutual information exchange between aspect extraction and opinion extraction. Our-T: The third variant that eliminates the loss L T from the training. For the stage two evaluation, we cannot find a baseline to compare under identical settings. Thus we stack our stage two model directly on the best performed stage one baselines to construct different pipeline models. In addition to evaluating the triplets (eg. (Waiter-friendly-POS)) 6 , we also evaluate the performances on the pairs (eg. <ref type="figure">(Waiter-friendly)</ref>).</p><p>The implementations all use GloVe <ref type="bibr" target="#b19">(Pennington, Socher, and Manning 2014)</ref> embeddings of 300 dimension and remove domain embeddings for a fair comparison. We train up to 40 epochs with SGD optimizer with an initial learning rate 0.1 and decay rate at 0.001. Dropout rate of 0.5 is applied on the ultimate features before prediction. We report testing results of the epoch that has the best validation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head><p>Stage one. <ref type="table" target="#tab_5">Table 3</ref> presents the unified performance of the stage one for aspect extraction and sentiment classification. Our model outperforms existing strong baselines (i.e. RINANTE, CMLA, and Li-Unified) on all datasets, especially compared with the Li-unified model which is the state-of-the-art in the unified task. Interestingly, the baseline Li-unified-R, derived from Li-unified, performs very competitive, i.e. better than Li-unified on all datasets. It shows <ref type="bibr">6</ref> We switch TS-OPT pairs to target-opinion-sentiment triplets. that given the ground-truth label of opinion words, explicitly modeling opinion extraction can help upgrade the performance of aspect extraction. We also notice that Li-unified-R outperforms our full model on 14res and 14lap. Another insight is that the performance of RINANTE and CMLA reduced a lot in the unified tag setting, comparing with their original setting. We believe this is due to the lack of specific design to utilize sentiment information. Thus, for reference, we evaluate them under their original setting, i.e. only considering the target boundary and ignoring the sentiment polarity. The results shown in the last two rows increase drastically compared with those in the unified tag setting. <ref type="table" target="#tab_6">Table 4</ref> illustrates the stage one performances of opinion term extraction. In terms of F score, our core model has again achieved the best performance compared with all existing baselines. Li-unified-R is generally not as good as our model on the restaurant datasets, but still performs very competitive and event better than our model on 14lap. Our-TG variant model has outperformed all baselines in the laptop domain. RINANTE, CMLA and IOG only learned the mutual influence of aspect and opinion term. Compared with these baseline models, our model learns the multi-lateral information flow among the three tasks, i.e., aspect extraction, sentiment classification and opinion term extraction. In the case of opinion term extraction, it would be relatively straightforward to locate opinion terms if their sentiment polarities are given. Specifically, the h OPT is used for unified tag prediction and thus the sentiment classification signals are backpropagated to BLST M OP T , therefore, the opinion prediction can leverage such information.</p><p>Stage two. After obtaining all the possible candidate triplets from the stage one, each triplet is sent to a binary classifier. The classifier was trained on the ground truth aspect and opinion pairs in the training set. The model performing the best on the validation set was used as the stage two classifier for evaluating both our model and baselines.</p><p>(The performance of the classifier on the validation set is shown in the first row of <ref type="table" target="#tab_7">Table 5</ref>). The last section in <ref type="table" target="#tab_7">Table 5</ref> shows the performance for the final triplet extraction. We can observe that our model has achieved steady advantage over other baselines. In addition to evaluating the triplet, we also examine the pure pairing performance for coupling aspects and opinion terms. The results are shown in the middle section of <ref type="table" target="#tab_7">Table 5</ref>. In both sections, Li-unified-R+, a variant of Li-unified implemented by us, achieved competitive performance on the first three datasets, and even slightly better than ours on 15res in the pairing evaluation.</p><p>Ablation test. To evaluate the rationality of our model design, we also conducted ablation tests by introducing three model variants, our-BLSTM OPT , our-T and our-TG, where '-' means without the component followed behind.</p><p>As we introduced before, BLSTM OPT is expected to encode sentence contextual information which is beneficial to both unified tag prediction and opinion term extraction. From <ref type="table" target="#tab_5">Table 3</ref> and 4, for most datasets, we can find the apparent performance reduction after removing the BLSTM OPT module, which validates the effectiveness of this component. Nevertheless, the contribution of TG is more complex. In the unified tag prediction task, the removal of TG module brings down the performance in all datasets reasonably. In the opinion term extraction, the removal even boosts the performances on 14res and 14lap datasets, especially the latter.</p><p>Since TG module studies the mutual influence between aspects and opinion terms, we suspect that their mutual relation is not that strong. Instead of bringing useful information, TG module could potentially brings in noise as well.</p><p>Our assumption is validated by the classifier performance trained on gold labels in <ref type="table" target="#tab_7">Table 5</ref>. 14lap and 14res have lower performances than the other two, particularly 14lap, which indicates that their target-opinion pairs are intrinsically more heterogeneous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>Some triplet prediction cases are given in <ref type="table">Table 6</ref>. In general, our model outputs more reasonable results. For the first case, our model can predict more accurate opinions and sentiment polarity such as "was n't so fresh". However, it faces some problem in pairing prediction. The baselines cannot well capture the negated opinion. For the second case, all pipelines are hindered by the target "log on", our model can predict a partial target "log". For the third case, Li-unified-R predicts three opinions, but "feeding" is a wrong one, while RINANTE fails to predict tags and thus cannot output any triplet. One might notice that in the table, some aspects extracted in the stage one are coupled with multiple opinions, which usually brings in false positive triplets in the stage two. It might be plausible to set a heuristic rule to constrain that the pairing algorithm can only output a certain number (say equal to the number of extracted aspects) of triplets according to the classification probability. However, we did try it and found it was not consistently profitable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The framework of our proposed two stage model. E-NEG, S-NEG, B-NEU, I-NEU, E-NEU, S-NEU} ∪ {O}, which locates aspects and labels their sentiment. The opinion tag schema is Y OPT = {B, I, E, S} ∪ {O}. The unified SL predicts a tag sequence Y T S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Tagging schema and relative position index, where B denotes begin, I denotes inside, E denotes end, S denotes single and O denotes outside. The check and cross marks denote valid and invalid aspect-opinion pairs.</figDesc><table><row><cell></cell><cell>Input</cell><cell cols="5">Waiters are friendly and the</cell><cell>fugu</cell><cell cols="7">sashimi is out of the world .</cell></row><row><cell cols="2">Unified tag (aspect+sentiment)</cell><cell>S-POS</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell cols="5">O B-POS E-POS O O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell></row><row><cell></cell><cell>Opinion tag</cell><cell>O</cell><cell>O</cell><cell>S</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>B</cell><cell>I</cell><cell>I</cell><cell>E</cell><cell>O</cell></row><row><cell>Position index</cell><cell>(Waiters,friendly) (fugu sashimi, friendly)</cell><cell>2 0</cell><cell>0 0</cell><cell>2 3</cell><cell>0 0</cell><cell>0 0</cell><cell>0 3</cell><cell>0 3</cell><cell>0 0</cell><cell>0 0</cell><cell>0 0</cell><cell>0 0</cell><cell>0 0</cell><cell>0 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Dataset. (#s and #p denote number of sentences and target-opinion pairs, respectively.)</figDesc><table><row><cell>Dataset</cell><cell>#s</cell><cell>14res # p</cell><cell>#s</cell><cell>14lap #p</cell><cell>#s</cell><cell>15res #p</cell><cell>#s</cell><cell>16res #p</cell></row><row><cell>train</cell><cell cols="8">1300 2145 920 1265 593 923 842 1289</cell></row><row><cell>valid</cell><cell>323</cell><cell>524</cell><cell cols="2">228 337</cell><cell cols="4">148 238 210 316</cell></row><row><cell>test</cell><cell>496</cell><cell>862</cell><cell cols="2">339 490</cell><cell cols="4">318 455 320 465</cell></row><row><cell cols="9">more than one aspect/targets and opinions, we pair up indi-</cell></row><row><cell cols="9">vidual aspects/targets and their opinions. Below is an exam-</cell></row><row><cell>ple:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">The best thing about this laptop is the price along with some</cell></row><row><cell cols="3">of the newer features .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">The=O best=O thing=O about=O this=O laptop=O is=O</cell></row><row><cell cols="9">the=O price=T-POS along=O with=O some=O of=O</cell></row><row><cell cols="7">the=O newer=O features=TT-POS .=O</cell><cell></cell><cell></cell></row><row><cell cols="9">The=O best=S thing=O about=O this=O laptop=O is=O</cell></row><row><cell cols="9">the=O price=O along=O with=O some=O of=O the=O</cell></row><row><cell cols="5">newer=SS features=O .=O</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Our-BLSTM OPT : The first variant of our model that removes the BLSTM OPT component.</figDesc><table><row><cell>IOG (Fan et al. 2019):</cell></row><row><cell>A top performing opinion term extraction method with an</cell></row><row><cell>Inward-Outward LSTM. Li-unified: (Li et al. 2019a) The</cell></row><row><cell>state-of-the-art unified model for aspect extraction and sen-</cell></row><row><cell>timent classification. It also serves as a base model in our</cell></row><row><cell>design and its results are compared on aspect extraction and</cell></row><row><cell>sentiment classification. Note that it does not conduct opin-</cell></row><row><cell>ion extraction. Li-unified-R: A modified model variant of</cell></row><row><cell>Li-unified by us, which adapts their original OE component</cell></row><row><cell>for opinion extraction.</cell></row></table><note>Thus, it may fail to consider sentence contextual information for opinion term extraction. Our-TG: The second variant of our model that removes the TG component, which does not</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Stage one results of aspect extraction and sentiment classification. (All models were trained in the unified tag setting.)47.36 48.15 41.20 33.20 36.70 46.20 37.40 41.30 49.40 36.70  42.10 CMLA 67.80 73.69 70.62 54.70 59.20 56.90 49.90 58.00 53.60 58.90 63.60 61.20 Li-unified 74.43 69.26 71.75 68.01 56.72 61.86 61.39 67.99 64.52 66.88 71.40 69.06 Li-unified-R 73.15 74.44 73.79 66.28 60.71 63.38 64.95 64.95 64.95 66.33 74.55 70.20 Our-BLSTM OPT 70.00 74.20 72.04 65.99 54.62 59.77 63.41 65.19 64.29 69.74 71.62 70.67 67.84 71.95 63.15 61.55 62.34 67.65 64.02 65.79 71.18 72.30 71.73 The two rows below are results of aspect extraction only, without evaluating the correctness of sentiment polarity.</figDesc><table><row><cell></cell><cell></cell><cell>14res</cell><cell></cell><cell></cell><cell>14lap</cell><cell></cell><cell></cell><cell>15res</cell><cell></cell><cell></cell><cell>16res</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell cols="13">RINANTE 48.97 Our-TG 74.41 73.97 74.19 64.35 60.29 62.26 59.28 61.92 60.57 64.57 66.89 65.71</cell></row><row><cell>Our-T</cell><cell>69.42</cell><cell>72.2</cell><cell cols="8">70.79 64.14 60.63 62.34 62.28 66.35 64.25 62.65</cell><cell>71.4</cell><cell>66.74</cell></row><row><cell cols="13">Our 76.60 RINANTE 75.89 70.34 73.00 70.80 52.80 60.50 72.64 51.68 60.39 67.10 55.20 60.60</cell></row><row><cell>CMLA</cell><cell cols="12">84.21 89.83 86.93 71.50 82.20 76.40 75.10 89.30 81.50 72.00 87.60 79.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Stage one results of opinion term extraction. 84.80 83.21 76.87 75.31 76.03 75.98 76.32 76.10 82.33 85.16 83.67 Our-T 80.61 85.38 82.88 76.69 73.88 75.21 78.13 75.22 76.60 77.14 87.10 81.77 Our 84.72 80.39 82.45 78.22 71.84 74.84 78.07 78.07 78.02 81.09 86.67 83.73</figDesc><table><row><cell></cell><cell></cell><cell>14res</cell><cell></cell><cell></cell><cell>14lap</cell><cell></cell><cell></cell><cell>15res</cell><cell></cell><cell></cell><cell>16res</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Distance rule</cell><cell cols="12">58.39 43.59 49.92 50.13 33.86 40.42 54.12 39.96 45.97 61.90 44.57 51.83</cell></row><row><cell>Dependency rule</cell><cell cols="12">64.57 52.72 58.04 45.09 31.57 37.14 65.49 48.88 55.98 76.03 56.19 64.62</cell></row><row><cell>RINANTE</cell><cell cols="12">81.06 72.05 76.29 78.20 62.70 69.60 77.40 57.00 65.70 75.00 42.40 54.10</cell></row><row><cell>CMLA</cell><cell cols="12">69.47 74.53 71.91 51.80 65.30 57.70 60.80 65.30 62.90 74.50 69.00 71.70</cell></row><row><cell>IOG</cell><cell cols="12">82.85 77.38 80.02 73.24 69.63 71.35 76.06 70.71 73.25 85.25 78.51 81.69</cell></row><row><cell>Li-unified-R</cell><cell cols="12">81.20 83.18 82.13 76.62 74.90 75.70 79.18 75.88 77.44 79.84 86.88 83.16</cell></row><row><cell>Our-BLSTM OPT</cell><cell cols="12">80.41 86.19 83.15 78.06 68.98 73.19 74.29 80.48 77.21 82.12 84.95 83.46</cell></row><row><cell>Our-TG</cell><cell>81.77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Stage two results in both pair and triplet setting. (+ denotes cascading our stage two module.) 51.08 46.29 34.40 26.20 29.70 37.10 33.90 35.40 35.70 27.00 30.70 CMLA+ 45.17 53.42 48.95 42.10 46.30 44.10 42.70 46.70 44.60 52.50 47.90 50.00 Li-unified-R+ 44.37 73.67 55.34 52.29 52.94 52.56 52.75 61.75 56.85 46.11 64.55 53.75 Our 47.76 68.10 56.10 50.00 58.47 53.85 49.22 65.70 56.23 52.35 70.50 60.04 Triplet RINANTE+ 31.07 37.63 34.03 23.10 17.60 20.00 29.40 26.90 28.00 27.10 20.50 23.30 CMLA+ 40.11 46.63 43.12 31.40 34.60 32.90 34.40 37.60 35.90 43.60 39.80 41.60 Li-unified-R+ 41.44 68.79 51.68 42.25 42.78 42.47 43.34 50.73 46.69 38.19 53.47 44.51 Our 44.18 62.99 51.89 40.40 47.24 43.50 40.97 54.68 46.79 46.76 62.97 53.62</figDesc><table><row><cell></cell><cell></cell><cell>14res</cell><cell></cell><cell></cell><cell>14lap</cell><cell></cell><cell></cell><cell>15res</cell><cell></cell><cell></cell><cell>16res</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Classifier F1</cell><cell></cell><cell>97.59</cell><cell></cell><cell></cell><cell>94.36</cell><cell></cell><cell></cell><cell>99.61</cell><cell></cell><cell></cell><cell>97.91</cell></row><row><cell>RINANTE+</cell><cell>42.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pair</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the opinion expressions should be paired with the targets/aspects it modifies in a many-to-many setting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Each aspect could contain one or multiple terms. The same also applies to the opinion expression. In our model setting, there should be at least one aspect and one opinion expression.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://spacy.io/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We introduce a sentiment triplet extraction task that answers what is the aspect, how is its sentiment and why is the sentiment in one shot by coupling together aspect extraction, aspect term sentiment classification and opinion term extraction in a two-stage framework. The first stage generates candidate aspects with sentiment polarities and candidate opinion terms by utilizing mutual influence between aspects and opinion terms. The second stage pairs up the correct aspects and opinion terms. Experiments validate the feasibility and effectiveness of our model, and set a benchmark performance for this task.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(Rice-too dry-NEG), (tuna-was n't so fresh-NEG) (Rice-too dry-NEG), (tuna-was n't so fresh-NEG), (Rice-was n't so fresh-NEG), (tuna-too dry-NEG) (Rice-dry-POS), 2017. An unsupervised neural attention model for aspect extraction. In ACL.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural aspect and opinion term extraction with mined rules as weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bailin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>AAAI. [Dai and Song</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5268" to="5277" />
		</imprint>
	</monogr>
	<note>Learning latent opinions for aspect-level sentiment classification</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for targetdependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2509" to="2518" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting document knowledge for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="579" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An interactive multi-task learning network for end-to-end aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Can: Constrained attention networks for multi-aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep multi-task learning for aspect term extraction with memory interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning latent sentiment scopes for entity-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu ; Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3482" to="3489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aspect term extraction with history attention and selective transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified model for opinion target extraction and target sentiment prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6714" to="6721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting coarse-to-fine task transfer for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joty</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Syntactic patterns versus word alignment: Extracting opinion targets from online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extracting opinion targets and opinion words from online reviews with graph co-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>EMNLP. [Nguyen and Shirai</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
	<note>Open domain targeted sentiment</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning multi-grained aspect target sequence for chinese sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="167" to="176" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manning ; Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Pontiki 2014] Pontiki, M. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In SemEval</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 12: Aspect based sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 5: Aspect based sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu ; Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to attend via word-aspect associative fusion for aspectbased sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luu</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui ;</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive neural conditional random fields for aspectbased sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3316" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Target-sensitive memory networks for aspect sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="957" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aspect sentiment classification towards question-answering with reinforced bidirectional attention network</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Double embeddings and cnn-based sequence labeling for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
	<note>and</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised word and dependency path embeddings for aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00534</idno>
	</analytic>
	<monogr>
		<title level="m">Sentiment tagging with partial labels using modular architectures</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1701" to="1711" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural networks for open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vo ; Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="612" to="621" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
