<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gantavya</forename><surname>Bhatt</surname></persName>
							<email>gantavya.iitd@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
							<email>hbansal10n@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishubh</forename><surname>Singh</surname></persName>
							<email>rishubhsingh135@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumeet</forename><surname>Agarwal</surname></persName>
							<email>sumeet@iitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long short-term memory (LSTM) networks and their variants are capable of encapsulating long-range dependencies, which is evident from their performance on a variety of linguistic tasks. On the other hand, simple recurrent networks (SRNs), which appear more biologically grounded in terms of synaptic connections, have generally been less successful at capturing long-range dependencies as well as the loci of grammatical errors in an unsupervised setting. In this paper, we seek to develop models that bridge the gap between biological plausibility and linguistic competence. We propose a new architecture, the Decay RNN, which incorporates the decaying nature of neuronal activations and models the excitatory and inhibitory connections in a population of neurons. Besides its biological inspiration, our model also shows competitive performance relative to LSTMs on subject-verb agreement, sentence grammaticality, and language modeling tasks. These results provide some pointers towards probing the nature of the inductive biases required for RNN architectures to model linguistic phenomena successfully.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the last couple of decades, neural networks have been approached primarily from an engineering perspective, with the key motivation being efficiency, consequently moving further away from biological plausibility. Recent developments <ref type="bibr" target="#b27">(Song et al., 2016;</ref><ref type="bibr" target="#b7">Gao and Ganguli, 2015;</ref><ref type="bibr" target="#b29">Sussillo and Barak, 2013)</ref> have however incorporated explicit constraints in neural networks to model specific parts of the brain and have found a correlation between the learned activation maps and actual neural activity recordings. Thus, these trained networks can perhaps act as a proxy for a theoretical investigation into biological circuits. * * Equal Contribution Recurrent Neural Networks (RNNs) have been used to analyze the principles and dynamics of neural population responses by performing the same tasks as animals <ref type="bibr" target="#b21">(Mante et al., 2013)</ref>. However, these networks violate Dale's law <ref type="bibr" target="#b4">(Dale, 1935;</ref><ref type="bibr" target="#b28">Strata and Harvey, 1999)</ref>, which states that the neurons have either a purely excitatory or inhibitory effect on other neurons in the mammalian brain. The decaying nature of the potential in the neuron membrane after receiving signals (excitatory or inhibitory) from the surrounding neurons is also well-studied <ref type="bibr" target="#b9">(Gluss, 1967)</ref>. The goal of our work is to incorporate these biological features into the RNN structure, which gives rise to a neuro-inspired and computationally inexpensive recurrent network for language modeling, which we call a Decay RNN (Section 4). We perform learning using the backpropagation algorithm. Despite its differences with the way learning is believed to happen in the brain, it has been argued that the brain can implement its core principles <ref type="bibr" target="#b13">(Hinton, 2007;</ref><ref type="bibr" target="#b18">Lillicrap et al., 2020)</ref>. We assess our model's ability to capture syntax-sensitive dependencies via multiple linguistic tasks (Section 6): number prediction, grammaticality judgement <ref type="bibr" target="#b19">(Linzen et al., 2016)</ref> which entails subject-verb agreement, and a more complex language modeling task <ref type="bibr" target="#b22">(Marvin and Linzen, 2018)</ref>.</p><p>Subject-verb agreement, where the main noun and the associated verb must agree in number, is considered as evidence of hierarchical structure in English. This is exemplified using a sentence taken from the dataset made available by <ref type="bibr" target="#b19">Linzen et al. (2016)</ref>:</p><p>1. *All trips on the expressway requires a toll.</p><p>2. All trips on the expressway require a toll.</p><p>The effect of agreement attractors (nouns having number opposite to the main noun; expressway in the above example 1 ) between the main noun and main verb of a sentence has been well-studied <ref type="bibr" target="#b19">(Linzen et al., 2016;</ref><ref type="bibr" target="#b16">Kuncoro et al., 2018)</ref>. Our work also highlights the influence of non-attractor intervening nouns. For example,</p><p>â€¢ A chair created by a hobbyist as a gift to someone is not a commodity. 2 In the number prediction task, if a model correctly predicts the grammatical number of the verb (singular in case of 'is'), it might be due to the (helpful) interference of non-attractor intervening nouns ('hobbyist', 'gift', 'someone') rather than necessarily capturing its dependence the main noun ('chair'). From our investigation in Section 6.2, we find that the linear recurrent models take cues present in the vicinity of the main verb to predict its number, apart from the agreement with the main noun.</p><p>In the subsequent sections, we investigate the performance of the Decay RNN and other recurrent networks, showing that no single sequential model generalizes well on all (grammatical) phenomena, which include subject-verb agreements, reflexive anaphora, and negative polarity items as described in <ref type="bibr" target="#b22">Marvin and Linzen (2018)</ref>. Our major outcomes are:</p><p>1. Designing a relatively simple and bio-inspired recurrent model: the Decay RNN, which performs on-par with LSTMs for linguistic tasks such as subject-verb agreement and grammaticality judgement.</p><p>2. Pointing to some limitations of analyzing the intervening attractor nouns alone for the subject-verb agreement task and attempting joint analysis of non-attractor intervening nouns and attractor nouns in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Showing that there is no linear recurrent scheme which generalizes well on a variety of sentence types and motivating research in better understanding of the nature of biases induced by varied RNN structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been prior work on using LSTMs <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997)</ref> for language 1 Main noun and verb are highlighted in bold. Intervening nouns are underlined. Asterisks mark unacceptable sentences.</p><p>2 Sentence taken from the dataset made available by <ref type="bibr" target="#b19">Linzen et al. (2016)</ref>. modeling tasks. The work of Gers and <ref type="bibr" target="#b8">Schmidhuber (2001)</ref> has shown that LSTMs can learn simple context-free and context-sensitive languages. However, as per the investigations carried out in <ref type="bibr" target="#b16">Kuncoro et al. (2018)</ref>, it was observed that if the model capacity is not enough, then LSTMs may not generalize the long-range dependencies. Recently many architectures have explicitly incorporated the knowledge of phrase structure trees <ref type="bibr" target="#b16">(Kuncoro et al., 2018;</ref><ref type="bibr" target="#b0">Alvarez-Melis and Jaakkola, 2017;</ref><ref type="bibr" target="#b30">Tai et al., 2015)</ref> which have shown improvement in generalizing over long-range dependencies. At the same time, <ref type="bibr" target="#b26">Shen et al. (2019)</ref> proposed ON-LSTMs, a modification to LSTMs that provides an inductive tree bias to the structure. However, <ref type="bibr" target="#b5">Dyer et al. (2019)</ref> have shown that the success of ON-LSTMs was due to their proposed metric to analyze the model, not necessarily due to their architecture.</p><p>From the biological point of view, <ref type="bibr" target="#b2">Capano et al. (2015)</ref> used a hard reset of the membrane potential in contrast to a soft decay observed in a neuronal membrane. At the same time, their learning paradigm is similar to the Hebbian learning scheme <ref type="bibr" target="#b11">(Hebb, 1949)</ref>, which does not involve error backpropagation <ref type="bibr" target="#b25">(Rumelhart et al., 1986)</ref>. Our work is closely related to the idea of modeling the population of neurons as a dynamical system (EIRNN) proposed by <ref type="bibr" target="#b27">Song et al. (2016)</ref>. However, their time constant parameter was based on the concepts described in <ref type="bibr" target="#b32">Wang (2002)</ref> while the sampling rate was arbitrarily chosen. Given that the chosen values only considered a certain class of neurons <ref type="bibr" target="#b33">(Yang et al., 2019)</ref>, we believe that it is not necessary to have the same values of the parameters for each cognitive task. Thus, we build on their formulation by making the sampling rate and time constant learnable as manifested by our decay parameter, described in the next section. and excitability (synaptic strength) of a network maximizes the overall learning. This balance is governed by the ratio of inhibitory and excitatory neurons. They have further shown that this balance also maximizes the overall performance in multitask learning. <ref type="bibr" target="#b3">Catsigeras (2013)</ref> mathematically prove that Dale's principle is necessary for an optimal 3 neuronal network's dynamics.</p><p>In the postsynaptic neuron, the integration of synaptic potentials is realized by the addition of excitatory (+ve) and inhibitory (-ve) postsynaptic potentials (PSPs). PSPs are electronic voltages, that decay as a function of time due to spontaneous reclosure of the synaptic channels. The decay of the PSPs is controlled by the membrane constant Ï„ , i.e., the time required by the PSP to decay to 37% of its peak value <ref type="bibr" target="#b31">(Wallisch et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decay RNN</head><p>Here we present our proposed architecture, which we call the Decay RNN (DRNN). Our architecture aims to model the decaying nature of the voltage in a neuron membrane after receiving impulses from the surrounding neurons. At the same time, we incorporate Dale's principle in our architecture. Thus, our model captures both the microscopic and macroscopic properties of a group of neurons. Adhering to the stated phenomena, we define our model with the following update equations for given input x (t) at time t:</p><formula xml:id="formula_0">c (t) = (ReLU (W)W dale )h (tâˆ’1) + Ux (t) + b h (t) = f (Î±h (tâˆ’1) + (1 âˆ’ Î±)c (t) )</formula><p>Here f is a nonlinear activation function, W and U are weight matrices, b is the bias and h (t) represents the hidden state (analogous to voltage). We define Î± âˆˆ (0,1) as a learnable parameter to incorporate a decay effect in the hidden state (analogous to the decay in the membrane potential). Here Î± acts as a balancing factor between the hidden state h (tâˆ’1) and c (t) . 4 W dale is a diagonal matrix, and based on the empirical results on the mammalian brain (Hendry and Jones, 1981), we set the last 20% of entries to -1, representing the inhibitory connections, and the rest to 1 (See Appendix A.3). 5 Unlike <ref type="bibr" target="#b27">Song et al. (2016)</ref>, we keep self-connections in the network. Besides biological inspiration, our model also has the following salient features.</p><p>First, the presence of Î± acts as a coupled gating mechanism to the flow of information <ref type="figure" target="#fig_0">(Figure 1</ref>), at the same time maintaining an exponential moving average of the hidden state. Thus, Î± values close to 1 correspond to memories of the distant past. It is worth mentioning that Oliva et al. <ref type="formula">(2017)</ref> have considered the exponential moving average in the context of RNNs. However, their approach manually selected a set of scaling parameters, whereas we have a systematic way of arriving at the values of those parameters by making them learnable for the task at hand. Second, our model also has an intrinsic skip connection deriving out of its formulation. <ref type="bibr" target="#b34">Yue et al. (2018)</ref> has shown that the architectures with skip connections provide an alternate path for the flow of gradients during the error backpropagation. At the same time presence of coupled gates slows down the vanishing of gradient <ref type="bibr" target="#b1">(Bengio et al., 2013)</ref>. Thus, despite of its simple un-gated structure, the features discussed above provide safeguards against vanishing gradient.</p><p>To examine the importance of Dale's principle in the learning process, we made a variant of our Decay RNN without Dale's principle, which we call the Slacked Decay RNN (SDRNN), with updates to c (t) made as follows:</p><formula xml:id="formula_1">c (t) = Wh (tâˆ’1) + Ux (t) + b</formula><p>To understand the role of the correlation between the hidden states in the Decay RNN formulation, we devised an ablated version of our architecture, which we refer to as the Ab-DRNN. With the following update equation, we remove the mathematical factor (Wh (tâˆ’1) ) that gives rise to a correlation between hidden states: </p><formula xml:id="formula_2">h (t) = f (Î±h (tâˆ’1) + (1 âˆ’ Î±)(Ux (t) + b)) f Î± 1 âˆ’ Î± h (tâˆ’1) hidden x (t) input h (t) next hidden</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>For the number prediction (Section 6.1) and grammaticality judgment (Section 6.3) tasks, we used a corpus of 1.57 million sentences from Wikipedia <ref type="bibr" target="#b19">(Linzen et al., 2016)</ref>, of which 10% were used for training, 0.4% for validation, and the remaining were reserved for testing. On the other hand, for the language modeling task (Section 6.4), the model was trained on a 90 million word subset of Wikipedia comprising of 3 million training and 0.3 million validation sentences <ref type="bibr" target="#b10">(Gulordava et al., 2018)</ref>.</p><p>Despite having a large number of training points, these datasets have certain drawbacks, including the lack of a sufficient number of syntactically challenging examples leading to poor generalization over the sentences out of the training data distribution. Therefore, we construct a generalization set as described in <ref type="bibr" target="#b22">Marvin and Linzen (2018)</ref>, where we generate the sentences out of templates that can be described using a non-recursive context-free grammar. The use of the generalization set allows us to test on a much broader range of linguistic phenomena. We will use this dataset for the targeted syntactic evaluation of our trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Here we will describe our experiments 6 to assess the models' ability to capture syntax-sensitive dependencies. Details regarding the training settings are available in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Number Prediction Task</head><p>The number prediction task was proposed by <ref type="bibr" target="#b19">Linzen et al. (2016)</ref>. In this task, the model is required to predict the grammatical number of the verb when provided a sentence up to the verb.</p><p>1. The path to success is not straight forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The path to success</head><p>The model will take the second sentence as input and has to predict the number of the verb (here, singular). <ref type="table">Table 1</ref> shows the results on the number prediction task. All the models including SRNs performed well on this task. Thus, this indicates that even vanilla RNNs can identify singular and plural words and can associate the main subject with the upcoming verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Joint Analysis of Intervening Nouns</head><p>So far in the literature, when looking at intervening material in agreement tasks, the research has tended 6 Our code is available at https://github.com/bhattg/Decay-RNN-ACL-SRW2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>No. to focus on agreement attractors, the intervening nouns with the opposite number to the main noun <ref type="bibr" target="#b16">(Kuncoro et al., 2018)</ref>. However, we posit that the role of non-attractor intervening nouns may also be important when understanding a model's decisions. For long-range dependencies in agreement tasks, a model may be influenced by the presence of non-attractor intervening nouns instead of purely capturing the verb's relationship with the main subject. Hence an analysis done solely based on the number of agreement attractors may be misleading. <ref type="table">Table 2</ref> shows an improvement in the verb number prediction accuracy with an increasing number of non-attractors (n), even as the subject-verb distance and the attractor count are kept fixed. This indicates that the models are also using cues present in the vicinity of the main verb to predict its number, apart from agreement with the main noun.  <ref type="table">Table 2</ref>: Number prediction % accuracy with an increasing number of non-attractor intervening nouns (n). The distance between the main subject and the corresponding verb is held constant at 7 and the attractor count at 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Grammaticality Judgement</head><p>The previous objective was predicting the grammatical number of the verb after providing the model an input sentence only up to the verb. However, this way of training may give the model a cue to the syntactic clause boundaries. In this section, we describe the grammaticality judgment task. Given an input sentence, the model has to predict whether it is grammatical or not. To perform well on this task, the model would presumably need to allocate more resources to determine the locus of ungrammaticality. For example, consider the following pair of sentences 2 :</p><p>1. The roses in the vase by the door are red.</p><p>2. *The roses in the vase by the door is red.</p><p>The model has to decide, for input sentences such as the above, whether each one is grammatically correct or not. <ref type="table">Table 1</ref> shows the performance of different recurrent architectures on this task. It can be seen that SRNs, which were comparable to LSTMs and GRUs on the prediction experiment described in Section 6.1, are no better than random on the grammaticality judgment task. On the other hand, the Ab-DRNN performed better than the SRN. This highlights the importance of a balance between the uncorrelated hidden states (h (t) ), and the connected hidden states (Wh (t) ), which is modeled by the Decay RNN. Due to its architectural similarity with the Independent RNN <ref type="bibr" target="#b17">(Li et al., 2018)</ref>, which has independent connections among neurons in a layer, Ab-DRNN did not suffer from the vanishing gradient problem.</p><p>Importance of the generalization set <ref type="bibr" target="#b2">Capano et al. (2015)</ref> had argued that the inclusion of Dale's principle improved generalization abilities for multitask learning. For our models trained on a single task, we use the generalization set to determine the number prediction confidence profile over the sentences. <ref type="figure">Figure 2</ref> describes the average number prediction confidence at each part of speech for all prepositional phrases with inanimate subjects. We note the anomalously low confidence of the SDRNN at plural inanimate subjects (like 'movies', 'books'), unlike the DRNN.  <ref type="table">Table 3</ref>: Accuracy comparison of DRNN and SDRNN when tested on the generalization set for the grammaticality judgement task; 'anim' refers to an animated noun.</p><p>In <ref type="table">Table 3</ref>, 7 we present the result of the models trained for the grammaticality judgment task and tested on the synthetic generalization set. From the results, we can see that despite having nearly the same accuracy on the original testing data <ref type="table">(Table  Figure 2</ref>: Number prediction confidence (for the correct verb number) averaged over the generalization set (540 sentences) for prepositional phrases with plural inanimate subjects (IS). An example word for each position is indicated in parentheses. Values at ES indicate the confidence for the following verb/auxiliary. For the example sentence, confidence &lt; 0.5 implies singular verb number prediction, and confidence &gt; 0.5 plural. 1), there is a substantial difference in the generalization accuracies of the DRNN and SDRNN. The DRNN shows better generalization than the SDRNN in the experiments mentioned in <ref type="table">Table 3</ref> and <ref type="figure">Figure 2</ref>. This might be due to regularising effects induced by Dale's constraint. This is an interesting observation that merits further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Language Modeling</head><p>Word-level language modeling is a task that helps in the evaluation of the model's capacity to capture the general properties of language beyond what is tested in specialized tasks focused on, e.g., subjectverb agreement. We use perplexity to compare our model's performance against standard sequential recurrent architectures. <ref type="table">Table 4</ref> shows the validation perplexity of different language models along with the number of learnable parameters for the task. From the <ref type="table">Table 4</ref>, we observe that incorporating the components of the Ab-DRNN and the SRN in a coupled way might have led to the improved performance of the Decay RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Targeted Syntactic Evaluation</head><p>Targeted syntactic evaluation <ref type="bibr" target="#b22">(Marvin and Linzen, 2018)</ref> is a way to evaluate the language model across different classes of structure-sensitive phenomena. This includes subject-verb agreement, reflexive anaphora, and negative polarity items (NPI). 8 <ref type="table">Table 4</ref> shows that even with a simple architecture, the Decay RNN class of models performs  <ref type="table">Table 4</ref>: Accuracy of models on targeted syntactic evaluation. RC: Relative Clause, PP: Prepositional Phrase, VP : Verb Phrase. Closeness in the mean arithmetic rank of models (other than SRNs) across tasks suggests that within the current space of sequential recurrent models, none dominates the others.</p><p>fairly similarly to LSTMs and much better than SRNs for many tests. <ref type="bibr">9</ref> In the case of long-range dependencies and NPI involving relative-object clauses, our models perform substantially better than LSTMs. High variability in the performance of the models in the case of NPIs might be due to non-syntactic cues as pointed out by <ref type="bibr" target="#b22">Marvin and Linzen (2018)</ref>. Based on the mean ranks observed in <ref type="table">Table 4</ref>, we conjecture that there is no sequential recurrent structure at present which outperforms the others across the board. However, SRNs alone are not sufficient for most purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed the Decay RNN, a bioinspired recurrent network that emulates the decaying nature of neuronal activations after receiving excitatory and inhibitory impulses from upstream neurons. We have found that the balance between the free term (h (t) ) and the coupled term (Wh (t) ) enabled the model to capture syntax-level dependencies. As shown by <ref type="bibr" target="#b23">McCoy et al. (2020)</ref>; <ref type="bibr" target="#b16">Kuncoro et al. (2018)</ref>, explicitly modeling hierarchical structure helps to discover non-local structural dependencies. The contrast in the performance of 9 Results for the ON-LSTM are directly quoted from <ref type="bibr" target="#b26">Shen et al. (2019).</ref> the language models encourages us to look at the inductive biases, which might have led to better syntactic generalization in certain cases. Recently, <ref type="bibr" target="#b20">Maheswaranathan and Sussillo (2020)</ref> showed the existence of a line attractor in the dynamics of the hidden states for sentiment classification. Thus, similar dynamical-system-based analysis can be extended to our settings to further understand the working of the Decay RNN.</p><p>From the cognitive neuroscience perspective, it would be interesting to investigate if the proposed Decay RNN can capture some aspects of actual neuronal behaviour and language cognition. Our results here do at least indicate that the complex gating mechanisms of LSTMs (whose cognitive plausibility has not been established) may not be essential to their performance on many linguistic tasks, and that simpler and perhaps more cognitively plausible RNN architectures are worth exploring further as psycholinguistic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Effect of agreement attractors</head><p>In this section, we present the trends in the testing performance of the LSTM and the Decay RNN (DRNN) for the grammaticality judgment task. <ref type="figure">Figure 3</ref> shows the performance of the models when we fix the number of intervening nouns and vary the count of attractors between the main subject and the corresponding verb. The decreasing performance of the models with the introduction of more attractors indicates that they cause the models to get more confused about the upcoming verb number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Comparison between DRNN and SDRNN</head><p>In Section 6.3, we saw that in terms of testing accuracy for grammaticality judgment, the Slacked Decay RNN (SDRNN) outperformed the Decay RNN (DRNN). For a robust investigation of this behaviour, we tested our models on the generalization set and mentioned a subset of our results on grammaticality judgment in <ref type="table">Table 3</ref>. Here we present a bar graph <ref type="figure">(Figure 4)</ref> depicting the model performance when tested on the generalization set for the grammaticality judgment task. A substantial difference in the performance of the SDRNN and the DRNN reinforces the possibility of the regularizing effects of Dale's principle.</p><p>A.3 Implementation of Dale's constraint</p><formula xml:id="formula_3">âˆ€w i,j âˆˆ ReLU (W), w i,j â‰¥ 0 ReLU(W)W dale = ï£® ï£¯ ï£¯ ï£¯ ï£°</formula><p>w 1,1 w 1,2 . . . w 1,n w 2,1 w 2,2 . . . w 2,n . . . . . . . . . . . . For the number prediction task and the grammaticality judgment task the network is trained as a binary classifier. The network is single-layered, with ReLU activation and trained with embedding and hidden layer dimension being 50, and a batch size of 1. We have reported the average accuracies after 3 separate runs in <ref type="table">Table 1</ref>. For targeted syntactic evaluation, we have trained a language model to predict the grammaticality of a sentence. In our language model, we used a 2-layered network with tanh activation, a dropout rate of 0.2 with embedding dimension 200, hidden dimension 650, and a batch size of 128. All models are trained with a learning rate of 0.001 using the Adam optimizer <ref type="bibr" target="#b15">(Kingma and Ba, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Decay parameter (Î±) learning</head><p>In the main text, we describe the balancing effect of Î± in the Decay RNN model. We present the trend in the learned value of Î± throughout training for the grammaticality task for various initializations in <ref type="figure" target="#fig_2">Figure 5</ref>. We observe that for all Î± initializations in the range (0,1), the learned value converges to around 0.8. Hence, we initialize our Î± to 0.8 at the start of the training process. <ref type="figure">Figure 3</ref>: Trends in the performance of the LSTM (blue) and DRNN (orange) models with increasing numbers of intervening nouns. For each subplot corresponding to a fixed intervening noun number, the number of agreement attractors increases as we move from left to right on the x-axis. <ref type="figure">Figure 4</ref>: Performance of the LSTM (blue), DRNN (orange), and SDRNN (green) models for the different types of sentences in the generalization set, when trained for the grammaticality judgment task. There were at least 200 test sentences for each of these types. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Decay RNN cell, comprising of a skip connection and coupled scalar gates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Moving average of Î± over the course of training for different initializations. 1 unit of training length is 1 forward pass.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Biological PreliminariesAccording to Dale's principle, a neuron is either excitatory or inhibitory<ref type="bibr" target="#b6">(Eccles, 1976)</ref>. If a neuron output produces a negative (positive) change in the membrane potential of all the connected neurons via its synapse, then it is said to be an inhibitory (excitatory) neuron. In a set of N neurons, if W is the synaptic connection matrix, then the connection from the neuron j to neuron i is 'excitatory' if W ij &gt; 0, and 'inhibitory' if W ij â‰¤ 0.<ref type="bibr" target="#b2">Capano et al. (2015)</ref> have argued that a balance between structural and response variability (entropy),</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In the sense of showing the most diverse set of responses. 4 It was kept bounded using a sigmoid function. Our results did not change when we used a linear function instead.5  Our results did not change when we chose a different set of -1 entries instead of the last 20%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Here, we present three tests from the targeted syntactic evaluation framework. Others test results can be found in Appendix A.2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The definitions of these linguistic terms are provided in the supplementary material of<ref type="bibr" target="#b22">Marvin and Linzen (2018)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We wish to thank the anonymous reviewers, and Jakob Prange and ACL SRW for the postacceptance mentorship program; Pankaj Malhotra for valuable comments on earlier versions of this paper; and Tal Linzen for helpful discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tree-structured decoding with doubly-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8624" to="8628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal percentage of inhibitory synapses in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Capano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucilla</forename><forename type="middle">De</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arcangelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9895</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dale&apos;s principle is necessary for an optimal neuronal network&apos;s dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Catsigeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10B</biblScope>
			<biblScope unit="page" from="15" to="29" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pharmacology and nerve-endings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Dale</surname></persName>
		</author>
		<idno type="DOI">10.1177/003591573502800330</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of Medicine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="319" to="332" />
			<date type="published" when="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A critical analysis of biased parsers in unsupervised parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GÃ¡bor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09428</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From electrical to chemical transmission in the central nervous system: the closing address of the sir henry dale centennial symposium cambridge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eccles</forename></persName>
		</author>
		<idno type="DOI">https:/royalsocietypublishing.org/doi/abs/10.1098/rsnr.1976.0015</idno>
	</analytic>
	<monogr>
		<title level="j">Royal Society of London</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="230" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On simplicity and complexity in the brave new world of largescale neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="148" to="155" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LSTM recurrent networks learn simple context-free and contextsensitive languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1333" to="1340" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A model for neuron firing with exponential decay of potential resulting in diffusion equations for probability density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gluss</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1007/BF02476897</idno>
	</analytic>
	<monogr>
		<title level="j">The Bulletin of Mathematical Biophysics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="243" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Colorless green recurrent networks dream hierarchically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1195" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The organization of behavior: A neuropsychological theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sizes and distributions of intrinsic neurons incorporating tritiated gaba in monkey sensory-motor cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Hendry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="390" to="408" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">How to do backpropagation in a brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Invited talk at the NIPS2007 Deep Learning Workshop</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">https:/www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LSTMs can learn syntax-sensitive dependencies well, but modeling structure makes them better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (IndRNN): Building a longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Backpropagation and the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Marris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Akerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41583-020-0277-3</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">https:/www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00115</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">How recurrent networks implement contextual processing in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08013</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context-dependent computation by recurrent dynamics in prefrontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Valerio Mante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newsome</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature12742</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">503</biblScope>
			<biblScope unit="issue">7474</biblScope>
			<biblScope unit="page" from="78" to="84" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Targeted syntactic evaluation of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1151</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequenceto-sequence networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00304</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="125" to="140" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The statistical recurrent unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Junier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BarnabÃ¡s</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>PÃ³czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2671" to="2680" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ordered neurons: Integrating tree structures into recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training excitatory-inhibitory recurrent neural networks for cognitive tasks: a simple and flexible framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1004792</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dales principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piergiorgio</forename><surname>Strata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Research Bulletin</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="349" to="350" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Opening the black box: low-dimensional dynamics in highdimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Barak</surname></persName>
		</author>
		<idno type="DOI">https:/www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00409</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="649" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Synaptic transmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Wallisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lusignan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Benayoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">S</forename><surname>Dickey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">G</forename><surname>Hatsopoulos</surname></persName>
		</author>
		<idno type="DOI">10.1016/b978-0-12-374551-4.00027-0</idno>
	</analytic>
	<monogr>
		<title level="m">Matlab for Neuroscientists</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Probabilistic decision making by slow reverberation in cortical circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="955" to="968" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Task representations in neural networks trained to perform many cognitive tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guangyu Robert Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Madhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jing</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="306" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Residual recurrent neural networks for learning sequential representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxuan</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">56</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
