<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-based Interpolation of Feature Vectors for Accurate Few-Shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Hu</surname></persName>
							<email>yuqing.hu@imt-atlantique.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Electronics Dept</orgName>
								<orgName type="institution">IMT Atlantique Orange Labs</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
							<email>vincent.gripon@imt-atlantique.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Electronics Dept</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Pateux</surname></persName>
							<email>stephane.pateux@orange.com</email>
							<affiliation key="aff2">
								<address>
									<settlement>Orange Labs Cesson-Sévigné</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-based Interpolation of Feature Vectors for Accurate Few-Shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In few-shot classification, the aim is to learn models able to discriminate classes using only a small number of labeled examples. In this context, works have proposed to introduce Graph Neural Networks (GNNs) aiming at exploiting the information contained in other samples treated concurrently, what is commonly referred to as the transductive setting in the literature. These GNNs are trained all together with a backbone feature extractor. In this paper, we propose a new method that relies on graphs only to interpolate feature vectors instead, resulting in a transductive learning setting with no additional parameters to train. Our proposed method thus exploits two levels of information: a) transfer features obtained on generic datasets, b) transductive information obtained from other samples to be classified. Using standard few-shot vision classification datasets, we demonstrate its ability to bring significant gains compared to other works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep learning is the state-of-the-art solution for many problems in machine learning, specifically in the domain of computer vision. Relying on a huge number of tunable parameters, these systems are able to absorb subtle dependencies in the distribution of data in such a way that it can later generalize to unseen inputs. Numerous experiments in the field of vision suggest that there is a trade-off between the size of the model (for example expressed as the number of parameters <ref type="bibr" target="#b0">[1]</ref>) and its performance on the considered task. As such, reaching state-of-the-art performance often requires to deploy complex architectures. On the other hand, using large models in the case of data-thrifty settings would lead to a case of an underdetermined system. This is why few-shot learning is particularly challenging in the field.</p><p>In order to overcome this limitation of deep learning models, several works propose to use Graph Neural Networks (GNNs) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. GNNs are a natural way to exploit information available in other samples to classify, a setting often referred to as transductive in the literature. However, most often introduced GNNs come with their own set of parameters to be added to the already numerous parameters to tune to solve the considered task. As a consequence, many of these methods do not achieve top-tier results when compared to state-of-the-art solutions.</p><p>In this work, we propose to incorporate a graph-based method with no additional parameters, as a way to naturally bring transductive information in solving the considered task. The first step of the method consists in training a feature extractor with abundant data, followed by an interpolation strategy using well designed graphs. The graphs considered in this paper use vertices to represent each sample of the batch, and their edges are weighted depending on the similarity of corresponding feature vectors. The graph is thus used to interpolate features and thus share information between inputs. Once the features have been interpolated, we simply use a classical Logistic Regression (LR) to classify them. This work comes with the following claims:</p><p>• We introduce a three-stage method for few-shot classification of input images that combines state-of-the-art transfer learning <ref type="bibr" target="#b5">[6]</ref>, a graph-based interpolation technique and logistic regression. <ref type="bibr">•</ref> We empirically demonstrate that the proposed method reaches competitive accuracy on standardized benchmarks in the field of few-shot learning and largely surpasses the current works using GNNs. <ref type="bibr">•</ref> We analyze the importance of each step of the method and discuss hyperparameters influence. The paper is organized as follows. In Section II, we present related works. In Section III we introduce our proposed methodology. In Section IV, we show experimental results on standard vision datasets and discuss hyperparameters influence. Finally, Section V is a conclusion. The source code can be found at https://github.com/yhu01/transfer-sgc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Optimization based methods: Recent work on few-shot classification contains a variety of approaches, some of which can be categorized as meta-learning <ref type="bibr" target="#b6">[7]</ref> where the goal is to train an optimizer that initializes the network parameters using a first generic dataset, so that the model is able to reach good performance with only a few more steps on actual considered data. The well-known MAML method <ref type="bibr" target="#b7">[8]</ref> trains on different tasks with a basic stochastic gradient decent optimizer <ref type="bibr" target="#b8">[9]</ref> and Meta-LSTM <ref type="bibr" target="#b9">[10]</ref> utilizes a LSTM-based meta-learner that is thus memory-augmented. Meta-learning can be thought of as a refined transfer method, where the fewshot setting is taken into consideration directly when training on the generic dataset. Although both MAML and Meta-LSTM Pretraining Graph-based feature interpolation Logistic regression using lots of training data 1.train feature extractor</p><formula xml:id="formula_0">x i → f ϕ (x i ) = v i novel input feature vector v 1 v 2 v 3 v 4 v 5 v 6 based on cos(v 2 , v 3 )</formula><p>2.construct similarity graph  <ref type="figure">Fig. 1</ref>. Illustration of the proposed method. The proposed method is composed of three stages. During the pretraining stage, a classical backbone is trained using large datasets (step 1.). This trained backbone is then used to extract features of a novel dataset, comprising few supervised inputs. During feature interpolation, first is built a similarity graph depending on the cosine similarity between extracted features of both labeled and unlabeled available data (step 2.). Then this graph is used to diffuse (i.e. interpolate) features of similar (neighbor) samples (step 3.). The obtained representations are used to train a simple logistic classifier (step 4.) using the supervised data. Finally, in step 5., the trained classifier is used to perform predictions on unlabeled data.</p><formula xml:id="formula_1">V new = (αI + E) κ V 3.</formula><p>achieve good performance with quick adaptation, this type of solution suffers from the domain shift problem <ref type="bibr" target="#b8">[9]</ref> as well as the sensitivity of hyperparameters. Embedding based methods: Another popular approach aims at finding compact embedding for the input data by learning a metric that measures the distance in a low-dimensional way. Matching Nets <ref type="bibr" target="#b10">[11]</ref> and Proto Nets <ref type="bibr" target="#b11">[12]</ref> learn a nearestneighbor classifier by comparing the distance between the query inputs and labeled inputs with a certain metric, while Relation Nets <ref type="bibr" target="#b12">[13]</ref> construct a new neural network that learns the metric itself. If some of these methods are able to outperform MAML, they mainly suffer from over-fitting and a lack of task specific information.</p><p>Therefore, ideas have been proposed to address these issues. For example in <ref type="bibr" target="#b13">[14]</ref>, a plug network is added to find taskrelevant features inside embeddings so that the model can tell the inter-class uniqueness and intra-class commonality for a specific task. In <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>, the authors create a classweight generator by training the model with a linear classifier (e.g. SVM) in order for the model to minimize generalization error across a distribution of tasks. More recently, the use of graph methods <ref type="bibr" target="#b16">[17]</ref> [18] starts to gain momentum in the fewshot learning problems. For example, in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, the authors incorporate the idea of semi-supervised learning <ref type="bibr" target="#b18">[19]</ref> as a mean to benefit from the unlabeled query input data when solving a task, what is referred to as the transductive setting. Many recent works propose neural networks able to handle inputs supported on graphs <ref type="bibr" target="#b19">[20]</ref>. For example, in GCN <ref type="bibr" target="#b20">[21]</ref>, the authors introduce a graph convolution operator, that can be used in cascade to generate deep learning architectures. In GAT <ref type="bibr" target="#b21">[22]</ref>, the authors enrich GCN with additional learnable attention kernels. In SGC <ref type="bibr" target="#b22">[23]</ref>, the authors propose to simplify GCN by using only one-layer systems on powers of the adjacency matrix of considered graphs. Interestingly, they reach state-of-the-art accuracy with fewer parameters.</p><p>Hallucination based methods: Other methods propose to augment the training sets by learning a generator that can hallucinate novel class data using data-augmentation tech-niques <ref type="bibr" target="#b8">[9]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, the authors extract labeled data into different components and then combine them using learned transformations, while in <ref type="bibr" target="#b24">[25]</ref>, the authors aim at constructively deforming original samples with new samples drawn from another dataset. However, these methods lack precision as in the way the data is generated, which results in coarse and low-quality synthesized data that can sometimes lead to unsignificant gains in performance <ref type="bibr" target="#b25">[26]</ref>.</p><p>Transfer based methods: As in our work, transfer learning is another possible solution to solve few-shot classification problems. The main idea is to first train a feature extractor using a generic dataset <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, then process these features directly when solving the new task. In <ref type="bibr" target="#b8">[9]</ref> a distance-based classifier is applied to train the backbone (i.e. the feature extractor), and in <ref type="bibr" target="#b5">[6]</ref>, the authors aims at improving the feature quality by adding self-supervised learning and dataaugmentation techniques during training. These methods have been proven to perform generally well, yet the challenge remains to fine-tune using the limited amount of labeled data.</p><p>In our work, we propose to align multiple ingredients that have been introduced in this section. Namely, we use transfer with graph-based interpolation. We mainly use transfer to exploit information contained in massive generic datasets, and we use a graph method to leverage the additional information available in both labeled and unlabeled inputs. Following the transductive setting, our proposed method can be considered as similar to <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, but contrary to their works, we adopt a strategy in which the considered graph-based method contains no additional parameters to be trained. Our method can also be seen as a modification of Simplified Graph Convolutions <ref type="bibr" target="#b22">[23]</ref>, where contrary to their work we infer a graph structure from the latent representations of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem statement</head><p>Consider the following problem. We are given two datasets, termed D base and D novel with disjoint classes. The first one (called "base") contains a large number of labeled examples from K b different classes. The second one (called "novel") contains a small number of labeled examples, along with some unlabeled ones, all from K n new classes. Our aim is to accurately predict the class of the unlabeled inputs of the novel dataset. There are a few important parameters to this problem: the number of classes in the novel dataset K n , the number of training samples s for each corresponding class, and the total number of unlabeled inputs Q.</p><p>Note that in previous works <ref type="bibr" target="#b4">[5]</ref>, authors consider that there are exactly q = Q/K n unlabeled inputs for each class. We consider that this is non-practical, since in most applications there is no reason to think that this holds. We shall see in Section IV that this has strong implications in terms of performance, especially when q is small. Indeed, in practice the Q unlabeled examples are drawn uniformly at random in a pool containing the same amount of unlabeled inputs for each class. So, when Q is large, the central limit theorem tells us that the number of drawn inputs from each class should be similar, whereas it can be highly contrasted when Q is small, leading to an imbalanced case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed solution</head><p>Our method is illustrated in <ref type="figure">Figure 1</ref>. We first train a backbone deep neural network able to discriminate inputs from the base dataset</p><formula xml:id="formula_2">D base = {(x 1 , 1 ), ..., (x m , m )}, where x i ∈ R d and 1 ≤ i ≤ K b .</formula><p>The proposed methodology builds upon using this pretrained architecture as a generic feature extractor, what is referred to as transfer in the literature <ref type="bibr" target="#b26">[27]</ref>. Usually, a common way to extract features is to process data belonging to the novel dataset using the penultimate activation layer. Here, we obtain the extractor f ϕ : R d → R h , where ϕ are the learnable parameters trained using only the base dataset.</p><p>We then directly make use of the transferred representations</p><formula xml:id="formula_3">f ϕ (D novel ) = {f ϕ (x), x ∈ D novel }.</formula><p>Based on these, we build a k nearest neighbor graph using cosine similarity:</p><formula xml:id="formula_4">cos(f ϕ (x), f ϕ (y)) = f ϕ (x) f ϕ (y) f ϕ (x) 2 f ϕ (y) 2 .</formula><p>This graph contains as many vertices as the total number of inputs in the novel dataset (both labeled and unlabeled ones). Then, we train a model of simplified graph convolution model, that is supervised only for labeled inputs. The rationale behind this method is twofold: 1) the pretrained backbone should be able to find good discriminative features since it is trained on a sufficiently large labeled dataset 2) the graph-based interpolation technique should be able to benefit from both the supervised inputs and the unlabeled ones, resulting in significant gains in accuracy when compared to methods that would ignore the unlabeled data.</p><p>We show in the experiments that this method is also able to outperform other methods that use the unlabeled data especially when the number of labeled inputs is very limited.</p><p>The details of the proposed method are provided in the following paragraphs, first the pre-training stage (i.e. training the generic backbone), followed by the feature interpolation and logistic regression stages.</p><p>Pre-training: We follow the methodology introduced in <ref type="bibr" target="#b5">[6]</ref>. In more details the feature extractor f ϕ and a distance-based classifier D W b (parametrized by W b ) <ref type="bibr" target="#b28">[29]</ref> are trained on D base , where we compute the cosine distance between an input feature f ϕ (x i ) and each weight vector in W b in order to reduce the intra-class variations <ref type="bibr" target="#b8">[9]</ref>. The training process consists of two sub-stages: the first sub-stage utilizes rotationbased self-supervised learning technique <ref type="bibr" target="#b29">[30]</ref> where each input image is randomly rotated by a multiple of 90 degrees. We then co-train a linear classifier to tell which rotation was applied. Therefore, the total loss function of this sub-stage is given by:</p><formula xml:id="formula_5">L A = L class + L rotation .<label>(1)</label></formula><p>The second sub-stage fine-tunes the model with Manifold Mixup <ref type="bibr" target="#b30">[31]</ref> technique for a few more epochs, where the outputs of hidden layers in the neural network are linearly combined to help the trained model generalize better. The total loss in this sub-stage is given by:</p><formula xml:id="formula_6">L B = L ManifoldMixup + 0.5(L class + L rotation ).<label>(2)</label></formula><p>With this training process, we are able to obtain robust input representations that generalize well to novel classes. Feature interpolation: We consider fixed the pretrained parameters ϕ of f ϕ . Before training a new classifier C Wn on the transferred representations of the novel dataset, we propose to interpolate features using a graph.</p><p>In details, we define a graph G T (V, E) <ref type="bibr" target="#b20">[21]</ref> where vertices matrix V ∈ R (sKn+Q)×h contains the stacked features of labeled and unlabeled inputs <ref type="bibr" target="#b1">[2]</ref>. To build the adjacency matrix E ∈ R (sKn+Q)×(sKn+Q) , we first compute:</p><formula xml:id="formula_7">S[i, j] = cos(V[i, :], V[j, :]) if i = j 0 otherwise ,<label>(3)</label></formula><p>where V[i, :] denotes the i-th row of V. Note that in all backbone architectures we use in the experiments, the penultimate layers are obtained by applying a ReLU function, so that all coefficients in V are nonnegative. As a result, coefficients in S are nonnegative as well. Also, note that S is symmetric. Then, we only keep the value S[i, j] if it is one of the k largest values on the corresponding row or on the corresponding column in S. So, as soon as k ≥ (sK n + Q − 1), all values are kept. Otherwise, S contains many 0s.</p><p>Finally, we apply normalization on the resulting matrix:</p><formula xml:id="formula_8">E = D −1/2 SD −1/2 ,<label>(4)</label></formula><p>where D is the degree diagonal matrix defined as:</p><formula xml:id="formula_9">D[i, i] = j S[i, j].</formula><p>Therefore, the graph vertices represent all inputs (both labeled and unlabeled) of the novel dataset. Its nonzero weights are based on the cosine similarity between corresponding transferred representations.</p><p>We then apply feature propagation <ref type="bibr" target="#b22">[23]</ref> to obtain new features for each vertex. The formula is:</p><formula xml:id="formula_10">V new = (αI + E) κ "diffusion matrix" V,<label>(5)</label></formula><p>in which κ and α are both hyperparameters, and I is the identity matrix. The role of κ is important: providing κ is too small, the new feature of a vertex will only depend on its direct neighbors in the graph. Using larger values of κ allows to encompass for more indirect relationships. Using a too large value of κ might drown out the information by averaging over all inputs. Similarly, α allows to balance between the neighbors representations and self-ones. Logistic regression: Finally, a softmax classifier is trained using only the labeled vertices. We denote by V labeled new the subset of V new corresponding to labeled vertices, then the predicted resultsŶ can be written following this formula:</p><formula xml:id="formula_11">Y labeled = sof tmax(V labeled new W n ),<label>(6)</label></formula><p>where V labeled new ∈ R (sKn)×h ,Ŷ ∈ R (sKn)×Kn andŶ[i, j] denotes the probability of vertex i being categorized as being in the j-th class.</p><p>Prediction is performed using the same principle, but using unlabeled inputs instead: denote by V unlabeled new the subset of V new corresponding to unlabeled inputs, then we have the decision:Ŷ</p><formula xml:id="formula_12">unlabeled [i] = arg max j ((V unlabeled new W n )[i, j]).<label>(7)</label></formula><p>In <ref type="table" target="#tab_1">Table I</ref> we summarize the main parameters and hyperparameters of the considered problem and proposed solution. Let us point out that the proposed graph-based method does not contain any parameter to train. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We perform our experiments on 3 standardized few-shot classification datasets: miniImageNet <ref type="bibr" target="#b10">[11]</ref>, CUB <ref type="bibr" target="#b31">[32]</ref> and CIFAR-FS <ref type="bibr" target="#b15">[16]</ref>. These datasets are split into two parts: a) K b classes are chosen to train the backbone, called base classes, b) K n classes are drawn uniformly in the remaining classes to form the novel dataset, called novel classes. Among the K n drawn novel classes, s labeled inputs per class and a total of Q unlabeled inputs are drawn uniformly at random. As in most related works, unless mentioned otherwise all our experiments are performed using K n = 5 and Q/K n = 15. We perform a run of 10,000 random draws to obtain an accuracy score and indicate confidence scores (95%) when relevant.</p><p>miniImageNet: It consists of a subset of ImageNet <ref type="bibr" target="#b32">[33]</ref> that contains 100 classes and 600 images of size 84×84 pixels per class. According to the standard <ref type="bibr" target="#b9">[10]</ref>, we use 64 base classes to train the backbone and 20 novel classes to draw the novel datasets from. So, for each run, 5 classes are drawn uniformly at random among these 20 classes.</p><p>CUB: The dataset contains 200 classes and has a total of 11,788 images of size 84 × 84 pixels. We split it into 100 base classes to train the backbone and 50 novel classes to draw the novel datasets from.</p><p>CIFAR-FS: This dataset has 100 classes, each class contains 600 images of size 32 × 32 pixels. We use the same numbers as for the miniImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Backbone models and implementation details</head><p>We perform experiments using 2 different backbones as the structure of feature extractor f ϕ (x).</p><p>Wide residual networks (WRN) <ref type="bibr" target="#b33">[34]</ref>: We follow the settings in <ref type="bibr" target="#b5">[6]</ref> by choosing a WRN with 28 convolutional layers and a widening factor of 10. The output feature size h is 640.</p><p>Residual networks (ResNet18) <ref type="bibr" target="#b34">[35]</ref>: Our ResNet18 contains a total of 18 convolutional layers grouped into 8 blocks. Following the settings in <ref type="bibr" target="#b35">[36]</ref>, we remove the first two down-sampling layers and change the kernel size of the first convolutional layer to 3 × 3 pixels instead of 7 × 7 pixels. Here, h = 512.</p><p>For the pre-training stage and miniImageNet, we train all backbones for a total of 470 epochs from scratch using Adam optimizer <ref type="bibr" target="#b36">[37]</ref> and cross-entropy loss, including 400 epochs on the first sub-stage and 70 epochs on the second sub-stage. For the logistic regression, we train with the same optimizer and loss function for 1000 epochs with learning rate being 1e − 3 and weight decay being 5e − 6, which typically requires of the order of one second of computation on a modern GPU. Note that we observed that convergence usually occurs much quicker than 1000 epochs. In the In-Domain settings two stages are trained on the same dataset with base classes and novel classes respectively, while in the Cross-Domain settings we use these splits from two different datasets (e.g. base classes from miniImageNet and novel classes from CUB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with state-of-the-art methods</head><p>As a first experiment, we compare the raw performance of the proposed method with state-of-the-art solutions with WRN and ResNet18 as backbones. The results are presented in <ref type="table" target="#tab_1">Table II</ref>. We fixed α, k and κ respectfully with s = 1 and s = 5 for the proposed method, as it empirically gave the best results. Note that the sensitivity of these hyperparameters is discussed later in this section.</p><p>We point out that the proposed method reaches state-of-theart performance in both case of 1-shot and 5-shot classification for most of the time, whatever the choice of all considered datasets. Note that the gain we observe is higher in the 1-shot case than in the 5-shot case, this is expected as in the case of 1-shot, the unlabeled samples bring proportionally more information compared to the case of 5-shot. In the extreme case of s-shot, with s large enough, we expect the unlabeled samples to be almost useless.</p><p>We also perform experiments where the backbone has been trained using the base classes of miniImageNet but the fewshot task is performed using the novel classes of the CUB dataset. According to the results, we can draw conclusions very similar to the previous study, where the proposed method performs well for this specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparaison with other GNN methods</head><p>In this experiment we compare our performance on miniIm-ageNet with others that use Graph Neural Network to address the few-shot classification. As we can see in <ref type="table" target="#tab_1">Table III</ref>, with a three-stage training strategy, our proposed method has largely surpassed the current GNN based methods that train an entire model at once, given the transductive setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Importance of the parameter-free graph interpolation</head><p>In our work, we considered using a parameter-free graph interpolation technique to diffuse features between inputs. As mentioned in the related work section, there are many alternatives, but they come with additional parameters. In the next experiment, we compare the accuracy of the method when using GCN <ref type="bibr" target="#b20">[21]</ref> and GAT <ref type="bibr" target="#b21">[22]</ref>, instead of a simple interpolation. Results are presented in <ref type="table" target="#tab_1">Table IV</ref>. We note that the best results are obtained using our designed graph interpolation, which we believe to be due to the fact we use fewer parameters in total. Graph interpolation also has the interest of being many times faster to train. In our experiments, each run took about 0.65 seconds to train using graph interpolation versus 1.18 seconds for GCN and 22.42 seconds with GAT, which happens to lead to the worst performance of our considered methods.</p><p>It is worth pointing out that a drawback of the proposed method is that it requires to train a logistic regression model each time a batch prediction is required. In other words, it can be limiting in settings where predictions to make are streamed. However, the time required to train the logistic regression model remains very small in our experiments (less than one second).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Influence of Parameters</head><p>We then inquire the importance of various parameters of the task to the performance of the proposed method. We begin by varying the number of supervised inputs s, and consider two settings: one where we dispose of an average of Q/K n = 5 unsupervised inputs for each class and one where we dispose of Q/K n = 100 of them. Results are depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. As we can see, the performance of the method is highly influenced by the number of supervised inputs, as expected. Interestingly, there is a significant gap in accuracy between Q/K n = 5 and Q/K n = 100 for 1-shot setting, even if this gap diminishes as the number of supervised inputs is increased. In the next experiment, we draw in <ref type="figure">Figure 3</ref> the evolution of the performance of the method as a function of the number of unsupervised inputs Q, for 1-shot, 3-shot and 5-shot settings. This curve confirms two observations: a) in the case of 5-shot setting, the influence of the number of unsupervised inputs is little, and the accuracy of the method quickly reaches its pick and b) in the case of 1-shot setting, the number of unsupervised inputs significantly influences accuracy up to a few dozens. It is interesting to point out that about the same accuracy is achieved for 5-shot using Q = 1 and 1-shot using Q = 100, suggesting that 100 unsupervised inputs bring about the same usable information as 4 labeled inputs per class. In the next experiment we look at the influence of the parameters κ and α which respectively control to which power the diffusion matrix is taken and the importance of self-representations. In <ref type="figure">Figure 4</ref>, we draw the obtained mean accuracy as a function of κ, α and k. We use s = 1 and Q/K n = 15 in this experiment. There are multiple interesting conclusions to draw from this figure.</p><p>1) This curve justifies the previously mentioned choice of parameters, leading to the best performance. 2) We observe that when k is large and α is small, it is better not to use powers of the diffusion matrix. This is the only setting where this statement holds, emphasizing the fact that if the graph is not sparse and self-importance is low, powers of the diffusion   <ref type="figure">≤ 3)</ref>. This is an asset as it makes it simpler to find good hyperparameters.</p><p>4) The best results are achieved for smaller values of k, suggesting that cosine similarity between distant representations can be noisy and damaging to the performance of the method. 5) Note that in this experiment s + Q/K n = 16. So using k = 15 would ideally select exactly 15 neighbors of the same class for each input. Interestingly, this choice of k does not lead to the best performance, showing the graph structure is not perfectly aligned with classes. It is often disregarded the impact of class imbalance in the context of few-shot learning. As a matter of fact, since we only consider very few labeled examples, it does not make much sense to consider such a scenario. But in the context of transductive setting, it is highly probable that unlabeled inputs are imbalanced between classes. So we perform the next experiment by varying the number of examples chosen in two random classes from miniImageNet. We always make sure that the total number of queries to classify remains the same, that is 100. But we select q 1 of them in class 1 and 100 − q 1 of them in class 2.</p><p>In <ref type="figure" target="#fig_3">Figure 5</ref>, we depict the evolution of the accuracy of the proposed method, as a function of q 1 . As one can clearly see from this figure, there is an important influence of class imbalance towards the performance of the proposed method. This is expected as the generated graphs will have imbalanced communities as a consequence. This could be problematic to some application domains where such imbalance is expected to happen in considered datasets, as there is no direct way of correcting it. Obviously, if one has insights about the relative distribution between classes, simple data augmentation or sampling could be used for mitigation. However, this could be problematic to some application domains where such imbalance is expected to happen in considered datasets, as there is no direct way of correcting it. Obviously, if one has insights about the relative distribution between classes, simple data augmentation or sampling could be used for balancing this negative effect.</p><p>Finally, in <ref type="figure" target="#fig_4">Figure 6</ref>, we draw a representation of a typical graph obtained with the miniImageNet dataset, using Laplacian embedding <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. On this figure, we colored vertices depending on which class they belong to. Interestingly, this figure shows that some classes are easily separated in the graph, whereas others are much harder to discriminate. We believe that the main reason why these graphs are not perfectly segregating classes is because some dimensions obtained using the backbone are specialized on features completely irrelevant for the novel task. V. CONCLUSION In this paper we introduced a novel method to solve the few-shot classification problem. It consists in combining three steps: a pretrained transfer, a graph-based interpolation technique and a logistic regression.</p><p>By performing experiments on standardized vision datasets, we obtained state-of-the-art results, with the most important gains in the case of 1-shot classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Evolution of the accuracy of few-shot classification with miniImageNet (backbone: WRN) as a function of the number of supervised inputs s, and for various number of unsupervised queries q. We use α = 0.5, κ = 3 and k = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 Fig. 3 .</head><label>53</label><figDesc>Evolution of the accuracy of few-shot classification with miniImageNet (backbone: WRN) as a function of the number of query inputs Q, and for various number of unsupervised inputs s. We use α = 0.5, κ = 3 and k = min(10, sKn + Q − 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>20 Fig. 4 .</head><label>204</label><figDesc>Evolution of the accuracy of few-shot classification with miniImageNet (backbone: WRN) as a function of κ, α and k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Accuracy of 2-ways classification with unevenly distributed query data for each class, where the total number of query inputs remains constant. When q 1 = 1, we obtain the most imbalanced case, whereas q 1 = 50 corresponds to a balanced case. We use α = 0.5, κ = 3 and k = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Visualisation of a graph obtained using miniImageNet. Colors represent various classes. Vertices are placed close if they share many connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PARAMETERS</head><label>I</label><figDesc>AND HYPERPARAMETERS OF THE CONSIDERED PROBLEM AND PROPOSED SOLUTION (# STANDS FOR "NUMBER").</figDesc><table><row><cell cols="2">Novel dataset parameters</cell></row><row><cell>K n</cell><cell># classes</cell></row><row><cell>s</cell><cell># supervised inputs per class</cell></row><row><cell>Q</cell><cell>total # of unsupervised inputs</cell></row><row><cell cols="2">Proposed method hyperparameters</cell></row><row><cell>1 ≤ k &lt; sK n + Q</cell><cell># nearest neighbors to keep</cell></row><row><cell>κ ∈ N  *</cell><cell>power of the diffusion matrix</cell></row><row><cell>0 ≤ α ≤ 1</cell><cell>strength of self-representations</cell></row><row><cell cols="2">IV. EXPERIMENTAL VALIDATION</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II 1</head><label>II</label><figDesc>-SHOT AND 5-SHOT ACCURACY OF STATE-OF-THE-ART METHODS IN THE LITERATURE, COMPARED WITH THE PROPOSED SOLUTION. WE PRESENT RESULTS USING WRN AND RESNET18 AS BACKBONES. FOR THE PROPOSED SOLUTION, WE USE THE HYPERPARAMETERS α = 0.5, k = 10 AND κ = 3 FOR s = 1; α = 0.75, k = 15 AND κ = 1 FOR s = 5. SHOT AND 5-SHOT PERFORMANCE (ON MINIIMAGENET) COMPARISON WITH OTHER GNN BASED METHODS. IN OUR EXPERIMENT WE USE THE SAME HYPERPARAMETERS AS TABLE II. When k is small (here: k = 5 or k = 10), there is little</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>miniImageNet</cell></row><row><cell cols="2">Method</cell><cell></cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell cols="2">MAML [8]</cell><cell></cell><cell>ResNet18</cell><cell>49.61 ± 0.92%</cell><cell>65.72 ± 0.77%</cell></row><row><cell cols="2">Baseline++ [9]</cell><cell></cell><cell>ResNet18</cell><cell>51.87 ± 0.77%</cell><cell>75.68 ± 0.63%</cell></row><row><cell cols="2">Matching Networks [11]</cell><cell></cell><cell>ResNet18</cell><cell>52.91 ± 0.88%</cell><cell>68.88 ± 0.69%</cell></row><row><cell cols="2">ProtoNet [12]</cell><cell></cell><cell>ResNet18</cell><cell>54.16 ± 0.82%</cell><cell>73.68 ± 0.65%</cell></row><row><cell cols="2">SimpleShot [36]</cell><cell></cell><cell>ResNet18</cell><cell>63.10 ± 0.20%</cell><cell>79.92 ± 0.14%</cell></row><row><cell cols="2">S2M2 R [6]</cell><cell></cell><cell>ResNet18</cell><cell>64.06 ± 0.18%</cell><cell>80.58 ± 0.12%</cell></row><row><cell cols="2">LaplacianShot [38]</cell><cell></cell><cell>ResNet18</cell><cell>72.11 ± 0.19%</cell><cell>82.31 ± 0.14%</cell></row><row><cell cols="3">Transfer+Graph Interpolation (ours)</cell><cell>ResNet18</cell><cell>72.40 ± 0.24%</cell><cell>82.89 ± 0.14%</cell></row><row><cell cols="2">ProtoNet [12]</cell><cell></cell><cell>WRN</cell><cell>62.60 ± 0.20%</cell><cell>79.97 ± 0.14%</cell></row><row><cell cols="2">Matching Networks [11]</cell><cell></cell><cell>WRN</cell><cell>64.03 ± 0.20%</cell><cell>76.32 ± 0.16%</cell></row><row><cell cols="2">S2M2 R [6]</cell><cell></cell><cell>WRN</cell><cell>64.93 ± 0.18%</cell><cell>83.18 ± 0.11%</cell></row><row><cell cols="2">SimpleShot [36]</cell><cell></cell><cell>WRN</cell><cell>65.87 ± 0.20%</cell><cell>82.09 ± 0.14%</cell></row><row><cell cols="2">SIB [39]</cell><cell></cell><cell>WRN</cell><cell>70.00 ± 0.60%</cell><cell>79.20 ± 0.40%</cell></row><row><cell cols="2">BD-CSPN [40]</cell><cell></cell><cell>WRN</cell><cell>70.31 ± 0.93%</cell><cell>81.89 ± 0.60%</cell></row><row><cell cols="2">LaplacianShot [38]</cell><cell></cell><cell>WRN</cell><cell>74.86 ± 0.19%</cell><cell>84.13 ± 0.14%</cell></row><row><cell cols="3">Transfer+Graph Interpolation (ours)</cell><cell>WRN</cell><cell>76.50 ± 0.23%</cell><cell>85.23 ± 0.13%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CUB</cell></row><row><cell cols="2">Method</cell><cell></cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell cols="2">S2M2 R [6]</cell><cell></cell><cell>ResNet18</cell><cell>71.43 ± 0.28%</cell><cell>85.55 ± 0.52%</cell></row><row><cell cols="2">ProtoNet [12]</cell><cell></cell><cell>ResNet18</cell><cell>72.99 ± 0.88%</cell><cell>86.64 ± 0.51%</cell></row><row><cell cols="2">Matching Networks [11]</cell><cell></cell><cell>ResNet18</cell><cell>73.49 ± 0.89%</cell><cell>84.45 ± 0.58%</cell></row><row><cell cols="2">LaplacianShot [38]</cell><cell></cell><cell>ResNet18</cell><cell>80.96%</cell><cell>88.68%</cell></row><row><cell cols="3">Transfer+Graph Interpolation (ours)</cell><cell>ResNet18</cell><cell>86.05 ± 0.20%</cell><cell>90.87 ± 0.10%</cell></row><row><cell cols="2">S2M2 R [6]</cell><cell></cell><cell>WRN</cell><cell>80.68 ± 0.81%</cell><cell>90.85 ± 0.44%</cell></row><row><cell cols="3">Transfer+Graph Interpolation (ours)</cell><cell>WRN</cell><cell>88.35 ± 0.19%</cell><cell>92.14 ± 0.10%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>miniImageNet−→CUB</cell></row><row><cell cols="2">Method</cell><cell></cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell cols="2">Baseline++ [9]</cell><cell></cell><cell>ResNet18</cell><cell>40.44 ± 0.75%</cell><cell>56.64 ± 0.72%</cell></row><row><cell cols="2">SimpleShot [36]</cell><cell></cell><cell>ResNet18</cell><cell>48.56%</cell><cell>65.63%</cell></row><row><cell cols="2">LaplacianShot [38]</cell><cell></cell><cell>ResNet18</cell><cell>55.46%</cell><cell>66.33%</cell></row><row><cell cols="3">Transfer+Graph Interpolation (ours)</cell><cell>ResNet18</cell><cell>51.67 ± 0.24%</cell><cell>69.83 ± 0.18%</cell></row><row><cell cols="2">Manifold Mixup [31]</cell><cell></cell><cell>WRN</cell><cell>46.21 ± 0.77%</cell><cell>66.03 ± 0.71%</cell></row><row><cell cols="2">S2M2 R [6]</cell><cell></cell><cell>WRN</cell><cell>48.24 ± 0.84%</cell><cell>70.44 ± 0.75%</cell></row><row><cell cols="3">Transfer+Graph Interpolation (ours)</cell><cell>WRN</cell><cell>58.63 ± 0.25%</cell><cell>73.46 ± 0.17%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR-FS</cell></row><row><cell cols="2">Method</cell><cell></cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell cols="2">BD-CSPN [40]</cell><cell></cell><cell>WRN</cell><cell>72.13 ± 1.01%</cell><cell>82.28 ± 0.69%</cell></row><row><cell cols="2">S2M2 R [6]</cell><cell></cell><cell>WRN</cell><cell>74.81 ± 0.19%</cell><cell>87.47 ± 0.13%</cell></row><row><cell cols="2">SIB [39]</cell><cell></cell><cell>WRN</cell><cell>80.00 ± 0.60%</cell><cell>85.30 ± 0.40%</cell></row><row><cell cols="3">Transfer+Graph Interpolation (ours)</cell><cell>WRN</cell><cell>83.90 ± 0.22%</cell><cell>88.76 ± 0.15%</cell></row><row><cell cols="2">TABLE III</cell><cell></cell><cell></cell></row><row><cell>1-Method</cell><cell>1-shot</cell><cell>5-shot</cell><cell></cell></row><row><cell>GNN [2]</cell><cell>50.33 ± 0.36%</cell><cell cols="2">66.41 ± 0.63%</cell></row><row><cell>TPN [5]</cell><cell>55.51 ± 0.86%</cell><cell cols="2">69.86 ± 0.65%</cell></row><row><cell>wDAE-GNN [4]</cell><cell>61.07 ± 0.15%</cell><cell cols="2">76.75 ± 0.11%</cell></row><row><cell>Transfer+Graph Interpolation (ours)</cell><cell>76.50 ± 0.23%</cell><cell cols="2">85.23 ± 0.13%</cell></row><row><cell cols="4">matrix are likely to over-smooth the representations of</cell></row><row><cell>neighbors.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV 1</head><label>IV</label><figDesc>-SHOT AND 5-SHOT ACCURACY ON MINIIMAGENET, WHEN USING THE WRN BACKBONE AND VARIOUS GRAPH NEURAL NETWORKS. WE USE THE SAME HYPERPARAMETERS AS TABLE II AND APPLY THEM TO ALL METHODS (WITH THE EXCEPTION OF κ FOR GCN AND GAT).</figDesc><table><row><cell>Method</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Transfer+GAT</cell><cell>65.38 ± 0.89%</cell><cell>76.00 ± 0.67%</cell></row><row><cell>Transfer+GCN</cell><cell>75.88 ± 0.23%</cell><cell>84.51 ± 0.13%</cell></row><row><cell>Transfer+Graph Interpolation</cell><cell>76.47 ± 0.23%</cell><cell>85.23 ± 0.13%</cell></row><row><cell>*GAT is evaluated with 600 test runs.</cell><cell></cell><cell></cell></row></table><note>sensitivity to both α and κ (for κ</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interestingly, the proposed method requires to tune few hyperparameters, and these have a little impact on accuracy. We thus believe that it is an applicable solution to many practical problems.</p><p>There are still open questions to be addressed, such as the case of imbalanced classes, or settings where prediction must be performed on streaming data, one input at a time.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01102</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding taskrelevant features for few-shot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Metalearning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08136</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised learning (chapelle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Few-shot learning via saliencyguided hallucination of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2770" to="2779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8680" to="8689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Few-shot learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05046</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A two-stage approach to few-shot learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at nearzero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="488" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05236</idno>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Laplacian regularized few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15486</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Empirical bayes transductive metalearning with synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12696</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Prototype rectification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10713</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A short tutorial on graph laplacians, laplacian embedding, and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
