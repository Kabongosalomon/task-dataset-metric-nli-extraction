<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Canonical Tensor Decomposition for Knowledge Base Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
						</author>
						<title level="a" type="main">Canonical Tensor Decomposition for Knowledge Base Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of Knowledge Base Completion can be framed as a 3rd-order binary tensor completion problem. In this light, the Canonical Tensor Decomposition (CP) (Hitchcock, 1927) seems like a natural solution; however, current implementations of CP on standard Knowledge Base Completion benchmarks are lagging behind their competitors. In this work, we attempt to understand the limits of CP for knowledge base completion. First, we motivate and test a novel regularizer, based on tensor nuclear p-norms. Then, we present a reformulation of the problem that makes it invariant to arbitrary choices in the inclusion of predicates or their reciprocals in the dataset. These two methods combined allow us to beat the current state of the art on several datasets with a CP decomposition, and obtain even better results using the more advanced ComplEx model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In knowledge base completion, the learner is given triples <ref type="bibr">(subject, predicate, object)</ref> of facts about the world, and has to infer new triples that are likely but not yet known to be true. This problem has attracted a lot of attention <ref type="bibr" target="#b26">(Nickel et al., 2016a;</ref><ref type="bibr" target="#b24">Nguyen, 2017)</ref> both as an example application of large-scale tensor factorization, and as a benchmark of learning representations of relational data.</p><p>The standard completion task is link prediction, which consists in answering queries <ref type="bibr">(subject, predicate, ?)</ref> or <ref type="bibr">(?, predicate, object)</ref>. In that context, the canonical decomposition of tensors (also called CANDECOMP/PARAFAC or CP) <ref type="bibr" target="#b13">(Hitchcock, 1927)</ref> is known to perform poorly compared to more specialized methods. For instance, DistMult <ref type="bibr" target="#b36">(Yang et al., 2014)</ref>, a particular case of CP which shares the fac-Proceedings of the 35 th International Conference on Machine <ref type="bibr">Learning, Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s). tors for the subject and object modes, was recently shown to have state-of-the-art results <ref type="bibr" target="#b16">(Kadlec et al., 2017)</ref>. This result is surprising because DistMult learns a tensor that is symmetric in the subject and object modes, while the datasets contain mostly non-symmetric predicates.</p><p>The goal of this paper is to study whether and how CP can perform as well as its competitors. To that end, we evaluate three possibilities.</p><p>First, as <ref type="bibr" target="#b16">Kadlec et al. (2017)</ref> showed that performances for these tasks are sensitive to the loss function and optimization parameters, we re-evaluate CP with a broader parameter search and a multiclass log-loss.</p><p>Second, since the best performing approaches are less expressive than CP, we evaluate whether regularization helps. On this subject, we show that the standard regularization used in knowledge base completion does not correspond to regularization with a tensor norm. We then propose to use tensor nuclear p-norms <ref type="bibr" target="#b10">(Friedland &amp; Lim, 2018)</ref>, with the goal of designing more principled regularizers. Third, we propose a different formulation of the objective, in which we model separately predicates and their inverse: for each predicate pred, we create an inverse predicate pred −1 and create a triple (obj, pred −1 , sub) for each training triple <ref type="bibr">(sub, pred, obj)</ref>. At test time, queries of the form <ref type="bibr">(?, pred, obj)</ref> are answered as (obj, pred −1 , ?). Similar formulations were previously used by <ref type="bibr" target="#b29">Shen et al. (2016)</ref> and <ref type="bibr" target="#b15">Joulin et al. (2017)</ref>, but for different models for which there was no clear alternative, so the impact of this reformulation has never been evaluated.</p><p>To assess whether the results we obtain are specific to CP, we also carry on the same experiments with a state-of-theart model, ComplEx <ref type="bibr" target="#b34">(Trouillon et al., 2016)</ref>. ComplEx has the same expressivity as CP in the sense that it can represent any tensor, but it implements a specific form of parameter sharing. We perform all our experiments on 5 common benchmark datasets of link prediction in knowledge bases.</p><p>Our results first confirm that within a reasonable time budget, the performance of both CP and ComplEx are highly dependent on optimization parameters. With systematic parameter searches, we obtain better results for ComplEx than what was previously reported, confirming its status as a state-of-the-art model on all datasets. For CP, the results arXiv:1806.07297v1 [stat.ML] 19 Jun 2018 are still way below its competitors.</p><p>Learning and predicting with the inverse predicates, however, changes the picture entirely. First, with both CP and ComplEx, we obtain significant gains in performance on all the datasets. More precisely, we obtain state-of-the-art results with CP, matching those of ComplEx. For instance, on the benchmark dataset FB15K <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, the mean reciprocal rank of vanilla CP and vanilla ComplEx are 0.40 and 0.80 respectively, and it grows to 0.86 for both approaches when modeling the inverse predicates.</p><p>Finally, the new regularizer we propose based on the nuclear 3-norm, does not dramatically help CP, which leads us to believe that a careful choice of regularization is not crucial for these CP models. Yet, for both CP and Com-plEx with inverse predicates, it yields small but significant improvements on the more difficult datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Tensor Factorization of Knowledge Bases</head><p>We describe in this section the formal framework we consider for knowledge base completion and more generally link prediction in relational data, the learning criteria, as well as the approaches that we will discuss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Link Prediction in Relational Data</head><p>We consider relational data that comes in the form of triples <ref type="bibr">(subject, predicate, object)</ref>, where the subject and the object are from the same set of entities. In knowledge bases, these triples represent facts about entities of the world, such as (Washington, capital_of, USA). A training set S contains triples of indices S = {(i 1 , j 1 , k 1 ), ..., (i |S| , j |S| , k |S| )} that represent predicates that are known to hold. The validation and test sets contain queries of the form (?, j, k) and (i, j, ?), created from triples (i, j, k) that are known to hold but held-out from the training set. To give orders of magnitude, the largest datasets we experiment on, FB15K and YAGO3-10, contain respectively 15k/1.3k and 123k/37 entities/predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Tensor Decomposition for Link Prediction</head><p>Relational data can be represented as a {0, 1}-valued third order tensor Y ∈ {0, 1} N ×P ×N , where N is the total number of entities and P the number of predicates, with Y i,j,k = 1 if the relation (i, j, k) is known. In the rest of the paper, the three modes will be called the subject mode, the predicate mode and the object mode respectively. Tensor factorization algorithms can thus be used to infer a predicted tensorX ∈ R N ×P ×N that approximates Y in a sense that we describe in the next subsection. Validation/test queries (?, j, k) are answered by ordering entities i by decreasing values ofX i ,j,k , whereas queries (i, j, ?) are answered by ordering entities k by decreasing values ofX i,j,k .</p><p>Several approaches have considered link prediction as a low-rank tensor decomposition problem. These models then differ only by structural constraints on the learned tensor. Three models of interest are:</p><p>CP. The canonical decomposition of tensors, also called CANDECOM/PARAFAC <ref type="bibr" target="#b13">(Hitchcock, 1927)</ref>, represents a tensor X ∈ R N1×N2×N3 as a sum of R rank one tensors u</p><formula xml:id="formula_0">(1) r ⊗ u (2) r ⊗ u (3) r (with ⊗ the tensor product) where r ∈ {1, ..., R}, and u (m) r ∈ R Nm : X = R r=1 u (1) r ⊗ u (2) r ⊗ u (3) r .</formula><p>A representation of this decomposition, and the score of a specific triple is given in <ref type="figure" target="#fig_0">Figure 1</ref> (a). Given X, the smallest R for which this decomposition holds is called the canonical rank of X.</p><p>DistMult. In the more specific context of link prediction, it has been suggested in <ref type="bibr" target="#b1">Bordes et al. (2011);</ref><ref type="bibr" target="#b25">Nickel et al. (2011)</ref> that since both subject and object mode represent the same entities, they should have the same factors. DistMult <ref type="bibr" target="#b36">(Yang et al., 2014</ref>) is a version of CP with this additional constraint. It represents a tensor X ∈ R N ×P ×N as a sum of rank-1 tensors u</p><p>(1)</p><formula xml:id="formula_1">r ⊗ u (2) r ⊗ u (1) r : X = R r=1 u (1) r ⊗ u (2) r ⊗ u (1) r .</formula><p>ComplEx. By contrast with the first models that proposed to share the subject and object mode factors, Dist-Mult yields a tensor that is symmetric in the object and subject modes. The assumption that the data tensor can be properly approximated by a symmetric tensor for Knowledge base completion is not satisfied in many practical cases (e.g., while (Washington, capital_of, USA) holds, (USA, capital_of, Washington) does not). Com-plEx <ref type="bibr" target="#b34">(Trouillon et al., 2016)</ref> proposes an alternative where the subject and object modes share the parameters of the factors, but are complex conjugate of each other. More precisely, this approach represents a real-valued tensor X ∈ R N1×N2×N3 as the real part of a sum of R complex-valued rank one tensors u</p><p>(1)</p><formula xml:id="formula_2">r ⊗ u (2) r ⊗ u (1) r where r ∈ {1, ..., R}, and u (m) r ∈ C Nm X = Re R r=1 u (1) r ⊗ u (2) r ⊗ u (1) r , where u<label>(1)</label></formula><p>r is the complex conjugate of u</p><p>r . This decomposition can represent any real tensor <ref type="bibr" target="#b34">(Trouillon et al., 2016)</ref>. The good performances of DistMult on notoriously nonsymmetric datasets such as FB15K or WN18 are surprising. First, let us note that for the symmetricity to become an issue, one would have to evaluate queries (i, j, ?) while also trying to answer correctly to queries of the form (?, j, i) for a non-symmetric predicate j. The ranking for these two queries would be identical, and thus, we can expect issues with relations such as capital_of . In FB15K, those type of problematic queries make up only 4% of the test set and thus, have a small impact. On WN18 however, they make up 60% of the test set. We describe in appendix 8.1 a simple strategy for DistMult to have a high filtered MRR on the hierarchical predicates of WN18 despite its symmetricity assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training</head><p>Previous work suggested ranking losses <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, binary logistic regression <ref type="bibr" target="#b34">(Trouillon et al., 2016)</ref> or sampled multiclass log-loss <ref type="bibr" target="#b16">(Kadlec et al., 2017)</ref>. Motivated by the solid results in <ref type="bibr" target="#b15">Joulin et al. (2017)</ref>, our own experimental results, and with a satisfactory speed of about two minutes per epoch on FB15K, we decided to use the full multiclass log-loss.</p><p>Given a training triple (i, j, k) and a predicted tensor X, the instantaneous multi-class log-</p><formula xml:id="formula_4">loss i,j,k (X) is i,j,k (X) = (1) i,j,k (X) + (3) i,j,k (X) (1) (1) i,j,k (X) = −X i,j,k + log k exp(X i,j,k ) (3) i,j,k (X) = −X i,j,k + log i exp(X i ,j,k ) .</formula><p>These two partial losses are represented in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>. For CP, the final tensor is computed by finding a minimizer of a regularized empirical risk formulation, where the factors u</p><p>r are weighted in a data-dependent manner by w</p><p>S , which we describe below:</p><formula xml:id="formula_7">min (u (d) r ) d=1..3 r=1..R (i,j,k)∈S i,j,k R r=1 u (1) r ⊗ u (2) r ⊗ u (3) r + λ R r=1 3 d=1 w (d) S u (d) r 2 2 ,<label>(2)</label></formula><p>where is the entry-wise multiplication of vectors. For DistMult and ComplEx, the learning objective is similar, up to the appropriate parameter sharing and computation of the tensor.</p><p>As discussed in Section 3.2, the weights w</p><formula xml:id="formula_8">(d)</formula><p>S may improve performances when some rows/columns are sampled more than others. They appear naturally in optimization with stochastic gradient descent when the regularizer is applied only to the parameters that are involved in the computation of the instantaneous loss. For instance, in the case of the logistic loss with negative sampling used by <ref type="bibr" target="#b34">Trouillon et al. (2016)</ref>, denoting by q d i the marginal probability (over S) that index i appears in mode d of a data triple, these weights are w (d) S,i = q d i + α for some α &gt; 0 that depends on the negative sampling scheme.</p><p>We focus on redefining the loss (2.3) and the regularizer (2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>We discuss here in more details the work that has been done on link prediction in relational data and on regularizers for tensor completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Link Prediction in Relational Data</head><p>There has been extensive research on link prediction in relational data, especially in knowledge bases, and we review here only the prior work that is most relevant to this paper. While some approaches explicitly use the graph structure during inference <ref type="bibr" target="#b21">(Lao et al., 2011)</ref>, we focus here on representation learning and tensor factorization methods, which are the state-of-the-art on the benchmark datasets we use. We also restrict the discussion to approaches that only use relational information, even though some approaches have been proposed to leverage additional types <ref type="bibr" target="#b20">(Krompass et al., 2015;</ref><ref type="bibr" target="#b22">Ma et al., 2017)</ref> or external word embeddings <ref type="bibr" target="#b33">(Toutanova &amp; Chen, 2015)</ref>.</p><p>We can divide the first type of approaches into two broad categories. First, two-way approaches score a triple (i, j, k) depending only on bigram interaction terms of the form subject-object, subject-predicate, and predicate-object. Even though they are tensor approximation algorithms of limited expressivity, two-way models based on translations TransE, or on bag-of-word representations <ref type="bibr" target="#b15">(Joulin et al., 2017)</ref> have proved competitive on many benchmarks. Yet, methods using three-way multiplicative interactions, as described in the previous section, show the strongest performances <ref type="bibr" target="#b1">(Bordes et al., 2011;</ref><ref type="bibr" target="#b11">Garcia-Duran et al., 2016;</ref><ref type="bibr" target="#b27">Nickel et al., 2016b;</ref><ref type="bibr" target="#b34">Trouillon et al., 2016)</ref>. Compared to general-purpose tensor factorization methods such as CP, a common feature of these approaches is to share parameters between objects and subjects modes <ref type="bibr" target="#b25">(Nickel et al., 2011)</ref>, an idea that has been widely accepted except for the twoway model of <ref type="bibr" target="#b15">Joulin et al. (2017)</ref>. DistMult <ref type="bibr" target="#b36">(Yang et al., 2014)</ref> is the extreme case of this parameter sharing, in which the predicted tensor is symmetric in the subject and object modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Regularization for Matrix Completion</head><p>Norm-based regularization has been extensively studied in the context of matrix completion. The trace norm (or nuclear norm) has been proposed as a convex relaxation of the rank  for matrix completion in the setting of rating prediction, with strong theoretical guarantees <ref type="bibr" target="#b5">(Candès &amp; Recht, 2009</ref>). While efficient algorithms to solve the convex problems have been proposed (see e.g. <ref type="bibr" target="#b4">Cai et al., 2010;</ref><ref type="bibr" target="#b14">Jaggi et al., 2010)</ref>, the practice is still to use the matrix equivalent of the nonconvex formulation (2.3). For the trace norm (nuclear 2-norm), in the matrix case, the regularizer simply becomes the squared 2-norm of the factors and lends itself to alternating methods or SGD optimization <ref type="bibr" target="#b19">Koren et al., 2009)</ref>. When the samples are not taken uniformly at random from a matrix, some other norms are preferable to the usual nuclear norm. The weighted trace norm reweights elements of the factors based on the marginal rows and columns sampling probabilities, which can improve sample complexity bounds when sampling is non-uniform <ref type="bibr" target="#b9">(Foygel et al., 2011;</ref><ref type="bibr" target="#b23">Negahban &amp; Wainwright, 2012)</ref>. Direct SGD implementations on the nonconvex formulation implicitly take this reweighting rule into account and were used by the winners of the Netflix challenge (see <ref type="bibr">Srebro &amp; Salakhutdinov, 2010, Section 5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Tensor Completion and Decompositions</head><p>There is a large body of literature on low-rank tensor decompositions (see <ref type="bibr" target="#b18">Kolda &amp; Bader, 2009</ref>, for a comprehensive review). Closely related to our work is the canonical decomposition of tensor (also called CANDECOMP/PARAFAC or CP) <ref type="bibr" target="#b13">(Hitchcock, 1927)</ref>, which solves a problem similar to (4.1) without the regularization (i.e., λ = 0), and usually the square loss.</p><p>Several norm-based regularizations for tensors have been proposed. Some are based on unfolding a tensor along each of its modes to obtain matricizations, and either regularize by the sum of trace norms of the matricizations <ref type="bibr" target="#b32">(Tomioka et al., 2010)</ref> or write the original tensor as a sum of tensors T k , regularizing their respective kth matricizations with the trace norm <ref type="bibr" target="#b35">(Wimalawarne et al., 2014)</ref>. However, in the large-scale setting, even rank-1 approximations of matricizations involve too many parameters to be tractable.</p><p>Recently, the tensor trace norm (nuclear 2-norm) was proposed as a regularizer for tensor completion <ref type="bibr" target="#b37">Yuan &amp; Zhang (2016)</ref>, and an algorithm based on the generalized conditional gradient has been developed by <ref type="bibr" target="#b6">Cheng et al. (2016)</ref>. This algorithm requires, in an inner loop, to compute a (constrained) rank-1 tensor that has largest dot-product with the gradient of the data-fitting term (gradient w.r.t. the tensor argument). This algorithm is efficient in our setup only with the square error loss (instead of the multiclass log-loss), because the gradient is then a low-rank + sparse tensor when the argument is low-rank. However, on large-scale knowledge bases, the state of the art is to use a binary log-loss or a multiclass log-loss <ref type="bibr" target="#b34">(Trouillon et al., 2016;</ref><ref type="bibr" target="#b16">Kadlec et al., 2017)</ref>; in that case, the gradient is not adequately structured, thereby causing the approach of <ref type="bibr" target="#b6">(Cheng et al., 2016)</ref> to be too computationally costly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Nuclear p-Norm Regularization</head><p>As discussed in Section 3, norm-based regularizers have proved useful for matrices. We aim to reproduce these successes with tensor norms. We use the nuclear p-norms defined by <ref type="bibr" target="#b10">Friedland &amp; Lim (2018)</ref>. As shown in Equation (2.3), the community has favored so far a regularizer based on the square Frobenius norms of the factors <ref type="bibr" target="#b36">(Yang et al., 2014;</ref><ref type="bibr" target="#b34">Trouillon et al., 2016)</ref>. We first show that the unweighted version of this regularizer is not a tensor norm. Then, we propose4 a variational form of the nuclear 3-norm to replace the usual regularization at no additional computational cost when used with SGD. Finally, we discuss a weighting scheme analogous to the weighted trace-norm proposed in <ref type="bibr" target="#b30">Srebro &amp; Salakhutdinov (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">From Matrix Trace-Norm to Tensor Nuclear Norms</head><p>To simplify notation, let us introduce the set of CP decompositions of a tensor X of rank at most R:</p><formula xml:id="formula_9">U R (X) = (u (d) r ) d=1..3 r=1..R X = R r=1 u (1) r ⊗ u (2) r ⊗ u (3) r , ∀r, d, u (d) r ∈ R N d .</formula><p>We will study the family of regularizers:</p><formula xml:id="formula_10">Ω α p (u) = 1 3 R r=1 3 d=1 u (d) r α p .</formula><p>Note that with p = α = 2, we recover the familiar squared Frobenius norm regularizer used in (2.3). Similar to showing that the squared Frobenius norm is a variational form of the trace norm on matrices (i.e., its minimizers realize the trace norm, inf M =U V T 1 2 ( U 2 F + V 2 F ) = M * ), we start with a technical lemma that links our regularizer with a function on the spectrum of our decompositions.</p><formula xml:id="formula_11">Lemma 1. min u∈U R (X) 1 3 R r=1 3 d=1 u (d) r α p = min u∈U R (X) R r=1 3 d=1 u (d) r α/3 p .</formula><p>Moreover, the minimizers of the left-hand side satisfy:</p><formula xml:id="formula_12">u (d) r p = 3 3 d =1 u (d ) r p .</formula><p>Proof. See Appendix 8.2. This Lemma motivates the introduction of the set of p-norm normalized tensor decompositions:</p><formula xml:id="formula_13">U p R (X) = (σ r , (ũ r )) r=1..R σ r = 3 d=1 u (d) r p , u (d) r = u (d) r u (d) r p , ∀r, d, u ∈ U R (X) .</formula><p>Lemma 2, shows that Ω α p behaves as an α/D penalty over the CP spectrum for tensors of order D. We recover the nuclear norm for matrices when α = p = 2.</p><p>Using Lemma 2, we have :</p><formula xml:id="formula_14">min u∈U R (X) Ω 2 2 (u) ≤ η ⇐⇒ min (σ,ũ)∈U 2 R (X) σ 2/3 ≤ η 3/2<label>(3)</label></formula><p>We show that the sub-level sets of the term on the right are not convex, which implies that Ω 2 2 is not the variational form of a tensor norm, and hence, is not the tensor analog to the matrix trace norm.</p><p>Proposition 1. The function over third order-tensors of R N1×N2×N3 defined as</p><formula xml:id="formula_15">|||X||| = min σ 2/3 (σ,ũ) ∈ U 2 R (X), R ∈ N is not convex. Proof. See Appendix 8.2.</formula><p>Remark 1. <ref type="bibr">Cheng et al. (2016, Appendix D)</ref> already showed that regularizing with the square Frobenius norm of the factors is not related to the trace norm for tensors of order 3 and above, but their observation is that the regularizer is not positively homogeneous, i.e., min u∈αU R (X) Ω 2 2 (u) = |α| min u∈U R (X) Ω 2 2 (u). Our result in Proposition 1 is stronger in that we show that this regularizer is not a norm even after the rescaling (4.1) to make it homogeneous.</p><formula xml:id="formula_16">The nuclear p-norm of X ∈ R N1×N2×N3 for p ∈ [1, +∞], is defined in Friedland &amp; Lim (2018) as X * ,p := min σ 1 (σ,ũ) ∈ U p R (X), R ∈ N .</formula><p>Given an estimated upper bound on the optimal R, the original problem (2.3) can then be re-written as a non-convex problem using the equivalence in Lemma 2:</p><formula xml:id="formula_17">min (u (d) r ) d=1..3 r=1..R (i,j,k)∈S i,j,k R r=1 u (1) r ⊗ u (2) r ⊗ u (3) r + λ 3 R r=1 3 d=1 u (d) r 3 p .<label>(4)</label></formula><p>This variational form suggests to use p = 3, as a means to make the regularizer separable in each coefficients, given that then u</p><formula xml:id="formula_18">(d) r 3 p = n d i=1 u (d) r,i | 3 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weighted Nuclear p-Norm</head><p>Similar to the weighted trace-norm for matrices, the weighted nuclear 3-norm can be easily implemented by keeping the regularization terms corresponding to the sampled triplets only, as discussed in Section 3.2. This leads to a formulation of the form</p><formula xml:id="formula_19">min (u (d) r ) d=1..3 r=1..R (i,j,k)∈S i,j,k R r=1 u (1) r ⊗u (2) r ⊗u (3) r (5) + λ 3 R r=1 u (1) r,i | 3 + u (2) r,j | 3 + u (3) r,k | 3 .</formula><p>For an example (i, j, k), only the parameters involved in the computation ofX i,j,k are regularized. The computational complexity is thus the same as the currently used weighted Frobenius norm regularizer. With q (1) (resp. q (2) , q (3) ) the marginal probabilities of sampling a subject (resp. predicate, object), the weighting implied by this regularization scheme is</p><formula xml:id="formula_20">X * ,3,w = ( 3 q (1) ⊗ 3 q (2) ⊗ 3 q (3) ) X * ,3</formula><p>We justify this weighting only by analogy with the matrix case discussed by <ref type="bibr" target="#b30">(Srebro &amp; Salakhutdinov, 2010)</ref>: to make the weighted nuclear 3-norm of the all 1 tensor independent of its dimensions for a uniform sampling (since the nuclear 3-norm grows as 3 √ M N P for an (M, N, P ) tensor).</p><p>Comparatively, for the weighted version of the nuclear 2norm analyzed in <ref type="bibr" target="#b37">Yuan &amp; Zhang (2016)</ref>, the nuclear 2-norm of the all 1 tensor scales like √ N M P . This would imply a formulation of the form</p><formula xml:id="formula_21">min (u (d) r ) d=1..3 r=1..R (i,j,k)∈S i,j,k R r=1 u (1) r ⊗ u (2) r ⊗ u (3) r + λ 3 R r=1 3 d=1 q (d) u (d) r 3 2 .<label>(6)</label></formula><p>Contrary to formulation (4.2), the optimization of formulation (4.2) with a minibatch SGD leads to an update of every coefficients for each mini-batch considered. Depending on the implementation, and size of the factors, there might be a large difference in speed between the updates of the weighted nuclear {2, 3}-norm. In our implementation, this difference for CP is of about 1.5× in favor of the nuclear 3-norm on FB15K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">A New CP Objective</head><p>Since our evaluation objective is to rank either the lefthand side or right-hand side of the predicates in our dataset, what we are trying to achieve is to model both predicates and their reciprocal. This suggests appending to our input the reciprocals of each predicates, thus factorizing [Y ; 2Ỹ ] rather than Y , where [ ; 2 ] is the mode-2 concatenation, and Y i,j,k =Ỹ k,j,i . After that, we only need to model the object fibers of this new tensor Y . We represent this transformation in <ref type="figure" target="#fig_0">Figure 1 (c)</ref>. This reformulation has an important side-effect: it makes our algorithm invariant to the arbitrary choice of including a predicate or its reciprocal in the dataset. This property was introduced as "Semantic Invariance" in <ref type="bibr" target="#b0">Bailly et al. (2015)</ref>. Another way of achieving this invariance property would be to find the flipping of predicates that lead to the smallest model. In the case of a CP decomposition, we would try to find the flipping that leads to lowest tensor rank. This seems hopeless, given the NP-hardness of computing the tensor rank. <ref type="table" target="#tab_0">Dataset  N  P  Train Valid Test  WN18  41k  18  141k  5k  5k  WN18RR  41k  11  87k  3k  3k  FB15K  15k  1k  500k  50k  60k  FB15K-237 15k 237 272k</ref> 18k 20k YAGO3-10 123k 37 1M 5k 5k (i, j, k) becomes :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More precisely, the instantaneous loss of a training triple</head><formula xml:id="formula_22">i,j,k (X) = − X i,j,k + log k exp(X i,j,k ) (7) − X k,j+P,i + log i exp(X k,j+P,i ) .</formula><p>At test time we useX i,j,: to rank possible right hand sides for query (i, j, ?) andX k,j+P,: to rank possible left hand sides for query (?, j, k).</p><p>Using CP to factor the tensor described in <ref type="formula">(5)</ref>, we beat the previous state of the art on many benchmarks, as shown in <ref type="table">Table 2</ref>. This reformulation seems to help even the ComplEx decomposition, for which parameters are shared between the entity embeddings of the first and third mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We conducted all experiments on a Quadro GP 100 GPU. The code is available at https://github.com/ facebookresearch/kbc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets and Experimental Setup</head><p>WN18 and FB15K are popular benchmarks in the Knowledge Base Completion community. The former comes from the WordNet database, was introduced in <ref type="bibr" target="#b3">Bordes et al. (2014)</ref> and describes relations between words. The most frequent types of relations are highly hierarchical (e.g., hypernym, hyponym). The latter is a subsampling of Freebase limited to 15k entities, introduced in <ref type="bibr" target="#b2">Bordes et al. (2013)</ref>. It contains predicates with different characteristics (e.g., oneto-one relations such as capital_of to many-to-many such as actor_in_film).</p><p>Toutanova &amp; Chen (2015) and <ref type="bibr" target="#b7">Dettmers et al. (2017)</ref> identified train to test leakage in both these datasets, in the form of test triplets, present in the train set for the reciprocal predicates. Thus, both of these authors created two modified datasets: FB15K-237 and WN18RR. These datasets are harder to fit, so we expect regularization to have more impact. <ref type="bibr" target="#b7">Dettmers et al. (2017)</ref> also introduced the dataset YAGO3-10, which is larger in scale and doesn't suffer from leakage. All datasets statistics are shown in <ref type="table" target="#tab_0">Table 1</ref>.  <ref type="bibr" target="#b7">Dettmers et al. (2017)</ref> and <ref type="bibr" target="#b16">Kadlec et al. (2017)</ref>. † Results taken as best from <ref type="bibr" target="#b7">Dettmers et al. (2017)</ref> and <ref type="bibr" target="#b34">Trouillon et al. (2016)</ref>. We give the origin of each result on the Best Published row in appendix.</p><p>In all our experiments, we distinguish two settings: Reciprocal, in which we use the loss described in equation <ref type="formula">(5)</ref> and Standard, which uses the loss in equation <ref type="formula" target="#formula_7">(2.</ref>3). We compare our implementation of CP and ComplEx with the best published results, then the different performances between the two settings, and finally, the contribution of the regularizer in the reciprocal setting. In the Reciprocal setting, we compare the weighted nuclear 3-norm (N3) against the regularizer described in (2.3) (FRO). In preliminary experiments, the weighted nuclear 2-norm described in (4.2) did not seem to perform better than N3 and was slightly slower. We used Adagrad <ref type="bibr" target="#b8">(Duchi et al., 2011)</ref> as our optimizer, whereas <ref type="bibr" target="#b16">Kadlec et al. (2017)</ref> favored Adam <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2014)</ref>, because preliminary experiments didn't show improvements.</p><p>We ran the same grid for all algorithms and regularizers on the FB15K, FB15K-237, WN18, WN18RR datasets, with a rank set to 2000 for ComplEx, and 4000 for CP. Our grid consisted of two learning rates: 10 −1 and 10 −2 , two batch-sizes: 25 and 100, and regularization coefficients in [0, 10 −3 , 5.10 −3 , 10 −2 , 5.10 −2 , 10 −1 , 5.10 −1 ]. On YAGO3-10, we limited our models to rank 1000 and used batch-sizes 500 and 3000, the rest of the grid was identical. We used the train/valid/test splits provided with these datasets and measured the filtered Mean Reciprocal Rank (MRR) and Hits@10 <ref type="bibr" target="#b2">(Bordes et al. (2013)</ref>). We used the filtered MRR on the validation set for early stopping and report the corresponding test metrics. In this setting, an epoch for ComplEx with batch-size 100 on FB15K took about 110s and 325s for a batch-size of 25. We trained for 100 epochs to ensure convergence, reported performances were reached within the first 25 epochs.</p><p>All our results are reported in <ref type="table">Table 2</ref> and will be discussed in the next subsections. Besides our implementations of CP and ComplEx, we include the results of ConvE and DistMult in the baselines. The former because <ref type="bibr" target="#b7">Dettmers et al. (2017)</ref> includes performances on the WN18RR and YAGO3-10 benchmarks, the latter because of the good performances on FB15K of DistMult and the extensive experiments on WN18 and FB15K reported in <ref type="bibr" target="#b16">Kadlec et al. (2017)</ref>. The performances of DistMult on FB15K-237, WN18RR and YAGO3-10 may be slightly underestimated, since our baseline CP results are better. To avoid clutter, we did not include in our table of results algorithms that make use of external data such as types <ref type="bibr" target="#b20">(Krompass et al., 2015)</ref>, external word embeddings <ref type="bibr" target="#b33">(Toutanova &amp; Chen, 2015)</ref>, or using path queries as regularizers <ref type="bibr" target="#b12">(Guu et al., 2015)</ref>. The published results corresponding to these methods are subsumed in the "Best Published" line of <ref type="table">Table 2</ref>, which is taken, for every single metric and dataset, as the best published result we were able to find.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Reimplementation of the Baselines</head><p>The performances of our reimplementation of CP and Com-plEx appear in the middle rows of previously published results used a rank of around 200; the more extensive search for optimization/regularization parameters and the use of nuclear 3-norm instead of the usual regularization are also most likely part of the explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Standard vs Reciprocal</head><p>In this section, we compare the effect of reformulation (5), that is, the middle and bottom rows of <ref type="table">Table 2</ref>. The largest differences are obtained for CP, which becomes a state of the art contender going from 0.2 to 0.95 filtered MRR on WN18, or from 0.46 to 0.86 filtered MRR on FB15K.For ComplEx, we notice a weaker, but consistent improvement by using our reformulation, with the biggest improvements observed on FB15K and YAGO3-10. Following the analysis in <ref type="bibr" target="#b2">Bordes et al. (2013)</ref>, we show in <ref type="table">Table 3</ref> the average filtered MRR as a function of the degree of the predicates. We compute the average in and out degrees on the training set, and separate the predicates in 4 categories : 1-1, 1-m, m-1 and m-m, with a cut-off at 1.5 on the average degree. We include reciprocal predicates in these statistics. That is, a predicate with an average in-degree of 1.2 and average out-degree of 3.2 will count as a 1-m when we predict its right-hand side, and as an m-1 when we predict its left-hand side. Most of our improvements come from the 1-m and m-m categories, both on ComplEx and CP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Frobenius vs Nuclear 3</head><p>We focus now on the effect of our norm-based N3 regularizer, compared to the Frobenius norm regularizer favored by the community. Comparing the four last rows of Table 2, we notice a small but consistent performance gain across datasets. The biggest improvements appear on the harder datasets WN18RR, FB15K-237 and YAGO3-10. We checked on WN18RR the significance of that gain with a Signed Rank test on the rank pairs for CP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Effect of Optimization Parameters</head><p>During these experiments, we noticed a heavy influence of optimization hyper-parameters on final results. This influence can account for as much as 0.1 filtered MRR and is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The difference is large even after 100 epochs and the effect is inverted in the two settings, making it hard to choose the batchsize a priori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Discussion</head><p>The main contribution of this paper is to isolate and systematically explore the effect of different factors for large-scale knowledge base completion. While the impact of optimization parameters was well known already, neither the effect of the formulation (adding reciprocals doubles the mean reciprocal rank on FB15K for CP) nor the impact of the regularization was properly assessed. The conclusion is that the CP model performs nearly as well as the competitors when each model is evaluated in its optimal configuration. We believe this observation is important to assess and prioritize directions for further research on the topic.</p><p>In addition, our proposal to use nuclear p-norm as regularizers with p = 2 for tensor factorization in general is of independent interest.</p><p>The results we present leave several questions open. Notably, whereas we give definite evidence that CP itself can perform extremely well on these datasets as long as the problem is formulated correctly, we do not have a strong theoretical justification as to why the differences in performances are so significant.</p><p>1 0 0 0 = 0 0 0 1 = 1</p><p>Let A = 1 2 I 2 , the mean of these two matrices. Identifying A with a 2 × 2 × 1 tensor A to obtain the decomposition (σ, u) yielding |||A|||, we have that the matrix A can be written as r . This comes from the fact that u (3) r is a normalized 1 × 1 vector, so its only entry is equal to 1. We then write that trace Tr(A) = R r=1 σ r Tr(u (1) r ⊗ u (2) r ) ≤ R r=1 σ r by Cauchy-Schwarz. Hence σ 1 ≥ Tr(A) = 1. Moreover, we have σ 2/3 ≥ σ 1 with equality only for σ with at most one non-zero coordinate. Since A is of rank 2, its representation has at least 2 non-zero coordinates, hence |||A||| = σ 2/3 &gt; 1, which contradicts convexity. This proof can naturally be extended to tensors of any sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Best Published results</head><p>We report in <ref type="table" target="#tab_4">Table 4</ref> the references for each of the results in <ref type="table">Table 2</ref>    <ref type="table">Table 2</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) On the left, the link between the score of a triple (i,j,k) and the tensor estimated via CP. (b) In the middle, the two type of fiber losses that we will consider. (c) On the right, our semantically invariant reformulation, the first-mode fibers become third-mode fibers of the reciprocal half of the tensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Effect of the batch-size on FB15K in the Standard (top) and Reciprocal (bottom) settings, other parameters being equal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 (Table 3 .</head><label>23</label><figDesc>Standard setting). We only kept the results for the nuclear 3-norm, which didn't seem to differ from those with the Frobenius norm. Our results are slightly better than their published counterparts, going from 0.33 to 0.46 filtered MRR on FB15K for CP and 0.70 to 0.80 for ComplEx. This might be explained in part by the fact that in the Standard setting (2.3) we use a multi-class log-loss, whereas<ref type="bibr" target="#b34">Trouillon et al. (2016)</ref> used binomial negative log-likelihood. Another reason for this increase can be the large rank of 2000 that we chose, where ComplEx Reciprocal 0.88 0.92 0.71 0.87 Average MRR per relation type on FB15K.</figDesc><table><row><cell></cell><cell>1-1 m-1 1-m m-m</cell></row><row><cell>CP Standard CP Reciprocal ComplEx Standard</cell><cell>0.45 0.71 0.24 0.44 0.77 0.92 0.71 0.86 0.87 0.92 0.59 0.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>in the article.</figDesc><table><row><cell>Model</cell><cell cols="2">Metric Result</cell><cell>Reference</cell></row><row><cell>WN18</cell><cell cols="2">MRR H@10 0.97 0.94</cell><cell>Trouillon et al. (2016) Ma et al. (2017)</cell></row><row><cell>WN18RR</cell><cell cols="3">MRR H@10 0.51 Dettmers et al. (2017) 0.46 Dettmers et al. (2017)</cell></row><row><cell>FB15K</cell><cell cols="2">MRR H@10 0.93 0.84</cell><cell>Kadlec et al. (2017) Shen et al. (2016)</cell></row><row><cell>FB15K-237</cell><cell>MRR H@10</cell><cell>0.32 0.49</cell><cell>Dettmers et al. (2017) Dettmers et al. (2017)</cell></row><row><cell>YAGO3-10</cell><cell>MRR H@10</cell><cell>0.52 0.66</cell><cell>Dettmers et al. (2017) Dettmers et al. (2017)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>References for the Best Published row in</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Armand Joulin and Maximilian Nickel for valuable discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Leaves and the root only appear on one side of the queries (i, p, j) and hence won't have any problems with the symmetricity. We now focus on an internal node i. It has n children (c i k ) k=1..n and one ancestor a i . Assuming n &gt; 2, the MRR associated with this node will be higher if the query (i, p, ?) yields the ranked list [c i 1 , ..., c i n , a i ]. Indeed, the filtered rank of the n queries (i, p, c i k ) will be 1 while the filtered rank of the query (a i , p, i) will be n + 1.</p><p>Counting the number of queries for which the filtered rank is 1, we see that they far outweigh the queries for which the filtered rank is n + 1 in the final filtered MRR. For each internal nodes, n queries lead to a rank of 1, and only 1 to a rank of n + 1. For the root, n queries with a rank of 1, for the leaves, n d queries with a rank of 1.</p><p>Our final filtered MRR is :</p><p>Hence for big hierarchies such as hyponym or hypernym in WN, we expect the filtered MRR of DistMult to be high even though its modeling assumptions are incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Proofs</head><p>Lemma 2.</p><p>Moreover, the minimizers of the left-hand side satisfy:</p><p>Proof. First, we characterize the minima :</p><p>We study a summand, for c i , a i &gt; 0 :</p><p>Using constrained optimization techniques, we obtain that this minimum is obtained for :</p><p>and has value ( 3 d=1 a i ) α/3 , which completes the proof.</p><p>Proposition 2. The function over third order-tensors of R N1×N2×N3 defined as |||X||| = min σ 2/3 (σ,ũ) ∈ U R (X), R ∈ N is not convex.</p><p>Proof. We first study elements of R 2×2×1 , tensors of order 3 associated with matrices of size 2 × 2. We have that</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semantically Invariant Tensor Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakhnenko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<editor>Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exact matrix completion via convex optimization. Foundations of Computational mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">717</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable and sound low-rank tensor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1114" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01476</idno>
		<title level="m">Convolutional 2d knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning with the weighted trace-norm under arbitrary sampling distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2133" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
		<title level="m">Nuclear norm of higher-order tensors. Mathematics of Computation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1255" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining two and three-way embedding models for link prediction in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="715" to="742" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple algorithm for nuclear norm regularized problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sulovsk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast linear model for knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10881</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge Base Completion: Baselines Strike Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleindienst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Type-constrained representation learning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krompass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Type-based multiple embedding representations for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="717" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Restricted strong convexity and weighted matrix completion: Optimal bounds with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Negahban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1665" to="1697" />
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An overview of embedding models of entities and relationships for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Review of Relational Machine Learning for Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast maximum margin matrix factorization for collaborative prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="713" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Implicit reasonet: Modeling large-scale structured relationships with shared memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Collaborative filtering in a non-uniform world: Learning with the weighted trace norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2056" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Maximum-margin matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1329" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Estimation of low-rank tensors via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1010.0789</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multitask learning meets tensor factorization: task imputation via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wimalawarne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2825" to="2833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On tensor completion via nuclear norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1031" to="1068" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">We forget any modeling issues we might have</title>
		<imprint/>
	</monogr>
	<note>but focus on the symmetricity assumption in Distmult</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
