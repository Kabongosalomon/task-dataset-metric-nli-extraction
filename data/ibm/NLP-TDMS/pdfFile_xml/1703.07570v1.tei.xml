<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
							<email>1florian.chabot@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CEA-LIST Vision and Content Engineering Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
							<email>mohamed.chaouch@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CEA-LIST Vision and Content Engineering Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
							<email>jaonary.rabarisoa@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CEA-LIST Vision and Content Engineering Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Céline</forename><surname>Teulière</surname></persName>
							<email>celine.teuliere@univ-bpclermont.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Pascal Institute</orgName>
								<orgName type="institution" key="instit2">Blaise Pascal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
							<email>thierry.chateau@univ-bpclermont.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Pascal Institute</orgName>
								<orgName type="institution" key="instit2">Blaise Pascal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel approach, called Deep MANTA (Deep Many-Tasks), for many-task vehicle analysis from a given image. A robust convolutional network is introduced for simultaneous vehicle detection, part localization, visibility characterization and 3D dimension estimation. Its architecture is based on a new coarse-to-fine object proposal that boosts the vehicle detection. Moreover, the Deep MANTA network is able to localize vehicle parts even if these parts are not visible. In the inference, the network's outputs are used by a real time robust pose estimation algorithm for fine orientation estimation and 3D vehicle localization. We show in experiments that our method outperforms monocular state-of-the-art approaches on vehicle detection, orientation and 3D location tasks on the very challenging KITTI benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last years, traffic scene analysis has been improved thanks to deep learning approaches which paves the way to multiple applications, especially, autonomous driving. Impressive recent work in 2D object detection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> already provides important information related to scenes content but does not yet allow to describe objects in the 3D real world scene. In this paper, we are interested in both 2D and 3D vehicle analysis from monocular images in the context of self-driving cars. This is a relevant research field because currently most cars are equipped with a single camera. For an autonomously driving vehicle, it is essential to understand the traffic and predict critical situations based on the information extracted from the image of the scene. For the recovery of speed and direction of the surrounding cars, 3D vehicle localization and orientation jointly used with temporal description are necessary. Additionally, for proper traffic understanding it is important to describe surrounding vehicles in a fine way. For example, <ref type="bibr">Figure 1</ref>. System outputs. Top: 2D vehicle bounding boxes, vehicle part localization and part visibility. In this example, red dots correspond to visible parts, green dots to occluded parts and blue dots to self-occluded parts. Bottom: 3D vehicle bounding box localization and 3D vehicle part localization. The camera is represented in blue. correct localization of high lights is required to interpret vehicle direction indicators, for which knowledge of the exact location of vehicle parts is needed. Finally, for interpretation of the overall scene the characterization of the visibility of vehicle parts needs also to be obtained. Thus it will be known if a vehicle is hidden by other vehicles or environment obstacles. Here we propose an approach that, given a single image, provides accurate vehicle detections, vehicle part localization, vehicle part visibility, fine orientation, 3D localization and 3D template (3D dimension). <ref type="figure">Figure 1</ref> illustrates the outputs of our approach.</p><p>Our first contribution is to encode 3D vehicle information using characteristic points of vehicles. The underlying idea is that 3D vehicle information can be recovered using monocular images because vehicles are rigid objects with well known geometry. Our approach localizes vehicle parts even if these parts are hidden due to occlusion, truncation or self-occlusion in the image. These parts are found using regression instead of using a part detector. In this way, the approach predicts the position of hidden parts which are essential for robust 3D information recovering. We use a 3D vehicle dataset composed of 3D meshes with real dimensions. Several vertices are annotated for each 3D model. These 3D points correspond to vehicle parts (such as wheels, headlights, etc) and define a 3D shape for each 3D model. The main idea of the approach is to recover the projection of these 3D points (2D shape) in the input image for each detected vehicle. Then, the best corresponding 3D model for each detection box is chosen. 2D/3D matching is performed between 2D shapes and selected 3D shapes to recover vehicle orientation and 3D location.</p><p>The second contribution is the introduction of the Deep Coarse-to-fine Many-Task Convolutional Neural Network called Deep MANTA. This network outputs accurate 2D vehicle bounding boxes, 2D shapes, part visibility and 3D vehicle templates. Its architecture contains several originalities. Firstly, inspired by the Region proposal network <ref type="bibr" target="#b32">[33]</ref>, the MANTA model is able to propose coarse 2D bounding boxes which are then iteratively refined, by multi-pass forward, to provide accurate scored 2D detections. Secondly, this network is based on the many-task concept. That means that the same feature vector can be used to predict many tasks. We optimize in the same time six tasks: region proposal, detection, 2D box regression, part localization, part visibility and 3D template prediction.</p><p>The last contribution is related to the training dataset. Deep neural networks require many samples and labels to be efficiently learned. Furthermore, it is very fastidious and almost impossible to annotate manually vehicle parts which are not visible. For this purpose, we propose a semiautomatic annotation process using 3D models to generate labels on real images for the Deep MANTA training. Labels from 3D models (geometry information, visibility, etc) are automatically projected onto real images providing a large training dataset without labour-intensive annotation work.</p><p>In the next section, related work is reviewed. The section 3 explains the proposed model. Finally, we show that our approach outperforms monocular state-of-the-art methods related to vehicle detection, orientation and 3D localization on the very challenging KITTI dataset <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Object analysis is a well studied topic and we divide it into two main categories: 2D object detection/coarse pose estimation and 3D object detection/fine pose estimation.</p><p>2D Object detection and coarse pose estimation. There are two ways to perform 2D object detection. The first one is the standard sliding window scheme used in many detection systems as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref>. The second one is the 2D object proposal based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1]</ref>. The goal of object proposal methods is to propose several boxes with high objectness confidence score. These proposals are then given to a detector which is able to classify objects and background. The main advantage of object proposal methods is the processing time because that considerably reduces the search space. In parallel, Deep Convolutional Neural Networks (CNN) have proven their effectiveness in many computer vision fields such as object classification <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>, object detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref> and scene segmentation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9]</ref>. Thus, the success of object proposal methods as well as CNN, leads people to directly learn Region Proposal Networks (RPN) sharing weights with the down-stream detection network <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b17">18]</ref>. RPN provides strong objectness confidence regions of interest computed on deep feature maps. Experiments show that this kind of method increases detection accuracy. The proposed approach uses the RPN framework but uses several steps of 2D bounding box refinement to significantly increase object detection performance. 2D object detection is often associated with pose estimation and many methods address the two issues. They generally divide the viewing sphere in several bins to learn multi-class models where each bin corresponds to a class <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>. These approaches allow to get coarse information on objects and do not provide continuous viewpoint estimation.</p><p>3D Object detection and fine pose estimation. To go further than 2D reasoning, several approaches are designed to detect vehicles in 3D space and are able to give a detailed 3D object representation. A part of them consists in fitting 3D models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17]</ref>, active shape model <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref> or predicting 3D voxel patterns <ref type="bibr" target="#b38">[39]</ref> to recover the exact 3D pose and detailed object representation. These methods generally use an initialization step providing the 2D bounding box and the coarse viewpoint information. More recently, people have proposed to use 3D object proposals generated while using monocular images <ref type="bibr" target="#b6">[7]</ref> or disparity maps <ref type="bibr" target="#b7">[8]</ref>. In these approaches, 3D object proposals are projected in 2D bounding boxes and given to a CNN based detector which jointly predicts the class of the object proposal and the object fine orientation (using angle regression). In the proposed approach, vehicle fine orientation estimation is found using a robust 2D/3D vehicle part matching: the 2D/3D pose matrix is computed using all vehicle parts (visible or hidden) in contrast to other methods such as <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24]</ref> which focus on visible parts. That clearly increases the precision of orientation estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep MANTA approach</head><p>In this section, we describe the proposed approach for 2D/3D vehicle analysis from monocular images. Our system has two main steps. First, the input image is passed through the Deep MANTA network that outputs 2D scored bounding boxes, associated vehicle geometry (vehicle part   Using these outputs, the inference step allows to choose the best corresponding 3D template using template similarity Tj and then performs 2D/3D pose computation using the associated 3D shape.</p><formula xml:id="formula_0">{B i ,2 } {V i } {t j } {B i ,1 } {B i ,3 } {S i } {S j 3 d }, {B j 3 d } RPN Conv</formula><formula xml:id="formula_1">{B j },{S j },{V j },{T j } {T j } {S j }</formula><p>coordinates, 3D template similarity) and part visibility properties. The Deep MANTA network architecture is detailed in the section 3.3. The second step is the inference which uses Deep MANTA outputs and a 3D vehicle dataset to recover 3D orientations and locations. This step is detailed in the section 3.4. In this method, we use a dataset of 3D shapes and one of 3D templates. These two datasets encode the variability of vehicles in terms of dimension, type, and shape. These datasets are presented in the section 3.1. In the section 3.2, we define the adopted 2D/3D vehicle model for a given vehicle in a monocular image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D shape and template datasets</head><p>We use a dataset of M 3D models corresponding to several types of vehicles (Sedan, SUV, etc). For each 3D model m, we annotate N vertices (called 3D parts). These parts correspond to relevant vehicle regions. For one 3D model m, we denote its 3D shape aligned in canonical view as S 3d m = (p 1 , p 2 , .., p N ) with p k = (x k , y k , z k ) corresponding to the 3D coordinate of the k th part. The 3D template (i.e 3D dimension) associated to the 3D model m is de-</p><formula xml:id="formula_2">fined ast 3D m = (w m , h m , l m )</formula><p>where w m , h m , l m are the width, the height and the length of the 3D model respectively. <ref type="figure">Figure 3</ref> shows some examples from the 3D shape dataset {S 3d m } m∈{1,..,M } and the 3D template dataset {t 3d m } m∈{1,..,M } .</p><p>. . . <ref type="figure">Figure 3</ref>. Some examples from the 3D template and 3D shape dataset. Each 3D model m (first line) is associated to a 3D templatet 3d m (second line) and a 3D shapeS 3d m (third line). The 3D shape corresponds to manually annotated vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">2D/3D vehicle model</head><p>We represent each vehicle in a monocular image with a 2D/3D model. It is formally defined by the following attributes:</p><formula xml:id="formula_3">(B, B 3d , S, S 3d , V) B = (c x , c y , w, h) is the 2D vehicle bounding box in the image where (c x , c y )</formula><p>is the center and (w, h) represents the width and the height respectively.</p><formula xml:id="formula_4">B 3d = (c x , c y , c z , θ, t)</formula><p>is the 3D bounding box characterized by its 3D center (c x , c y , c z ), its orientation θ and its 3D template t = (w, h, l) corresponding to its 3D real size.</p><formula xml:id="formula_5">S = {q k = (u k , v k )} k∈{1,..,N } is the vehicle 2D part coor- dinates in the image. S 3d = {p k = (x k , y k , z k )} k∈{1,..,N } is the vehicle 3D part coordinates in the 3D real word co- ordinate system. V = {v k } k∈{1,.</formula><p>.,N } is the part visibility vector where v k denotes the visibility class of the k th part. Four classes of visibility are defined: (1) visible if the part is observed in the image, (2) occluded if the part is occluded by another object, (3) self-occluded if the part is occluded by the vehicle and (4) truncated if the part is out of the image. <ref type="figure" target="#fig_2">Figure 4</ref> shows an example of a 2D/3D vehicle model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep MANTA Network</head><p>The Deep MANTA network is designed to detect vehicles using a coarse-to-fine bounding box proposal as well as to output other finer attributes such as vehicle part localization, part visibility, and template similarity.</p><p>Coarse-to-fine forward. Given an entire input image, the network returns a first set of K object proposals B 1 = {B i,1 } i∈{1,..,K} as the region proposal network proposed by <ref type="bibr" target="#b32">[33]</ref>. These regions are then extracted from a feature map and pooled to a fixed size using ROI Pooling introduced by <ref type="bibr" target="#b13">[14]</ref>. Extracted regions are forwarded in a net-work (sharing some weights with the first level) and refined by offset transformations. A second set of K objects</p><formula xml:id="formula_6">B 2 = {B i,2 } i∈{1,.</formula><p>.,K} is proposed. This operation is repeated one last time to provide the final set of bounding box B 3 . These three levels of refinement are illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. This procedure differs than Faster-RCNN <ref type="bibr" target="#b32">[33]</ref> in that our iterative refinement steps overcome the constraints of large object scale variations and provide more accurate detection. Furthermore, in our approach, ROI pooled regions are extracted on the first convolution feature maps for keeping high resolution to detect hard vehicles.</p><p>Many-task prediction. The Deep MANTA architecture outputs a final bounding box set <ref type="bibr" target="#b2">3</ref> , the MANTA network also returns all 2D vehicle part coordinates S i , part visibility V i and 3D template similarity T i . The template similarity vector T i is defined as T i = {r m } m∈{1,..,M } . r m = (r x , r y , r z ) corresponds to the three scaling factors to apply on the 3D templatet 3d m to fit the real 3D template of the detected vehicle i. This vector encodes the similarity between the detected vehicle and all the 3D templates {t 3d m } m∈{1,..,M } of the 3D template dataset.</p><formula xml:id="formula_7">B 3 = {B i,3 } i∈{1,..,K} . For each bounding box B i,</formula><p>At this stage of the approach, non-maximum suppression is performed to remove redundant detections. This provides a new set of K detections and associated attributes</p><formula xml:id="formula_8">{B j , S j , V j , T j } j∈{1,..,K } .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Deep MANTA Inference</head><p>The inference step uses the Deep MANTA network outputs, the 3D shape dataset {S 3d m } m∈{1,..,M } and the 3D template dataset {t 3d m } m∈{1,..,M } defined in 3.1 to recover 3D information. Given a vehicle detection j provided by the Deep MANTA network, the inference consists in two steps. In the first step, we choose the closest 3D template c ∈ {1, .., M } in the 3D template dataset {t 3d m } m∈{1,..,M } using the template similarity T j = {r m } m∈{1,..,M } returned by the network. For each samplet 3d m of the 3D template dataset we apply the scaling transformation r m . The resulting 3D templates are defined by {t 3d m } m∈{1,..,M } . The best 3D template c is the one that minimizes the distance between t 3d m andt 3d m :</p><formula xml:id="formula_9">c = argmin m∈{1,..,M } d(t 3d m , t 3d m ).</formula><p>In other words, the best 3D template is the one that is predicted closer to (1, 1, 1) by the Deep MANTA network.</p><p>In the second step, 2D/3D matching is applied using 3D shapeS 3d c . It is rescaled to fit the 3D template t j = t 3d c . Then, a pose estimation algorithm is performed to match the rescaled 3D shapeS 3d c with the 2D shape S j using a standard 2D/3D matching <ref type="bibr" target="#b19">[20]</ref>. This last step provides the 3D bounding box B 3d j and the 3D part coordinates S 3d j . The last block in <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the inference step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep MANTA Training</head><p>This section defines all the tasks of the MANTA network and the associated loss functions. In the following, we consider three levels of refinement l ∈ {1, 2, 3} and five functions to minimize: L rpn , L det , L parts , L vis and L temp . L rpn is the RPN loss function defined in <ref type="bibr" target="#b32">[33]</ref>. L det is the detection loss function focusing on discriminating vehicle and background bounding box as well as regressing bounding boxes. L parts is the loss corresponding to vehicle part localization. L vis is the loss related to part visibility. L temp is the loss related to template similarity. We use the Faster-RCNN framework <ref type="bibr" target="#b32">[33]</ref> based on RPN to learn the end-toend MANTA model. Given an input image, the network joint optimization minimizes the global function:</p><formula xml:id="formula_10">L = L 1 + L 2 + L 3 with L 1 = L rpn , L 2 = i L 2 det (i) + L 2 parts (i), L 3 = i L 3 det (i) + L 3 parts (i) + L vis (i) + L temp (i),</formula><p>where i is the index of a proposal object. These three losses correspond to the three levels of refinement of the Deep MANTA architecture: finer is the level, bigger is the amount of information learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Many-task loss functions</head><p>Here, we will detail the different task losses used in the global function presented above. In the following, each object proposal at each level of refinement l, is indexed by i and it is represented by its box B i,l = (c x i,l , c y i,l , w i,l , h i,l ). The closest ground-truth vehicle box B to B i,l is selected. Associated ground-truth parts S, ground-truth visibility V and ground-truth template t are also selected (see section 3.2). We denote the standard log softmax loss as P and the robust SmoothL1 loss defined in <ref type="bibr" target="#b13">[14]</ref> as R.</p><p>Detection loss. The object proposal i at the refinement level l is assigned to a class label C i,l . C i,l is 1 if the object proposal is a vehicle and 0 otherwise. The classification criteria is the overlap between the box B i,l and the groundtruth box B. The predicted class returned by Deep MANTA network for the proposal is C * i,l . A target box regression vector ∆ i,l = (δ x , δ y , δ w , δ h ) is also defined as follows:</p><formula xml:id="formula_11">δ x = (c x i,l − c x )/w δ w = log(w i,l /w) δ y = (c y i,l − c y )/h δ h = log(h i,l /h)</formula><p>The predicted regression vector returned by Deep MANTA network is ∆ * i,l . The detection loss function is defined by:</p><formula xml:id="formula_12">L l det (i) = λ cls P (C * i,l , C i,l ) + λ reg C i,l R(∆ * i,l − ∆ i,l )</formula><p>with λ cls and λ reg the regularization parameters of box classification and box regression respectively. Part loss. Using the ground-truth parts S = (q 1 , .., q N ) and the box B i,l associated to the object proposal i at level l, normalized vehicle parts S i,l = (q 1 , ..,q N ) are computed as follows:q</p><formula xml:id="formula_13">k = ( u k − c x i,l w i,l , v k − c y i,l h i,l ).</formula><p>The predicted normalized parts are S * i,l . The part loss function is defined as:</p><formula xml:id="formula_14">L l parts (i) = λ parts C i,l R(S * i,l − S i,l )</formula><p>with λ parts the regularization parameter of part loss. Visibility loss. This loss is only optimized on the final level of refinement l = 3. The ground-truth visibility vector V i = V is assigned to the object proposal i. The predicted visibility vector is V * i . The visibility loss function is defined as:</p><formula xml:id="formula_15">L vis (i) = λ vis C i,3 P (V * i , V i )</formula><p>with λ vis the regularization parameter of visibility loss. Template similarity loss. This loss is only optimized on the final level of refinement l = 3. Instead of directly optimizing the three dimensions of the 3D template t, we encode it as a vector T using the 3D template dataset as explained in 3.3. For training, the log function is applied to each element of T for better normalization (similarity values are thus in <ref type="figure">[−1, 1]</ref>). The ground-truth template similarity vector vector T i = T is assigned to the object proposal i. The predicted template similarity vector is T * i . The template similarity loss function is defined as:</p><formula xml:id="formula_16">L temp (i) = λ temp C i,3 R(T * i − T i )</formula><p>with λ temp the regularization parameter of template similarity loss. Notice that if the object proposal i is not positive (i.e C i,l = 0) the loss functions associated to bounding box regression, part location, visibility and template similarity are null because it does not make sense to optimize vehicle properties on background regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semi-automatic annotation</head><p>A semi-automatic annotation process is used to provide useful labels to train our Deep MANTA network (vehicles part coordinates, part visibility, 3D template). To perform the annotation process, we only need a weakly annotated real dataset providing 3D bounding boxes of vehicle and a 3D CAD dataset. For this purpose, we use a 3D CAD dataset composed of M 3D car models. We manually annotate N vertices on each 3D model. For each vehicle in the weakly annotated real dataset, we choose automatically the best corresponding 3D model in the 3D model dataset. This is done by choosing the 3D model which has its 3D bounding box closest to the real 3D vehicle bounding box in the image (in terms of 3D dimensions). 3D parts associated to the chosen CAD are projected onto the image to get 2D part coordinates. The visibility of each projected part is computed using a mesh of visibility. This mesh is a low resolution 3D model where each face is associated to an annotated vehicle 3D part. <ref type="figure" target="#fig_3">Figure 5</ref> illustrates this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the proposed approach on the challenging KITTI object detection benchmark dedicated to autonomous driving <ref type="bibr" target="#b11">[12]</ref>. This dataset is composed of 7481 training images and 7518 testing images. The calibration matrix is given. Since ground truth annotations for the testing set are not released, we use train/validation splits from the training set to validate our method. To compare our approach to other state-of-the-art methods, we use two train/val splits: val1 used by <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref> and val2 used by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>. This is a means to compare our approach to these methods for tasks which are not initially evaluated on the KITTI benchmark. We use the 3D CAD dataset provided by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6]</ref> composed of M = 103 3D vehicle models for semi-automatic annotation. We annotate N = 36 vehicle parts on each 3D model. We train the Deep MANTA using the GoogLenet <ref type="bibr" target="#b35">[36]</ref> and the VGG16 <ref type="bibr" target="#b34">[35]</ref> architectures with the standard stochastic gradient descent optimization. The Deep MANTA is initialized using pre-trained weights learned on ImageNet. We use 7 aspect ratios and 10 scales for the RPN providing 70 anchors at each feature map location as proposed by <ref type="bibr" target="#b39">[40]</ref>. During training, an object proposal is considered positive if its overlap with a groundtruth box is greater than 0.7. For experiments, all regularization parameters λ are set to 1 except for the part localization task where λ parts = 3. The choice of these parameters are discussed at the end of this section.</p><p>We present results for several tasks: 2D vehicle detec-tion and orientation, 3D localization, 2D part localization, part visibility and 3D template prediction. In all presented results, we use 200 object proposals and an overlapping threshold of 0.5 for non-maximum suppression. Results are presented for three levels of difficulty (Easy, Moderate and Hard) as proposed by the KITTI Benchmark <ref type="bibr" target="#b11">[12]</ref>.</p><p>2D vehicle detection and orientation. We use mean Average Precision (mAP) with overlapping criteria of 0.7 to evaluate 2D vehicle detection. We use average orientation similarity (AOS) to evaluate vehicle orientation as proposed by the KITTI Benchmark <ref type="bibr" target="#b11">[12]</ref>. <ref type="table">Table 1</ref> shows results for these two tasks on the two train/val splits. <ref type="table" target="#tab_2">Table 2</ref> shows results on the KITTI testing set. We can see that our method outperforms others for the two tasks on the two train/val split as well as on the test set. In addition, our approach is less time consuming. This is due to the resolution of the input image. Many state-of-the-art object proposal based approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> upscale the input image by a factor of 3 on the KITTI dataset. This is done to not lose information on spatially reduced feature maps. Our coarse-to-fine approach overcomes this loss of information and that allows to give an input image at initial resolution. The coarse-to-fine architecture of the Deep MANTA is also evaluated and results are shown in <ref type="table">Table 3</ref>. We compare the presented Deep MANTA to two other networks. The first line is a network which does not use refinement steps and where pooling regions are extracted on the feature map at the 5th level of convolution (as the original Faster-RCNN <ref type="bibr" target="#b32">[33]</ref>). The second line is a network without refinement steps and where pooling regions are extracted at the first level of convolution. We can see that extracting regions on the first convolution level clearly boosts detection and orientation score (around 24% up for moderate). The last line is the presented Deep MANTA architecture (with refinement step and regions extracted on the first convolution maps). These results shows that the coarse-to-fine architecture increases detection and orientation estimation (around 4% up for moderate). 3D localization. We use Average Localization Precision (ALP) metric proposed by <ref type="bibr" target="#b38">[39]</ref>. It consists in replacing orientation similarity in AOS with localization precision. A 3D location is correct if its distance from the ground truth 3D location is smaller than a threshold. <ref type="table">Table 4</ref> presents results on the two train/val splits for a threshold distance of 1 meter and 2 meters. Our Deep MANTA approach clearly outperforms other monocular approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref> for the 3D localization task (around 16% up compared to Mono3D <ref type="bibr" target="#b6">[7]</ref>). <ref type="figure">Figure 6</ref> shows recall/3D localization precision curves of Deep MANTA and Mono3D <ref type="bibr" target="#b6">[7]</ref>. Compared to 3DOP <ref type="bibr" target="#b7">[8]</ref>, which uses stereo information, the Deep MANTA performances are equivalent at a threshold error distance of 2 meters but less accurate at 1 meter: Deep MANTA only uses a single image contrarily to the 3DOP approach which uses disparity information.</p><p>3D template, part localization and visibility. We also evaluate the precision of part localization, part visibility classification accuracy as well as 3D template prediction. Given a correct detection, we use the following three metrics. For part localization, a part is considered well local- <ref type="figure">Figure 6</ref>. Recall/3D localization precision curves for 1 meter (left) and 2 meters (right) precision on the val2 used by Mono3D <ref type="bibr" target="#b6">[7]</ref>.</p><p>ized if the normalized distance to the ground-truth part is less than a threshold (20 pixels). Distances are normalized using a fixed bounding box height (155 pixels) as proposed by <ref type="bibr" target="#b44">[45]</ref>. The visibility metric is the accuracy over the four visibility classes. Finally, we evaluate 3D template prediction by comparing the three predicted dimensions (w, h, l) to the ground-truth 3D box dimensions (w gt , h gt , l gt ) pro-  <ref type="table">Table 3</ref>. Coarse-to-fine comparison for 2D vehicle detection (AP) and orientation estimation (AOS) on the validation set val2. These experiments show the importance of the refinement step as well as the influence of the feature maps chosen for region extraction. Many-task and regularization parameters. <ref type="table">Table 6</ref> shows results with different sets of regularization parameters. These results also aim to compare performances of the Deep MANTA approach with networks optimized on fewer tasks. In <ref type="table">Table 6</ref>, D corresponds to the detection task, P to the part localization task, V to the part visibility task and T to the template similarity task. With these notations, the first line of <ref type="table">Table 6</ref> is the Deep MANTA trained only on the detection task (λ parts = λ vis = λ temp = 0). As part localization and template similarity are not trained, orientation and 3D localization cannot be predicted in this case. The second line is the Deep MANTA trained without the visibility task (λ vis = 0) and with λ parts = 3. The third line is the complete Deep MANTA (all tasks) but with the regularization parameter associated to part localization λ parts = 1. Finally, the last line is the Deep MANTA with λ parts = 3 (the one presented in all above results). These results are interesting for several reasons. First, we can see that increasing the number of learned tasks (i.e enriching the vehicle description) does not significantly affect performances (it is slightly higher for detection and orientation accuracy but slightly lower on 3D localization). That proves the relevance of the Many-Task concept: a neural network is able to learn one feature representation which can be used to pre-dict many tasks. Secondly, we can see that the parameter λ parts is very important for 3D localization. Learning the Deep MANTA with λ parts = 3 improves the 3D localization by 6% for 1 meter distance precision.  <ref type="table">Table 6</ref>. The influence of the amount of tasks learned as well as different regularization parameters. This table gives results for vehicle detection (AP), orientation (AOS), and 3D localization for 1 meter and 2 meters precision (ALP). Given results are averaged over the two validation sets and over the three levels of difficulty (Easy, Moderate, Hard). See text for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>To conclude, we propose a new approach for joint 2D and 3D vehicle analysis from monocular image. It is based on the Many-task CNN (Deep MANTA) which proposes accurate 2D vehicle bounding boxes using multiple refinement steps. The MANTA architecture also provides vehicle part coordinates (even if these parts are hidden), part visibility and 3D template for each detection. These fine features are then used to recover vehicle orientation and 3D localization using robust 2D/3D point matching. Our approach outperforms state-of-the-art methods for vehicle detection and fine orientation estimation and clearly increases vehicle 3D localization compared to monocular approaches. One perspective is to adapt this framework to other rigid objects and build a multi-class Deep MANTA network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the Deep MANTA approach. The entire input image is forwarded inside the Deep MANTA network. Conv layers with the same color share the same weights. Moreover, these three convolutional blocks correspond to the split of existing CNN architecture. The network provides object proposals {Bi,1} which are iteratively refined ({Bi,2} and then the final detection set {Bi,3}). 2D part coordinates {Si}, part visibility {Vi} and template similarity {Ti} are associated to the final set of detected vehicle {Bi,3}. A non-maximum suppression (NMS) is then performed. It removes redundant detections and provides the new set {Bj, Sj, Vj, Tj}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Example of one 2D/3D vehicle model. (a) the bounding box B, (b) 2D part coordinates S and part visibility V: visible parts (red), occluded parts (green) and self-occluded parts (blue). (c) the 3D bounding box B 3d and (d) the associated 3D shape S 3d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Semi-automatic annotation process. (a) weak annotations on a real image (3D bounding box). (b) best corresponding 3D models in green. (c) projection of these 3D models in the image. (d) corresponding mesh of visibility (each color represents a part). (e) Final annotations (part localization and visibility). Red dots: visible parts, green dots: occluded parts, bleu dots: self-occluded parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>97.90 / 97.58 91.01 / 90.89 83.14 / 82.72 97.60 / 97.44 90.66 / 90.66 82.66 / 82.35 / 91.85 81.79 / 85.15 97.10 / 97.09 91.01 / 91.57 81.14 / 84.72Table 1. Results for 2D vehicle detection (AP) and orientation (AOS) on KITTI val sets. Results on the two validation sets: val1 / val2. Results for 2D vehicle detection (AP) and orientation (AOS) on the KITTI test set.</figDesc><table><row><cell>AP</cell><cell>AOS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>70.90 / 65.71 58.05 / 53.79 49.00 / 47.21 90.12 / 89.29 77.02 / 75.92 66.09 / 67.28</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 meter</cell><cell></cell><cell>2 meters</cell></row><row><cell cols="2">Method</cell><cell cols="3">Type Time</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell cols="2">3DVP [39]</cell><cell cols="2">Mono</cell><cell cols="5">40 s 45.61 / 00-00 34.28 / 00-00 27.72 / 00-00 65.73 / 00-00 54.60 / 00-00 45.62 / 00-00</cell></row><row><cell cols="2">3DOP [8]</cell><cell cols="2">Stereo</cell><cell>3 s</cell><cell cols="4">00-00 / 81.97 00-00 / 68.15 00-00 / 59.85 00-00 / 91.46 00-00 / 81.63 00-00 / 72.97</cell></row><row><cell cols="2">Mono3D [7]</cell><cell cols="7">Mono 4.2 s 00-00 / 48.31 00-00 / 38.98 00-00 / 34.25 00-00 / 74.77 00-00 / 60.91 00-00 / 54.24</cell></row><row><cell cols="5">Ours GoogLenet Mono 0.7 s Ours VGG16 Mono 2 s</cell><cell cols="4">66.88 / 69.72 53.17 / 54.44 44.40 / 47.77 88.32 / 91.01 74.31 / 76.38 63.62 / 67.77</cell></row><row><cell cols="9">Table 4. 3D localization accuracy (ALP) on KITTI val sets for 1 meter and 2 meters precision. Results on the two validation sets: val1 /</cell></row><row><cell>val2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">vided by KITTI. A 3D template (w, h, l) is considered cor-</cell><cell></cell></row><row><cell>rect if |</cell><cell cols="3">wgt−w wgt | &lt; 0.2 and |</cell><cell cols="2">hgt−h hgt | &lt; 0.2 and |</cell><cell>lgt−l lgt | &lt; 0.2.</cell><cell></cell></row><row><cell cols="7">Table 5 shows the good performances for these tasks.</cell><cell></cell></row><row><cell></cell><cell>Metric</cell><cell></cell><cell cols="4">Easy Moderate Hard</cell><cell></cell></row><row><cell cols="5">Part localization 97.54</cell><cell>90.79</cell><cell>82.64</cell><cell></cell></row><row><cell></cell><cell cols="2">Part visibility</cell><cell cols="2">92.48</cell><cell>85.08</cell><cell>76.90</cell><cell></cell></row><row><cell></cell><cell>3D template</cell><cell></cell><cell cols="2">94.04</cell><cell>86.62</cell><cell>78.72</cell><cell></cell></row><row><cell cols="7">Table 5. Part localization, part visibility, 3D template evaluation</cell><cell></cell></row><row><cell cols="3">on the validation set val2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>parts = 3 89.73 89.39 58.37 78.11 DPVT / λ parts = 1 89.58 89.27 51.47 73.93 DPVT / λ parts = 3 90.54 90.23 57.44 77.58</figDesc><table><row><cell></cell><cell></cell><cell>AOS</cell><cell>1 m</cell><cell>2 m</cell></row><row><cell>D</cell><cell>89.86</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DPT / λ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">Multiscale combinatorial grouping. CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast, modular scene understanding system using context-aware object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Beat the mturkers: Automatic image labeling from weak 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Monocular 3d object detection for autonomous driving. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>3d object proposals for accurate object class detection. NIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d object detection and viewpoint estimation with a deformable 3d cuboid model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint 3d estimation of objects and scene layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fpm : Fine pose partsbased model with 3d cad models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Grained Recognition without Part Annotations. CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Epnp: An accurate o(n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Integrating context and occlusion for car detection by hierarchical and-or model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-view object class detection with a 3d geomtrique model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Parsing ikea objects: Fine pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Jointly Optimizing 3D Model Fitting and Fine-Grained Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Accurate object detection with location relaxation and regionlets relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deformable part models revisited: A performance evaluation for object category pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lopez-Sastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarase</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to detect vehicles by clustering appearance patterns. T-ITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Teaching 3d geometry to deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schielen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Occlusion patterns for object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-view and 3d deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>3d object class detection in the wild. CVPR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Z D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deepface : Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04693</idno>
		<title level="m">Subcategoryaware convolutional neural networks for object proposals and detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Beyond pascal : A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Estimating the aspect layout of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savareses</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Revisiting 3d geometric models for accurate object shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV-WS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Detailed 3d representations for object modeling and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Explicit occlusion modeling for 3d object class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pose estimation for category specific multiview object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
