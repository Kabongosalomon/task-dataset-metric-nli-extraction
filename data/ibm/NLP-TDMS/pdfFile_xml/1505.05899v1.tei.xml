<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The IBM 2015 English Conversational Telephone Speech Recognition System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Saon</surname></persName>
							<email>gsaon@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kwang</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Picheny</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The IBM 2015 English Conversational Telephone Speech Recognition System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: recurrent neural networks</term>
					<term>convolutional neural networks</term>
					<term>conversational speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the latest improvements to the IBM English conversational telephone speech recognition system. Some of the techniques that were found beneficial are: maxout networks with annealed dropout rates; networks with a very large number of outputs trained on 2000 hours of data; joint modeling of partially unfolded recurrent neural networks and convolutional nets by combining the bottleneck and output layers and retraining the resulting model; and lastly, sophisticated language model rescoring with exponential and neural network LMs. These techniques result in an 8.0% word error rate on the Switchboard part of the Hub5-2000 evaluation test set which is 23% relative better than our previous best published result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Ever since <ref type="bibr" target="#b1">[1]</ref> demonstrated the large accuracy gains from using deep neural network acoustic models versus Gaussian mixture models, the Switchboard corpus has become the de facto standard experimental testbed for reporting believable and, more importantly, reproducible results for LVCSR. We surmise that this is because it is the largest publicly available dataset (up to 2300 hours of training data) composed of truly conversational speech and because, in general, techniques which result in improvements on Switchboard tend to work well on both small and large vocabulary tasks. One can think of LDA/STC, VTLN, FMLLR and lattice-based model and feature-space discriminative training which were developed first on Switchboard and then became ubiquitous as prime examples of such techniques.</p><p>Since Switchboard is such a well-studied corpus, we thought we would take a step back and reflect on how far we have come in terms of speech recognition technology. To set the baseline, the human word error rate on this task is estimated to be around 4% <ref type="bibr" target="#b2">[2]</ref>. Quoting <ref type="bibr" target="#b2">[2]</ref> again, in 1995, "a high-performance HMM recognizer" achieved a 43% WER on Switchboard <ref type="bibr" target="#b3">[3]</ref>. In 2000, Cambridge University achieved an at the time impressive error rate of 19.3% during the Hub5e DARPA evaluation <ref type="bibr" target="#b4">[4]</ref> which they attributed to "careful engineering". At the height of technological development for GMM-based systems, the winning IBM submission scored 15.2% WER during the 2004 DARPA EARS Rich Transcription evaluation <ref type="bibr" target="#b5">[5]</ref> largely due to the Attila ASR toolkit <ref type="bibr" target="#b6">[6]</ref> and fMPE <ref type="bibr" target="#b7">[7]</ref>. Nowadays, deep neural networks have levelled the playing field and multiple sites can easily reach 12-14% WER using much simpler systems <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12]</ref> as shown in <ref type="table">Table 6</ref>.</p><p>To achieve an error rate of 8.0% on this task is not trivial. In our opinion, a successful recipe has to contain several ingredients. The first and most obvious one is to train larger acoustic and language models on more data. The second (a little less obvious) is to train neural nets that have diverse architectures and operate on different input representations so that we get accuracy gains from both feature and model combination. Third, extra "spice" such as networks with maxout nonlinearities and exponential and NN language models were also found to significantly lower the error rate of our system. Last but not least, it is our experience that having a strong GMM-HMM baseline system <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b13">13]</ref> which provides high-quality alignments used for the various speaker adaptation techniques and for DNN crossentropy training helps.</p><p>The paper is organized as follows: in section 2 we describe the processing steps that are common across all models, in section 3 we present a set of system improvements, and in section 4 we summarize our findings and ponder future opportunities for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">General processing</head><p>Here we describe the common processing steps for all the models detailed in this paper. In particular, we discuss frontend processing, speaker adaptation and neural network training specifics which are largely similar to <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b13">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Training and test data</head><p>The training data consists of 1975 hours of segmented audio from English telephone conversations between two strangers on a preassigned topic and is divided as follows: 262 hours from the Switchboard 1 data collection, 1698 hours from the Fisher data collection and 15 hours of CallHome audio. The test set is the Hub5 2000 evaluation set and contains two parts: 2.1 hours (21.4K words, 40 speakers) of Switchboard data and 1.6 hours (21.6K words, 40 speakers) of CallHome audio. The decoding vocabulary has 30.5K words and 32.9K pronunciations and all decodings were performed with a 4M 4-gram language model (and rescored with different LMs in subsection 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature extraction</head><p>Speech is coded into 25 ms frames, with a frame-shift of 10 ms. Each frame is represented by a feature vector of 13 VTL-warped perceptual linear prediction (PLP) cepstral coefficients which are mean and variance normalized per conversation side. Every 9 consecutive cepstral frames are spliced together and projected down to 40 dimensions using LDA. The range of this transformation is further diagonalized by means of a global semi-tied covariance transform. Next, the LDA features are transformed with one feature-space MLLR (FMLLR) transform per conversation side at both training and test time. Convolutional nets are trained on VTL-warped logmel features augmented with first and second temporal derivatives. The Mel filterbank has 40 filters and the input to the CNNs are blocks of 11 consecutive 40×3-dimensional frames (as described in <ref type="bibr" target="#b13">[13]</ref>).</p><p>In addition to VTLN and FMLLR, DNNs are adapted to the speaker by appending 100-dimensional i-vectors to every block of 11 FMLLR frames as described in <ref type="bibr" target="#b14">[14]</ref>. The i-vectors are extracted using a universal background model given by a GMM with 2048 diagonal covariance mixture components which was trained with maximum likelihood on the speaker-adapted features. The i-vectors are extracted once per conversation side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neural network training</head><p>All models have sigmoid hidden layers and softmax output layers (except for the models from subsection 3.1) and are trained with 10-15 epochs of SGD on frame-randomized minibatches of 250 frames and a cross-entropy criterion. The targets correspond to the context-dependent HMM states obtained by aligning the audio with a GMM-HMM system with 300K Gaussians trained with maximum likelihood on the FMLLR features. The same alignments are mapped to the leaves of various phonetic decision trees which differ in phone context size (±2 or ±3) and number of leaves (16K, 32K and 64K). Prior to CE training, the networks are initialized with layerwise discriminative pretraining as suggested in <ref type="bibr" target="#b1">[1]</ref>. Additionally, we applied 20-30 iterations of hessian-free sequence discriminative training (ST) by using the state-based minimum Bayes risk (MBR) objective function as described in <ref type="bibr" target="#b15">[15]</ref>. The trained networks are used directly in a hybrid decoding scenario by subtracting the logarithm of the HMM state priors from the log of the DNN output scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System improvements</head><p>In this section we discuss specific improvements related to acoustic and language modeling. More concretely, we describe the following techniques: maxout models with annealed dropout (subsection 3.1); training DNNs, CNNs and RNNs with a very large number of outputs (subsection 3.2); improved joint training of convolutional and non-convolutional nets (subsection 3.3); and language model rescoring with exponential and neural network LMs (subsection 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Maxout networks with annealed dropout</head><p>Maxout networks <ref type="bibr" target="#b16">[16]</ref> generalize rectified linear (ReLU, max[0, a]) units, employing non-linearities of the form:</p><formula xml:id="formula_0">sj = max i∈C(j) ai<label>(1)</label></formula><p>where the activations ai = w T i x + bi are based, as usual, on inner products, and the sets of activations {C(j)} utilized by different hidden units are typically disjoint. Maxout networks are conditionally linear and so avoid the vanishing gradient problem, and are well suited for the dropout training procedure <ref type="bibr" target="#b17">[17]</ref>, which for a linear model, trains an exponentially sized model ensemble (2 D models for input dimension D), whose geometric average can be computed by simply renormalizing at test time.</p><p>Maxout networks for ASR have recently been investigated by several researchers, and found to produce significant gains when training data is limited <ref type="bibr" target="#b18">[18]</ref>, but negligible gains in our personal experience when the amount of training data exceeds approximately 100 hours. However, recently we showed that by annealing the dropout rate over the course of training, Maxout networks can produce substantial gains, even in big data scenarios <ref type="bibr" target="#b19">[19]</ref>. The annealing procedure effectively initializes the ensemble of models being learned at a given iteration with an ensemble of models with lower mean and higher variance in the number of active units. This stochastic regularization procedure retains the benefits of the standard dropout training procedure (a strong exploration-phase; a preference for population-based predictions) without compromising the capacity of the network being learned. <ref type="table">Table 1</ref> compares the performance of our annealed dropout Maxout networks (Maxout-AD) to corresponding sigmoidbased DNNs and CNNs from <ref type="bibr" target="#b13">[13]</ref> learned using our standard training procedure, using only the SWB-1 training data (262 hours). All Maxout networks utilize 2 filters per hidden unit, and the same number of layers and roughly the same number of parameters per layer as the sigmoid-based DNN/CNN counterparts. Parameter equalization is achieved by having a factor of √ 2 more neurons per hidden layer for the maxout nets since the maxout operation reduces the number of outputs by a factor of 2. Note that ReLU networks, in our experience, perform on-par with sigmoid-based DNNs in this data regime. Maxout networks trained with AD (Maxout-AD), on the other hand, show a clear advantage over our traditional networks. Also, note that the convolutional layers of the Maxout-AD CNN have only 128 and 256 feature map outputs, whereas those of the sigmoid CNN has 512/512 outputs. Training of the Maxout-AD CNN with a 512/512 filter configuration is in progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>WER SWB (ST) sigmoid Maxout-AD DNN 11.9 11.0 CNN 11.8 11.6 DNN+CNN 10.5 10.2 <ref type="table">Table 1</ref>: Word error rates of sigmoid vs. Maxout networks trained with annealed dropout (Maxout-AD) for ST CNNs, DNNs and score fusion on Hub5'00 SWB. Note that all networks are trained only on the SWB-1 data (262 hours).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Networks with very large output layers</head><p>When training on 2000 hours of data, we found it beneficial to increase the number of context-dependent HMM output targets to values that are far larger than commonly reported. To keep the computation and the number of parameters in check, we also had to use a bottleneck layer before the output layer <ref type="bibr" target="#b20">[20]</ref> with typically 512 neurons. Back in the days when we were training GMM-based acoustic models, we did not notice accuracy improvements when using more than, say, 10000 HMM states <ref type="bibr" target="#b5">[5]</ref>. We conjecture that this is because GMMs are a distributed model and require more data for each state to reliably estimate the mixture components, whereas the DNN output layer is shared between states. This allows DNNs to have a much richer target space. Additionally, we experimented with growing acoustic decision trees where the phonetic context is increased to heptaphones (±3 phones within words and ±2 phones across words). This was a distinct feature of our EARS RT'04 evaluation system which made a significant difference <ref type="bibr" target="#b5">[5]</ref>. The effect of varying the number of outputs and phonetic context is shown in  • Regular DNNs that operate on 11 spliced 40dimensional FMLLR frames and 100-dimensional ivectors. These models have 5 hidden sigmoid layers (4 with 2048 units and 1 with 512 units) and their architecture is shown on the left side of <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>• Convolutional neural networks with two convolutional layers with 128 and 256 filters respectively. The CNNs operate on blocks of 11 consecutive VTL-warped 40dimensional logmel frames augmented with first and second derivatives with 9×9 convolution windows. The convolution and pooling layer configuration is taken from <ref type="bibr" target="#b21">[21]</ref> and the architecture is also shown on the left side of <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>• Partially unfolded recurrent neural networks <ref type="bibr" target="#b22">[22]</ref> which operate on a sliding window of 6 40-dimensional FM-LLR frames (from t . . . t + 5) and 100-dimensional ivectors. The 6-frame window slides backwards in time from t to t − 5 (so that the RNN and the DNN have exactly the same input). The first hidden layer is recurrent and is followed by 4 non-recurrent hidden layers (3 with 2048 neurons and 1 with 512 neurons) and one output layer with 32000 softmax units.</p><p>All nets are trained with 10-15 passes of cross-entropy on 2000 hours of audio and 30 iterations of sequence discriminative training using Hessian-free optimization <ref type="bibr" target="#b15">[15]</ref>. The performance of the individual networks as well as their score fusion combination is shown in <ref type="table" target="#tab_3">Table 3</ref> on the Hub5'00 test set (SWB and CallHome parts). For score fusion, we decode with a framelevel sum of the outputs of the nets prior to the softmax with uniform weights.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Improved joint training of recurrent and convolutional nets</head><p>In <ref type="bibr" target="#b13">[13]</ref>, we proposed a method for jointly modeling and training a CNN and a DNN. The crux of the method is to have the first layers be network specific (convolution and pooling for CNN operating on spectral features and input layer for DNN operating on PLP-based and i-vector features) and the remaining layers be shared. The outputs of the network-specific layers are merged into one common hidden layer followed by additional (common) hidden layers and one output layer. This graph structure for the joint network extends the standard linear sequence of layers for DNNs (or CNNs). By using this architecture, we reported a 12% relative gain on a Switchboard 300 hours setup over the best single model (from 11.8% for the CNN to 10.4% for the joint CNN/DNN). We also showed that performing score fusion of a CNN and a DNN trained separately achieves a similar WER of 10.5%. Hence, the main benefit of the joint model in <ref type="bibr" target="#b13">[13]</ref> over the score fusion approach is the shared computation for the common hidden and output layers which is considerably faster than having to do two separate forward passes.</p><p>A different approach that we are advocating here is to initialize the joint model such that it is equivalent to the score fusion of the separate models. The reasoning behind this is that, after retraining, the objective function for sequence discriminative training can only improve (or, at worst, remain the same). For the case of log-linear score combination of multiple neural networks with the same number (and type) of outputs, this initialization is done by concatenating the individual weight matrices between the bottleneck and output layers and by dividing the resulting matrix by the number of models (assuming uniform weights). An example of a joint CNN/DNN model initialized in such a way is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. For convenience, we have indicated the sizes of the weight matrices in the oval boxes and the dimensionality of the layers is attached to the arrows. We have experimented with jointly training the unfolded RNN and the CNN from subsection 3.2. Two experimental scenarios were considered. The first is where the joint model was initialized with the fusion of the cross-entropy trained RNN and CNN whereas the second uses ST models as the starting point. For both scenarios we generate numerator and denominator lattices with the initial joint model and optimize the lattice-based MBR loss using distributed hessian-free training <ref type="bibr" target="#b15">[15]</ref>. In <ref type="table">Ta</ref>  We observe that joint modeling and sequence discriminative retraining helps by 0.5% on the CallHome part and only 0.1% on SWB over score fusion of the ST models. Also, the performance of the joint model after sequence training appears to be slightly better for the initialization from CE models (we expected it to be the other way around).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Language model</head><p>In experiments comparing acoustic models reported in previous sections, we used a baseline legacy language model that had been used for previous publications: a 4M 4-gram language model with a vocabulary of 30.5K words. While keeping the vocabulary the same, we rebuilt the LM using publicly available (e.g. LDC) training data, including Switchboard, Fisher, Gigaword, and Broadcast News and Conversations. The most relevant data are the transcripts of the 1975 hour audio data used for training the acoustic model, consisting of about 24M words.</p><p>To build the new n-gram language model, we trained a 4gram model with modified Kneser-Ney smoothing <ref type="bibr" target="#b23">[23]</ref> for each corpus, and then linearly interpolated the component models with weights chosen to optimize perplexity on a held-out set. Then we applied entropy pruning <ref type="bibr" target="#b24">[24]</ref>, resulting in a single 4gram LM consisting of 37M n-grams. This new n-gram LM was used in combination with our best acoustic model to decode and generate word lattices for further LM rescoring experiments. The first two lines of <ref type="table" target="#tab_7">Table 5</ref> show the improvement using this larger n-gram LM trained on more data. The WER improved by 0.5% for SWB and 0.3% for CallHome. Part of this improvement (0.1-0.2%) was due to also using a larger beam for decoding.  For LM rescoring, we used two types of LMs: model M, a class-based exponential model <ref type="bibr" target="#b25">[25]</ref> and neural network LM (NNLM) <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref>. We built a model M LM on each corpus and interpolated the models, together with the 37M ngram LM. As shown in <ref type="table" target="#tab_7">Table 5</ref>, using model M results in an improvement of 0.4% on SWB and 1.0% on CallHome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LM</head><p>We built two NNLMs for interpolation. One was trained on just the most relevant data: the 24M word corpus (Switchboard/Fisher/CallHome acoustic transcripts). Another was trained on a 560M word subset of the LM training data: in order to speed up training for this larger set, we employed a hierarchical NNLM approximation <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b30">30]</ref>. <ref type="table" target="#tab_7">Table 5</ref> shows that, compared with the n-gram LM baseline, interpolating NNLM to model M and n-gram LM results in an improvement of 0.8% on SWB (8.8% to 8.0%) and 1.2% on CallHome (15.3% to 14.1%).</p><p>Lastly, in <ref type="table">Table 6</ref> we compare our results with those obtained by various other systems from the literature. For clarity, we also specify the type of training data that was used for acoustic modeling in each case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>AM training data SWB CH Vesely et al. <ref type="bibr">[</ref>  <ref type="table">Table 6</ref>: Comparison of word error rates on Hub5'00 (SWB and CH) for existing systems ( * note that the 19.1% CallHome WER is not reported in <ref type="bibr" target="#b13">[13]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We have presented a set of improvements to our English Switchboard system that lowered the error rate substantially compared to our previous best result <ref type="bibr" target="#b13">[13]</ref>. In decreasing order of importance these are: rescoring with strong language models trained on diverse data sources; joint training of an RNN and a CNN with 32000 outputs on 2000 hours of audio and maxout networks with annealed dropout. We expect additional accuracy gains by training the maxout nets and larger CNNs with a 512/512 filter configuration on all the data. Extrapolating from historical trends, we believe that human accuracy on this task can be reached within the next decade. We think that the way to get there will most likely involve an increase of several orders of magnitude in training data and the use of more sophisticated neural network architectures that tightly integrate multiple knowledge sources (acoustics, language, pragmatics, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ackowledgment</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Forming a joint CNN/DNN model out of separately trained networks by fusing the bottleneck and output layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>for DNNs with 5 hidden</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of word error rates for CE-trained DNNs with different number of outputs and phonetic context size on Hub5'00 SWB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of word error rates for CE and ST CNN, DNN, RNN and various score fusions on Hub5'00.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of word error rates for CE and sequence trained unfolded RNN and DNN with score fusion and joint modeling on Hub5'00. The WERs for the joint models are after sequence training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of word error rates for different language models.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors wish to thank the following present and former IBM colleagues: H. Soltau, D. Povey, S. Chen, A. Emami, V. Goel, B. Kingsbury, L. Mangu, B. Ramabhadran, T. Sainath and G. Zweig for significant contributions to the Switchboard system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cnn</forename><surname>Joint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dnn</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feature engineering in context-dependent deep neural networks for conversational speech transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech recognition by machines and humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance of the IBM LVCSR system on the Switchboard corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Speech Research Symposium</title>
		<meeting>Speech Research Symposium</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page">189</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The CU-HTK march 2000 HUB5E transcription system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Speech Transcription Workshop</title>
		<meeting>Speech Transcription Workshop<address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The IBM 2004 conversational telephony system for rich transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mangu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2005 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="205" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The IBM Attila speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Workshop on Spoken Language Technology (SLT)</title>
		<meeting>of IEEE Workshop on Spoken Language Technology (SLT)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">fMPE: Discriminatively trained features for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mangu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="961" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequence-discriminative training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">1-bit stochastic gradient descent and its application to dataparallel distributed training of speech dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deepspeech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence training of multiple deep neural networks for better performance and faster training speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5627" to="5631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Increasing deep neural network acoustic model size for large vocabulary continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Lengerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.7806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint training of convolutional and non-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">to Proc. ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speaker adaptation of neural network acoustic models using i-vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable minimum Bayes risk training of deep neural network acoustic models using distributed Hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving deep neural network acoustic models using generalized maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="215" to="219" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Annealed dropout training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for deep neural network training with high-dimensional output targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8614" to="8618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unfolded recurrent neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="393" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entropy-based pruning of backoff language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Broadcast News Transcription and Understanding Workshop</title>
		<meeting>DARPA Broadcast News Transcription and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="270" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shrinking exponential language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="468" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A neural syntactic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Baltimore, MD, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Empirical study of neural network language models for Arabic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mangu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large scale hierarchical neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arısoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vozila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
