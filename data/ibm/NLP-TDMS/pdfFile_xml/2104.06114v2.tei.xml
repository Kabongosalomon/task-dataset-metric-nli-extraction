<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Software</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Software</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Software</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection in point clouds is a challenging vision task that benefits various applications for understanding the 3D visual world. Lots of recent research focuses on how to exploit end-to-end trainable Hough voting for generating object proposals. However, the current voting strategy can only receive partial votes from the surfaces of potential objects together with severe outlier votes from the cluttered backgrounds, which hampers full utilization of the information from the input point clouds. Inspired by the back-tracing strategy in the conventional Hough voting methods, in this work, we introduce a new 3D object detection method, named as Back-tracing Representative Points Network (BRNet), which generatively back-traces the representative points from the vote centers and also revisits complementary seed points around these generated points, so as to better capture the fine local structural features surrounding the potential objects from the raw point clouds. Therefore, this bottom-up and then top-down strategy in our BR-Net enforces mutual consistency between the predicted vote centers and the raw surface points and thus achieves more reliable and flexible object localization and class prediction results. Our BRNet is simple but effective, which significantly outperforms the state-of-the-art methods on two large-scale point cloud datasets, ScanNet V2 (+7.5% in terms of mAP@0.50) and SUN RGB-D (+4.7% in terms of mAP@0.50), while it is still lightweight and efficient. Code will be available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As one of the fundamental tasks that aims at understanding 3D visual world, 3D object detection would like to predict amodal 3D bounding boxes and associated semantic labels of objects in real 3D scenes. 3D object detection technologies would significantly benefit various down-* Lu Sheng is the corresponding author.  <ref type="figure">Figure 1</ref>. The votes generated by VoteNet <ref type="bibr" target="#b19">[20]</ref> and its variants usually suffer from (a) partial coverage of the object surfaces, (b) outliers from the cluttered background. By examining the corresponding seed points, the generated proposals from these votes receive erratic features with respect to the objects, and may be less reliable for predicting accurate bounding boxes, orientations and even semantic classes. Best viewed on screen.</p><p>stream real world applications such as augmented reality, robotics and etc. In this work, we focus on 3D object detection from point clouds. It is even more challenging because the irregular, sparse and orderless characteristics of this special 3D input make it a hard task to design reliable point-based 3D object detection systems by leveraging the recent progress in 2D object detection. While earlier works resorted to reordering point clouds into regular forms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b43">43]</ref>, or applying predefined shape templates <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">40]</ref>, VoteNet <ref type="bibr" target="#b19">[20]</ref> and its variants <ref type="bibr">[36,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref> have shown a great success in designing end-to-end 3D object detection networks based on raw point clouds. VoteNet reformulates the traditional Hough voting process into a point-wise regression problem, and generates an object proposal by sampling a number of seed points from the input point cloud whose votes are within the same cluster. The aggregated feature in each vote cluster is then used to estimate the 3D bounding box (e.g. center, size and orientation) and the associated semantic label.</p><p>Therefore, the quality of the regressed votes principally determine the reliability of the generated proposals, and then the performance on the object detector. However, al- <ref type="figure">Figure 2</ref>. Back-tracing representative points and revisiting seed points. We show the vote cluster center for the two chairs (in purple points). The representative points are back-traced from the vote cluster center (in red points). We set the number of representative points per proposal as 12 in this case, which are illustrated on the right chair. Then, the seed points within a fixed distance of the representative points are revisited, shown in blue points on the left chair. The revisited seed points provide good coverage of the chair's surface, which imply the object shape and keep the structural details as the chair armrest. Best viewed on screen.</p><p>though the clustered vote centers are quite accurate, the votes are usually not as representative as our expectation. For example, as illustrated in <ref type="figure">Fig. 1</ref>, by retrieving the seed points of votes from the given vote clusters, these corresponding seed points either partially cover the underlying objects ( <ref type="figure">Fig. 1(a)</ref>) or contain severe outliers from the cluttered background ( <ref type="figure">Fig. 1(b)</ref>). Therefore as shown in <ref type="figure">Fig. 1(a)</ref>, it is undoubted that we cannot accurately predict the bounding box of a long bookshelf if the votes only capture a small area surrounding the vote center. Likewise as shown in <ref type="figure">Fig. 1(b)</ref>, the severe outliers make it impossible to accurately detect the chair based on the vote features. Moreover, these seed points are less informative due to the lack of knowledge from the votes, so that there will be less significant gains if we simply back-trace these seed features (as in conventional Hough voting <ref type="bibr" target="#b13">[14]</ref>) to improve the votingbased 3D object detection methods.</p><p>However, in our point of view, back-tracing is still necessary and could partially address the aforementioned issues with a special design. To be specific, as shown in <ref type="figure">Fig. 2</ref>, we would like to backwardly generate (or trace) the virtual representative points from the center of each vote cluster, and use these virtual points to revisit their surrounding seed points. This generative back-tracing operation indicates possible object shape distributions around the vote center, while the revisited seed features provide complementary local structural clues that may not be fully discovered by the votes. This bottom-up and then top-down process can end up with a mutual interaction that associates the seed features and the vote features, which has the potential to enhance each other features and enable more robust object class prediction and more accurate bounding box regression.</p><p>To this end, we propose a new point cloud-based 3D object detection method, named as Back-tracing Representa-tive Point Network (BRNet), by incorporating the end-toend learnable back-tracing and revisiting operations into the voting-based framework. Specifically, we propose a representative points generation module that generatively samples uniformly distributed representative points within the 3D area of a candidate object, based on the features of a vote cluster center. The generated points can coarsely infer the object bounding boxes even though their sampling process is class-agnostic. The revisited seed points of each representative point are aggregated in a similar way as ROI grid pooling <ref type="bibr" target="#b27">[28]</ref>, but based on the spatial layout of the representative points. After fusing the aggregated features of the revisited seed points and the features of the vote cluster center, we obtain the refined proposals to eventually detect the objects. Note that the proposed bounding box regression scheme explicitly depends on the spatial distribution of the representative points, thus improves robustness with respect to shape variations within and across object categories.</p><p>The contributions of this work are three-fold: (1) the first 3D object detection network, named as BRNet, that successfully adapts the back-tracing step of Hough voting to 3D object detection. (2) an end-to-end learnable network that can generatively back-trace the representative points, reliably revisit the seed points, and then mutually refine the object proposals for more robust object classification and more accurate bounding box regression. (3) the state-ofthe-art 3D object detection performance on two benchmark datasets, ScanNet V2 <ref type="bibr" target="#b3">[4]</ref> (50.9% in terms of mAP@0.50) and the SUN RGB-D [31] (43.7% in terms of mAP@0.50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>3D object detection on point clouds. Object detection from 3D point clouds is challenging due to the irregular, sparse and orderless characteristics of 3D points. Earlier attempts usually relied on projections onto regular grids such as multi-view images <ref type="bibr" target="#b2">[3]</ref> and voxel grids <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30]</ref>, or based on the candidates from RGB-driven 2D proposal generation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b10">11]</ref> or segmentation hypotheses <ref type="bibr" target="#b7">[8]</ref>, where the existing 2D object detection or segmentation methods based on regular image coordinates can be effortlessly adapted. Other approaches also studied how to exploit discriminative <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> or generative shape templates <ref type="bibr" target="#b40">[40]</ref>, and high-order contextual potentials to regularize the proposal objectness <ref type="bibr" target="#b15">[16]</ref>, or used sliding shapes <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b31">32]</ref>, or clouds of oriented gradients (COG) <ref type="bibr" target="#b26">[27]</ref>.</p><p>Thanks to PointNet <ref type="bibr" target="#b21">[22]</ref>, deep neural networks have become extensively employed onto raw point clouds. For instance, PointRCNN <ref type="bibr" target="#b28">[29]</ref> introduced a two-stage 3D object detector, which is analogous to the two-stage 2D object detection methods such as Faster RCNN <ref type="bibr" target="#b25">[26]</ref>. Inspired by the Hough voting strategy for 2D object detection and instance segmentation <ref type="bibr" target="#b13">[14]</ref>, VoteNet <ref type="bibr" target="#b19">[20]</ref> was built upon the backbone of PointNet++ <ref type="bibr" target="#b22">[23]</ref> and presented an end-to-end trainable 3D object detector. Later on, the extensions of VoteNet <ref type="bibr" target="#b19">[20]</ref>, such as MLCVNet <ref type="bibr">[36]</ref>, HGNet <ref type="bibr" target="#b1">[2]</ref> and 3DSSD <ref type="bibr" target="#b39">[39]</ref>, employed the contextual clues, the hierarchical graph neural networks and the feature-FPS sampling strategy to enable better generation of object proposals. However, these methods heavily depend on the unreliable vote clustering proposed in <ref type="bibr" target="#b19">[20]</ref>, which is inevitably affected by outliers and usually overlooks inlier seed points. H3DNet <ref type="bibr" target="#b41">[41]</ref> partially tackled this issue by introducing a hybrid set of overcomplete geometric primitives to refine the initial bounding boxes predicted by the clustered votes. But these primitives centers are learned with less accurate supervisions and also collected by a similar clustering strategy, thus may still fail to eliminate the outliers or capture sufficient geometric clues to infer the target objects. In this work, we show how to leverage the representative points back-traced from the vote centers to complementarily profile the target objects, which enables more discriminative categorization and more robust bounding box regression.</p><p>Anchor-free 2D object detection. The implementation of the back-tracing representative points in our BRNet adopts similar anchor-free localization strategies in 2D object detection. Unlike two-stage 2D object detectors such as Faster RCNN <ref type="bibr" target="#b25">[26]</ref>, SSD <ref type="bibr" target="#b16">[17]</ref> and YOLOv2 <ref type="bibr" target="#b24">[25]</ref> that generate proposals with the predefined anchors, the anchor-free detectors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5]</ref>, especially the regressionbased approaches <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5]</ref>, either directly regress borders <ref type="bibr" target="#b23">[24]</ref>, regress the object boundaries with an iterative dynamic sampling strategy <ref type="bibr" target="#b38">[38]</ref>, or regress 4D offsets as the surrogate of the localization results <ref type="bibr" target="#b34">[34]</ref>. Inspired by these methods, in our method, the back-tracing process relies on a class-agnostic offset regressor to retrieve the representative points that indicate the likely shape profile surrounding each vote center and thus provides more local structural clues for latter inference. Rather than localization constrained by predefined class-aware statistics, as in VoteNet <ref type="bibr" target="#b19">[20]</ref> and its successors, the proposed BRNet benefits more flexible regression without losing its discriminative power.</p><p>Back-tracing in voting-based object detection and instance segmentation. Leibe et al. <ref type="bibr" target="#b13">[14]</ref> applied the hough voting strategy for simultaneously 2D object detection and instance segmentation. The core part of this approach is a learned highly flexible representation for object shapes in a probabilistic extension of Generalized Hough Transform. Moreover, the work in <ref type="bibr" target="#b8">[9]</ref> combined the top-down clues available from object detection and the bottom-up power of Markov Random Fields (MRFs) when performing class-specific object detection and segmentation in 3D scenes. These methods rely on a top-down strategy such as back-tracing object hypotheses to enhance the bottom-up strategy such as Hough voting. Their mutual agreement enhances each other, and thus devotes to the success of more reliable object detection. The proposed BRNet also fol-lows this idea with a new end-to-end trainable back-tracing process based on the representative points. Recently, as a 3D instance segmentation method, 3D-MPA [6] applied a "direct" back-tracing strategy to cluster the surface points from the corresponding votes in one cluster. In contrast, our method alleviates the inherent partial coverage and outlier issues from the "generative" back-tracing strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we describe the technical details of our BRNet. Sec. 3.1 presents an overview of our method. In Sec. 3.2 to Sec. 3.5, we elaborate the network architecture and the learning objective of our BRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>As illustrated in <ref type="figure">Fig. 3</ref>, the input of our BRNet is a point cloud P ∈ R N ×3 , with a 3D coordinate for each of the N points. Such an input typically comes from multi-view stereo (e.g. ScanNet <ref type="bibr" target="#b3">[4]</ref>) or depth sensors (e.g. SUN RGB-D <ref type="bibr" target="#b30">[31]</ref>). The output is a collection of (oriented) bounding boxes</p><formula xml:id="formula_0">B, each box b ∈ B is associated with a predefined category label l b ∈ C, a center c b = [c x b , c y b , c z b ] ∈ R 3 in a world coordinate system, the size of bounding box s b = [s x b , s y b , s z b ] ∈ R 3</formula><p>, an orientation angle θ b in the xy-plane of the same world coordinate system.</p><p>BRNet consists of four main modules: (1) vote generation and clustering, (2) back-traced representative points generation, (3) seed point revisiting, and (4) proposal refinement and classification followed by standard 3D NMS. In the first module, we follow the same network and training strategy as in VoteNet <ref type="bibr" target="#b19">[20]</ref> to generate the seed points, the votes and the vote clusters. We will elaborate the other three modules in the following parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generating Back-traced Representative Points</head><p>The conventional back-tracing step of Hough voting for identifying object boundaries <ref type="bibr" target="#b13">[14]</ref> is less reliable for amodal object detection from partial observations, as it just picks up seed points that contribute to the selected votes. For example, in VoteNet <ref type="bibr" target="#b19">[20]</ref>, these back-traced seed points can only capture local geometric area near the cluster center while containing the outliers from the cluttered background in the meantime. VoteNet <ref type="bibr" target="#b19">[20]</ref> circumvents this issue by removing the back-tracing step and using a PointNet-like set aggregation block just for votes, and then generates the object proposals and classifies them. However, the aforementioned incompleteness issue and the outliers within the votes (delivered from the seed points) are clearly harmful for the detection task. To this end, we argue that it is still beneficial to use back-tracing in point-based 3D object detection, but it requires a better tracing strategy to effectively find the representative seed points. In contrast to the conventional back-tracing strategy, we propose a representative  <ref type="figure">Figure 3</ref>. An overview of our proposed BRNet for 3D object detection in point clouds. Given an input point cloud consisting of N points with the XYZ coordinates, we generate votes from it and group the votes into M clusters as in VoteNet <ref type="bibr" target="#b19">[20]</ref>. For each of the M vote cluster centers, we back-trace K representative points from it. The back-traced representative points imply the possible area of the object. We then revisit the seed points around the representative points and aggregate the surrounding seed point features to the representative points. The clustered vote features and the revisited seed features are fused and processed by the proposal refinement and classification module to produce the refined representative points and object's semantic category, which can be easily transformed into 3D object bounding boxes. The standard 3D NMS is eventually used to generate the final detection results. Best viewed on screen.</p><p>point generation (RPG) module to backwardly regress the virtually generated representative points from the votes in a generative manner. The generated representative points are uniformly distributed within the potential 3D area of a candidate object, which can also indicate 3D object shapes when interacted with their actual surrounding seed points.</p><p>To be specific, the vote sampling and grouping block generates a set of vote cluster centers</p><formula xml:id="formula_1">{v i } M i=1 , where v i = [p i , f i ] with p i ∈ R 3</formula><p>as the vote's geometric position in the 3D space and f i ∈ R C as its feature extracted from the preceding network, M is the number of vote clusters. Then, the RPG module generates a set of representative points for each vote cluster center. Rather than directly sampling the 3D coordinates of these points, this module simultaneously predicts the tentative orientation θ i ∈ [0, 2π] of the potential object, and regresses the offset distances x i ∈ R 6 from v i to the tentative object's surface in 6 canonical directions (i.e. front/back/left/right/up/down), and then uniformly samples distributed representative points</p><formula xml:id="formula_2">R i = {r k i = (x k i , y k i , z k i )} K k=1</formula><p>along these directions (which are skewed by the predicted orientation) within the range of the offset distances. K is the number of representative points. In this work, we sample 2 uniformly distributed points within the range of each offset, thus K = 2 × 6 = 12 in total.</p><p>Network architecture and learning. The RPG module is implemented by using multi-layer perceptrons (MLP) with the ReLU activation function and batch normalization. It takes the feature f i from the vote center v i as the input, and its output is the set {x i , θ i }. We employ exp(·) to map any real number to (0, ∞) on the output of x i . This module is supervised by the ground-truth (GT) offsets as the vote center can be assigned to a GT object, i.e.</p><formula xml:id="formula_3">L rep-off = 1 M pos M i=1 x i − x * i ρ · I[v i is positive],<label>(1)</label></formula><p>where I[v i is positive] indicates whether the vote center v i is around a GT object center (within a radius of 0.3). M pos is the number of positive vote centers. ρ means smooth-1 norm. And x * i is the GT offsets from the vote center v i to the 6 faces of the GT bounding box. This module is also supervised by the GT orientation of the same GT object. To better predict the orientation angle, we adopt the binbased angle prediction scheme as in <ref type="bibr" target="#b20">[21]</ref>, which predicts a classification score for each orientation bin and a regression offset in each bin, and then uses the cross-entropy loss for orientation bins, and the smooth-1 loss for the regression offset. We term the orientation loss as L rep-ang . Therefore, the final learning objective for this module is</p><formula xml:id="formula_4">L rep = λL rep-off + L rep-ang ,<label>(2)</label></formula><p>where λ = 20 is used to balance the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Revisiting Seed Points</head><p>By back-tracing the representative points R i in a generative manner from a vote center v i , i = 1, . . . , M , we can roughly obtain the size and the position of a possible object in a class-agnostic way, but it still requires mutual consistency from the actual seed points in order to reliably gener-ate the object proposals for more accurate object localization, bounding box estimation and object class prediction. To be specific, we revisit the seed points {q j | q j − r k i ≤ δ} within a fixed radius (δ=0.2 in the work) surrounding a back-traced representative point r k i , k = 1, . . . , K, and aggregate the revisited seed features by using a PointNet-like block <ref type="bibr" target="#b21">[22]</ref>, denoted asg r k i . This process is similarly implemented as ROI-grid pooling proposed in PV-RCNN <ref type="bibr" target="#b27">[28]</ref>, but with a different griding and radius selection strategy.</p><p>Thereafter, to each vote center (or called proposal) v i , the set of aggregated seed point featuresg r k i from each representative point r k i can be further fused into a single featureg vi , which is implemented by concatenating {g r k i } K k=1 in a predefined order before being projected to a 128-dimensional feature. The predefined order should be consistent for each proposal, but different ordering strategies do not affect the performance. The revisited seed features are summarized intog vi , which thus captures the local object-level features from the relatively precise raw point clouds instead of the predicted vote points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Proposal Refinement and Classification</head><p>The back-traced representative point set R i helps to revisit the seed points and aggregate the local geometric clues from the potential object indicated by the vote center v i . The aggregated featureg vi can be concatenated with the feature f i of the vote center v i , and then refine the proposal and use for more discriminative object class prediction. To this end, the fused featuref i = [g vi , f i ] ∈ R 256 is fed into a shared MLP to predict the residuals ∆x i and ∆θ i based on the preceding estimation results x i and θ i , and produce the final output set {x i + ∆x i , θ i + ∆θ i }. Meanwhile, we predict the objectness score and the semantic classification score for each fused feature, similarly as in <ref type="bibr" target="#b19">[20]</ref>. Note that the final offsets x i + ∆x i can be reformulated as the bounding box size s i = [s x i , s y i , s z i ] ∈ R 3 and the object center c i = [c x i , c y i , c z i ] ∈ R 3 , by min-max clipping the final representative point setR i in the canonical coordinate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">The Learning Objective</head><p>In summary, the loss function of the entire framework of the newly proposed BRNet is defined as following:</p><formula xml:id="formula_5">L = L vote-reg + λ obj-cls L obj-cls + λ sem-cls L sem-cls + λ rep L rep + λ refine L refine (3)</formula><p>Following the terms and label assignment strategy used in VoteNet <ref type="bibr" target="#b19">[20]</ref>, the loss terms L vote-reg , L obj-cls , L sem-cls indicate the per-point vote regression loss, the objectness loss and the semantic classification loss, respectively. L rep is defined in Sec. 3.2. L refine is used to supervise the residuals from the initial representative point sets to the final repre-sentative point sets:</p><formula xml:id="formula_6">L refine = 1 M pos M i=1 (λ x i + ∆x i − x * i ρ + (θ i + ∆θ i − θ * i ρ ) · I[v i</formula><p>is positive] (4) ρ denotes the smooth-1 norm. θ * i is the orientation angle of the ground-truth object bounding box. L refine is computed only on the positive vote clusters. The weighting factors are λ obj-cls = 1, λ sem-cls = 0.1, λ rep = 1, λ refine = 1 and λ = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setups and Implementation Details</head><p>Datasets. We evaluate our method on two large-scale indoor scene datasets, i.e. SUN RGB-D <ref type="bibr" target="#b30">[31]</ref> and ScanNet V2 <ref type="bibr" target="#b3">[4]</ref>. SUN RGB-D consists of 10, 355 single-view indoor RGB-D images annotated with the oriented 3D bounding boxes and the semantic labels for 37 categories. The point clouds are converted from the depth maps based on the provided camera parameters. The captured point clouds contain severe occlusions and holes, thus are challenging for 3D object detection. ScanNet V2 is a 3D mesh dataset about 1, 500 3D reconstructed indoor scenes. It contains 18 object categories with densely annotated axis-aligned bounding boxes. The scans in the ScanNet V2 dataset are more complete with more objects than those in the SUN RGB-D dataset. For both datasets, we use the same data preparation and training/validation split as in VoteNet <ref type="bibr" target="#b19">[20]</ref>. Input and data augmentation. The input of our method is a point cloud randomly sub-sampled from the raw data of each dataset, i.e., 20, 000 points from a point cloud in the SUN RGB-D dataset, and 40, 000 points from a 3D mesh in the ScanNet V2 dataset. We also include the height feature to each point. To augment the training data, we add random flipping, rotating and scaling to the input point clouds, as the way employed by VoteNet <ref type="bibr" target="#b19">[20]</ref>. Network training details. Our network is end-to-end optimized by using the Adam optimizer with the batch size as 8. The base learning rates are 0.001 for the SUN RGB-D <ref type="bibr" target="#b30">[31]</ref> dataset and 0.005 for the ScanNet V2 <ref type="bibr" target="#b3">[4]</ref> dataset. We train the network for 220 epochs on both datasets. The cosine annealing learning rate strategy <ref type="bibr" target="#b17">[18]</ref> is adopted for learning rate decay. Based on PyTorch platform equipped with one NVIDIA GeForce RTX 2080 Ti GPU card, it takes around 4 hours to train the model on the ScanNet V2 dataset, while it takes around 12 hours on the SUN RGB-D dataset. Inference and evaluation. Our method takes the point clouds of the entire scenes as the inputs and outputs the object proposals. The proposals are post-processed by a 3D NMS module with an IoU threshold of 0.25. The evaluation follows the same protocol as in <ref type="bibr" target="#b33">[33]</ref> using mean average precision, especially mAP@0.25 and mAP@0.50.  <ref type="bibr" target="#b33">[33]</ref> Geo + RGB 42.1 -COG <ref type="bibr" target="#b26">[27]</ref> Geo + RGB 47.6 -2D-driven <ref type="bibr" target="#b10">[11]</ref> Geo + RGB 45.1 -F-PointNet <ref type="bibr" target="#b20">[21]</ref> Geo + RGB 54.0 -VoteNet <ref type="bibr" target="#b19">[20]</ref> Geo only 57.7 32.9 HGNet <ref type="bibr" target="#b1">[2]</ref> Geo only 61.6 -MLCVNet <ref type="bibr">[36]</ref> Geo only 59.8 -H3DNet (1BB)* <ref type="bibr" target="#b41">[41]</ref> Geo only --H3DNet (4BB)* <ref type="bibr" target="#b41">[41]</ref> Geo only 60.1 39.0 Ours Geo only 61.1 43.7 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with the State-of-the-art Methods</head><p>We compare our method with a list of reference methods, for example the earlier attempts, such as COG <ref type="bibr" target="#b26">[27]</ref>, DSS <ref type="bibr" target="#b33">[33]</ref> and 3D-SIS <ref type="bibr" target="#b6">[7]</ref>, 2D-driven <ref type="bibr" target="#b10">[11]</ref> and F-PointNet <ref type="bibr" target="#b20">[21]</ref>, and GSPN <ref type="bibr" target="#b40">[40]</ref>, and the recent point cloudbased state-of-the-art methods such as VoteNet <ref type="bibr" target="#b19">[20]</ref> and its successors MLCVNet <ref type="bibr">[36]</ref>, HGNet <ref type="bibr" target="#b1">[2]</ref> and H3DNet <ref type="bibr" target="#b41">[41]</ref>.</p><p>Quantitative results. The comparison results are summarized in <ref type="table" target="#tab_1">Table 1</ref>. Our method outperforms all baseline methods by remarkable performance gains, for example more than 7.5% and 4.7% improvement in terms of the mAP@0.50 metric on the validation sets of ScanNet V2 and SUN RGB-D respectively. Note that mAP@0.50 is a fairly challenging metric as it basically requires more than 79% coverage in each dimension of a bounding box, which indicates that back-tracing representative points can significantly improve the localization accuracy. Notably, ML-CVNet <ref type="bibr">[36]</ref> works well on the ScanNet dataset but achieves relatively poor performance on the SUN RGB-D dataset, while HGNet <ref type="bibr" target="#b1">[2]</ref> works well on the SUN RGB-D dataset but achieves poor result on the ScanNet dataset, especially in terms of the mAP@0.50 metric. Our method works well on both datasets, which indicates its stronger generalization ability for different detection scenarios. ScanNet contains relative complete 3D reconstructed meshes, while SUN RGB-D consists of single-view RGB-D scans with severe occlusions and holes. Moreover, H3DNet <ref type="bibr" target="#b41">[41]</ref> ensembles 4 PointNet++ <ref type="bibr" target="#b22">[23]</ref> backbones to achieve the reported result on the SUN RGB-D dataset, while our model only needs one backbone as the base feature extractor. It fur-ther validates it is effective to back-trace the representative points for reliably parsing the object proposals. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our method performs the best on 12 classes among 18 total classes from the ScanNet dataset in terms of mAP@0.50. While our method only uses one PointNet++ backbone for point cloud feature extraction, it outperforms H3DNet <ref type="bibr" target="#b41">[41]</ref> with 4 PointNet++ backbones. Moreover, it achieves better performance on the categories (e.g. "cabinet", "chair", "sofa", "table", "counter" and "desk") with irregular sizes or shapes, as its back-tracing and revisiting process removes the outliers from the votes and enables better mutual agreement between the votes and the local object surfaces, whilst its class-agnostic regression strategy makes the estimation process robust to shape variations.</p><p>Qualitative results. In <ref type="figure">Fig. 4</ref> and <ref type="figure">Fig. 6</ref>, we visualize the representative 3D object detection results, from our method and the baseline methods, such as VoteNet <ref type="bibr" target="#b19">[20]</ref>, MLCVNet <ref type="bibr">[36]</ref> and H3DNet <ref type="bibr" target="#b41">[41]</ref>. These results demonstrate that our method achieves more reliable detection results with more accurate bounding boxes and orientations. Our method also eliminates false positives and discovers more missing objects when compared with the baseline methods 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study and Discussions</head><p>Class-agnostic bounding box regression. Our method regresses the representative points in a class-agnostic way, which are then converted to the proposal's bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>MLCVNet VoteNet H3DNet <ref type="figure">Figure 4</ref>. Qualitative results of different 3D object detection methods on ScanNet V2 dataset <ref type="bibr" target="#b3">[4]</ref>. The baseline methods are VoteNet <ref type="bibr" target="#b19">[20]</ref>, MLCVNet <ref type="bibr">[36]</ref> and H3DNet <ref type="bibr" target="#b41">[41]</ref>. Best viewed on screen. Accuracy gain Intra-category size variance <ref type="figure">Figure 5</ref>. Class-agnostic bounding box regression works better on the categories with high intra-category size variances. For each category we show the relative accuracy gain (in blue dots) of the alternative method "VoteNet+CA-Reg" over VoteNet <ref type="bibr" target="#b19">[20]</ref> and intracategory size variance(in red squares), which is normalized by the mean category size.</p><p>Note VoteNet <ref type="bibr" target="#b19">[20]</ref> and its variants <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b41">41]</ref> have to estimate the sizes of object proposals in a class-aware way. Thus these baseline methods usually output the object sizes that can only moderately vary around the class-aware templates, and tend to falsely detect the objects when their sizes are unusual. To validate this observation, we implement an alternative method that employs a similar regression strategy as in our method but shares the same network as VoteNet <ref type="bibr" target="#b19">[20]</ref>. We term this variant as "VoteNet+CA-Reg". As shown in <ref type="table" target="#tab_3">Table 3</ref>, this variant significantly outperforms VoteNet. As shown in <ref type="figure">Figure 5</ref>, we also observe that this alternative method works better for the categories with high intra-category variance in sizes, and the mAP@0.50 gains of this alternative method over VoteNet on the SUN RGB-D dataset are positively related to size variances. Back-tracing, revisiting and refinement. Back-tracing the representative points should also be combined with the subsequent revisiting and refinement modules. As shown in <ref type="table" target="#tab_3">Table 3</ref>, we find this complete method has significant performance gains (∼ 10% mAP improvement on ScanNet and ∼ 6% mAP improvement on SUN RGB-D in terms of mAP@0.50) over the aforementioned baseline. The backtracing operation gives rough estimation of the object extent, and the revisiting and refining operations further update the proposal features with the reliable seed features in the neighborhood, thus offering better chance to produce more accurate detection results. Moreover, as shown in <ref type="figure">Figure 7</ref>, the revisited seed points by our method compactly cover the object's surface, while the corresponding seed points retrieved by the votes can only partially cover the surface, and also suffer from the outliers. Moreover, to validate whether the seed points can help improve the object detection results, we consider another variant (termed as "VoteNet+Seed-Pts") that VoteNet has its vote features fused with the corresponding seed points' features. In comparison to VoteNet, this alternative method also achieves non-trivial gains on both datasets, especially  image <ref type="figure">Figure 6</ref>. Qualitative comparison results of the 3D object detection methods on the SUN RGB-D dataset <ref type="bibr" target="#b30">[31]</ref>. The baseline methods are VoteNet <ref type="bibr" target="#b19">[20]</ref> and H3DNet <ref type="bibr" target="#b41">[41]</ref>. Best viewed on screen.</p><p>(a) Corresponding seed points (b) Revisited seed points <ref type="figure">Figure 7</ref>. Comparison between the corresponding seed points and the revisited seed points. The seed points are marked as blue points and the predicted bounding boxes are the green boxes. The revisited seed points completely cover the chair, while the corresponding seed points suffer from partial coverage and the outliers.</p><p>on ScanNet V2 in terms of mAP@0.50.</p><p>Sampling strategy of representative points. In <ref type="table" target="#tab_5">Table 4</ref>, we compare different sampling strategies to generate our representative points. "Ray" means uniform sampling along 6 directions between 0 and the maximum offsets. "Grid" means uniform sampling within the 3D bounding box spanned based on the predicted offsets. "#Pts" is the number of sampled points. Our methods using different strategies are generally comparable.</p><p>Model size and speed. As listed in <ref type="table" target="#tab_6">Table 5</ref>, our proposed method is efficient in comparison to VoteNet, and is 3× faster than the current state-of-the-art H3DNet <ref type="bibr" target="#b41">[41]</ref>, when evaluated on both datasets. Its model size is marginally increased from that of VoteNet, and around 4× smaller than that of H3DNet. Knowing that the proposed method has significant performance gains than these reference methods (as discussed in Sec. 4.2), its lightweight model validates that the proposed back-tracing strategy is significant for 3D object detection in point clouds 2 .</p><p>Number of Backbones. Our BRNet can also be improved <ref type="bibr" target="#b1">2</ref> Note that MLCVNet does not provide a checkpoint for the SUN RGB-D dataset, we omit its comparison on this dataset.  after using 4 backbones, and it achieves the result of 51.8% in terms of mAP@0.50 on ScanNet <ref type="bibr" target="#b3">[4]</ref>, which outperforms H3DNet (4 backbones) with a remarkable margin (+3.7%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have introduced a new approach to improve the voting-based 3D object detection method by generatively and class-agnostically back-tracing the representative points. We revisit the seed points around the backtraced representative points and extract fine object surface features to generate the high-quality object proposals. Comprehensive ablation studies show the importance and effectiveness of the proposed back-tracing, revisiting and refinement operations. Qualitative and quantitative results further demonstrate that our method remarkably outperforms the existing methods while bringing negligible increases in model size and executive time compared with VoteNet <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary</head><p>This supplementary provides more quantitative results of our method (Sec. A.1), more qualitative results (Sec. A.1), and finally implementation details (Sec. A.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. More Quantitative Results</head><p>Finer performance evaluations. We try to evaluate our method using mean average precision with multiple IoU thresholds for finer performance evaluations in <ref type="table" target="#tab_1">Table S1</ref> and S2. We use mAP@0.25, mAP@0.50, mAP@0.75 to evaluate different methods, i.e. VoteNet <ref type="bibr" target="#b19">[20]</ref>, HGNet <ref type="bibr" target="#b1">[2]</ref>, MLCVNet <ref type="bibr">[36]</ref>, H3DNet <ref type="bibr" target="#b41">[41]</ref> and our BRNet .</p><p>Our method performs the best on the metrics mAP@0.50 and mAP@0.75. Notably, mAP@0.75 requires more than 90% coverage in each dimension of a bounding box, which is very challenging for a detector. Our method gains −1.1%, 2.8%, 3.7% increase on mAP@0.25, mAP@0.50, mAP@0.75 compared with H3DNet [41] using 4 Point-Net++ backbones and doubled input point clouds (i.e., 20, 000 points by our BRNet , and 40, 000 points by H3DNet) on the ScanNetV2 dataset. On more challenging evaluation metrics, our method has more gain, which shows the importance of our representative point generation, and its benefits for seed points revisiting and finer surface feature extraction to accurately detect objects with more reliable bounding boxes. Per-category results. We show the per-category results on ScanNet V2 dataset with 3D IoU threshold 0.25 in <ref type="table" target="#tab_3">Table S3</ref>, and the per-category results on SUN RGB-D with both 3D IoU thresholds 0.25 and 0.50 in <ref type="table" target="#tab_5">Table S4</ref> and S5. In terms of the accuracy about the object detection, our approach outperforms the baseline VoteNet <ref type="bibr" target="#b19">[20]</ref> and prior state-of-the-art method H3DNet <ref type="bibr" target="#b41">[41]</ref> significantly. For objects in the SUN RGB-D dataset, our approach can gain 7.9%, 10.9%, 6.7%, 7.6%, 6.3% increase on Bathtub, Bed, Dresser, Nightstand and Sofa compared with H3DNet <ref type="bibr" target="#b41">[41]</ref>. These improvements are achieved by using back-tracing and seed points revisiting to better capture object surface features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. More Qualitative Results</head><p>We provide more qualitative comparisons between our method and the top-performing reference methods, such as VoteNet <ref type="bibr" target="#b19">[20]</ref>, MLCVNet <ref type="bibr">[36]</ref> and H3DNet <ref type="bibr" target="#b41">[41]</ref>, on the ScanNet V2 and SUN RGB-D datasets, as shown in <ref type="figure">Fig. S2</ref> and <ref type="figure">Fig. S3</ref>, respectively. Our method can generate highquality and compact predicted bounding boxes compared with the other reference methods.</p><p>We also show two typical failure cases in <ref type="figure">Fig. S1</ref>. Our BRNet cannot avoid the existence of false positive predicted bounding boxes which appear on the hollow floor. Also, it is hard for our method to detect objects on the smooth wall, especially windows and pictures. We need to mention that these failure cases are also common, and hard for the reference methods. It is an interesting and significant future direction of our work to tackle these false positives when points are over sparse and increase the robustness when perceiving objects within the cluttered background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Implementation Details</head><p>As mentioned in the main paper, the BRNet consists of four modules: (1) vote generation and clustering, (2) backtraced representative points generation, (3) seed points revisiting, and (4) proposal refinement and classification followed by 3D NMS. Here we elaborate the implementation details with respect to each module.</p><p>Vote generation and clustering. We follow the same network architecture and vote regression loss as in VoteNet <ref type="bibr" target="#b19">[20]</ref>.</p><p>Representative point generation. It has output sizes of 128, 128, 6 + 2 × N H for the three MLP layers, where N H is the number of heading bins for estimating the orientations, 6 is the 6 distance offsets from vote point to object surface (front/back/left/right/up/down) in the canonical coordinate centered at the vote point. Then we sample 2 representative points on each skewed direction as the back-traced representative points, thus we have 12 representative points per proposal.</p><p>Seed point revisiting. We use the set abstraction module (SA module) to aggregate seed points features within 0.2m radius surrounding a back-traced representative point. The SA module has the output size of 128, 64, 32 for the MLP layers. After revisiting seed points, we get a 32 di-mensional feature vector for each representative point. We concatenate the representative point features in a predefined local-structure-aware order to a 32 × 12 = 384 dimensional feature vector per proposal. The feature vector is then projected to 128-dimensional as the captured surface feature of the object proposal. Proposal refinement and classification. The input is the 256 dimensional fused feature vector which is the concatenation of 128-D vote cluster feature and 128-D revisited seed point feature. Then the fused feature is fed into a threelayer MLP, whose output sizes are 128, 128, 9 + N C . N C is the number of semantic classes, i.e., N C = 10 for the SUN RGB-D dataset <ref type="bibr" target="#b30">[31]</ref> and N C = 18 for ScanNet V2 dataset <ref type="bibr" target="#b3">[4]</ref>. In the first 9 channels, the first two are for objectness classification, the following one is for heading angle refinement and the last six are for distance offsets refinement. GT Ours <ref type="figure">Figure S1</ref>. Samples of failure cases on ScanNet V2 dataset. <ref type="table" target="#tab_6">Table S5</ref>. 3D object detection results on SUN RGB-D val dataset. We show per-category results of average precision (AP) with 3D IOU threshold 0.50 as proposed in <ref type="bibr" target="#b30">[31]</ref>, and mean of AP across all semantic classes. For fair comparison with previous methods, the evaluation is on the SUN RGB-D V1 data. *Note that H3DNet <ref type="bibr" target="#b41">[41]</ref> sub-samples 40, 000 points from every scene in SUN RGB-D dataset, while others use 20, 000 points. Also, H3DNet <ref type="bibr" target="#b41">[41]</ref>  GT Ours VoteNet MLCVNet H3DNet <ref type="figure">Figure S2</ref>. More qualitative results on ScanNet V2 dataset <ref type="bibr" target="#b3">[4]</ref>. The reference methods are VoteNet <ref type="bibr" target="#b19">[20]</ref>, MLCVNet <ref type="bibr">[36]</ref> and H3DNet <ref type="bibr" target="#b41">[41]</ref>. Best viewed on screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours H3DNet VoteNet Bed</head><p>Chair Sofa image <ref type="figure">Figure S3</ref>. More qualitative results on SUN RGB-D dataset <ref type="bibr" target="#b30">[31]</ref>. The reference methods are VoteNet <ref type="bibr" target="#b19">[20]</ref> and H3DNet <ref type="bibr" target="#b41">[41]</ref>. Best viewed on screen.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>A) False positive predicted bounding box. GT Ours (B) Miss some objects in ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Input: point cloud Clustered votes (XYZ + feature) Back-traced representative points Revisited seeds Output: 3D bounding boxes chair table Input point cloud Clustered votesObject proposals 3D NMS Style-3 boxes Back-traced representative points</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Revisited</cell><cell></cell></row><row><cell>Vote Generation</cell><cell>Representative Points</cell><cell>Seed Points</cell><cell>seed features</cell><cell>Proposal Refinement &amp;</cell></row><row><cell>&amp; Clustering</cell><cell>Generation</cell><cell>Revisiting</cell><cell></cell><cell>Classification</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>3D object detection results on the ScanNet V2 validation set(left) and the SUN RGB-D V1 validation set(right). Evaluation metric is average precision with 3D IOU thresholds as 0.25 and 0.50. *Note for fair comparison, we report the results of H3DNet on the ScanNet V2 dataset under both 1 and 4 PointNet++ backbones (BB) settings. While we only report the result of H3DNet with 4 PointNet++ backbones (BB) on the SUN RGB-D dataset, as the work<ref type="bibr" target="#b41">[41]</ref> only reports the result under this setting.</figDesc><table><row><cell>ScanNet V2</cell><cell>Input</cell><cell cols="2">mAP@0.25 mAP@0.50</cell><cell>SUN RGB-D</cell><cell>Input</cell><cell>mAP@0.25 mAP@0.50</cell></row><row><cell>DSS [33]</cell><cell>Geo + RGB</cell><cell>15.2</cell><cell>6.8</cell><cell>DSS</cell><cell></cell></row><row><cell>F-PointNet [21]</cell><cell>Geo + RGB</cell><cell>19.8</cell><cell>10.8</cell><cell></cell><cell></cell></row><row><cell>GSPN [40]</cell><cell>Geo + RGB</cell><cell>30.6</cell><cell>17.7</cell><cell></cell><cell></cell></row><row><cell>3D-SIS [7]</cell><cell>Geo + 5 views</cell><cell>40.2</cell><cell>22.5</cell><cell></cell><cell></cell></row><row><cell>VoteNet [20]</cell><cell>Geo only</cell><cell>58.6</cell><cell>33.5</cell><cell></cell><cell></cell></row><row><cell>HGNet [2]</cell><cell>Geo only</cell><cell>61.3</cell><cell>34.4</cell><cell></cell><cell></cell></row><row><cell>MLCVNet [36]</cell><cell>Geo only</cell><cell>64.7</cell><cell>42.1</cell><cell></cell><cell></cell></row><row><cell>H3DNet (1BB)* [41]</cell><cell>Geo only</cell><cell>64.4</cell><cell>43.4</cell><cell></cell><cell></cell></row><row><cell>H3DNet (4BB)* [41]</cell><cell>Geo only</cell><cell>67.2</cell><cell>48.1</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>Geo only</cell><cell>66.1</cell><cell>50.9</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>3D object detection results on the ScanNet V2 validation set. The evaluation metric is the average precision with 3D IOU threshold as 0.50. *Note that for H3DNet only the per-category results with 4 PointNet++ backbones are reported in<ref type="bibr" target="#b41">[41]</ref>.</figDesc><table><row><cell>ScanNet V2</cell><cell>cab</cell><cell cols="5">bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn avg</cell></row><row><cell>VoteNet [20]</cell><cell cols="2">8.1 76.1 67.2 68.8 42.4 15.3</cell><cell>6.4</cell><cell>28.0 1.3 9.5 37.5 11.6 27.8</cell><cell>10.0</cell><cell>86.5 16.8 78.9 11.7 33.5</cell></row><row><cell cols="4">MLCVNet [36] 16.6 83.3 78.1 74.7 55.1 28.1 17.0</cell><cell>51.7 3.7 13.9 47.7 28.6 36.3</cell><cell>13.4</cell><cell>70.9 25.6 85.7 27.5 42.1</cell></row><row><cell cols="4">H3DNet* [41] 20.5 79.7 80.1 79.6 56.2 29.0 21.3</cell><cell>45.5 4.2 33.5 50.6 37.3 41.4</cell><cell>37.0</cell><cell>89.1 35.1 90.2 35.4 48.1</cell></row><row><cell>Ours</cell><cell cols="3">28.7 80.6 81.9 80.6 60.8 35.5 22.2</cell><cell>48.0 7.5 43.7 54.8 39.1 51.8</cell><cell>35.9</cell><cell>88.9 38.7 84.4 33.0 50.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Quantitative ablation experiments on ScanNet V2 and</cell></row><row><cell cols="5">SUN RGB-D datasets. "+CA-Reg" means VoteNet [20] with</cell></row><row><cell cols="5">a class-agnostic bounding box regressor, "+Seed-Pts" indicates</cell></row><row><cell cols="5">VoteNet with votes fused with their corresponding seed points.</cell></row><row><cell></cell><cell cols="2">ScanNet V2</cell><cell cols="2">SUN RGB-D</cell></row><row><cell></cell><cell cols="4">mAP@0.25 mAP@0.50 mAP@0.25 mAP@0.50</cell></row><row><cell>VoteNet</cell><cell>58.6</cell><cell>33.5</cell><cell>57.7</cell><cell>32.9</cell></row><row><cell>+CA-Reg</cell><cell>59.3</cell><cell>40.8</cell><cell>58.2</cell><cell>37.6</cell></row><row><cell>+Seed-Pts</cell><cell>59.1</cell><cell>37.6</cell><cell>59.5</cell><cell>33.6</cell></row><row><cell>Ours</cell><cell>66.1</cell><cell>50.9</cell><cell>61.1</cell><cell>43.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table Bookshelf</head><label>Bookshelf</label><figDesc></figDesc><table><row><cell>Toilet</cell></row><row><cell>Desk</cell></row><row><cell>Bathtub</cell></row><row><cell>Dresser</cell></row><row><cell>Nght. Std.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Results of BRNet using different RP sampling strategies.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ScanNet V2</cell><cell cols="2">SUN RGB-D</cell></row><row><cell cols="2">Types #Pts</cell><cell cols="4">mAP@0.25 mAP@0.50 mAP@0.25 mAP@0.50</cell></row><row><cell>Ray</cell><cell>6</cell><cell>65.0</cell><cell>48.3</cell><cell>60.3</cell><cell>42.7</cell></row><row><cell>Ray</cell><cell>12</cell><cell>66.1</cell><cell>50.9</cell><cell>61.1</cell><cell>43.7</cell></row><row><cell>Ray</cell><cell>18</cell><cell>65.8</cell><cell>48.4</cell><cell>60.4</cell><cell>42.9</cell></row><row><cell>Grid</cell><cell>8</cell><cell>65.4</cell><cell>49.1</cell><cell>59.9</cell><cell>42.2</cell></row><row><cell>Grid</cell><cell>27</cell><cell>66.0</cell><cell>49.2</cell><cell>60.2</cell><cell>42.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Model size and processing time comparison of different methods, which are evaluated on a NVIDIA GeForce RTX 2080 Ti GPU card with the same configuration. #BB means the number of backbones used for feature extraction.</figDesc><table><row><cell>Method</cell><cell cols="4">#BB Model size ScanNet SUN RGB-D</cell></row><row><cell>VoteNet [20]</cell><cell>1</cell><cell>11.2MB</cell><cell>0.130s</cell><cell>0.076s</cell></row><row><cell>MLCVNet [36]</cell><cell>1</cell><cell>13.9MB</cell><cell>0.141s</cell><cell>-</cell></row><row><cell>H3DNet [41]</cell><cell>4</cell><cell>56.0MB</cell><cell>0.330s</cell><cell>0.241s</cell></row><row><cell>Ours</cell><cell>1</cell><cell>12.9MB</cell><cell>0.133s</cell><cell>0.079s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S1 .</head><label>S1</label><figDesc>3D object detection results on ScanNetV2 dataset with multiple IoU thresholds. *Note that H3DNet<ref type="bibr" target="#b41">[41]</ref> only provide the checkpoint with 4 PointNet++ backbones as we use here.Table S2. 3D object detection results on SUN RGB-D dataset with multiple IoU thresholds. *Note that H3DNet<ref type="bibr" target="#b41">[41]</ref> only provide the checkpoint with 4 PointNet++ backbones as we use here. Also H3DNet use 40, 000 points for SUN RGB-D dataset as input, while others use 20, 000 points.</figDesc><table><row><cell>ScanNet V2</cell><cell cols="3">mAP@0.25 mAP@0.50 mAP@0.75</cell></row><row><cell>VoteNet [20]</cell><cell>58.6</cell><cell>33.5</cell><cell>3.4</cell></row><row><cell>HGNet [2]</cell><cell>61.3</cell><cell>34.4</cell><cell>-</cell></row><row><cell>MLCVNet [36]</cell><cell>64.7</cell><cell>42.1</cell><cell>7.4</cell></row><row><cell>H3DNet* [41]</cell><cell>67.2</cell><cell>48.1</cell><cell>15.4</cell></row><row><cell>Ours</cell><cell>66.1</cell><cell>50.9</cell><cell>19.1</cell></row><row><cell>SUN RGB-D</cell><cell cols="3">mAP@0.25 mAP@0.50 mAP@0.75</cell></row><row><cell>VoteNet [20]</cell><cell>57.7</cell><cell>32.9</cell><cell>1.2</cell></row><row><cell>HGNet [2]</cell><cell>61.6</cell><cell>-</cell><cell>-</cell></row><row><cell>MLCVNet [36]</cell><cell>59.8</cell><cell>-</cell><cell>-</cell></row><row><cell>H3DNet* [41]</cell><cell>60.1</cell><cell>39.0</cell><cell>3.5</cell></row><row><cell>Ours</cell><cell>61.1</cell><cell>43.7</cell><cell>5.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>only reports the per-category results with 4 PointNet++ backbones, while others with only 1 PointNet++ backbone. SUN RGB-D bathtub bed bookshelf chair desk dresser nightstand sofa table toilet mAP@0.50</figDesc><table><row><cell>VoteNet[20]</cell><cell>49.9</cell><cell>47.3</cell><cell>4.6</cell><cell>54.1</cell><cell>5.2</cell><cell>13.6</cell><cell>35.0</cell><cell>41.4 19.7 58.6</cell><cell>32.9</cell></row><row><cell>H3DNet*[41]</cell><cell>47.6</cell><cell>52.9</cell><cell>8.6</cell><cell>60.1</cell><cell>8.4</cell><cell>20.6</cell><cell>45.6</cell><cell>50.4 27.1 69.1</cell><cell>39.0</cell></row><row><cell>Ours</cell><cell>55.5</cell><cell>63.8</cell><cell>9.3</cell><cell cols="2">61.6 10.0</cell><cell>27.3</cell><cell>53.2</cell><cell>56.7 28.6 70.9</cell><cell>43.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table Bookshelf</head><label>Bookshelf</label><figDesc></figDesc><table><row><cell>Toilet</cell></row><row><cell>Desk</cell></row><row><cell>Bathtub</cell></row><row><cell>Dresser</cell></row><row><cell>Nght. Std.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">MLCVNet does not provide a checkpoint for the SUN RGB-D dataset<ref type="bibr" target="#b30">[31]</ref> thus we cannot provide its visualization results on this dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Density-based clustering for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariam</forename><surname>Syeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Meng</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10608" to="10617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical graph network for 3D object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biwen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view 3D object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alireza Fathi, Bastian Leibe, and Matthias Nießner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9031" to="9040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D-SIS: 3D semantic instance segmentation of RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4421" to="4430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate localization of 3D objects from RGB-D data using segmentation hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shili</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3182" to="3189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scene cut: Class-specific object detection and segmentation in 3D scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukta</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="7389" to="7398" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2D-driven 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Bastian Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Database-assisted object retrieval for real-time 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3D object detection with RGBD cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A search-classify approach for cluttered indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Sharf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3D object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PV-RCNN: Pointvoxel feature set abstraction for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PointR-CNN: 3D object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3D object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DeNet: Scalable real-time object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="428" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MLCVNet: Multi-level context votenet for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">RepPoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D-SSD: Point-based 3D single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GSPN: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3947" to="3956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">H3DNet: 3D object detection using hybrid geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-end learning for point cloud based 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evaluation metric is the average precision with 3D IOU threshold 0.25. *Note that H3DNet [41] only reports the per-category results with 4 PointNet++ backbones, while others with only 1 PointNet++ backbone. ScanNet V2 cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP@0</title>
	</analytic>
	<monogr>
		<title level="m">Table S3. 3D object detection results on ScanNet V2 validation dataset</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Votenet</surname></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">We show per-category results of average precision (AP) with 3D IOU threshold 0.25 as proposed in [31], and mean of AP across all semantic classes. For fair comparison with previous methods, the evaluation is on the SUN RGB-D V1 data. *Note that H3DNet [41] sub-samples 40, 000 points from every scene in SUN RGB-D dataset</title>
	</analytic>
	<monogr>
		<title level="m">Table S4. 3D object detection results on SUN RGB-D val dataset</title>
		<imprint/>
	</monogr>
	<note>while others use 20, 000 points. Also, H3DNet [41] only reports the per-category results with 4 PointNet++ backbones, while others with only 1 PointNet++ backbone. SUN RGB-D bathtub bed bookshelf chair desk dresser nightstand sofa table toilet mAP@0.25</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
