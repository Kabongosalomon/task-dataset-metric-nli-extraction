<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face Search at Scale: 80 Million Gallery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-07-28">28 Jul 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Dayong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Charles</forename><surname>Otto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
						</author>
						<title level="a" type="main">Face Search at Scale: 80 Million Gallery</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-07-28">28 Jul 2015</date>
						</imprint>
					</monogr>
					<note>MSU TECHNICAL REPORT MSU-CSE-15-11, JULY 24, 2015 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-face search</term>
					<term>unconstrained face recognition</term>
					<term>deep learning</term>
					<term>big data</term>
					<term>cascaded system</term>
					<term>scalability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the prevalence of social media websites, one challenge facing computer vision researchers is to devise methods to process and search for persons of interest among the billions of shared photos on these websites. Facebook revealed in a 2013 white paper that its users have uploaded more than 250 billion photos, and are uploading 350 million new photos each day. Due to this humongous amount of data, large-scale face search for mining web images is both important and challenging. Despite significant progress in face recognition, searching a large collection of unconstrained face images has not been adequately addressed. To address this challenge, we propose a face search system which combines a fast search procedure, coupled with a state-of-the-art commercial off the shelf (COTS) matcher, in a cascaded framework. Given a probe face, we first filter the large gallery of photos to find the top-k most similar faces using deep features generated from a convolutional neural network. The k retrieved candidates are re-ranked by combining similarities from deep features and the COTS matcher. We evaluate the proposed face search system on a gallery containing 80 million web-downloaded face images. Experimental results demonstrate that the deep features are competitive with state-of-the-art methods on unconstrained face recognition benchmarks (LFW and IJB-A). More specifically, on the LFW database, we achieve 98.23% accuracy under the standard protocol and a verification rate of 87.65% at FAR of 0.1% under the BLUFR protocol. For the IJB-A benchmark, our accuracies are as follows: TAR of 51.4% at FAR of 0.1% (verification); Rank 1 retrieval of 82.0% (closed-set search); FNIR of 61.7% at FPIR of 1% (open-set search). Further, the proposed face search system offers an excellent trade-off between accuracy and scalability on datasets consisting of millions of images. Additionally, in an experiment involving searching for face images of the Tsarnaev brothers, convicted of the Boston Marathon bombing, the proposed cascade face search system could find the younger brother's (Dzhokhar Tsarnaev) photo at rank 1 in 1 second on a 5M gallery and at rank 8 in 7 seconds on an 80M gallery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Social media has become pervasive in our society. It is hence not surprising that a growing segment of the population has a Facebook, Twitter, Google, or Instagram account. One popular aspect of social media is the sharing of personal photographs. Facebook revealed in a 2013 white paper that its users have uploaded more than 250 billion photos, and are uploading 350 million new photos each day 1 . To enable automatic tagging of these images, strong face recognition capabilities are needed. Given an uploaded photo, Facebook and Google's tag suggestion systems automatically detect faces and then suggest possible name tags based on the similarity between facial templates generated from the input photo and previously tagged photographs in their datasets. In the law enforcement domain, the FBI plans to include over 50 million photographs in its Next Generation Identification (NGI) dataset 2 , with the goal of providing investigative leads by searching the gallery for images similar to a suspect's photo. Both tag suggestion in social networks and searching for a suspect in criminal investigations are examples of the face search problem ( <ref type="figure" target="#fig_0">Fig. 1)</ref>. We address the large-scale face search problem in the context of social media and other web applications where face images are generally unconstrained in terms of pose, expression, and illumination <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>The major focus in face recognition literature lately has been to improve face recognition accuracy, particularly on the Labeled Faces in the Wild (LFW) dataset <ref type="bibr" target="#b2">[3]</ref>. But, the problem of scale  in face recognition has not been adequately addressed <ref type="bibr" target="#b2">3</ref> . It is now accepted that the small size of the LFW dataset (13, 233 images of 5, 749 subjects) and the limitations in the LFW protocol do not address the two major challenges in large-scale face search: (i) loss in search accuracy with the size of the dataset, and (ii) increase in computational complexity with dataset size.</p><p>The typical approach to scalability (e.g. content-based image retrieval <ref type="bibr" target="#b1">[2]</ref>) is to represent objects with feature vectors and employ an indexing or approximate search scheme in the feature space. A vast majority of face recognition approaches, irrespective of the representation scheme, are ultimately based on fixed length feature vectors, so employing feature space methods is feasible.</p><p>However, some techniques are not compatible with feature space approaches such as pairwise comparison models (e.g. Joint-Bayes <ref type="bibr" target="#b4">[5]</ref>), which have been shown to improve face recognition accuracy. Additionally, most COTS face recognition SDKs define pairwise comparison scores but do not reveal the underlying feature vectors, so they are also incompatible with feature-space approaches. Therefore, using a feature space based approximation method alone may not be sufficient.</p><p>To address the issues of search performance and search time for large datasets (80M face images used here), we propose a cascaded face search framework <ref type="figure" target="#fig_1">(Fig. 2)</ref>. In essence, we decompose the search problem into two steps: (i) a fast filtering step, which uses an approximation method to return a short candidate list, and (ii) a re-ranking step, which re-ranks the candidate list with a slower pairwise comparison operation, resulting in a more accurate search. The fast filtering step utilizes a deep convolutional network (ConvNet), which is an efficient implementation of the architecture in <ref type="bibr" target="#b5">[6]</ref>, with product quantization (PQ) <ref type="bibr" target="#b6">[7]</ref>. For the re-ranking step, a COTS face matcher (one of the top performers in the 2014 NIST FRVT <ref type="bibr" target="#b7">[8]</ref>) is used. The main contributions of this paper are as follows:</p><p>• An efficient deep convolutional network for face recognition, trained on a large public domain data (CASIA <ref type="bibr" target="#b5">[6]</ref>), which improves upon the baseline results reported in <ref type="bibr" target="#b5">[6]</ref>.</p><p>• A large-scale face search system, leveraging the deep network representation combined with a state-of-the-art COTS face matcher in a cascaded scheme.</p><p>• Studies on three types of face datasets of increasing complexity: the PCSO mugshot dataset, the LFW dataset (only includes faces detectable by a Viola-Jones face detector), and the IJB-A dataset (includes faces which are not automatically detectable).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The largest face search experiments conducted to date on the LFW <ref type="bibr" target="#b2">[3]</ref> and IJB-A <ref type="bibr" target="#b8">[9]</ref> face datasets with an 80M gallery.</p><p>• Using face images of the Tsarnaev brothers involved in the Boston Marathon bombing as queries, we show that Dzhokhar Tsarnaev's photo could be identified at rank 8 when searching against the 80M gallery.</p><p>The rest of this paper is organized as follows. Section 2 reviews related work on face search. Section 3 details the proposed deep learning architecture and large-scale face search framework. Section 4 introduces the face image datasets used in our experiments. Section 5 presents experiments illustrating the performance of the deep face representation features on face recognition tasks of increasing difficulty (including public domain benchmarks). Section 6 presents large-scale face search results (with 80M web downloaded face images). Section 7 presents a case study based on the Tsarnev brothers, convicted in the 2013 Boston Marathon bombing. Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Face search has been extensively studied in multimedia and computer vision literature <ref type="bibr" target="#b19">[20]</ref>. Early studies primarily focused on faces captured under constrained conditions, e.g. the FERET dataset <ref type="bibr" target="#b13">[14]</ref>. However, due to the growing need for strong face recognition capability in the social media context, ongoing research is focused on faces captured under more challenging conditions in terms of pose, expression, illumination and aging, similar to images in the public domain datasets LFW <ref type="bibr" target="#b2">[3]</ref> and IJB-A <ref type="bibr" target="#b8">[9]</ref>.</p><p>The three main challenges in large-scale face search are: i) face representation, ii) approximate k-NN search, and iii) gallery selection and evaluation protocol. For the face representation, features learned from deep networks (deep features) have been shown to saturate performance on the standard LFW evaluation protocol <ref type="bibr" target="#b3">4</ref> . For example, the best recognition performance reported to date on LFW (99.65%) <ref type="bibr" target="#b20">[21]</ref> used a deep learning approach leveraging training with 1M images of 20K individuals (outside the protocol). A comparable result (99.63%) was achieved by a Google team <ref type="bibr" target="#b21">[22]</ref> by training a deep model with about 150M images of 8M subjects. It has even been reported that deep features exceed the human face recognition accuracy (99.20% <ref type="bibr" target="#b9">[10]</ref>) on the LFW dataset. To push the frontiers of unconstrained face recognition, the IJB-A dataset was released in 2015 <ref type="bibr" target="#b8">[9]</ref>. IJB-A contains face images that are more challenging than LFW in terms of both face detection and face recognition. In order to recognize web downloaded unconstrained face images, we also adopt a deep learning based face representation by improving the architecture outlined in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Given our goal of using deep features to filter a large gallery to a small set of candidate face images, we use approximate k-NN search to improve scalability. There are three main approaches for approximate face search:</p><p>• Inverted Indexing. Following the traditional bag-of-words representation <ref type="bibr" target="#b22">[23]</ref>, Wu et al. <ref type="bibr" target="#b1">[2]</ref> designed a componentbased local face representation for inverted indexing. They first split aligned face images into a set of small blocks around the detected facial landmarks and then quantized each block into a visual word using an identity-based quantization scheme. The candidate images were retrieved from the inverted index of visual words. Chen et al. <ref type="bibr" target="#b0">[1]</ref> improved the search performance in <ref type="bibr" target="#b1">[2]</ref> by leveraging human attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Hashing. Yan et al. <ref type="bibr" target="#b14">[15]</ref> proposed a spectral regression algorithm to project facial features into a discriminative space; a cascaded hashing scheme (similarity hashing) was used for efficient search. Wang et al. <ref type="bibr" target="#b23">[24]</ref> proposed a weak label regularized sparse coding to enhance facial features and adopted the Locality-Sensitive Hash (LSH) <ref type="bibr" target="#b24">[25]</ref> to index the gallery.</p><p>• Product Quantization (PQ). Unlike the previous two approaches which require index vectors to be stored in main memory, PQ <ref type="bibr" target="#b6">[7]</ref> is a compact discrete encoding method that can be used either for exhaustive search or inverted indexing search. In this work, we adopt product quantization for fast filtering.</p><p>Face search systems published in the literature have been mainly evaluated under closed-set protocols <ref type="table" target="#tab_1">(Table 1)</ref>, which assume that the subject in the probe image is present in the gallery. However, in many large scale applications (e.g., surveillance and watch list scenarios), open-set search performance, where the probe subject may not be present in the gallery, is more relevant and appropriate.</p><p>A search operating in open-set protocol requires two steps: first determine if the identity associated with the face in the probe is present in the gallery, and if so find the top-k most similar faces in 4. http://vis-www.cs.umass.edu/lfw/results.html  <ref type="bibr" target="#b18">[19]</ref>. However, even in these two protocols used on benchmark datasets, the gallery sizes are fairly small (3, 143 and 1, 000 gallery images), due to the inherent small size of the LFW dataset. <ref type="table" target="#tab_1">Table 1</ref> shows that the largest face gallery size reported in the literature to date is about 1M, which is not even close to being a representative of social media and forensic applications. To tackle these two limitations, we evaluate the proposed cascaded face search system with an 80M face gallery under closed-set and open-set protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FACE SEARCH FRAMEWORK</head><p>Given a probe face image, a face search system aims to find the top-k most similar face images in the gallery. To handle large galleries (e.g. tens of millions of images), we propose a cascaded face search structure, designed to speed up the search process while achieving acceptable accuracy <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b25">[26]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> outlines the proposed face search architecture consisting of three main steps: i) template generation module which extracts features for the N gallery faces offline as well as from the probe face; ii) face filtering module which compares the probe representation against the gallery representations using product quantization to retrieve the top-k most similar candidates (k ≪ N ); and (iii) re-ranking module which fuses similarity scores of deep features with scores from a COTS face matcher to generate a new ordering of the k candidates. These three modules are discussed in detail in the remainder of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Template Generation</head><p>Given a face image I, the template generator is a non-linear mapping function</p><formula xml:id="formula_0">F (I) = x ∈ R d<label>(1)</label></formula><p>which projects I into a d-dimensional feature space. The discriminative ability of the template is critical for the accuracy of the search system. Given the impressive performance of deep learning techniques in various machine learning applications, particularly face recognition, we adopt deep learning for template generation. The architecture of the proposed deep ConvNet <ref type="figure" target="#fig_2">(Fig. 3</ref>) is inspired by <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b26">[27]</ref>. There are four main differences between the proposed network and the one in <ref type="bibr" target="#b5">[6]</ref>: i) input to the network is color images instead of gray images; ii) a robust face alignment procedure; iii) an additional data argumentation step that randomly crops a 100 × 100 region from the 110 × 110 input color image; and iv) deleting the contrastive cost layer for computational efficiency (experimentally, this did not hinder recognition accuracy).</p><p>The proposed deep convolutional network has three major parts: i) convolution and pooling layers, ii) a feature representation layer, and iii) an output classification layer. For the convolution layers, we adopt a very deep architecture <ref type="bibr" target="#b27">[28]</ref> (10 convolution layers in total) and filters with small supports (3 × 3). The small filters reduce the total number of parameters to be learned, and the very deep architecture enhances the nonlinearity of the network. Based on the basic assumption that face images usually lie on a low dimensional manifold, the network outputs 320 dimensional feature vector.</p><p>The input layer accepts the RGB values of the aligned face image pixels. Faces are aligned as follows: i) Use the DLIB 5 implementation of Kazemi and Sullivan's ensemble of regression 5. http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html trees method <ref type="bibr" target="#b28">[29]</ref> to detect 68 facial landmarks (see <ref type="figure">Fig. 4</ref>); ii) rotate the face in the image plane to make it upright based on the eye positions; iii) find a central point on the face (the blue point in <ref type="figure">Fig. 4</ref>) by taking the mid-point between the leftmost and rightmost landmarks; the center points of the eyes and mouth (red points in <ref type="figure">Fig. 4</ref>) are found by averaging all the landmarks in the eye and mouth regions, respectively; iv) center the faces in the x-axis, based on the central point (blue point); v) fix the position along the y-axis by placing the eye center point at 45% from the top of the image and the mouth center point at 25% from the bottom of the image, respectively; vi) resize the image to a resolution of 110×110. Note that the computed midpoint is not consistent across pose. In faces exhibiting significant yaw, the computed midpoint will be different from the one computed in a frontal image, so facial landmarks are not aligned consistently across yaw.</p><p>(a) (b) (c) <ref type="figure">Fig. 4</ref>. A face image alignment example. The original image is shown in (a); (b) shows the 68 landmark points detected by the method in <ref type="bibr" target="#b28">[29]</ref>, and (c) is the final aligned face image, where the blue circle was used to center the face image along the x-axis, and the red circles denote the two points used for face cropping.</p><p>We augment our training set using a couple of image transform operations: transformed versions of the input image are obtained by randomly applying horizontal reflection, and cropping random 100 × 100 sub-regions from the original 110 × 110 aligned faces images.</p><p>Following the input layer, there are 10 convolutional layers, 4 max-pooling layers, and 1 average-pooling layer. To enhance the nonlinearity, every pair of convolutional layers is grouped together and connected sequentially. The first four groups of convolutional layers are followed by a max-pooling layer with a window size of 2 × 2 and a stride of 2, while the last group of convolutional layers is followed by an average-pooling layer with window size 7 × 7. The dimensionality of the feature representation layer is the same as the number of filters in the last convolutional layer. As discussed in <ref type="bibr" target="#b5">[6]</ref>, the ReLU <ref type="bibr" target="#b29">[30]</ref> neuron produces a sparse vector, which is undesirable for a face representation layer. In our network, we use ReLU neurons <ref type="bibr" target="#b29">[30]</ref> in all the convolutional layers, except the last one, which is combined with an average-pooling layer to generate a low dimensional face representation with a dimensionality of 320.</p><p>Although multiple fully-connected layers are used in <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, in our network we directly feed the deep features generated by the feature layer to an N -way softmax (where N = 10, 575 is the number of subjects in our training set). We regularize the feature representation layer using dropout <ref type="bibr" target="#b30">[31]</ref>, keeping 60% of the feature components as-is and randomly setting the remaining 40% to zero during training.</p><p>We use a softmax loss function for our network, and train it using the standard back-propagation method. We implement the network using the open source cuda-convnet2 6 library. We set the weight decay of all layers to 5 × 10 −4 . The learning rate for stochastic gradient descent (SGD) is initialized to 10 −2 , and gradually reduced to 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Face Filtering</head><p>Given a probe face I and a template generation function F , finding the top-k most similar faces C k (I) in the gallery G is formulated as follows:</p><formula xml:id="formula_1">C k (I) = Rank k ({S(F (I), F (J i ))|J i=1,2,...,N ∈ G}) (2)</formula><p>where N is the size of gallery G, S is a function, which measures the similarity of the probe face I and the gallery image J i , and Rank is a function that finds the top-k largest values in an array. The computational complexity of naive face comparison functions is linear with respect to the gallery size N and the feature dimensionality d. However, approximate nearest neighbor (ANN) algorithms, which improve runtime without a significant loss in accuracy, have become popular for large galleries.</p><p>Various approaches have been proposed for ANN search. Hashing based algorithms use compact binary representations to conduct an exhaustive nearest neighbor search in Hamming space. Although multiple hash tables <ref type="bibr" target="#b24">[25]</ref> can significantly improve performance and reduce distortion, their performance degrades quickly with increasing gallery size in face recognition applications. Product quantization (PQ) <ref type="bibr" target="#b6">[7]</ref>, where the feature template space is decomposed into a Cartesian product of low dimensional subspaces (each subspace is quantized separately) has been shown to achieve excellent search results <ref type="bibr" target="#b6">[7]</ref>. Details of product quantization used in our implementation are described below.</p><p>Under the assumption that the dimensionality d of the feature vectors is a multiple of m, where m is an integer, any feature vector x ∈ R d can be written as a concatenation (x 1 , x 2 , . . . , x m ) of m sub-vectors, each of dimension d/m. In the i-th subspace R d/m , given a sub-codebook C i = {c i j=1,2,...,z |c i j ∈ R d/m }, where z is the size of codebook, the sub-vector x i can be mapped to a codeword c i j in the codebook C i , with j as the index value. The index j can then be represented by a binary code with log 2 (z) bits. In our system, each codebook is generated using the k-means clustering algorithm. Given all the m sub-codebooks {C 1 , C 1 , . . . , C m }, the product quantizer of feature template x is q(x) = (q 1 (x 1 ), . . . , q m (x m )) 6. https://code.google.com/p/cuda-convnet2/ where q j (x j ) ∈ C j is the nearest sub-centroid of sub-vector x j in C j , for j = 1, 2, . . . , m, and the quantizer q(x) requires m log 2 (z) bits. Given another feature template y, the asymmetric squared Euclidean distance between x and y is approximated by</p><formula xml:id="formula_2">D(y, x) = y − q(x) 2 = m j=1 y j − q j (x j ) 2</formula><p>where q j (x j ) ∈ C j , and the distances y j − q j (x j ) are pre-computed for each sub-vector of y j , j = 1, 2, . . . , m and each sub-centroid in C j , j = 1, 2, . . . , m. Since the distance computation requires O(m) lookup and add operations <ref type="bibr" target="#b6">[7]</ref>, approximate nearest neighbor search with product quantizers is fast, and significantly reduces the memory requirements with binary coding.</p><p>To further reduce the search time, a non-exhaustive search scheme was proposed in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b31">[32]</ref> based on an inverted file system and a coarse quantizer; the query image is only compared against a portion of the image gallery, based on the coarse quantizer. Although a non-exhaustive search framework is essential for general image search problems based on local descriptors (where billions of local descriptors are indexed, and thousands of descriptors per query are typical), we found that non-exhaustive search significantly reduces face search performance when used with the proposed feature vector.</p><p>Two important parameters in product quantization are the number of sub-vectors m and the size of the sub-codebook z, which together determine the length of the quantization code: m log 2 z. Typically, z is set to 256. To find the optimal m, we empirically evaluate search accuracy and time per query for various values of m, based on a 1 million face gallery and over 3, 000 queries. We noticed that the performance gap between product quantization (PQ) and brute force search becomes small when the length of the quantization code is longer than 512 bits (m = 64). Considering search time, the PQ-based approximate search is an order of magnitude faster than the brute force search. As a trade-off between efficiency and effectiveness, we set the number of sub-vectors m to 64; The length of the quantization code is 64 log 2 (256) = 512 bits.</p><p>Although we use product quantization to compute the similarity scores, we also need to pick a distance or similarity metric. We empirically evaluated cosine similarity, L1 distance, and L2 distance using a 5M gallery. The cosine similarity achieves the best performance, although after applying L2 normalization, L2 distance has an identical performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Re-Ranking</head><p>After the short candidate list is acquired, the re-ranking module aims to improve search accuracy by using several face matchers to re-rank the candidate list. In particular, given a probe face I and the corresponding k topmost nearest similar faces, C k (I) returned from the filtering module, the k candidate faces are re-ranked by fusing the similarity scores from l different matchers. The ranking module is formulated as:</p><formula xml:id="formula_3">Sort d ({Fusion(S j=1,...,l (I, J i ))|J i=1,...,k ∈ C k (I)}) (3)</formula><p>where S j is the j-th matcher, and Sort d is a descending order sorting function. In general, there is a trade-off between accuracy and computational cost when using multiple face recognition approaches. To make our system simple yet effective, we set l = 2 and generate the final similarity score using the sumrule fusion <ref type="bibr" target="#b32">[33]</ref> of the cosine similarity from the proposed deep network, and the scores generated by a stat-of-the-art COTS face matcher with z-score normalization <ref type="bibr" target="#b33">[34]</ref>.</p><p>The main benefits of combining the proposed deep features and a COTS matcher are threefold: 1) the cosine similarities can be easily acquired from the fast filtering module; 2) an important guideline in fusion is that the matchers should have some diversity <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>. We noticed that the set of impostor face images that are incorrectly assigned high similarity scores by deep features and COTS matcher do not overlap. 3) COTS matchers are widely deployed in many real world applications <ref type="bibr" target="#b7">[8]</ref>, so the proposed cascade fusion scheme can be easily integrated in existing applications to improve scalability and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Impact of Size of Candidate Set (k)</head><p>In the proposed cascaded face search system, the size of candidate list k is a key parameter. In general, we expect the optimal value of k to be related to the gallery size N (a larger gallery would require a larger candidate list to maintain good search performance). We evaluate the relationship between k and N by computing the mean average precision (mAP) as the gallery size (N ) from 100K to 5M and the size of candidate list (k) from 50 to 100K.   <ref type="figure" target="#fig_3">6</ref> shows the search performance, as expected, decreases with increasing gallery size. Further, if we increase k for a fixed N , search performance will initially increase, then drop off when k gets too large. We find that the optimal candidate set size k scales linearly with the size of the gallery N . Because the plots in <ref type="figure" target="#fig_3">Fig 6 flatten</ref> out, a near optimal value of k (e.g., k = 0.01N) can drastically reduce the candidate list with only a very small loss in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Fusion Method</head><p>Another important issue for the proposed cascaded search system is the fusion of similarity scores from deep features (DF) and COTS. We empirically evaluated the following strategies:  • DF→COTS rank : Rank all the k candidate faces with COTS and deep features scores separately, then combine the two ranked lists using rank-level fusion. This is useful when the COTS matcher does not report similarity scores.</p><p>We evaluated the different fusion methods on a 1M face gallery. The average precision vs. average recall curves of these four fusion strategies are shown in <ref type="figure">Fig. 7</ref>. As a base line, we also show the performance of just using DF and COTS alone. The fusion scheme (DF→COTS) consistently outperforms the other fusion methods as well as simply using DF and COTS alone. Note that omitting the filtering step results does not perform as well as the cascaded approach, which is consistent with results in the previous section: when k is too large (e.g. k = N ), the search accuracy decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FACE DATASETS</head><p>We use four web face datasets and one mugshot dataset in our experiments: PCSO, LFW <ref type="bibr" target="#b2">[3]</ref>, IJB-A <ref type="bibr" target="#b8">[9]</ref>, CASIA-WebFace <ref type="bibr" target="#b5">[6]</ref> (abbreviated as "CASIA" in the following sections), and general web face images, referred to as "Web Faces", which we downloaded from the web to augment the gallery. We briefly introduce these datasets, and show example face images from each dataset ( <ref type="figure">Fig. 5</ref>).</p><p>• PCSO: This dataset is a subset of a larger collection of mugshot images acquired from the Pinellas County Sheriffs Office (PCSO) dataset, which contains 1, 447, 607 images of 403, 619 subjects.</p><p>• LFW <ref type="bibr" target="#b2">[3]</ref>: The LFW dataset is a collection of 13, 233 face images of 5, 749 individuals, downloaded from the web. Face images in this dataset contain significant variations in pose, illumination, and expression. However, the face images in this dataset were selected on the bias that they could be detected by the Viola-Jone detector <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p><p>• IJB-A <ref type="bibr" target="#b8">[9]</ref> IARPA Janus Benchmark-A (IJB-A) contains 500 subjects with a total of 25, 813 images (5, 399 still images and 20, 414 video frames). Compared to the LFW dataset, the IJB-A dataset is more challenging due to: i) full pose variation making it challenging to detect all the faces using a commodity face detector, ii) a mix of images and videos, and iii) wider geographical variation of subjects. To make evaluation of face recognition methods feasible in the absence of automatic face detection and landmarking methods for images with full-pose variations, ground-truth eye, nose and face locations are provided with the IJB-A dataset (and used in our experiments when needed). <ref type="figure">Fig. 5</ref> (c) shows the images of two different subjects in the IJB-A dataset, captured in various conditions (video/photo, indoor/outdoor, pose, expression, illumination).</p><p>• CASIA <ref type="bibr" target="#b5">[6]</ref> dataset provides a large collection of labeled (based on subject names) training set for deep learning networks. It contains 494, 414 images of 10, 575 subjects.</p><p>• Web Faces To evaluate the face search system on a large-scale gallery, we used a crawler to automatically download millions of web images, which were filtered to only include images with faces detectable by the OpenCV implementation of the Viola-Jones face detector <ref type="bibr" target="#b39">[40]</ref>. A total of 80 million face images were collected in this manner, which were used to augment the gallery in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FACE RECOGNITION EVALUATION</head><p>In this section, we first evaluate the proposed deep models on a mugshot dataset (PCSO), then we evaluate the performance of the proposed deep model on two publicly available unconstrained face recognition benchmarks (LFW <ref type="bibr" target="#b2">[3]</ref> and IJB-A <ref type="bibr" target="#b8">[9]</ref>) to establish its performance relative to the state of the art. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mugshot Evaluation</head><p>We evaluate the proposed deep model using the PCSO mugshot dataset. Some example mugshots are shown in <ref type="figure">Fig. 5 (a)</ref>. Images are captured in constrained environments with a frontal view of the face. We compare the performance of our deep features with a COTS face matcher. The COTS matcher is designed to work with mugshot-style images, and is one of the top performers in the 2014 NIST FRVT <ref type="bibr" target="#b7">[8]</ref>. Since mugshot dataset is qualitatively different from the CASIA <ref type="bibr" target="#b8">[9]</ref> dataset that we used to train our deep network, similar to <ref type="bibr" target="#b3">[4]</ref>, we first retrained the network with a mugshot training set taken from the full PCSO dataset, consisting of 471, 130 images of 29, 674 subjects. Then, we compared the performance of deep features with the COTS matcher on a test subset of the PCSO dataset containing 89, 905 images of 13, 665 subjects, which contains no overlapping subjects with the training set. We evaluate performance in the verification scenario, and make a total of about 340K genuine pairwise comparisons and over 4 billion impostor pairwise comparisons. The experimental results are shown in <ref type="table" target="#tab_3">Table 3</ref>. We observe that the COTS matcher outperforms the deep features consistently, especially at low false accept rates (FAR) (e.g. 0.01%). However, a simple score-level fusion between the deep features and COTS scores results in improved performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LFW Evaluation</head><p>While mugshot data is of interest in some applications, many others require handling more difficult, unconstrained face images.</p><p>In this section, we evaluate the proposed deep models on a more difficult dataset, the LFW <ref type="bibr" target="#b2">[3]</ref> unconstrained face dataset, using two protocols: the standard LFW <ref type="bibr" target="#b2">[3]</ref> protocol and the BLUFR protocol <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Standard Protocol</head><p>The standard LFW evaluation protocol defines 3, 000 pairs of genuine comparisons and 3, 000 pairs of impostor comparisons, involving 7, 701 images of 4, 281 subjects. These 6, 000 face pairs are divided into 10 disjoint subsets for cross validation, with each subset containing 300 genuine pairs and 300 impostor pairs. We compare the proposed deep model with several state-of-theart deep models: DeepFace <ref type="bibr" target="#b35">[36]</ref>, DeepID2 <ref type="bibr" target="#b36">[37]</ref>, DeepID3 <ref type="bibr" target="#b37">[38]</ref>, Face++ <ref type="bibr" target="#b38">[39]</ref>, DeepNet <ref type="bibr" target="#b21">[22]</ref>, Tencent-BestImage <ref type="bibr" target="#b20">[21]</ref>, and Li et al. <ref type="bibr" target="#b5">[6]</ref>. Additionally, we report the performance of a state-ofthe-art commercial face matcher (COTS), as well as human performance on "funneled" LFW images <ref type="bibr" target="#b40">[41]</ref>. Based on the experimental results shown in <ref type="table" target="#tab_2">Table 2</ref>, we can make the following observations: (i) the COTS matcher performs poorly relative to the deep learning based algorithms. This is to be expected since most COTS matchers have been trained to handle face images captured in constrained environments, e.g. mugshot or driver license photos. (ii) The superior performance of deep learning based algorithms can be attributed to (a) large number of training images (&gt; 100K), (b) data augmentation methods, e.g., use of multiple deep models, and (c) supervised learning algorithms, such as Joint-Bayes <ref type="bibr" target="#b4">[5]</ref>, used to learn a verification model for a pair of faces in the training set.</p><p>To generate multiple deep models, we cropped 6 different subregions from the aligned face images (by centering the positions of the left-eye, right-eye, nose, mouth, left-brow, and right-brow) and trained six additional networks. As a result, by combining seven models together and using Joint-Bayes <ref type="bibr" target="#b4">[5]</ref>, the performance of our deep model can be improved to 98.23% from 96.95% for a single network using the cosine similarity. Despite using only publicly available training data, the performance of our deep model is highly competitive with state-of-the-art on the standard LFW protocol (see <ref type="table" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">BLUFR Protocol</head><p>It has been argued in the literature that the standard LFW evaluation protocol is not appropriate for real-world face recognition systems, which require high true accept rates (TAR) at low false accept rates ( FAR = 0.1%). A number of new protocols for unconstrained face recognition have been proposed, including the open-set identification protocol <ref type="bibr" target="#b17">[18]</ref> and the Benchmark of Largescale Unconstrained Face Recognition (BLUFR) protocol <ref type="bibr" target="#b18">[19]</ref>. In this experiment, we follow the BLUFR protocol, which defines 10fold cross-validation face verification and open-set identification tests, with corresponding training sets for each fold.</p><p>For face verification, in each trial, the test set contains the 9, 708 face images of 4, 249 subjects, on average. As a result, over 47 million face comparison scores need to be computed in each trial. For open-set identification, the dataset in the previous verification task (9, 708 face images of 4, 294 subjects) is randomly partitioned into three subsets: gallery set, genuine probe set, and impostor probe set. In each trial, 1, 000 subjects from the test set are randomly selected to constitute the gallery set; a single image per subject is put in the gallery. After the gallery is selected, the remaining images from the 1, 000 selected subjects are used to form the genuine probe set, and all other images in the test set are used as the impostor probe set. As a result, in each trial, the genuine probe set contains 4, 350 face images of 1, 000 subjects, the impostor probe set contains 4, 357 images of 3, 249 subjects, on average, and the gallery set contains 1, 000 images.</p><p>Following the protocol in <ref type="bibr" target="#b18">[19]</ref>, we report the verification rate (VR) at FAR = 0.1% for the face verification and the detection and identification rate (DIR) at Rank-1 corresponding to an FAR of 1% for open-set identification. As yet, only a few other deep learning based algorithms have reported their performance using this protocol. We report the published results on this protocol, along with the performance of our deep network, and a state of the art COTS matcher in <ref type="table" target="#tab_4">Table 4</ref>. We notice that the verification rates at low FAR (0.1%) under the BLUFR protocol are much lower than the accuracies reported on the standard LFW protocol. For example, the performance of the COTS matcher is only 58.56% under the BLUFR protocol compared to 90.35% in the standard LFW protocol. This indicates that the performance metrics for the BLUFR protocol are much harder as well as realistic than those of the standard LFW protocol. The deep learning based algorithms still perform better than the COTS matcher, as well as the high-dimensional LBP based features. Using cosine similarity and a single deep model, our method achieves better performance (83.39%) than the one in <ref type="bibr" target="#b5">[6]</ref>, which indicates that our modifications to the network design (using RGB input, random cropping, and improved face alignment) does improve the recognition performance. Our performance is further improved to 87.65% when we fuse 7 deep models. In this experiment, Joint-Bayes approach <ref type="bibr" target="#b4">[5]</ref> did not improve the performance. In the open-set recognition results, our single deep model achieves a significantly better performance (46.31%) than the previous best reported result of 28.90% <ref type="bibr" target="#b5">[6]</ref> and the COTS matcher (36.44%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">IJB-A Evaluation</head><p>The IJB-A dataset <ref type="bibr" target="#b8">[9]</ref> was released in an attempt to push the frontiers of unconstrained face recognition. Given that the recognition performance on the LFW dataset was getting saturated and the deficiencies in the LFW protocols, the IJB-A dataset contains more challenging face images and defines both verification and identification (open and close sets) protocols. The basic protocol consists of 10-fold cross-validation on pre-defined splits of the dataset, with a disjoint training set defined for each split. One unique aspect of the IJB-A evaluation protocol is that it defines "templates," consisting of one or more images (still images or video frames), and defines set-to-set comparisons, rather than using face-to-face comparisons. <ref type="figure" target="#fig_7">Fig. 8</ref> shows examples of templates in the IJB-A protocol (one per row), with varying number of images per template. In particular, in the IJB-A evaluation protocol the number of images per template ranges from a single image to a maximum of 202 images. Both the search task (1:N search) and verification task (1:1 matching) are defined in terms of comparisons between templates (consisting of several face images), rather than single face images.</p><p>The verification protocol in IJB-A consists of 10 sets of predefined comparisons between templates (groups of images). Each set contains about 11, 748 pairs of templates (1, 756 genuine plus 9, 992 impostor pairs), on average. For the search protocol, which evaluates both closed-set and open-set search performance, 10 corresponding gallery and probe sets are defined, with both the gallery and probe sets consisting of templates. In each search fold, there are about 1, 187 genuine probe templates, 576 impostor probe templates, and 112 gallery templates, on average.</p><p>Given an image or video frame from the IJB-A dataset, we first attempt to automatically detect 68 facial landmarks with DLIB. If the landmarks are successfully detected, we align the detected face using the alignment method proposed in Section 3.1. We call the images with automatically detected landmarks well-aligned images. If the landmarks cannot be automatically detected, as is the case for profile faces or when only the back of the head is showing <ref type="figure" target="#fig_8">(Fig. 9)</ref>, we align the face based on the ground-truth landmarks provided with the IJB-A protocol. All possible ground truth landmarks (left eye, right eye, and nose tip) may be visible in every image, and so the M-Turk workers who manually marked the landmarks skipped the missing ones. For example, in faces exhibiting a high degree of yaw, only one eye is typically visible. If all the three landmarks are available, we estimate the mouth position and align the face images using the alignment method in Section 3.1; otherwise, we directly crop a square face region using the provided ground-truth face region. We call images for which the automatic landmark detection fails poorly-aligned images. <ref type="figure" target="#fig_8">Fig. 9</ref> shows some examples from these two categories in the IJB-A dataset. The IJB-A protocol allows participants to perform training for each fold. Since the IJB-A dataset is qualitatively different from the CASIA dataset that we used to train our network, we retrain our deep model using the IJB-A training set. The final face representations consists of a concatenation of the deep features from five deep model trained just on the CASIA dataset and one re-trained (on the IJB-A training set following the protocol) deep model. We then reduce the dimensionality of the combined face representation to 100 using PCA.</p><p>Since all the IJB-A comparisons are defined between sets of faces, we need to determine an appropriate set-to-set comparison method. We choose to prioritize well-aligned images, since they are most consistent with the data used to train our deep models. Our set-to-set comparison strategy is to check if there are one or more well-aligned images in a template. If so, we only use the well-aligned images for the set comparison, we call the corresponding template well-aligned templates. Otherwise we use the poorly-aligned images, with naming the corresponding template poorly-aligned templates. The pairwise face-to-face similarity scores are computed using the cosine similarity, and the average score over the selected subset of images is the final set-to-set similarity score.</p><p>In terms of evaluation, verification performance is summarized using True Accept Rates (TAR) at a fixed False Accept Rate (FAR). The TAR is defined as the fraction of genuine templates correctly accepted at a particular threshold, and FAR is defined as the fraction of impostor templates incorrectly accepted at the same threshold. Closed-set recognition performance is evaluated based on the Cumulative Match Characteristic (CMC) curve, which computes the fraction of genuine samples retrieved at or below a specific rank. Open-set recognition performance is evaluated using the False Positive Identification Rate (FPIR), and the False Well-aligned Templates Poorly-aligned Templates <ref type="figure" target="#fig_0">Fig. 11</ref>. Distribution of well-aligned templates and poorly-aligned templates in 1:N search protocol of IJB-A, averaged over 10 folds. Correct Match@Rank-1 means that the mated gallery template is correctly retrieved at rank 1. Well-aligned images use the landmarks automatically detected by DLIB <ref type="bibr" target="#b28">[29]</ref>. Poorly-aligned images mainly consist of side-views of faces. We align these images using the three ground-truth landmarks where available, or else by cropping the entire face region.</p><p>Negative Identification Rate (FNIR), where FPIR is the fraction of impostor probe images accepted at a given threshold, while FNIR is the fraction of genuine probe images rejected at the same threshold. Key results of the proposed method, along with the baseline results reported in <ref type="bibr" target="#b8">[9]</ref> are shown in <ref type="table" target="#tab_7">Table 5</ref>. Our deep network based method performs significant better than the two baselines at all evaluated operating points. <ref type="figure" target="#fig_0">Fig. 10</ref> shows three sets of face search results. We failed to find the mated templates at rank 1 for the third probe template. A template containing a single poorly-aligned image is much harder to recognize than the templates containing one or more well-aligned images. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the distribution of well-aligned images and poorly-aligned images in probe templates. Compared to the distribution of poorly aligned templates in the overall dataset, we fail to recognize a disproportionate number of templates containing only poorlyaligned face images at rank 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LARGE-SCALE FACE SEARCH</head><p>In this section, we evaluate our face search system using an 80M gallery. The test datasets we use include LFW and IJB-A data, but now we do not follow the protocols associated with these two datasets, and instead use those images as the mated portion of a retrieval database with an enhanced gallery. We report search results, both under open-set and closed-set protocols, with increasing size of the gallery up to 80M faces. We evaluate the following three face search schemes:</p><p>• Deep Features (DF): Use our deep features and product quantization (PQ) to directly retrieve the top-k most similar faces to the probe (no re-ranking step).</p><p>• COTS: Use a state-of-the-art COTS face matcher to compare the probe image with each gallery face, and output the top-k most similar faces to the probe (no filtering step).</p><p>• DF→COTS: Filter the gallery using deep features and then re-rank the k candidate faces by fusing cosine similarities computed from deep features with the COTS matcher's similarity scores.</p><p>For closed-set face search, we assume that the probe always has at least one corresponding face image in the gallery. For</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probe Template</head><p>Retrieved templates from the gallery under the closed-set 1:N search protocol of IJB-A  <ref type="figure" target="#fig_0">Fig. 10</ref>. Examples of face search in first fold of the IJB-A closed-set 1:N search protocol, using "templates." The first column contains the probe templates, and the following 5 columns contain the corresponding top-5 ranked gallery templates, where red text highlights the correct mated gallery template. There are 112 gallery templates in total; only a subset (four) of the gallery images for each template are shown. open-set face search, given a probe we first decide whether a corresponding image is present in the gallery. If it is determined that the probe's identity is represented in the gallery, then we return the search results for the probe image. For open-set performance evaluation, the probe set consists of two groups: i) genuine probe set that has mated images in the gallery set, and ii) impostor probe set that has no mated images in the gallery set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Search Dataset</head><p>We construct a large-scale search dataset using the four web face datasets introduced in Section 4. The dataset consists of five parts, as shown in <ref type="table" target="#tab_8">Table 6</ref>: 1) training set, which is used to train our deep network; 2) genuine probe set, the probe set which has corresponding gallery images; 3) mate set, the part of the gallery containing the same subjects as the genuine probe set; 4) impostor probe set, which has no overlapping subjects with the genuine probe set; 5) background set, which has no identity labels and is simply used as background images to enlarge the gallery size.</p><p>We use the LFW and IJB-A datasets to construct the genuine probe set and corresponding mate set. For the LFW dataset, we first remove all the subjects who have only a single image, resulting in 1, 507 subjects with 2 or more images. For each of these subjects, we take half of the images for the genuine probe set and use the remaining images for the mate set in the gallery. We repeat this process 10 times to generate 10 groups of probe and mate sets. To construct the impostor probe set, we collect 4, 000 images from the LFW subjects with only a single image. For the IJB-A dataset, a similar process is adopted to generate 10 groups of probe and mate sets. To build a large-scale background set, we use a crawler to download millions of web images from the Internet, then filter them to only include those with faces detectable by the OpenCV implementation of the Viola-Jones face detector. By combining mate set and background set, we compose an 80 million web face gallery. More details are shown in <ref type="table" target="#tab_8">Table 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance Measures</head><p>We evaluate face search performance in terms of precision, the fraction of the search set consisting of mated face images and recall, the fraction of all mated face images for a given probe face that were returned in the search results. Various trade-offs between precision and recall are possible (for example, high recall can be achieved by returning a large search set, but a large search set will also lead to lower precision), so we summarize overall closed-set face search performance using mean Average Precision (mAP). The mAP measure is widely used in image search applications, and is defined as follows: given a set of n probe face images Q = {x 1 q , x 2 q , . . . , x n q } and a gallery set with N images, the average precision of x i q is:</p><formula xml:id="formula_4">avgP(x i q ) = N k=1 P (k) × [R(k) − R(k − 1)]<label>(4)</label></formula><p>where P (k) is precision at the position k and R(k) is recall at the position k with R(0) = 0. The mean Average Precision (mAP) of the entire probe set is then:</p><p>mAP(Q) = mean(avgP(x i q )), i = 1, 2, . . . , n In the open-set scenario, we evaluate search performance as a trade-off between mean average precision (mAP) and false accept rate (FAR) (the fraction of impostor probe images which are not rejected at a given threshold). Given a genuine probe, its average precision is set to 0 if it is rejected at a given threshold, otherwise, its average precision is computed with Eq. 4. A basic assumption in our search performance evaluation is that none of the query images are present in the 80M downloaded web faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Closed-set Face Search</head><p>We examine closed-set face search performance with varying gallery size N , from 100K to 80M. Enrolling the complete 80M gallery in the COTS matcher would take a prohibitive amount of time (over 80 days), due to limitations of the SDK we have, so the maximum gallery set used for the COTS matcher is 5M. For the proposed face search scheme DF→COTS, we chose the size of candidate set k using the heuristic k = 1/100N when the gallery size is smaller than 5M and k = 1, 000 when the gallery set size is 80M. We use a fixed k for the full 80M gallery since using a larger k would take a prohibitive amount of time, due to the need to enroll the top-ranking images in the COTS matcher. Experimental results for the LFW and IJB-A datasets are shown in Figs. 12, respectively. Closed-set Search Evaluation on LFW and IJB-A datasets For both LFW and IJB-A face images, the recognition performance of all three face search schemes evaluated here decreases with increasing gallery set size. In particular, for all the search schemes, mAP linearly decreases with the gallery size N on log scale; the performance gap between a 100K gallery and a 5M gallery is about the same as the performance gap between a 5M gallery and an 80M gallery. While deep features outperform the COTS matcher alone, the proposed cascaded face search system (which leverages both deep features and the COTS matcher) gives better search accuracy than either method individually. Results on the IJB-A dataset are overall similar to the LFW results. It is important to note that the overall performance on the IJB-A dataset is much lower than the LFW dataset, which is to be expected given the nature of the IJB-A dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Open-set Face Search</head><p>Open-set search is important for several practical applications where it is unreasonable to assume that a gallery contains images of all potential probe subjects. We evaluate open-set search performance on an 80M gallery, and plot the search performance (mAP) at varying FAR in Figs. 13.</p><p>For both the LFW and IJB-A datasets, the open-set face search problem is much harder than closed-set face search. At a FAR of 1%, the search performance (mAP) of the compared algorithms is much lower than the closed-set face search, indicating that a large number of genuine probe images are rejected at the threshold needed to attain 1% FAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Scalability</head><p>In addition to mAP, we also report the search times in <ref type="table" target="#tab_9">Table 7</ref>. We run all the experiments on a PC with an Intel(R) Xeon(R) CPU (E5-2687W) clocked at 3.10HZ. For a fair comparison, all the compared algorithms use only one CPU core. The deep features are extracted using a Tesla K40 graphics card.</p><p>In our experiments, template generation is applied over the entire gallery off-line, meaning that deep features are extracted </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>False Accept Rate (%) Detection and Mean Average Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Features (LFW) DF → COTS (LFW) Deep Features (IJB−A) DF → COTS (IJB−A)</head><p>Open-set Search Evaluation on LFW and IJB-A datasets <ref type="figure" target="#fig_0">Fig. 13</ref>. Open-set face search performance (mAP) vs. false accept rate (FAR) on LFW and IJB-A datasets, using an 80M face gallery. The performance of COTS matcher is not shown, since enrolling the complete 80M gallery with the COTS matcher would have taken a prohibitive amount of time (over 80 days). for all gallery images and the gallery is indexed using product quantization before we begin processing probe images. We also enroll the gallery images using the COTS matcher and store the templates on disk. The run time of the proposed face search system after the gallery is enrolled and indexed consists of two parts: i) enrollment time including face detection, alignment and feature extraction, and ii) search time consisting of the time taken to find the top-k search results given the probe template. Since we did not enroll all 80M gallery images using the COTS matcher, we estimate the query time for the 80M gallery by assuming that search time increases linearly with the gallery size. Using product quantization for fast matching based on deep features, we can retrieve the top-k candidate faces in about 0.9 seconds for a 5M image gallery and in about 6.7 seconds for an 80M gallery. On the other hand, the COTS matcher takes about 30 and 480 seconds to carry out brute-force comparison over the complete galleries of 5 and 80 million images, respectively. In the proposed cascaded face search system, we mitigate the impact of the slow exhaustive search required by the COTS matcher by only using them on a short candidate list. The proposed cascaded scheme takes about 1 second for the 5M gallery and about 6.9 seconds for the 80M gallery, which is only a minor increase over the time taken using deep features alone (6.68 seconds). The search time could be further reduced by using a non-exhaustive search method, but that most likely will result in a significant loss in search accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">BOSTON MARATHON BOMBING CASE STUDY</head><p>In addition to the large-scale face search experiments reported above, we report on a case-study: finding the identity of Boston marathon bombing suspects 7 in an 80M face gallery. Klontz and Jain <ref type="bibr" target="#b41">[42]</ref> made an attempt to identify the face images of the Boston marathon bombing suspects in a large gallery of mugshot images. Video frames of the two suspects were matched against a background set of mugshots using two state-ofthe-art COTS face matcher. Five low resolution images of the two suspects, released by the FBI (shown in left side of <ref type="figure" target="#fig_0">Fig. 14)</ref> were used as probe images, and six images of the suspects released by the media (shown in the right side of <ref type="figure" target="#fig_0">Fig. 14)</ref> were used as the mates in the gallery. These gallery images were augmented with 1 million mugshot images. One of the COTS matchers was successful in finding the true mate (2y) of one of the probe image (2c) of Tamerlan Tsarnaev at rank 1.</p><p>To evaluate the face search performance of our cascaded face search system, we construct a similar search problem under more challenging conditions by adding the six gallery images to a background set of up to 80 million web faces. We argue that the unconstrained web faces are more consistent with the quality of the images of the suspects used in the gallery than mugshot images and therefore comprise a more meaningful gallery set. We evaluate the search results using gallery sizes of 5M and 80M leveraging the same background set used in our prior search experiments. Since there is no demographic information available for the web face images we downloaded, we only conduct a "blind search" <ref type="bibr" target="#b41">[42]</ref>, and do not filter the gallery using any demographic information.</p><p>The search results are shown in <ref type="table" target="#tab_11">Table 8</ref>. Both the deep features and the COTS matcher fail on probe images 1a, 2b, 2a, and 2b, similar to the results in <ref type="bibr" target="#b41">[42]</ref>. On the other hand, for probe 2c, the deep features perform much better than the COTS matcher. For the 5M gallery, the COTS matcher found a mate for probe 2c at rank 625; however, the deep features returned the gallery image 2x at rank 9. The proposed cascaded search system returned gallery 7. https://en.wikipedia.org/wiki/Boston Marathon bombing image 2y at rank 1 in the 5M image gallery, by using the COTS matcher to re-rank the deep features results, demonstrating the value of the proposed cascade framework. Results are somewhat worse for the 80M image gallery. For probe 2c, using deep features alone, we find gallery image 2x at rank 109 and gallery image 2y at rank 2, 952. However, using the cascaded search system, we find gallery image 2x at rank 46 by re-ranking the top-1K faces, and find gallery image 2y at rank 8 by re-ranking the top-10K faces. So, even with an 80M image gallery, we can successfully find a match for one of the probe image (2c) within the top-10 retrieved faces. The face search results for the 80M galleries are shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. One interesting observation is that deep features will typically return faces taken under similar conditions to the probe image. For example, a list of candidate images with sunglasses are returned for probe image, which exhibits partial occlusion due to sunglasses. Similarly, a list of blurred candidate faces are returned for probe, which is of low resolution due to blur. Another interesting observation is that the deep features based search found several near-duplicate images which happened to be present in the unlabeled background dataset, images which we were not aware of prior to viewing these search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>We have proposed a cascaded face search system suitable for large-scale search problems. We have developed a deep learning based face representation trained on the publicly available CASIA dataset <ref type="bibr" target="#b5">[6]</ref>. The deep features are used in a product quantization based approximate k-NN search to first obtain a short list of candidate faces. This short list of candidate faces is then reranked using the similarity scores provided by a state-of-the-art COTS face matcher. We demonstrate the performance of our deep features on three face recognition datasets, of increasing difficulty: the PCSO mugshot dataset, the LFW unconstrained face dataset, and the IJB-A dataset. On the mugshot data, our performance (TAR of 93.5% at FAR of 0.01%) is worse than a COTS matcher (98.5%), but fusing our deep features with the COTS matcher still improves overall performance (99.2%). Our performance on the standard LFW protocol (98.23% accuracy) is comparable to state of the art accuracies reported in the literature. On the BLUFR protocol for the LFW database we attain the best reported performance to date (verification rate of 87.65% at FAR of 0.1%). We outperform the benchmarks reported in <ref type="bibr" target="#b8">[9]</ref> on the IJB-A dataset, as follows: TAR of 51.4% at FAR of 0.1% (verification); Rank 1 retrieval of 82.0% (closed-set search); FNIR of 61.7% at FPIR of 1% (open-set search). In addition to the evaluations on the LFW and the IJB-A benchmarks, we evaluate the proposed search scheme on an 80 million face gallery, and show that the proposed scheme offers an attractive balance between recognition accuracy and runtime. We also demonstrate search performance on an operational case study involving the video frames of the two persons (Tsarnaev brothers) implicated in the 2013 Boston marathon bombing. In this case study, the proposed system can find one of the suspects' images at rank 1 in 1 second on a 5M gallery and at rank 8 in 7 seconds on an 80M gallery.</p><p>We consider non-exhaustive face search an avenue for further research. Although we made an attempt to employ indexing methods, they resulted in a drastic decrease in search performance. If only a few searches need to be made, the current system's search speed is adequate, but if the number of searches required is on the order of the gallery size, the current runtime is inadequate. We are also interested in improving the underlying face representation, via improved network architectures, as well as larger training sets.  <ref type="figure" target="#fig_0">Fig. 15</ref>. Top 10 search results for the two Boston marathon bombers on the 80M face gallery. The first two probe faces are of the older brother (Dzhokhar Tsarnaev) and the last three probe faces are of the younger brother (Tamerlan Tsarnaev). For each probe face, the retrieved image with green border is the correctly retrieved image. Images with the red border are near-duplicate images present in the gallery. Note that we were not aware of the existence of these near-duplicate images in the gallery before the search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example of large-scale face search problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the proposed large-scale face search system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The proposed deep convolutional neural network (ConvNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Impact of candidate set size k as a function of the size of the gallery (N ) on the search performance. Red points mark the optimal value of the candidate set (k) size for each gallery size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.</head><label></label><figDesc>Fig. 6 shows the search performance, as expected, decreases with increasing gallery size. Further, if we increase k for a fixed N , search performance will initially increase, then drop off when k gets too large. We find that the optimal candidate set size k scales linearly with the size of the gallery N . Because the plots in Fig 6 flatten out, a near optimal value of k (e.g., k = 0.01N) can drastically reduce the candidate list with only a very small loss in accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>•</head><label></label><figDesc>DF+COTS: Score-level fusion of similarities based on deep features and the COTS matcher, without any filtering.• DF→COTS: Filter the gallery using deep features, then re-rank the candidate list based on score-level fusion between the deep features and the COTS scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .Fig. 7 .</head><label>57</label><figDesc>Examples of face images in five face datasets. Comparison of fusion strategies based on a 1M gallery.• DF→COTS only : Only use the similarity scores of COTS matcher to rank the k candidate faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>(a) Probe template (ID=110), #Images=1 (b) Gallery template (ID=319), #Images=4 (c) Gallery template (ID=5, 762), #Images=95 Examples of probe/gallery "templates" in the first folder of IJB-A protocol in 1:N face search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Examples of web images in the IJB-A dataset with overlayed landmarks (top row), and the corresponding aligned face images (bottom row); (a) example of a well-aligned image obtained using automatically detected landmarks by DLIB [29]; (b), (c), and (d) examples of poorly-aligned images with 3, 2, and 0 ground-truth landmarks provided in IJB-A, respectively. DLIB fails to output landmarks for (b)-(d). The web images in the top row have been cropped around the relevant face regions from the original images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Closed-set face search performance (mAP) vs. gallery size N (log-scale), on LFW and IJB-A datasets. The performance of COTS matcher on 80M gallery is not shown, since enrolling the complete 80M gallery with the COTS matcher would have taken a prohibitive amount of time (over 80 days).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>UYlT8p Law Enforcement Social Media Who is this? Large-Scale Face Dataset One of them? Face Search System</head><label></label><figDesc></figDesc><table /><note>1. https://goo.gl/FmzROn 2. goo.gl/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 A</head><label>1</label><figDesc>summary of face search systems reported in the literature Web Faces are downloaded from the Internet and used to augment the gallery; different face search systems use their own web faces.</figDesc><table><row><cell>Authors</cell><cell>Probe</cell><cell></cell><cell>Gallery</cell><cell></cell><cell>Dataset</cell><cell>Search Protocol</cell></row><row><cell></cell><cell cols="4"># Images # Subjects # Images # Subjects</cell><cell></cell><cell></cell></row><row><cell>Wu et al. [2]</cell><cell>220</cell><cell>N/A</cell><cell>1M+</cell><cell cols="2">N/A LFW [3] + Web Faces a</cell><cell>closed set</cell></row><row><cell>Chen et al. [1]</cell><cell>120</cell><cell>12</cell><cell>13, 113</cell><cell>5, 749</cell><cell>LFW [3]</cell><cell>closed set</cell></row><row><cell></cell><cell>4, 300</cell><cell>43</cell><cell>54, 497</cell><cell>200</cell><cell>Pubfig [10]</cell><cell>closed set</cell></row><row><cell>Miller et al. [11]</cell><cell>4, 000</cell><cell>80</cell><cell>1M+</cell><cell cols="2">N/A FaceScrub [12] + Yahoo Images b</cell><cell>closed set</cell></row><row><cell>Yi et al. [13]</cell><cell>1, 195</cell><cell>N/A</cell><cell>201, 196</cell><cell cols="2">N/A FERET [14] + Web Faces</cell><cell>closed set</cell></row><row><cell>Yan et al. [15]</cell><cell>16, 028</cell><cell>N/A</cell><cell>116, 028</cell><cell cols="2">N/A FRGC [16] + Web Faces</cell><cell>closed set</cell></row><row><cell>Klare et al. [17]</cell><cell>840</cell><cell>840</cell><cell>840</cell><cell>840</cell><cell>LFW [3]</cell><cell>closed set</cell></row><row><cell></cell><cell>25, 000</cell><cell>25, 000</cell><cell>25, 000</cell><cell>25, 000</cell><cell>PCSO [17]</cell><cell>closed set</cell></row><row><cell>Best-Rowden et al. [18]</cell><cell>10, 090</cell><cell>5, 153</cell><cell>3, 143</cell><cell>596</cell><cell>LFW [3]</cell><cell>open set</cell></row><row><cell>Liao et al. [19]</cell><cell>8, 707</cell><cell>4, 249</cell><cell>1, 000</cell><cell>1, 000</cell><cell>LFW [3]</cell><cell>open set</cell></row><row><cell>Proposed System</cell><cell>7, 370</cell><cell>5, 507</cell><cell>80M+</cell><cell cols="2">N/A LFW [3] + Web Faces</cell><cell>closed &amp; open set</cell></row><row><cell></cell><cell>5, 828</cell><cell>4, 500</cell><cell>80M+</cell><cell cols="3">N/A IJB-A [9] + LFW [3] + Web Faces closed &amp; open set</cell></row><row><cell>a.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>b. http://labs.yahoo.com/news/yfcc100m/ the gallery. To address face search application requirements, sev- eral new protocols for unconstrained face recognition have been proposed, including the open-set identification protocol [18] and the Benchmark of Large-scale Unconstrained Face Recognition (BLUFR) protocol</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Performance of various face recognition methods on the standard LFW verification protocol. Method #Nets Training Set (private or Public face dataset) Training Setting Mean accuracy ± sd DeepFace [36] 1 4.4 million images of 4, 030 subjects, Private cosine 95.92%±0.29% DeepFace 7 4.4 million images of 4, 030 subjects, Private unrestricted, SVM 97.35%±0.25% DeepID2 [37] 1 202, 595 images of 10, 117 subjects, Private unrestricted, Joint-Bayes 95.43% DeepID2 25 202, 595 images of 10, 117 subjects, Private unrestricted, Joint-Bayes 99.15 ± 0.15% DeepID3 [38] 50 202, 595 images of 10, 117 subjects, Private unrestricted, Joint-Bayes 99.53 ± 0.10%</figDesc><table><row><cell>Face++ [39]</cell><cell cols="2">4 5 million images of 20, 000 subjects, Private</cell><cell>L2</cell><cell>99.50 ± 0.36%</cell></row><row><cell>FaceNet [22]</cell><cell cols="3">1 100 ∼ 200 million images of 8 million subjects, Private L2</cell><cell>99.63 ± 0.09%</cell></row><row><cell>Tencent-BestImage [21]</cell><cell>20</cell><cell>1, 000, 000 images of 20, 000 subjects, Private</cell><cell>Joint-Bayes</cell><cell>99.65 ± 0.25%</cell></row><row><cell>Li et al. [6]</cell><cell cols="2">1 494, 414 images of 10, 575 subjects, Public</cell><cell>cosine</cell><cell>96.13%±0.30%</cell></row><row><cell>Li et al.</cell><cell cols="2">1 494, 414 images of 10, 575 subjects, Public</cell><cell cols="2">unrestricted, Joint-Bayes 97.73%±0.31%</cell></row><row><cell>Human, funneled</cell><cell cols="2">N/A N/A</cell><cell>N/A</cell><cell>99.20%</cell></row><row><cell>COTS</cell><cell cols="2">N/A N/A</cell><cell>N/A</cell><cell>90.35%±1.30%</cell></row><row><cell>Proposed Deep Model</cell><cell cols="2">1 494, 414 images of 10, 575 subjects, Public</cell><cell>cosine</cell><cell>96.95%±1.02%</cell></row><row><cell>Proposed Deep Model</cell><cell cols="2">7 494, 414 images of 10, 575 subjects, Public</cell><cell>cosine</cell><cell>97.52%±0.76%</cell></row><row><cell>Proposed Deep Model</cell><cell cols="2">1 494, 414 images of 10, 575 subjects, Public</cell><cell cols="2">unrestricted, Joint-Bayes 97.45%±0.99%</cell></row><row><cell>Proposed Deep Model</cell><cell cols="2">7 494, 414 images of 10, 575 subjects, Public</cell><cell cols="2">unrestricted, Joint-Bayes 98.23%±0.68%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Performance of face verification on a subset of the PCSO dataset (89, 905 images of 13, 666 subjects). There are about 340K genuine pairs and over 4 billion imposter pairs.</figDesc><table><row><cell></cell><cell>TAR@FAR=0.01%</cell><cell>TAR@FAR=0.1%</cell><cell>TAR@FAR=1%</cell></row><row><cell>COTS</cell><cell>0.985</cell><cell>0.993</cell><cell>0.997</cell></row><row><cell>Deep Features</cell><cell>0.935</cell><cell>0.977</cell><cell>0.993</cell></row><row><cell>DF + COTS</cell><cell>0.992</cell><cell>0.996</cell><cell>0.997</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Performance of various face recognition methods using the BLUFR LFW protocol reported as Verification Rate (VR) and Detection and Identification Rate (DIR).</figDesc><table><row><cell>Method</cell><cell>Training Setting</cell><cell>VR</cell><cell>DIR@FAR=1%</cell></row><row><cell></cell><cell></cell><cell>@FAR=0.1%</cell><cell>Rank=1</cell></row><row><cell>HDLBP+JB [19]</cell><cell>Joint-Bayes</cell><cell>41.66%</cell><cell>18.07%</cell></row><row><cell>HDLBP+LDA [19]</cell><cell>LDA</cell><cell>36.12%</cell><cell>14.94%</cell></row><row><cell>Li et al. [6]</cell><cell>Joint-Bayes</cell><cell>80.26%</cell><cell>28.90%</cell></row><row><cell>COTS</cell><cell>N/A</cell><cell>58.56%</cell><cell>36.44%</cell></row><row><cell>Proposed Deep Model</cell><cell>#Nets = 1, Cosine</cell><cell>83.39%</cell><cell>46.31%</cell></row><row><cell>Proposed Deep Model</cell><cell>#Nets = 7, Cosine</cell><cell>87.65%</cell><cell>56.27%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Recognition accuracies under the IJB-A protocol. Results for GOTS and OpenBR are taken from<ref type="bibr" target="#b8">[9]</ref>. Results reported are the average ± standard deviation over the 10 folds specified in the IJB-A protocol.</figDesc><table><row><cell></cell><cell cols="3">TAR @ FAR (verification)</cell><cell cols="2">CMC (closed-set search)</cell><cell cols="2">FNIR @ FPIR (open-set search):</cell></row><row><cell>Algorithm</cell><cell>0.1</cell><cell>0.01</cell><cell>0.001</cell><cell>Rank-1</cell><cell>Rank-5</cell><cell>0.1</cell><cell>0.01</cell></row><row><cell>GOTS</cell><cell>0.627 ± 0.012</cell><cell>0.406 ± 0.014</cell><cell>0.198 ± 0.008</cell><cell>0.443 ± 0.021</cell><cell>0.595 ± 0.020</cell><cell>0.765 ± 0.033</cell><cell>0.953 ± 0.024</cell></row><row><cell>OpenBR</cell><cell>0.433 ± 0.006</cell><cell>0.236 ± 0.009</cell><cell>0.104 ± 0.014</cell><cell>0.246 ± 0.011</cell><cell>0.375 ± 0.008</cell><cell>0.851 ± 0.028</cell><cell>0.934 ± 0.017</cell></row><row><cell>Proposed Deep Model</cell><cell>0.895 ± 0.013</cell><cell>0.733 ± 0.034</cell><cell>0.514 ± 0.060</cell><cell>0.820 ± 0.024</cell><cell>0.929 ± 0.013</cell><cell>0.387 ± 0.032</cell><cell>0.617 ± 0.063</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Large-scale web face search dataset overview.</figDesc><table><row><cell></cell><cell>Source</cell><cell># Subjects</cell><cell># Images</cell></row><row><cell>Training Set</cell><cell>CASIA [6]</cell><cell>10,575</cell><cell>494,414</cell></row><row><cell cols="3">LFW based probe and mate sets</cell><cell></cell></row><row><cell>Genuine Probe Set</cell><cell>LFW [3]</cell><cell>1,507</cell><cell>3,370</cell></row><row><cell>Mate Set</cell><cell>LFW [3]</cell><cell>1,507</cell><cell>3,845</cell></row><row><cell cols="3">IJB-A based probe and mate sets</cell><cell></cell></row><row><cell>Genuine Probe Set</cell><cell>IJB-A [3]</cell><cell>500</cell><cell>10,868</cell></row><row><cell>Mate Set</cell><cell>IJB-A [3]</cell><cell>500</cell><cell>10,626</cell></row><row><cell cols="2">Impostor Probe Set LFW [3]</cell><cell>4,000</cell><cell>4,000</cell></row><row><cell>Background Set</cell><cell>Web Faces</cell><cell cols="2">N/A 80,000,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7</head><label>7</label><figDesc>The average search time (seconds) per probe face and the search performance (mAP).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">5M Face Gallery</cell><cell></cell><cell cols="2">80M Face Gallery</cell></row><row><cell></cell><cell>COTS</cell><cell>DF</cell><cell>DF→COTS @50K</cell><cell>COTS</cell><cell>DF</cell><cell>DF→COTS @1K</cell></row><row><cell>Enrollment</cell><cell>0.09</cell><cell>0.05</cell><cell>0.14</cell><cell>0.09</cell><cell>0.05</cell><cell>0.14</cell></row><row><cell>Search</cell><cell>30</cell><cell>0.84</cell><cell>1.15</cell><cell>480 ⋆</cell><cell>6.63</cell><cell>6.64</cell></row><row><cell>Total Time</cell><cell>30.09</cell><cell>0.89</cell><cell>1.29</cell><cell>480.1 ⋆</cell><cell>6.68</cell><cell>6.88</cell></row><row><cell>mAP</cell><cell>0.36</cell><cell>0.52</cell><cell>0.62</cell><cell>N/A</cell><cell>0.34</cell><cell>0.4</cell></row></table><note>⋆ Estimated by assuming that search time increases linearly with gallery size.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8</head><label>8</label><figDesc>Rank search results of Boston bombers face search based on 5M and 80M gallery. The five probe images are designated as 1a, 1b, 2a, 2b, and 2c. The six mated images are designated as 1x, 1y, 1z, 2x, 2y, and 2z. The corresponding images are shown inFig. 14</figDesc><table><row><cell></cell><cell>COTS (5M Gallery)</cell><cell></cell><cell></cell><cell cols="3">Deep Features (5M Gallery)</cell><cell cols="3">Deep Features (80M Gallery)</cell></row><row><cell></cell><cell>1x</cell><cell>1y</cell><cell>1z</cell><cell>1x</cell><cell>1y</cell><cell>1z</cell><cell>1x</cell><cell>1y</cell><cell>1z</cell></row><row><cell>1a</cell><cell>2,041,004</cell><cell>595,265</cell><cell>1,750,309</cell><cell>132,577</cell><cell>232,275</cell><cell>1,401,474</cell><cell>2,566,917</cell><cell>5,398,454</cell><cell>31,960,091</cell></row><row><cell>1b</cell><cell>3,816,874</cell><cell>3,688,368</cell><cell>2,756,641</cell><cell>1,511,300</cell><cell>1,152,484</cell><cell>1,699,926</cell><cell>33,783,360</cell><cell>27,439,526</cell><cell>44,282,173</cell></row><row><cell></cell><cell>2x</cell><cell>2y</cell><cell>2z</cell><cell>2x</cell><cell>2y</cell><cell>2z</cell><cell>2x</cell><cell>2y</cell><cell>2z</cell></row><row><cell>2a</cell><cell>67,766</cell><cell>86,747</cell><cell>301,868</cell><cell>174,438</cell><cell>39,417</cell><cell>105879</cell><cell>2,461,664</cell><cell>875,168</cell><cell>1,547,895</cell></row><row><cell>2b</cell><cell>352,062</cell><cell>48,335</cell><cell>865,043</cell><cell>71,783</cell><cell>26,525</cell><cell>84,012</cell><cell>1,417,768</cell><cell>972,411</cell><cell>1,367,694</cell></row><row><cell>2c</cell><cell>158,341</cell><cell>625</cell><cell>515,851</cell><cell>9</cell><cell>341</cell><cell>9,975</cell><cell>109</cell><cell>2,952</cell><cell>136,651</cell></row><row><cell></cell><cell>Proposed Cascaded Face Search System</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2c</cell><cell>DF→COTS@1K</cell><cell></cell><cell></cell><cell>7</cell><cell>1</cell><cell>9,975</cell><cell>46</cell><cell>2,952</cell><cell>136,651</cell></row><row><cell>2c</cell><cell>DF→COTS@10K</cell><cell></cell><cell></cell><cell>10</cell><cell>1</cell><cell>1,580</cell><cell>160</cell><cell>8</cell><cell>136,651</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>MethodProbe Top 10 most similar retrieved images from an 80M face gallery</figDesc><table><row><cell>Deep Features</cell><cell>1a</cell></row><row><cell>Deep Features</cell><cell>1b</cell></row><row><cell>Deep Features</cell><cell>2a</cell></row><row><cell>Deep Features</cell><cell>2b</cell></row><row><cell>Deep</cell><cell></cell></row><row><cell>Features</cell><cell>2c</cell></row><row><cell>DF→COTS</cell><cell></cell></row><row><cell>@10K</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable face image retrieval using attribute-enhanced sparse codewords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable face image retrieval with identity-based quantization and multi-reference re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2010. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face retriever: Pre-filtering the gallery via deep neural net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICB</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923v1</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face recognition vendor test (frvt): Performance of face identification algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIST Interagency Report</title>
		<imprint>
			<biblScope unit="volume">8009</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Megaface: A million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="347" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast matching by 2 lines of code for large scale face recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.7180</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Towards incremental and large scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ICB</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient face retrieval using synecdoches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICB</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unconstrained face recognition: Identifying a person of interest from a media collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A benchmark study of large-scale unconstrained face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCB</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Handbook of face recognition</title>
		<editor>A. K. Jain and S. Z. L.</editor>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tencent-Bestimage</surname></persName>
		</author>
		<ptr target="http://bestimage.qq.com/2" />
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey of content-based image retrieval with high-level semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="262" to="282" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Retrieval-based face annotation by weak label regularized local coordinate coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling LSH for performance tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CIKM</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discrimination of characters by a multi-stage recognition process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1539" to="1549" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning for content-based image retrieval: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM MM</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Locally optimized product quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Score normalization in multimodal biometric systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2270" to="2285" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the link between error correlation and error reduction in decision tree ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS-UCI</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4773</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deepid3: Face recognition with very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Naive-deep face recognition: Touching the limit of LFW benchmark or not</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04690</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised joint alignment of complex images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A case study of automated face recognition: The boston marathon bombings suspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klontz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
