<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Option Comparison Network for Multiple-choice Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-03-07">7 Mar 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Ran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Hu</surname></persName>
							<email>weiweihu@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Option Comparison Network for Multiple-choice Reading Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-03-07">7 Mar 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multiple-choice reading comprehension (MCRC) is the task of selecting the correct answer from multiple options given a question and an article. Existing MCRC models typically either read each option independently or compute a fixed-length representation for each option before comparing them. However, humans typically compare the options at multiple-granularity level before reading the article in detail to make reasoning more efficient. Mimicking humans, we propose an option comparison network (OCN) for MCRC which compares options at word-level to better identify their correlations to help reasoning. Specially, each option is encoded into a vector sequence using a skimmer to retain fine-grained information as much as possible. An attention mechanism is leveraged to compare these sequences vector-by-vector to identify more subtle correlations between options, which is potentially valuable for reasoning. Experimental results on the human English exam MCRC dataset RACE show that our model outperforms existing methods significantly. Moreover, it is also the first model that surpasses Amazon Mechanical Turker performance on the whole dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multiple-choice reading comprehension (MCRC) aims to selecting the correct answer from a set of options given a question and an article. As MCRC requires both understanding of natural language and world knowledge to distinguish correct answers from distracting options, it is challenging for machine and a good testbed for artificial intelligence. With the rapid development of deep learning, various neural models have been proposed for MCRC and achieve promising results in recent years <ref type="bibr" target="#b0">(Chen et al., 2016;</ref><ref type="bibr" target="#b16">Yin et al., *</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>indicates equal contribution</head><p>Article: Are you a crazy chocolate fan? Have you heard about Hershey's Kisses? Do you love the movie Charlie and the Chocolate Factory? If your answer was, "yes", to any of the questions, then my experience will make you jealous. I just went to the famous Hershey Chocolate Factory! ...... When we arrived at the factory, we realized that this was much more than just a factory. The whole town is a chocolate-themed amusement park ...... Jason, our tour guide, began telling us about this quiet little town ...... Jason went on, "The factory first started on a small farm. It developed very fast. So they built this town for factory workers to live in. Then they built hotels, hospitals, stadiums, theaters and even museums with the theme of chocolate. Isn't that cool?" "Yes, a hundred times yes!" I yelled ( ) with delight. Question: What can we know from the writer's answer to the guide? Options: A. The writer had never heard about Hershey Chocolate. B. The writer didn't want to visit the factory any more. C. The writer had visited the factory before. D. The writer couldn't wait to visit the factory. Answer: D 2016; <ref type="bibr" target="#b11">Trischler et al., 2016;</ref><ref type="bibr" target="#b2">Dhingra et al., 2017;</ref><ref type="bibr" target="#b10">Tay et al., 2018;</ref><ref type="bibr" target="#b5">Parikh et al., 2018;</ref><ref type="bibr" target="#b18">Zhu et al., 2018;</ref><ref type="bibr" target="#b12">Wang et al., 2018;</ref><ref type="bibr" target="#b15">Xu et al., 2017;</ref><ref type="bibr" target="#b9">Sun et al., 2018;</ref><ref type="bibr" target="#b17">Zhang et al., 2019)</ref>.</p><p>Comparing options before reading the article in detail is a commonly used strategy for humans when solving MCRC problems. By comparing the options, the correlations between the options can be identified and people only need to pay attention to the information related to the correlations when reading the article. As a result, questions can be answered more efficiently and effectively. <ref type="table" target="#tab_0">Taking  Table 1</ref> as an example, by comparing option B and D, people may identify that the key difference is whether the writer would like to visit the factory, which can be decided easily by skimming the article.</p><p>However, the strategy is not adopted by most existing MCRC methods. The Stanford AR <ref type="bibr" target="#b0">(Chen et al., 2016)</ref> and GA Reader <ref type="bibr" target="#b2">(Dhingra et al., 2017)</ref> variants used in <ref type="bibr" target="#b4">(Lai et al., 2017)</ref> encode question and article independent of options, ignoring their correlations.</p><p>In contrast, <ref type="bibr" target="#b12">Wang et al. (2018)</ref> and <ref type="bibr" target="#b17">Zhang et al. (2019)</ref> leverage sophisticated matching mechanisms to gather the correlation information, while <ref type="bibr" target="#b9">Sun et al. (2018)</ref> relies on a pre-trained language model <ref type="bibr" target="#b7">(Radford et al., 2018)</ref> to extract such information. Nevertheless, none of them consider the correlations between options explicitly. To the best of our knowledge, <ref type="bibr" target="#b18">(Zhu et al., 2018)</ref> is the only work that considers option correlations explicitly.</p><p>Whereas, the options are compressed into fixed-length vectors before being compared, which may make it hard for a model to identify subtle differences or similarities between options.</p><p>To gather option correlation information more effectively, we propose option comparison network (OCN), a novel method for MCRC which explicitly compares options at word-level to mimic the aforementioned human strategy. Specially, we first use a skimmer network to encode options into vector sequences independently as their features. Then for each option, it is compared with other options one-by-one at word-level using an attentionbased mechanism in vector space to identify their correlations. Finally, the article is reread with the gathered correlation information to do reasoning and select the correct answer. As options are compared one-by-one, the correlations between each pair of options can be explicitly identified. By comparing options at word-level, we allow the model to detect subtle correlations more easily.</p><p>With a BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> based skimmer, our method outperforms the state-of-the-art baselines with large margins on RACE, a human exam MCRC dataset created by experts for assessing the reading comprehension skills of students, indicating the effectiveness of our model. More importantly, it is the first time that a model surpasses the Amazon Mechanical Turker performance on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Option Comparison Network</head><p>Suppose we have a question Q with n tokens {w q 1 , w q 2 , · · · , w q n }, an article P with m tokens {w p 1 , w p 2 , · · · , w p m }, and a candidate answer</p><formula xml:id="formula_0">set O with K options {O 1 , O 2 , · · · , O K }. Each option O k consists of n k tokens {w o 1 , w o 2 , · · · , w o n k }.</formula><p>For-mally, MCRC is to select the correct answerÔ from the candidate answer set O given question Q and article P . Our model selects the correct answer from the candidate answer set in four stages. First, we concatenate each (article, question, option) triple into a sequence and use a skimmer to encode them into vector sequences (Sec. 2.1). Then an attentionbased mechanism is leveraged to compare the options (Sec. 2.2). Next the article is reread with the correlation information gathered in last stage as extra input (Sec. 2.3). And finally the probabilities for each option to be the correct answer are computed (Sec. 2.4). The details will be introduced in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Option Feature Extraction</head><p>A skimmer network is used to skim the options independently together with the question and article to extract option features. As BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> has been shown to be a powerful feature extractor for various tasks, it is used as the skimmer. Specially, for option O k , it is concatenated with the question Q and article P , denoted as P ; Q; O k 1 . Then the sequence is fed to BERT to compute their vector space encoding, which is denoted as</p><formula xml:id="formula_1">[P enc ; Q enc ; O enc k ] = BERT ( P ; Q; O k ) (1) where P enc ∈ R d×m , Q enc ∈ R d×n , O enc k ∈ R d×n k ,</formula><p>and BERT(·) denotes the network defined in <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> </p><formula xml:id="formula_2">2 .</formula><p>As question and options are closely related, we use</p><formula xml:id="formula_3">O q k = [Q enc |O enc k ] ∈ R d×n ′ k<label>(2)</label></formula><p>as features of O k , where n ′ k = n + n k and [·|·] denotes row-wise concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Option Correlation Features Extraction</head><p>This module is used to compare options at word level to extract option correlation information to support reasoning. For each option, an attentionbased mechanism is used to compare it with all the other options to gather the correlation information.</p><p>Given input matrices U ∈ R d×N and V ∈ R d×M , the attention weight function Att(·) spec-ified by the parameter v ∈ R 3d is defined as</p><formula xml:id="formula_4">s ij = v T [U :i ; V :j ; U :i • V :j ] (3) A = Att (U , V ; v) (4) = exp(s ij ) i exp(s ij ) i,j<label>(5)</label></formula><p>where [·; ·] denotes column-wise concatenation, • denotes the element-wise multiplication operation, and A ∈ R N ×M is the attention weight matrix.</p><p>The option correlation features are extracted in three steps as follows:</p><p>First, an option is compared with all other options one-by-one to collect the pairwise correlation information. Specially, for option O k , the in-</p><formula xml:id="formula_5">formation O (l) k ∈ R 2d×n ′ k gathered from option O l is computed as O (l) k = O q l Att(O q l , O q k ; v o ) (6) O (l) k = O q k −Ō (l) k ; O q k •Ō (l) k<label>(7)</label></formula><p>Then the pairwise correlation information gathered for each option is fused to get the option-wise correlation information, which is defined as</p><formula xml:id="formula_6">O c k = tanh W c O q k ; O (l) k l =k + b c<label>(8)</label></formula><p>where W c ∈ R d×(d+2d(|O|−1)) and b c ∈ R d . Note that option O k is not compared with itself. Finally, an element-wise gating mechanism is leveraged to fuse the option features with the option-wise correlation information to produce the option correlation features O c k . Specially, the gates g k ∈ R d×n ′ k are defined as</p><formula xml:id="formula_7">g k,:i = sigmoid W g [O q k,:i ; O c k,:i ; Q] + b g (9)</formula><p>where g k,:i denotes the i-th column of g, and Q ∈ R d is the attentive-pooling of Q enc defined as</p><formula xml:id="formula_8">A q = softmax v T a Q enc T , v a ∈ R d (10) Q = Q enc A q<label>(11)</label></formula><p>The option correlation</p><formula xml:id="formula_9">features O c k ∈ R d×n ′ k are computed as O c k,:i = g k,:i • O q k,:i + (1 − g k,:i ) • O c k,:i (12)</formula><p>Note that O c k is not compressed into a fixed-length vector, because we believe this will enable our model to utilize the correlation information in a more flexible way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Article Rereading</head><p>Mimicking humans, the article will be reread with the option correlation features as extra input to gain deeper understanding. Specially, the co-attention <ref type="bibr" target="#b14">(Xiong et al., 2017)</ref> and selfattention <ref type="bibr" target="#b13">(Wang et al., 2017)</ref> mechanisms are adopted for rereading. First, for each option O k , co-attention is performed as</p><formula xml:id="formula_10">A c k = Att (O c k , P enc ; v p ) ∈ R n ′ k ×m (13) A p k = Att (P enc , O c k ; v p ) ∈ R m×n ′ k (14) O p k = [P enc ; O c k A c k ]A p k ∈ R 2d×n ′ k (15) ThenÔ p k is fused with option correlation features O c k as O p k = ReLU(W p [O c k ;Ô p k ] + b p ) (16) where O p k ∈ R d×n ′ k , W p ∈ R d×3d , and b p ∈ R d . Finally, the full-info option representation O f k ∈ R d×n ′ k for option O k is computed with self- attention as O s k = O p k Att( O p k , O p k ; v r ) (17) O f k = [ O p k ; O s k ; O p k − O s k ; O p k • O s k ] (18) O f k = ReLU(W f O f k + b f )<label>(19)</label></formula><p>where W f ∈ R d×4d and b f ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Prediction</head><p>The score s k of option O k to be the correct answer is computed as</p><formula xml:id="formula_11">s k = v T s MaxPooling O f k<label>(20)</label></formula><p>where MaxPooling(·) performs row-wise max pooling and v s ∈ R d . The probability P (k|Q, P, O) of option O k to be the correct answer is computed as</p><formula xml:id="formula_12">P (k|Q, P, O) = exp(s k ) i exp(s i )<label>(21)</label></formula><p>And the loss function is defined as  3 Experiments</p><formula xml:id="formula_13">J(θ) = − 1 N i log(P (k i |Q i , P i , O i )) +</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluate our model on RACE <ref type="bibr" target="#b4">(Lai et al., 2017)</ref>, an MCRC dataset collected from the English exams for middle and high school students in China. The dataset is further devided into RACE-M and RACE-H, containing only data from middle school and high school examinations respectively. As the articles, questions and options are generated by English instructors for assessing the reading comprehension skills of humans, the dataset is inherently more difficult than other widely used reading comprehension datasets such as SQuAD <ref type="bibr" target="#b8">(Rajpurkar et al., 2016)</ref>. Analysis conducted in <ref type="bibr" target="#b4">(Lai et al., 2017)</ref> shows that 59.2% of the questions in RACE require reasoning, which is significantly higher than that of SQuAD (20.5%).</p><p>And the most frequent reasoning skills required are detail reasoning, whole-picture understanding, passage summarization, attitude analysis and world knowledge. Therefore, RACE is extremely challenging for MCRC models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Details</head><p>Adam optimizer (Kingma and Ba, 2014) is used to train our model. The model is trained for 3 epochs with batch size 12 and learning rate 3×10 −5 when BERT BASE is used as the skimmer, and trained for 5 epochs with batch size 24 and learning rate 1.5 × 10 −5 when BERT LARGE is used. For both cases, the learning rate linearly increases from 0.0 to the aforementioned value in the first 10% training steps and then linearly decays until training is completed. The L2 weight decay λ is set to 0.01. Articles, questions and options are trimmed to 400, 30 and 16 tokens respectively for memory and speed consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head><p>We compare our model with various state-of-theart methods and the results are shown in <ref type="table">Ta</ref>  has learned certain reasoning skills.</p><p>(3) There is still a large gap between human ceiling performance and our model's performance. We believe this is because our model still struggles in complex reasoning as expected. (4) All the models using pre-trained contextualized representations (GPT <ref type="bibr" target="#b7">(Radford et al., 2018)</ref> and BERT) outperform the other models with significantly large margins, indicating pre-training is a promising research direction for learning semantics from unsupervised data.</p><p>The ablation study results are shown in Table 3. Removing the option comparison component (Sec. 2.2) causes significant performance drop, especially on RACE-H, indicating the effectiveness of considering the correlations between options. The performance of our model drops seriously when BERT is replaced with ELMo <ref type="bibr" target="#b6">(Peters et al., 2018)</ref>, suggesting that BERT is a powerful feature extractor that can capture rich semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>To leverage option correlations to improve reasoning ability, we propose option comparison network (OCN) for multiple-choice reading comprehension in this work. By representing options as vector sequences and comparing them vector-byvector, we allow our model to identify the correlations between options more effectively. Experimental results show that our model outperforms the state-of-the-art baselines significantly and surpasses Amazon Mechanical Turker on the whole RACE dataset for the first time, indicating that our model is effective and has learned certain reasoning skills.</p><p>As shown in the ablation study, our model relies on the pre-trained BERT model heavily. However, BERT model is large and slow. How to reduce the model size and improve its speed with acceptable performance drop is an interesting future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An MCRC example from the RACE dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results. The best results in each group are in bold, and those better than Amazon Mechanical Turker are underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study. "Opt. Comp." denotes option comparison.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"> Delimiter [SEP]  are added between P , Q and O k . We omit [SEP] from the notation for brevity.2  We refer the readers to<ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> for details of BERT(·).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">   (22)   where θ denotes all trainable parameters, N is the training example number, andk i is the ground truth for the i-th example.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A thorough examination of the CNN/Daily Mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gatedattention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RACE: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ElimiNet: A model for eliminating options for reading comprehension with multiple choice questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preksha</forename><surname>Nema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4272" to="4278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training. Technical report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving machine reading comprehension with general reading strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno>abs/1810.13441</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-range reasoning for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno>abs/1803.09074</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A parallel-hierarchical model for machine comprehension on sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="432" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A co-matching model for multi-choice reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards humanlevel machine reading comprehension: Reasoning and inference with multiple strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1711.04964</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention-based convolutional neural network for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human-Computer Question Answering</title>
		<meeting>the Workshop on Human-Computer Question Answering<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dual comatching network for multi-choice reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuailiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1901.09381</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical attention flow for multiple-choice reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6084" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
