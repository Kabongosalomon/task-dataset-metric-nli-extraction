<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Video Generation, Prediction and Completion of Human Action Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Cai</surname></persName>
							<email>cbai@connect.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<email>yuwingtai@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tencent</forename><surname>Youtu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Video Generation, Prediction and Completion of Human Action Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current deep learning results on video generation are limited while there are only a few first results on video prediction and no relevant significant results on video completion. This is due to the severe ill-posedness inherent in these three problems. In this paper, we focus on human action videos, and propose a general, two-stage deep framework to generate human action videos with no constraints or arbitrary number of constraints, which uniformly address the three problems: video generation given no input frames, video prediction given the first few frames, and video completion given the first and last frames 1 . To make the problem tractable, in the first stage we train a deep generative model that generates a human pose sequence from random noise. In the second stage, a skeleton-to-image network is trained, which is used to generate a human action video given the complete human pose sequence generated in the first stage. By introducing the two-stage strategy, we sidestep the original ill-posed problems while producing for the first time high-quality video generation/prediction/completion results of much longer duration. We present quantitative and qualitative evaluation to show that our two-stage approach outperforms state-of-the-art methods in video generation, prediction and video completion. Our video result demonstration can be viewed at https://iamacewhite. github.io/supp/index.html.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper we propose a general, two-stage deep framework for human video generation, prediction and completion <ref type="figure" target="#fig_0">(Figure 1)</ref>, where each problem was previously addressed as separate problems. Previous video generation capitalizing state-of-the-art deep convolutional neural network (CNN), such as <ref type="bibr" target="#b34">[35]</ref>, has demonstrated the significant difficulty of the problem, where their first results were still far from photorealism. Current future prediction <ref type="bibr" target="#b19">[20]</ref> in the form of video prediction <ref type="bibr" target="#b36">[37]</ref> generates a short video from a * Equal Contribution. <ref type="bibr" target="#b0">1</ref> Without causing confusion, we refer unconstrained video generation as just video generation when no input frames are given, and we still use video prediction to refer to input situations where the first few frames are given, i.e., not just the first frame. given frame to predict future actions in a very limited scope. While there exist deep learning works on image completion <ref type="bibr" target="#b42">[43]</ref>, there is no known representative deep learning work on video completion.</p><p>To better address the general video synthesis problem, we need to understand how pixels change to generate a full temporal object action. With a higher level of uncertainty in the exact movement between frames of the moving object, as observed in <ref type="bibr" target="#b36">[37]</ref>, the problem is more tractable by modeling the uncertainty with underlying structure of the moving objects, which in our case is human poses. Human action videos are arguably the most interesting and useful videos in various computer vision applications. Thus, we focus on human action videos in this paper, and divide the task into human pose sequence generation (pose space) followed by image generation (pixel space) from the generated human pose sequences.</p><p>Specifically, our general deep framework for video generation has two stages: first, a new conditional generative adversarial network (GAN) is proposed to generate a plausible pose sequence that performs a given category of actions; we then apply our supervised reconstruction network with feature matching loss to transfer pose sequence to the pixel space in order to generate the full output video. Our general video generation framework can be specialized to video completion and video prediction by optimizing in the latent space to generate video results that best suit the given input constraints. Hence our approach can either generate videos from scratch, or complete/predict a video with arbitrary number of input frames available given the action class. We will provide extensive qualitative and quantitative experimental results to demonstrate that our model is able to generate and complete natural human motion video under the constraints prescribed by input frames if available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our general framework and implementation can be uniformly applied to video generation, prediction and completion. Video completion or prediction can be regarded as video generation constrained by the input frames. This paper focuses on human action videos, and our two-step approach consists of human pose estimation and image generation. We review here recent representative works leveraging deep learning to achieve state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Prediction/Generation</head><p>In video forecasting, research has been done to model uncertain human motion in pose space <ref type="bibr" target="#b36">[37]</ref>. Attempts have also been made to learn the deep predictive coding <ref type="bibr" target="#b17">[18]</ref> or various feature representation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>. To generate videos from scratch, work has been done to generate videos directly in pixel space <ref type="bibr" target="#b34">[35]</ref>. While these works shed some light on how we should model the uncertainty and temporal information in videos, our work aims at a different goal: video completion and generation in the same framework. Video prediction is useful in training computer agents to play computer games or how to act in complicated 2D or 3D environment <ref type="bibr" target="#b19">[20]</ref> by predicting the next frames (consisting of simple objects in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b7">8]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image/Video Completion</head><p>Much work in the completion area has been focused on image completion with Generative Models <ref type="bibr" target="#b42">[43]</ref>. However, video completion in the realm of deep learning has remain unexplored, although video completion is an important topic <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41]</ref>. If the temporal distance between the input frames is small, e.g., <ref type="bibr" target="#b23">[24]</ref> then videos frame interpolation can be performed to fill in the in-between frames. We are dealing with a different problem where input frames are far apart from each other in completion. The modeling of such uncertainty increased the difficulty of this task. In our paper, we aim to perform video completion by optimizing the latent space under the constraint of input frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Pose Estimation</head><p>Various research efforts have been made to produce state-of-the-art human pose estimation results, providing us with reliable human pose baseline. Realtime multi-person pose estimation was achieved in <ref type="bibr" target="#b2">[3]</ref>. Other important works include DeepPose <ref type="bibr" target="#b33">[34]</ref>, Pose Ma-chine <ref type="bibr" target="#b39">[40]</ref> and Adversarial PoseNet <ref type="bibr" target="#b4">[5]</ref>. The current stateof-the-art human pose estimation on our choice of dataset, Human3.6m <ref type="bibr" target="#b12">[13]</ref>, is achieved by Newell et al <ref type="bibr" target="#b22">[23]</ref>. In our paper, we leverage the reliable human pose estimation from <ref type="bibr" target="#b22">[23]</ref> in human motion videos and complete a pose sequence that looks smooth and natural.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Models</head><p>Our work is based on Generative Adversarial Networks, which has been undergoing rapid development recently. In the first GAN <ref type="bibr" target="#b10">[11]</ref>, a model that can implicitly generate any probabilistic distribution was proposed. Then the conditional version of GAN <ref type="bibr" target="#b20">[21]</ref> was proposed to enable generation under condition. Subsequent works include usage of convolution neural networks <ref type="bibr" target="#b27">[28]</ref>, and improve the training stability <ref type="bibr" target="#b29">[30]</ref> followed by Wasserstein GAN <ref type="bibr" target="#b0">[1]</ref> and Improved WGAN <ref type="bibr" target="#b11">[12]</ref> which further made Generative Adversarial Networks reliable in real world applications. In our paper, we first train a conditional WGAN to generate single frame human pose, then we train another conditional sequence GAN to find the optimal latent code combination on the single pose generator to generate a pose sequence that look real and natural in order to perform video completion in the latent space of sequence generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization over Input Data</head><p>To specialize our general generative model to video prediction and completion, we model them as constrained video generation and adopt randomization of the input data to find the motion sequence that best matches the input frames. In the most recent work, back-propagation on input data is performed on image inpainting <ref type="bibr" target="#b42">[43]</ref> to find the best match of corrupted image. Zhu et al <ref type="bibr" target="#b44">[45]</ref> utilized such method to enable generative visual manipulation by optimizing on the latent manifold. Google DeepDream <ref type="bibr" target="#b21">[22]</ref> also used back-propagation to generate dream-like images. Earlier, similar method has been employed to perform texture synthesis and style transfer <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>. In our paper, we similarly design specific loss and perform randomized optimization on the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton to Image Transformation</head><p>Our two-stage model involves a second stage to transform human pose to pixel level image in order to generate video in pixel space, which has been attempted by various deep learning methods. Recent methods like <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref> utilize GAN or multistage method to complete this task. We propose a simple yet effective supervised learning framework qualitatively comparable to state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We present a general generative model that uniformly addresses video generation, prediction and completion problems for human motions. The model itself is originally designed for video generation, i.e., generating human action videos from random noise. We split the generation process into two stages: first, we generate human skeleton sequences from random noise, and then we transform from the skeleton images to the real pixel-level images. In Sec- <ref type="figure">Figure 2</ref>. Overview of our two-stage video generation. In the first stage we generate skeleton motion sequences by G from random noise, while in the second stage we use our skeleton-to-image transformer F to transform skeleton sequence to image space. tion 3.1 we will elaborate the model and methods we use to generate human skeleton motion sequences, and in Section 3.2 we will present our novel method for solving the skeleton-to-image transformation problem. <ref type="figure">Figure 2</ref> illustrates our pipeline. Lastly, in Section 3.3, we will show that we can specialize this model without modification to accomplish video prediction and completion by regarding them as constrained video generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General Generative Model</head><p>We propose a two-step generation model that generates human skeleton motion sequences from random noise.</p><p>Let J be the number of joints of human skeleton in a video frame, and we represent each joint by its (x,y) location in image space. We formulate a skeleton motion sequence V as a collection of human skeletons across T consecutive frames in total, i.e., V ∈ R T ×2J , where each skeleton frame V t ∈ R 2J , t ∈ {1 · · · T } is a vector containing all (x, y) joint locations. Our goal is to learn a function G : R n → R T ×2J which maps an n-dimensional noise vector to a joint location vector sequence.</p><p>To find this mapping, our experiments showed that human pose constraints are too complicated to be captured by an end-to-end model trained from direct GAN method <ref type="bibr" target="#b10">[11]</ref>. Therefore, we switch to our novel two-step strategy, where we first train a Single Pose Generator G 0 : R m → R 2J which maps a m-dimensional latent vector to a single-frame pose vector, and then train a Pose Sequence Generator G P S : R n → R T ×m which maps the input random noise to the latent vector sequences, the latter of which can be transformed into human pose vector sequences through our Single Pose Generator in a frame-by-frame manner. <ref type="figure">Figure 3</ref> shows the overall pipeline and the results for each step in the procedure. The advantage of adopting this two-step method is that by training the single-frame generator, we enforce human pose constraints on each frame generated, which alleviate the difficulty compared to endto-end GAN training thus enabling the model to generate longer sequences. Additionally, in order to generate different types of motions, we employ the Conditional GAN <ref type="bibr" target="#b20">[21]</ref> method and concatenate an one-hot class vector indicating which class of motion to be produced to the input of our generators. <ref type="figure">Figure 3</ref>. Illustration of our two-step generation pipeline. In step one (left) G0 takes a random noise vector and outputs the generated pose vector. The D0 then differentiate between real and fake pose vectors. Both inputs to G0 and D0 are concatenated with conditional class vector. In step two (right), GP S takes the random noise z conditioned on the latent vector of the first frame and the class vector, and generates a sequence of latent vectors which can be transformed to pose vectors via G0. Then DP S takes as input real/fake frames to determine P (Real).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Single Pose Generator</head><p>In the first step, we employ the improved WGAN <ref type="bibr" target="#b11">[12]</ref> method with gradient penalty for our adversarial training. We build a multilayer perceptron (MLP) for both our generator and critic with similar structures and add condition to the input of both of them according to Conditional GAN <ref type="bibr" target="#b20">[21]</ref>. Our generator G 0 takes as input an m-dimensional latent vector z 0 concatenated with a one-hot class vector c and outputs a pose vector G 0 (z 0 |c). Our critic D 0 takes as input a real pose vector x 0 or a generated one, concatenated with c, yielding a critic score. The detailed architecture configurations are shown in <ref type="figure">Figure 4a</ref>, and are detailed in supplementary materials. Thus the WGAN objective is:</p><formula xml:id="formula_0">min G0 max D0∈D E c∼pc [E x0∼ppose [D 0 (x 0 |c)]− E z0∼pz 0 [D 0 (G 0 (z 0 |c)|c)]]<label>(1)</label></formula><p>where D is the set of 1-Lipschitz functions, p c is the distribution of different classes, p pose is the distribution of the real pose data, and p z0 is the uniform noise distribution. <ref type="figure" target="#fig_3">Figure 5a</ref> shows some of our generated pose subject to given action class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Pose Sequence Generator</head><p>In the second step, we use the normal GAN <ref type="bibr" target="#b10">[11]</ref> method instead for training our Pose Sequence Generator, since in practice normal GAN performs better than WGAN for this specific task. The generator G P S generates a sequence of latent vectors, which are then fed into the Single Pose Generator resulting in a sequence of pose vectorsV , from a random noise vector z conditioned on z 0 and c. Note that z 0 is a random noise vector describing the initial condition of the generated pose.</p><p>In our implementation we generate latent vector sequences by generating the shifts between two consecutive frames, namely, the output of the network is s 0 , s 1 , ..., s T −2 where z t+1 = s t + z t for all t ∈ {0...T − 2} and z t is the latent vector for the t-th frame (z 0 is given from the noise distribution).</p><p>For the discriminator, we employ a bi-directional LSTM structure, whose input of each time step t is the shift of consecutive frames ∆V t =V t+1 −V t conditioned onV t and c. The structural details are shown in <ref type="figure">Figure 4b</ref>. The objective function for the training in this step is:</p><formula xml:id="formula_1">min GP S max DP S E c∼pc [E V ∼p video [log D P S (V |c)]+ E z0∼pz 0 ,z∼pz [log(1 − D P S (G P S (z 0 |z, c)|c))]]<label>(2)</label></formula><p>where P c is the distribution of different classes, p video is the distribution of the real video sequence data, p z0 is the uniform noise distribution and p z is the Gaussian noise distribution. In our implementation, we also add an L2 regularization term on the generated latent vector shifts for temporal smoothness, and thus we restrict the output latent  vectors in the range of (−1, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use Adam Solver <ref type="bibr" target="#b15">[16]</ref> at a learning rate of 0.001 for training Single Pose Generator and 5e-5 for training Pose Sequence Generator , both decaying by 0.5 after 30 epochs with β 1 being 0.5 and β 2 being 0.9. We set the weight of gradient penalty to be 10 and the weight of L2 regularization term for generated latent vector shift to be 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Skeleton to Image Transformation</head><p>In this stage, we train a skeleton-to-image transformation to convert pose space to image space. Formally, given an input pose vector x ∈ R 2J and a reference image y 0 ∈ R w×h×3 where h and w are the width and height of images, we need to transform x to a pixel-level image y ∈ R w×h×3 . In order to make the dimensions of inputs well-aligned, we first convert the pose vector x to a set of heat maps S = (S 1 , S 2 , ..., S J ), where each heat map S j ∈ R w×h , j ∈ {1...J} is a 2D representation of the probability that a particular joint occurs at each pixel location. Specifically, let l j ∈ R 2 , (l j = (x j , x j+1 )) be the 2D position for joint j. The value at location p ∈ R 2 in the heat map S j is then defined as,</p><formula xml:id="formula_2">S j (p) = exp(− p − l j 2 2 σ 2 )<label>(3)</label></formula><p>where σ controls the variance. Then our goal is to learn a function F : R w×h×J → R w×h×3 that transforms joint heat maps into pixel-level human images, conditioned on the input reference image. This function F can thus be easily achieved by a feed-forward network. Note that we have the reference ground truth human images corresponding to each input poses in our training data, hence we can train this network in a supervised fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton-to-Image Network</head><p>To learn our function F , we employ a U-Net like network <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19]</ref> (i.e., convolutional autoencoder with skip connections as shown in <ref type="figure">Figure 6</ref>) that takes, as input, a set of joint heat maps S <ref type="figure">Figure 6</ref>. Skeleton to Image Network. Image sizes and feature dimensions are shown in the figure. Note that the input is of size <ref type="figure" target="#fig_0">(128, 128, 18)</ref>, which is the concatenation of (128, 128, 18) reference image and (128, 128, 15) heat maps for 15 joints and a reference image y 0 and produces, as output, a human imageŷ. For the encoder part, we employ a convolutional network which is adequately deep so that the final receptive field covers the entire image. For the decoder part, we use symmetric structure to gradually generate the image. To avoid inherit checkerboard artifact in transposed convolution layers, there has been several papers proposing solutions including sub-pixel convolution, resize and convolution etc <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b6">7]</ref>. In our case we apply nearest neighbor up-sampling followed by convolution layer in decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>To train our skeleton-to-image network, we compare the output image with the corresponding reference ground truth image by binary cross entropy loss. We calculate the binary cross entropy loss for intensity values at each pixel, i.e.</p><formula xml:id="formula_3">L bce = − 1 k (1 − y) log(1 − F (x|y 0 )) + y log(F (x|y 0 ))<label>(4)</label></formula><p>where y is the reference ground truth image, x is pixel and k is the number of pixels. Our experiments show that only using binary cross entropy loss tends to produce blurry results. Hence, in order to enforce details in the produced images, we further employ a feature-matching loss (in some paper also referred as perceptual loss), as suggested in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>. We match the activations in a pre-trained visual perception network that is applied to both the reference ground truth image and the generated image. Different layers in the network represent different levels of abstraction, providing comprehensive guidance for our transformation function F to generate more realistic images.</p><p>Specifically, let Φ be the visual perception network (We use VGG-19 <ref type="bibr" target="#b32">[33]</ref>), and Φ l be the activations in the l-th layer. Our feature-matching loss is defined as,</p><formula xml:id="formula_4">L 2 = l λ l Φ l (F (x|y 0 )) − Φ l (y) 1<label>(5)</label></formula><p>where λ l is the weight for the l-th layer, which are manually set to balance the contribution of each term. For layers Φ l , we use 'conv1 2', 'conv2 2', 'conv3 2', 'conv4 2' and 'conv5 2' in VGG-19 <ref type="bibr" target="#b32">[33]</ref>. The overall loss function for our skeleton-to-image network is therefore defined as</p><formula xml:id="formula_5">L = L 1 + λL 2<label>(6)</label></formula><p>where λ denotes the regularization factor of feature matching loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We train our network with Adam Solver <ref type="bibr" target="#b15">[16]</ref> at a learning rate of 0.001 and β 1 of 0.9. For the feature matching loss we set λ = 0.01. Our architecture details are shown in <ref type="figure">Figure 6</ref>. In the encoder we have 8 convolution layers with 5 × 5 filter size, alternating stride of 2, 1, 2, 1, 2, 1, 2, 1 and "same" padding. In the decoder we have 3 (upsample, conv, conv) modules followed by 1 (upsample, conv) module, where upsample stands for nearest neighbor resize with scalef actor = 2 and conv stands for convolution layer with 5 × 5 filter size and 1 stride size with "same" padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Prediction and Completion</head><p>To uniformly address video completion and video prediction, we model them as constrained video generation, which is ready to be defined by the general generative model. We optimize on the latent space in order to achieve our goal. For simplicity, the optimization is conducted based on generated pose sequence, and we can transform to complete video by our skeleton-to-image transformer using the completed pose sequence. We utilize state-of-the-art human pose estimation methods like <ref type="bibr" target="#b22">[23]</ref> to obtain pose sequences from videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Video Completion</head><p>To fill in missing frames of a video, our method utilizes the generator G trained with full-length human pose sequence. We assume that the learned encoding manifold z is effective in representing p data . We aim to perform video completion by finding the encodingẑ on the manifold that best fits the input frames constraint. As illustrated in <ref type="figure" target="#fig_5">Figure 8</ref>, we can generate the missing content by using the trained generative model G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective Function</head><p>We regard the constrained video generation problem as an optimization problem. Let I ∈ R t×2J be the input frames which acts as constraint and z denote the learned encoding manifold of G. With these notations, we define the optimal completion encodingẑ by: </p><formula xml:id="formula_6">z = arg min z {L c (z|I) + α × L p (z)},<label>(7)</label></formula><p>where L c denotes the contextual L1 loss between the constrained frames and corresponding generated frames and L p denotes the perceptual loss of generated frames, i.e. "realness" of the pose sequence. α denotes a regularization factor of the perceptual loss. L c and L p are defined as follows:</p><formula xml:id="formula_7">L c (z|I) = i∈I |G(z) i − I i | (8) L p (z) = − log(D(G(z)))<label>(9)</label></formula><p>where I is the set of constrained frames and z is some latent vector, i denotes the index of frames in I; i can be arbitrary numbers subject to the given constraints. By optimizing Eq. <ref type="formula" target="#formula_6">(7)</ref>, we obtain a full generated sequence G(ẑ) ∈ R T ×2J which is the "closest" match to the input frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-Step Optimization</head><p>In order to optimize Eq. (7), we employ a two-step method illustrated in <ref type="figure" target="#fig_5">Figure 8a</ref>.</p><p>To address the optimization of such highly non-convex latent space, we first randomly sample from the latent space and compare the loss of Eq. (7) to find the best initialization, namely z 0 .</p><p>As proposed in <ref type="bibr" target="#b44">[45]</ref>, taken the optimal initialization z 0 as the starting point, we apply Limited Broyden-Fletcher-Goldfarb-Shanno optimization (L-BFGS-B) <ref type="bibr" target="#b1">[2]</ref>, a quasinewton optimization algorithm well known for its optimality, on the (n + m)-dimension latent space in order to find the optimal completion result, namelyẑ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Blending</head><p>After generating G(ẑ) in pose space, the fully completed video can be simply obtained by stacking the input frames with the generated frames. However, slight shift and distortion in motion are observed as our method may not always produce perfect alignment with the input. To address this, we use Poisson blending <ref type="bibr" target="#b26">[27]</ref> to smooth our final pose sequence to make them more natural while satisfying the input constraints. The key idea is to maintain the gradients on the temporal direction of G(ẑ) to preserve motion smoothness while shifting the generated frames to match the input constraint. Our final solution,x, can be obtained bŷ</p><formula xml:id="formula_8">x = arg min x ∇ t x − ∇ t G(ẑ) 2 2 , s.t. x i = I i for i ∈ R t×2J<label>(10)</label></formula><p>where ∇ t is the gradient operator on the temporal dimension. The minimization problem contains a quadratic term, which has a unique solution <ref type="bibr" target="#b26">[27]</ref>. We will show qualitative results that the proposed blending preserves the naturalness of the videos while better aligning with the input frame constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Video Prediction</head><p>Video prediction can be solved under the same general framework as it can be essentially interpreted as video generation with first few frames as constraint. A figure of illustration is in <ref type="figure" target="#fig_5">Figure 8</ref>, where we exemplify how to perform future predictions by using the trained generative model G.</p><p>Formally, let I ∈ R t×2J be consecutive frames at time step 0 to t as input, we generate future frames G t , G t+1 , · · · G T so that I 0 , I 1 , · · · , I t , G t+1 , · · · G T form a natural and semantically meaningful video. To achieve such goal, we model video prediction as video generation with first few frames as constraint. In other words, we perform the same steps in 3.3.1 with the input described above, then we can obtain a completed video sequence where t + 1 to T frames are generated future frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We evaluate our model on Human3.6m dataset <ref type="bibr" target="#b12">[13]</ref>. This dataset consists of 896 human motion videos captured from 7 different subjects performing various classes of motion (e.g., walking, sitting and greeting, etc.). The videos were captured at high-resolution 1000×1000, at frame rate of 50 fps. The dataset provides reference ground truth 2D human poses (joint locations).</p><p>In our experiments, in order to reduce redundant frames and encourage larger motion variations, we subsample the video frames to 16 fps. The action classes we select are 'Direction', 'Greeting', 'Sitting', 'Sitting Down', 'Walking', all of which contain large human motions.</p><p>For our skeleton sequence generation task, we randomly select 5 subjects as training set and reserve 2 subjects as testing set. We normalize the ground truth 2D pose annotations so that the hip joints are all centered at the middle of the frames and the limb lengths are of the same scale. For our skeleton-to-image transformation task, we treat the unchosen action classes as training set, and our chosen 5 action classes as testing set. Human images are extracted from the original video frames using a 512×512 window, and then are resized into 128×128. Note that, since our major concern is human motion, we thus subtract all the backgrounds and generate the foreground human figure only. Background completion can be achieved by some traditional methods as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>Evaluating the quality of synthesized videos is a difficult problem in the case of video generation. Traditional methods such as per-pixel mean-squared error does not apply here as there can be multiple visually plausible solutions. Furthermore, the pixel based MSE does not measure the temporal smoothness and human-likeness which we aim to model in this paper.</p><p>In order to evaluate the visual quality of our results, we measure whether our generated videos are adequately realistic such that a pre-trained recognition network can recognize the object and action in the generated video. This method is inherently similar to the Inception Score in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30]</ref>, object detection evaluation in <ref type="bibr" target="#b38">[39]</ref> and Semantic Interpretability in <ref type="bibr" target="#b43">[44]</ref>. Two-stream model is first proposed by Yan et al <ref type="bibr" target="#b31">[32]</ref> in 2014 and further improved by Wang et al <ref type="bibr" target="#b37">[38]</ref>. Thus we fine-tune a state-of-the-art two-stream video action recognition <ref type="bibr" target="#b37">[38]</ref> network on our dataset. Then we evaluate the following two scores measuring the visual quality of generated image frames and video sequences respectively: Inception Score for frames One criterion regarding evaluating scores for video is that they should reflect if the video contains natural images along the sequence. Thus we calculate the inception score <ref type="bibr" target="#b29">[30]</ref> based on the output classification result of the RGB stream <ref type="bibr" target="#b37">[38]</ref> for each frame generated as the evaluation metric. The average score across the whole video should reflect the overall image quality. Additionally, we also show the Inception Score obtained at each time step, which gives us a detailed snapshot of how the quality of video vary over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inception Score for videos</head><p>As proposed in <ref type="bibr" target="#b36">[37]</ref>, we evaluate the inception score <ref type="bibr" target="#b29">[30]</ref> based on the fused classification results from our two-stream action classifier. By taking in to consideration the motion flow across the whole video, the output classes serve as an accurate indicator of the actions perceived in the video. Thus such score can give an overall quality of the full video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baselines</head><p>We present several baseline methods to provide comparisons of our results with results from previous methods.</p><p>For Video Generation, our baseline is Video-GAN (VGAN) <ref type="bibr" target="#b34">[35]</ref>. This approach trains a GAN that generates videos in pixel space. It is first successful attempt on video generation with deep learning methods.</p><p>For Video Prediction, the first baseline is PredNet, a predictive network <ref type="bibr" target="#b17">[18]</ref>. This approach is one of the latest results in video prediction. The second baseline is a Multi-Scale GAN (MS-GAN) in pixel space proposed by Mathieu et al <ref type="bibr" target="#b19">[20]</ref>. This approach has been quite successful in various video prediction tasks including both human action videos. The third baseline is PoseVAE, a sequential model proposed in <ref type="bibr" target="#b36">[37]</ref>, which utilized pose representation and have produced state-of-the-art results.</p><p>For Video Completion, our baseline is Conditional Video-GAN (cond-VGAN) <ref type="bibr" target="#b34">[35]</ref>. The model is capable of predicting next frames given input as shown in the paper, therefore we adopt it to video completion by changing its input to the first and last frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>For video generation, we generate videos from random noise vectors with dimensions consistent with the proposed models. For video prediction, we feed the first 4 frames as inputs, i.e. the baselines make prediction based on the input 4 frames, and our model generates videos with the first 4 frames as constraints. For video completion, we fix the the first and the last frames as constraints. In order to calculate the proposed metrics, we randomly generate 320 50-frame video samples for each method (except for the Video-GAN method <ref type="bibr" target="#b34">[35]</ref> which is fixed by architecture to generate only 32 frames). For prediction results, 4 preceding frames are fed as input. For completion results, the first and the last frames are fed as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Qualitative Results</head><p>In <ref type="figure">Figure 9</ref> we show the qualitative results of our model, in comparison with other state-of-the-art video generation and prediction methods. Since the results are videos, we strongly suggest readers to check our supplementary materials, which provide better visual comparisons than the sample frames shown in the paper. The baseline methods are all fine-tuned/re-trained on our Human3.6m dataset <ref type="bibr" target="#b12">[13]</ref>. We show generated results for each of our selected classes. Due to the page limit, we only show the beginning and the middle frames in the result videos.</p><p>By examining the results, we find that our model is capable of generating plausible human motion videos with high visual quality. By examining the image quality, we find that our model generates the most compelling human images, while other models tend to generate noisy (particularly Video-GAN) and blurry results due to their structural limitations. By examining the video sequences (provided in <ref type="figure">Figure 9</ref>. Qualitative comparisons. Each image-pair column corresponds to a generation method (the first column is real data), and columns are grouped together in the order of generation, prediction and completion, respectively. Each row corresponds to an action class, from top to bottom: Direction, Greeting, Sitting, Sitting Down, Walking. For each method we show the 10th and the 40th frames. For our method we also show the generated pose results. supplementary materials), we find that our model can generate natural and interpretable human motions. A key distinction here is that we are able to produce large-scale and detailed motion. For instance, the walking example shows clearly how the person moves his hands and legs in order to complete this action. Another important observation is that, our results maintain high quality over the entire time interval, while the others' quality (especially prediction models) tend to drop significantly after first few predictions. In the baseline prediction models, the human subjects are fading over time. <ref type="table" target="#tab_0">Table 1</ref> tabulates our quantitative evaluation results, "frame-IS" stands for Inception Score for frames, and "video-IS" stands for Inception Score for videos. While the ground truth (real) videos have the largest Inception Scores of both types, which coincides with our intuition, our generated videos have the highest scores among all the competing methods. This suggests that our model generates videos that have meaningful visual features in both image and video (spatial and temporal) domains closer to real videos, thus further indicating that our videos are more realistic. We also observe that other methods have much lower scores than ours, and VGAN and <ref type="bibr" target="#b19">[20]</ref> are even worse than PredNet. All the statistics are consistent with our qualitative results, which have demonstrated great discrepancy in visual perception. <ref type="figure" target="#fig_0">Figure 10</ref> shows a comparison of frame-by-frame Inception Score. We find that the ground truth videos maintain the highest scores at all time steps, and our results have considerably high scores closest to the ground truth quality. A more important observation is that, for the compared prediction models, PredNet <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b19">[20]</ref>, the scores tend to fall with time, indicating that the image quality is deteriorating over time in the long run. Although PoseVAE <ref type="bibr" target="#b36">[37]</ref> does not decline, its overall image quality is much lower than ours. This observation is consistent with our qualitative evaluation. An explanation might be the model's reliance on the input preceding frames, whose quality gradually drops since they are generated images. For completion baseline, the score tends to be higher at both ends, due to the restriction on the first and last frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We present a general generative model that addresses the problem of video generation, video completion and video prediction uniformly. By utilizing human pose as intermediate representation with our novel two-step generation strategy, we are able to generate large-scale human motion videos with longer duration from scratch. We are then able to solve the later two problems by constrained generation using our model. We find that our model is able to generated plausible human action videos both from scratch and under constraint, which surpasses current methods both quantitatively and visually.</p><p>Future directions include: in terms of improvement, more data can be incorporated in training both the motion generator and the skeleton-to-image transformer to increase the robustness and generality of our model. Multi-scale progressive training in second stage may also further increase the sharpness of the generated images. In terms of general video generation, it is possible to incorporate RNN to generate variable length videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>From top: video generation (from scratch), prediction and completion of human action videos using our general twostage deep framework. In all cases, a complete human pose skeleton sequence is generated in the first stage, shown in bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Step one: single pose (b) Step two: pose sequence Figure 4. Two-step generation architecture. Detailed architecture configuration of step one and step two are shown in (a) and (b) respectively. Here stands for element wise addition and stands for an LSTM cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Single Pose Generator (b) Pose Sequence Generator (for Walking class)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Results demonstration for our two-step generation model. (a) Generated results for Single Pose Generator for five action classes. (b) Generated results for Single Pose Generator. Here we only show several examples for the walking class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Illustration for skeleton-to-image training. From left to right: the input pose (encoded as heat maps), the input reference image, the corresponding ground truth and our results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Our completion/prediction pipeline. (a) Initialization: we randomly sample from the latent space and compare L1 error with the constraint frames. Dashed box shows the best initialization chosen. (b) We run randomized optimization algorithms starting at our initialization. We blend the constraints and the generated results as our final output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Generation, Completion and Prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Frame and Video Inception Score (IS)</figDesc><table><row><cell>Method</cell><cell>frame-IS</cell><cell>video-IS</cell></row><row><cell cols="3">Real VGAN[35] Ours PoseVAE[37] PredNet[18] MS-GAN[20] 1.48 ± 0.01 4.53 ± 0.01 4.63 ± 0.09 1.53 ± 0.04 1.40 ± 0.16 3.99 ± 0.02 3.99 ± 0.18 1.91 ± 0.01 2.17 ± 0.11 2.60 ± 0.04 2.94 ± 0.15 1.88 ± 0.10 Ours 3.87 ± 0.02 4.09 ± 0.15 cond-VGAN 2.35 ± 0.02 2.00 ± 0.06 Ours 3.91 ± 0.02 4.10 ± 0.07</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09405</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1705.00389</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video matting of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="248" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07404</idno>
		<title level="m">Learning visual predictive models of physics for playing billiards</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1704.00028</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video repairing under variable illumination using cyclic motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="832" to="839" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining markov random fields and convolutional neural networks for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2479" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09368</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep multiscale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1511.05440</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tyka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Google Research Blog. Retrieved</title>
		<imprint>
			<date type="published" when="2015-06-20" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01692</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Moving object removal and background completion in a video sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2003" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;14</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;14<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<idno>abs/1705.00053</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Space-time completion of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="476" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Skeleton-aided articulated motion generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01058</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
