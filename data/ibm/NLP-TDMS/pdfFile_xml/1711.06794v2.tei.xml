<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong Shanghai Key Laboratory of Trustworthy Computing</orgName>
								<orgName type="institution" key="instit2">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
							<email>jianyong@mail.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong Shanghai Key Laboratory of Trustworthy Computing</orgName>
								<orgName type="institution" key="instit2">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the Visual Question Answering (VQA) task has gained increasing attention in artificial intelligence. Existing VQA methods mainly adopt the visual attention mechanism to associate the input question with corresponding image regions for effective question answering. The freeform region based and the detection-based visual attention mechanisms are mostly investigated, with the former ones attending free-form image regions and the latter ones attending pre-specified detection-box regions. We argue that the two attention mechanisms are able to provide complementary information and should be effectively integrated to better solve the VQA problem. In this paper, we propose a novel deep neural network for VQA that integrates both attention mechanisms. Our proposed framework effectively fuses features from free-form image regions, detection boxes, and question representations via a multi-modal multiplicative feature embedding scheme to jointly attend question-related free-form image regions and detection boxes for more accurate question answering. The proposed method is extensively evaluated on two publicly available datasets, COCO-QA and VQA, and outperforms state-of-the-art approaches. Source code is available at https://github. com/lupantech/dual-mfa-vqa.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, multi-modal learning for language and vision has gained much attention in artificial intelligence. Great progress has been achieved for different tasks including image captioning <ref type="bibr" target="#b6">(Karpathy and Fei-Fei 2015)</ref>, visual question generation <ref type="bibr" target="#b17">(Mostafazadeh et al. 2016;</ref><ref type="bibr" target="#b12">Li et al. 2017b)</ref>, video question answering <ref type="bibr" target="#b27">(Ye et al. 2017</ref>) and text-to-image retrieval <ref type="bibr" target="#b24">(Xie, Shen, and Zhu 2016;</ref><ref type="bibr" target="#b11">Li et al. 2017a</ref>). The Visual Question Answering (VQA) <ref type="bibr" target="#b0">(Antol et al. 2015)</ref> task has recently emerged as a more challenging task. The algorithms are required to answer natural language questions about a given image's contents. Compared with the conventional visual-language tasks such as image captioning and text-to-image retrieval, the VQA task requires the algorithms to have a better understanding on both the input image and question in order to infer the answer. <ref type="figure">Figure 1</ref>: Co-attending free-form regions and detection boxes based on the question, the whole image, and detection boxes for better utilizing complementary information to solve the VQA task.</p><p>State-of-the-art VQA approaches utilize visual attention mechanism to relate the question to meaningful image regions for accurate question answering. Most visual attention mechanisms in VQA can be categorized into free-form region based methods <ref type="bibr" target="#b3">Fukui et al. 2016</ref>) and detection-based methods <ref type="bibr" target="#b10">(Li and Jia 2016;</ref><ref type="bibr" target="#b21">Shih, Singh, and Hoiem 2016)</ref>. For the free-form region based methods, the question features learned by a Long Short-Term Memory (LSTM) network and the image features learned by a Convolutional Neural Network (CNN) are fused by either addictive, multiplicative or concatenation operations at every image spatial location. The free-form attention map is obtained by applying a softmax non-linearity operation across the fused feature map. Since there is no restriction on the obtained attention map, the free-form attention region is able to attend both global visual context and specific foreground objects for inferring answers. However, since there is no restriction, the free-form attentive regions might focus on partial objects or irrelevant context sometimes. For instance, for an question, "What animal do you see?", a free-form region attention map might mistakenly focus on only a part of the foreground cat and generate an answer of "dog". On the other hand, for the detection-based attention methods, the attention mechanism is utilized for relating the question to pre-specified detection boxes (e.g., Edge boxes ). Instead of applying the softmax operation over all image spatial locations, the operation is calculated over all detection boxes. Therefore, the attended regions are restricted to pre-specified detection-box regions and such question-related regions could be more effective for answering questions about foreground objects. However, such restrictions also cause challenges for other types of questions. For instance, for the question "How is the weather today?", there might not exist a detection box in the sky, resulting in a failure in answering this question.</p><p>For better understanding the question and image contents and their relations, a good VQA algorithm needs to identify global scene attributes, locate objects, identify object attributes, quantity and categories to make accurate inference. We argue that the above mentioned two types of attended mechanisms provide complementary information and should be effectively integrated in a unified framework to take advantages of both the attended free-form regions and attended detection regions. Take the above mentioned two questions as examples, the question about animal could be more effectively answered with detection-based attention maps, while the question about the weather can be better answered with the free-form region based attention maps.</p><p>In this paper, we propose a novel dual-branch deep neural network for solving the VQA problem that combines free-form region based and detection-based attention mechanisms (see <ref type="figure">Figure 1</ref>). The overall framework consists of two attention branches, each of which associates the question with the most relevant free-form regions or with the most relevant detection regions in the input image. For obtaining more question-related attention weights for both types of regions, we propose to learn the joint feature representation of the input question, the whole image, and the detection boxes with a multiplicative feature embedding scheme. Such a multiplicative scheme does not share parameters between the two branches and is shown to result in more robust answering performance than existing methods.</p><p>The main contributions of our work can be summarized as twofold.</p><p>• We propose a novel dual-branch deep neural network that effectively integrates the free-form region based and detection-based attention mechanisms in a unified framework; • In order to better fuse features from different modalities, a novel multiplicative feature embedding scheme is proposed to learn joint feature representations from the question, the whole image, and the detection boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>State-of-the-art VQA algorithms are mainly based on deep neural networks for learning visual-question features and for predicting the final answers. Due to the establishment of the VQA dataset and the online evaluation platform by <ref type="bibr" target="#b0">(Antol et al. 2015)</ref>, there is an increasing number of VQA algorithms proposed every year.</p><p>Multi-modal feature embedding for VQA. The existing joint feature embedding methods for VQA typically combine the visual and question features learned by deep neural networks and then solve the task as a multi-class classification problem. <ref type="bibr" target="#b28">(Zhou et al. 2015)</ref> proposed a simple baseline, which learns image features with CNN and question features from LSTM, and concatenated these two features to predict the answer. Instead of using LSTM for learning question representations, <ref type="bibr" target="#b18">(Noh, Hongsuck Seo, and Han 2016)</ref> used GRU <ref type="bibr" target="#b2">(Cho et al. 2014)</ref> and <ref type="bibr" target="#b15">(Ma, Lu, and Li 2016)</ref> trained CNN for question embedding. Different from the above mentioned methods addressing the VQA task as a classification problem, the work by <ref type="bibr" target="#b16">(Malinowski, Rohrbach, and Fritz 2015)</ref> fed both image CNN features and question representations into an LSTM to generate the answer by sequence-tosequence learning. There are also high-order approaches for multi-modal feature embedding. MCB <ref type="bibr" target="#b3">(Fukui et al. 2016)</ref> and MLB <ref type="bibr" target="#b8">(Kim et al. 2017</ref>) both designed bilinear pooling approaches to learn multi-modal feature embedding for VQA. In <ref type="bibr" target="#b1">(Ben-Younes et al. 2017</ref>), a generalized multimodal pooling framework is proposed, which shows that MCB and MLB are its special cases. Inspired by ResNet , <ref type="bibr" target="#b7">(Kim et al. 2016)</ref> proposed element-wise multiplication for the joint residual mappings.</p><p>Attention mechanism for VQA. A large quantity of recent works focused on incorporating attention mechanisms for solving the VQA task. In (Xu and Saenko 2016), the attention weights over different image regions are calculated based on the semantic similarity between the question and image regions, and the updated question features are obtained as the weighted sum of the different regions. ) proposed a multi-stage attention framework, which stacks the attention modules to search question-related image regions by iterative feature fusion. The attention mechanism is not limited to image modality, ) proposed a co-attention mechanism that simultaneously attends both the question and image with joint visual-question feature representations.</p><p>Since some of the questions are related to objects in the images, object detection results are explored to replace the visual features obtained from the whole-image region. <ref type="bibr" target="#b21">(Shih, Singh, and Hoiem 2016)</ref> utilized the attention mechanism to generate visual features from 100 detection boxes for the VQA task. Similarly, <ref type="bibr" target="#b10">(Li and Jia 2016)</ref> proposed a framework that updates the joint visual-question feature embedding by iteratively attending the top detection boxes.</p><p>However, all the attention-based methods focus on one type of image regions for question-image association (i.e., either free-form image regions or detection boxes), and each of them have limitations on solving certain types of questions. In contrast, in order to better utilize the complementary information from both types of image regions, our proposed approach effectively integrates both attention mechanisms in a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The overall structure of our proposed deep neural network is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, which takes the question, the whole image, and the detection boxes as inputs, and learns to associate questions to free-form image regions and detection boxes simultaneously to infer the answer. Our proposed model for VQA consists of two co-attention branches for  free-form image regions and for detection boxes, respectively. Before calculating the attention weights for each type of regions, the joint feature representations of the question, the whole-image visual features, and the detection box visual features are obtained via a multiplicative approach. In this way, the attention weights for each type of regions are learned from all the three inputs. Importantly, the joint multimodal feature embedding has different parameters for each branch, which leads to better answering accuracy. The resulting visual features from the two branches are fused with the question representation and then added to obtain the final question-image embedding. A multi-class linear classifier is adopted for obtaining the final predicted answer.</p><p>The input feature encoding will be introduced in Section 3.1. Section 3.2 introduces the branch with visual attention mechanism for associating free-form image regions to the input questions with our multiplicative feature embedding scheme, and Section 3.3 describes the another branch with detection based attention mechanism. The final answer prediction will be introduced in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input feature encoding</head><p>Encoding whole-image features. We utilize an ImageNet pre-trained ResNet-152  for learning image visual features. The input image to ResNet is resized to 448 × 448 and the 2048 × 14 × 14 output feature map of the last convolution layer is used to encode the whole-image visual features R ∈ R 2048×14×14 , where 14 × 14 is its spatial size and 2048 is the number of visual feature channels.</p><p>Encoding detection-box features. We adopt the Faster-RCNN  framework to obtain object detection boxes in the input image. For all the object proposals and their associated detection scores generated by Faster-RCNN, the non-maximum suppression with an intersection over union (IoU) 0.3 is applied and the top-ranked 19 detection boxes are chosen as the detection-box features for our overall framework. The 4096-dimensional visual features of Faster-RCNN's fc7 layer concatenated with boxes' detection scores are utilized to encode the visual feature of each</p><formula xml:id="formula_0">detection box. Let D = [f 1 , · · · , f 19 ] ∈ R 4097×19 denote the visual features of the top-ranked 19 detection boxes.</formula><p>Encoding question features. The Gated Recurrent Unit (GRU) <ref type="bibr" target="#b2">(Cho et al. 2014</ref>) is adopted to encode the question features, which show its effectiveness in recent VQA methods <ref type="bibr" target="#b8">Kim et al. 2017)</ref>. A GRU cell consists of an update gate z and a reset gate r. Given a question q = [q 1 , ..., q T ], where q t is the one-hot vector of at position t, and T is the length of the question. We convert each word q t into a feature vector via a linear transformation x t = W e q t . At each time step, the word feature vector q t is sequentially fed into the GRU to encode the input question. At each step, the GRU updates the update gate z t and reset gate r t and outputs a hidden state h t . The GRU update process operates as</p><formula xml:id="formula_1">z t = σ(W z x t + U z h t−1 + b z ), (1) r t = σ(W r x t + U r h t−1 + b r ),<label>(2)</label></formula><formula xml:id="formula_2">h t = tanh(W h x t + U h (r t • h t−1 ) + b h ),<label>(3)</label></formula><formula xml:id="formula_3">h t = z t • h t−1 + (1 − z t ) •h t ,<label>(4)</label></formula><p>where σ represents the sigmoid activation function. The weight matrices W , U and bias vector b are learnable parameters of the GRU.</p><p>We take the final hidden state h T as the question embedding, i.e., Q = h T ∈ R k , where k denotes the embedding length. Following <ref type="bibr" target="#b10">(Li and Jia 2016;</ref><ref type="bibr" target="#b8">Kim et al. 2017)</ref>, we utilize the pre-trained skip-thought model  to initialize the embedding matrix W e in our question language model. Since the skip-thought model is previously trained on large text corpus, we are able to transfer external languagebased knowledge to our VQA task by fine-tuning the GRU from the initial point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning to attend free-form region visual features with multiplicative embedding</head><p>Our proposed framework has two branches, one for attending question-related free-form image regions to learn wholeimage features and the other for attending detection-box regions to learn question-related detection features. For each attention branch, it takes question feature Q, whole-image feature R and detection-box feature D as inputs, and outputs the question-attended visual features v 1 and v 2 for question answering.</p><p>Our free-form region attention branch tries to associate the input question to relevant regions of the input image. There is no restriction to the attended regions, which are of free-form and are able to capture the global visual context and attributes of the image. Unlike existing VQA methods that fuse question and image features via simple concatenation <ref type="bibr" target="#b21">(Shih, Singh, and Hoiem 2016)</ref> or addition  to guide the calculation of the attention weights, each of our attention branch fuses all three types of input features via a multiplicative feature embedding scheme for utilizing full information from the inputs. The network structure for attending visual features with our multiplicative feature embedding scheme is show in <ref type="figure" target="#fig_1">Figure 3(a)</ref>.</p><p>Given the question embedding Q ∈ R k , the whole-image representation R ∈ R 2048×14×14 , and the detection representation D ∈ R 4097×19 , we first embed them to a 1200dimensional common space via the following equations,</p><formula xml:id="formula_4">R 1 = tanh(W r1 R + b r1 ),<label>(5)</label></formula><formula xml:id="formula_5">D 1 = 1 19 · 1(tanh(W d1 D + b d1 ) T ),<label>(6)</label></formula><formula xml:id="formula_6">Q 1 = tanh(W q1 Q + b q1 ),<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">W r1 ∈ R 1200×2408 , W d1 ∈ R 1200×4097 , W q1 ∈ R 1200×k are the learnable weight parameters, b r1 , b d1 , b q1</formula><p>∈ R 1200 are the bias parameters, 1 ∈ R 19 represents an all-1 vector, and tanh is the hyperbolic tangent function. For learning the attention weights for the whole-image feature R, the transformed detection features are averaged across all detection boxes following Eq. (6) before feature fusion. After mapping all input features into the 1200-dimensional common space, the detection feature D 1 ∈ R 1200 and question feature Q 1 ∈ R 1200 , are spatially replicated to a 14×14 grid to formD 1 andR 1 , which match the spatial size of the whole-image feature R 1 ∈ R 1200×14×14 . The joint feature representation C 1 of the three inputs is obtained by element-wise multiplication (Hadamard product) ofQ 1 , R 1 andD 1 , and followed by a L 2 normalization to constrain the magnitude of the representation,</p><formula xml:id="formula_8">C 1 = Norm 2 (Q 1 • R 1 •D 1 ),<label>(8)</label></formula><p>where • indicates element-wise multiplication. The freeform attention map is then obtained by convolving the joint feature representation C 1 with a 1 × 1 convolution followed by a softmax operation over the 14 × 14 grid,</p><formula xml:id="formula_9">a 1 = softmax(W c1 * C 1 + b c1 ),<label>(9)</label></formula><p>where W c1 ∈ R 1200×1×1 and b c1 ∈ R 1200 are the learnable convolution kernel parameters. The attended whole-image feature over all spatial locations can be calculated by</p><formula xml:id="formula_10">v 1 = 14×14 i a 1 (i)R 1 (i),<label>(10)</label></formula><p>which can represent the attended whole-image visual feature that are most related to the input question.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning to attend detection-box visual features with multiplicative embedding</head><p>Our second branch focuses on learning question-related detection features by attending the detection-box features with joint input feature representation. Similar to the first branch, we fuse the question representation, the whole-image features, and the detection-box features for learning the attention weights for detection boxes. Unlike previous detection based attention methods <ref type="bibr" target="#b21">(Shih, Singh, and Hoiem 2016;</ref><ref type="bibr" target="#b10">Li and Jia 2016)</ref>, our proposed attention mechanism integrates whole-image features for better understanding the overall image contents. The structure for learning the joint feature representation is shown in <ref type="figure" target="#fig_1">Figure 3(b)</ref>. Similar to the joint representation for free-form image region attention, given the question embedding Q ∈ R k , the image representation R ∈ R 2048×14×14 , the detection representation D ∈ R 4097×19 , we transform each representation to a common semantic space, and then obtain the 1200dimensional joint feature representation C 2 as</p><formula xml:id="formula_11">D 2 = tanh(W d2 D + b d2 ),<label>(11)</label></formula><formula xml:id="formula_12">R 2 = 1 196 · 1(tanh(W r2 R + b r2 ) T ),<label>(12)</label></formula><formula xml:id="formula_13">Q 2 = tanh(W q2 Q + b q2 ),<label>(13)</label></formula><formula xml:id="formula_14">C 2 = Norm 2 (Q 2 •R 2 • D 2 ),<label>(14)</label></formula><p>where W d2 , W r2 , W q2 , b d1 , b r1 , b q2 are the learnable parameters for linear transformation. The transformed question feature Q 2 and whole-image feature R 2 are replicated across the number dimension to match the dimension of the transformed detection features D 2 ∈ R 1200×19 before calculating the joint representation C 2 following Eq. <ref type="formula" target="#formula_3">(14)</ref>. The final attended detection representation v 2 over all de-tection boxes is defined as</p><formula xml:id="formula_15">a 2 = softmax(W c2 C 2 + b c2 ),<label>(15)</label></formula><formula xml:id="formula_16">v 2 = 19 i a 2 (i)D 2 (i),<label>(16)</label></formula><p>where W c2 ∈ R 19×1200 and b c2 ∈ R 19 are the learnable parameters of learning the attention weights a 2 for the detection boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning for answer prediction</head><p>Similar to existing VQA approaches <ref type="bibr" target="#b0">(Antol et al. 2015;</ref><ref type="bibr" target="#b3">Fukui et al. 2016</ref>), we model VQA as a multi-class classification problem. Given the attended whole-image feature v 1 , attended detection feature v 2 with the input question feature Q, question-image joint encodings are obtained by element-wise multiplication of the transformed question features and the attended features from both branch,</p><formula xml:id="formula_17">h r = v 1 • tanh(W hr Q + b hr ), (17) h d = v 2 • tanh(W hd Q + b hd ),<label>(18)</label></formula><p>where W hr , W hd , b hr , b hd are the learnable parameters for transforming the input question feature Q. The reason why we choose different question transformation parameters for the two branches is that the attended visual features from the two different branches captures different information from the input image. One is from attended free-form region features and is able to capture the global context and attributes of the scene. The another is from attended detection features and is able to extract information about foreground objects. After merging question-image encodings from the two branches via addition, a linear classifier is trained for final answer prediction,</p><formula xml:id="formula_18">p ans = softmax(W p (h r + h d ) + b p ),<label>(19)</label></formula><p>where W p and b p are the classifier parameters, and p ans represents the probability of the final answer prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and evaluation metrics</head><p>We evaluate the proposed model on two public datasets, the VQA <ref type="bibr" target="#b0">(Antol et al. 2015)</ref> and COCO-QA <ref type="bibr" target="#b20">(Ren, Kiros, and Zemel 2015)</ref> datasets. VQA dataset <ref type="bibr" target="#b0">(Antol et al. 2015)</ref> is based on Microsoft COCO image data <ref type="bibr" target="#b13">(Lin et al. 2014)</ref>. The dataset consists of 248,349 training questions, 121,512 validation questions and 244,302 testing questions, generated on a total of 123,287 images. There are three types of questions including yes/no, number and other. For each question, 10 freeresponse answers are provided. We take the top 2000 most frequent answers as the candidate outputs for learning the classification model similar to <ref type="bibr" target="#b7">(Kim et al. 2016)</ref>, which covers 90.45% answers in the training and validation sets.</p><p>COCO-QA dataset <ref type="bibr" target="#b20">(Ren, Kiros, and Zemel 2015)</ref> is created based on the caption annotations of Microsoft COCO dataset <ref type="bibr" target="#b13">(Lin et al. 2014</ref>). There are 78,736 training samples and 38,948 testing samples, respectively. It contains four types of questions, object, number, color and location. All of the answers are single-words and considered as the valid answers, namely 430 answers, which are used for the possible answer classification.</p><p>Since we formulate VQA as a classification task, we use the accuracy metric to measure the performances of different models on both two datasets. In particular, for the VQA dataset, a predicted answer is regarded as correct if it matches more than three in ten ground truth answers. WUPS calculates the similarity between two words based on their common subsequence in the taxonomy tree. In addition, Wu-Palmer similarity (WUPS) <ref type="bibr" target="#b23">(Wu and Palmer 1994)</ref> is also reported for the COCO-QA dataset. Same as the previous work <ref type="bibr" target="#b10">Li and Jia 2016)</ref>, we report the WUPS scores with the thresholds of 0.9 and 0.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>For encoding questions, the length of questions is fixed to 26 and each word embedding is a vector of size 620. The hidden state of GRU is set to 2400. Given the question, image and detection representations, the joint feature embedding of these inputs h is set as 1200. Following <ref type="bibr" target="#b8">(Kim et al. 2017)</ref>, we take two glimpses of each attention map, that is to say, for each branch, another set of attention weights a 1 is trained for whole-image features R 1 in the first branch and another set of a 2 for detection features D 2 in the second branch. The two sets of attended features are concatenated in each branch as the final feature.</p><p>We implement our model with the Torch library. The RM-SProp method is used for training our network with an initial learning rate of 3×10 −4 , a momentum of 0.99 and a weightdecay of 10 −8 . The batch size is set to 300, and is trained for 250,000 iterations. The validation process is performed every 10,000 iterations with early stopping when validation accuracy stops improving for more than 5 validations. Dropout is applied after every linear transformation and gradient clipping techniques are used for regularization in GRU training. Multi-GPU parallel technology is adopted to accelerate the training process. <ref type="table" target="#tab_4">Table 1</ref> shows results on the VQA test set for both openended and multiple-choice tasks by our proposed approach and compared methods. The approaches shown in <ref type="table" target="#tab_4">Table 1</ref> are trained on the train+val split of the VQA dataset and evaluated on the test split, where the test-dev set is normally used for validation and the test-std set for standard testing. The models in the first part of <ref type="table" target="#tab_4">Table 1</ref> are based on simple joint question-image feature embedding methods. Models in the second part of <ref type="table" target="#tab_4">Table 1</ref> employ detection-based attention mechanism and models in the third part use freeform region-based attention mechanism. We only compare results of the single models since most approaches do not adopt the model ensemble strategy. We can see that our final model (denoted as Dual-MFA, where MFA stands for Multiplicative Feature Attention) improves the state-of-theart MLB approach <ref type="bibr" target="#b8">(Kim et al. 2017</ref>) from 65.07% to 66.09% for the open-ended task, and from 68.89% to 69.97% for the multiple-choice task on the test-std set. Specifically, in the  <ref type="bibr" target="#b18">(Noh, Hongsuck Seo, and</ref><ref type="bibr">Han 2016) 57.22 80.71 37.24 41.69 62.48 57.36 80.28 36.92 42.24 62.69 FDA (Ilievski, Yan, and</ref><ref type="bibr" target="#b5">Feng 2016)</ref> 59  question types of number and other, our proposed approach brings 2.49% and 2.12% improvements on the test-std set. QRU <ref type="bibr" target="#b10">(Li and Jia 2016)</ref> is the state-of-the-art detection-based attention method, and our model significantly outperforms it by 5.33% on the test-std set. We also compare our approach with a baseline model Dual-MLB, which consists of two attention branches based on the MLB tensor fusion module. The baseline model fuses the question and free-form image region embedding by MLB in one branch, and fuses question and image detection embedding by MLB in another branch. The baseline Dual-MLB also outperforms MLB by 0.23% on the test-dev set, which shows that our improvements are not only from the integration of the two attention mechanisms, but are also caused by effective joint feature embedding of question, image, and detection features. <ref type="table" target="#tab_7">Table 3</ref> compares our approach with the state-of-the-art approaches on the COCO-QA test set. Our final model Dual-MFA improves the state-of-the-art HieCoAtt  from 65.40% to 66.49%. In particular, our model achieves an improvement of 2.99% for the question type color. Similar to the results on the VQA dataset, our model significantly outperforms the state-of-the-art detection-based attention method QRU by 3.49%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-arts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>In this section, we conduct ablation experiments to study effectiveness of individual component designs in our model. <ref type="table">Table 2</ref> shows the results of baseline models replacing different components in our model, which are trained on the VQA training set and tested on the validation set. Following other compared approaches, the test set is not used in the study due to online submission restrictions. Specifically, we compare different multi-modal feature embedding designs and investigate the roles of the two attention mechanisms.  <ref type="table">Table 2</ref>: Ablation study on the VQA dataset, where "*" denotes our model's design.</p><p>The first part of <ref type="table">Table 2</ref> shows different element-wise operations used for joint embedding of three input features. Element-wise multiplication (denoted as MFA-MUL) in Eq. (8) performs better than element-wise addition (denoted as MFA-ADD) by 3.67%. The second part of <ref type="table">Table 2</ref> shows L 2 normalization (denoted as MFA-Norm) in joint feature embedding works better than the model with unsigned power operation of 1/3 (denoted as <ref type="bibr">MFA-Power)</ref>    mance than element-wise multiplication (denoted as Dual-MFA-MUL) and concatenation of two vectors (denoted as Dual-MFA-CAT). The third and fourth parts of <ref type="table">Table 2</ref> compare our multimodal multiplicative feature embedding for a single attention mechanism (MFA-R or MFA-D) with detection-based attention baseline models (QRU-D and MLB-D) and regionbased attention baseline models (MUTAN and MLB-R), which replace our multiplicative feature embedding with QRU <ref type="bibr" target="#b10">(Li and Jia 2016)</ref> or MLB <ref type="bibr" target="#b8">(Kim et al. 2017</ref>) respectively. The MUTAN (Ben-Younes et al. 2017) approach is also used for comparison. Results show that our models with a single attention branch outperform all compared detection-based and region-based baselines. Finally, we compare our final model Dual-MFA with the baseline model Dual-MLB. Our Dual-MFA model benefits from its effective multi-modal multiplicative feature embedding scheme and achieves an improvement of 0.75%.</p><p>The parameter size of our final full model is 82.6M, while the best region-based model (MFA-R) is 61.7M, and the best detection-based model (MFA-D) is 56.8M. As the parameters in the language model and the classifier are shared by two attention branches, parameters of our full model increase in a minor degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative evaluation</head><p>We visualize some co-attention maps generated by our model in <ref type="figure" target="#fig_2">Figure 4</ref> and present five examples from the VQA test set. <ref type="figure" target="#fig_2">Figures 4(a)</ref> and (b) show that our model attends to the corresponding image regions with two attention branches, which leads to correct answers with higher confidence. There are also cases where only one attention branch is able to attend the correct image region to obtain the correct answer. In <ref type="figure" target="#fig_2">Figure 4(c)</ref>, the free-form region based attention map is able to attend the blue sky and dry ground, while the detection-based attention map fails as there are no pre-given detection boxes covering these image regions. In <ref type="figure" target="#fig_2">Figure 4(d)</ref>, the detection-based attention map has higher weights on all five giraffes to generate the correct answer, while the free-form attention map attends incorrect regions. A failure case is also presented in <ref type="figure" target="#fig_2">Figure 4</ref>(e). The model fails to generate the correct answer as the classification label "no turning right" does not exit in the training set despite attending the correct image region.</p><p>In this paper, we propose a novel deep neural network with co-attention mechanism for visual question answering. The deep model contains two branches for visual attention which aim to select the free-form image regions and detection boxes most related to the input question. We generate visual attention weights by a novel multiplicative embedding scheme, which fuses the question, whole-image and detection-box features effectively. Ablation study demonstrates the effectiveness of individual components of our proposed model. Experimental results on two large VQA datasets show that our proposed model outperforms stateof-the-art approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the overall network structure for solving the VQA task. The network has two attention branches with the proposed multiplicative featucre embedding scheme, where one branch attends free-form image regions and another branch attends detection boxes for encoding question-related visual features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Learning to attend visual features with multimodal multiplicative feature embedding for (a) free-form image regions and for (b) detection boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization examples on the VQA test-dev set. (First row) input images. (Second row) free-form region based attention maps. (Third row) detection-based attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Antol et al. 2015) 53.74 78.94 35.24 36.42 57.17 54.06 79.01 35.55 36.80 57.57 iBOWING (Zhou et al. 2015) 55.72 76.55 35.03 42.62 61.68 55.89 76.76 34.98 42.62 61.97 DPPnet</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Test-dev</cell><cell></cell><cell></cell><cell></cell><cell>Test-std</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Open-Ended</cell><cell>MC</cell><cell></cell><cell cols="2">Open-Ended</cell><cell>MC</cell></row><row><cell>Method</cell><cell>All</cell><cell>Y/N</cell><cell>Num. Other</cell><cell>All</cell><cell>All</cell><cell>Y/N</cell><cell>Num. Other</cell><cell>All</cell></row><row><cell>LSTM Q+I (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results by our proposed method and compared methods on the VQA dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>and the model without normalization (denoted as MFA w/o Norm). For fusing outputs h r and h d from the two attention branches, element-wise addition in Eq. (19) achieves better perfor-</figDesc><table><row><cell>Method</cell><cell>All</cell><cell>Obj.</cell><cell cols="2">Num. Color</cell><cell>Loc.</cell><cell cols="2">WUPS0.9 WUPS0.0</cell></row><row><cell cols="6">2VIS+BLSTM (Ren, Kiros, and Zemel 2015) 55.09 58.17 44.79 49.53 47.34</cell><cell>65.34</cell><cell>88.64</cell></row><row><cell>IMG-CNN (Ma, Lu, and Li 2016)</cell><cell>58.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.50</cell><cell>89.67</cell></row><row><cell cols="2">DDPnet (Noh, Hongsuck Seo, and Han 2016) 61.16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.84</cell><cell>90.61</cell></row><row><cell>SAN (Yang et al. 2016)</cell><cell cols="5">61.60 65.40 48.60 57.90 54.00</cell><cell>71.60</cell><cell>90.90</cell></row><row><cell>QRU (Li and Jia 2016)</cell><cell cols="5">62.50 65.06 46.90 60.50 56.99</cell><cell>72.58</cell><cell>91.62</cell></row><row><cell>HieCoAtt (Lu et al. 2016)</cell><cell cols="5">65.40 68.00 51.00 62.90 58.80</cell><cell>75.10</cell><cell>92.00</cell></row><row><cell>Dual-MFA (ours)</cell><cell cols="5">66.49 68.86 51.32 65.89 58.92</cell><cell>76.15</cell><cell>92.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results by our proposed method and compared methods on the COCO QA dataset.</figDesc><table><row><cell>Q: What sport is this?</cell><cell>Q: What is the color of the surfboard?</cell><cell>Q: Is it a sunny day?</cell><cell>Q: How many giraffes are there?</cell><cell>Q: What does the red circle sign mean?</cell></row><row><cell>A: tennis</cell><cell>A: white</cell><cell>A: yes</cell><cell>A: 5</cell><cell>A: no parking</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by National Natural Science Foundation of China under Grant No. 61532010  and No. 61702190 </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<title level="m">Vqa: Visual question answering. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual question answering with question representation update (qru)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07192</idno>
		<title level="m">Visual question generation as dual task of visual question answering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to answer questions from image using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating natural questions about an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The vqa-machine: Learning how to use existing vision algorithms to answer new questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Online cross-modal hashing for web image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>ICML. Xu, H.,</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Dynamic memory networks for visual and textual question answering</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video question answering via attribute-augmented attention network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<title level="m">Simple baseline for visual question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
