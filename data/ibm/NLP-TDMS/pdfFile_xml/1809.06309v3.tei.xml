<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Commonsense for Generative Multi-Hop Question Answering Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Bauer</surname></persName>
							<email>lbauer6@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Mohit Bansal UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Wang</surname></persName>
							<email>yicheng@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Mohit Bansal UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Commonsense for Generative Multi-Hop Question Answering Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Equal contribution (published at EMNLP 2018). We publicly release all our code, models, and data at: https://github.com/yicheng-w/CommonSenseMultiHopQA</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multihop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-theart span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model's performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show promising initial results of the generalizability of our background knowledge enhancements by demonstrating some improvement on QAngaroo-WikiHop, another multihop reasoning dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we explore the task of machine reading comprehension (MRC) based QA. This task tests a model's natural language understanding capabilities by asking it to answer a question based on a passage of relevant content. Much progress has been made in reasoning-based MRC-QA on the bAbI dataset <ref type="bibr">(Weston et al., 2016)</ref>, which contains questions that require the combination of multiple disjoint pieces of evidence in the context. However, due to its synthetic nature, bAbI evidences have smaller lexicons and simpler passage structures when compared to humangenerated text.</p><p>There also have been several attempts at the MRC-QA task on human-generated text. Large scale datasets such as CNN/DM <ref type="bibr">(Hermann et al., 2015)</ref> and <ref type="bibr">SQuAD (Rajpurkar et al., 2016)</ref> have made the training of end-to-end neural models possible. However, these datasets are fact-based and do not place heavy emphasis on multi-hop reasoning capabilities. More recent datasets such as QAngaroo <ref type="bibr">(Welbl et al., 2018)</ref> have prompted a strong focus on multi-hop reasoning in very long texts. However, QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context; hence, this is more focused on fact finding and linking, and does not require models to synthesize and generate new information.</p><p>We focus on the recently published Narra-tiveQA generative dataset <ref type="bibr">(Kočiskỳ et al., 2018)</ref> that contains questions requiring multi-hop reasoning for long, complex stories and other narratives, which requires the model to go beyond fact linking and to synthesize non-span answers. Hence, models that perform well on previous reasoning tasks <ref type="bibr">(Dhingra et al., 2018)</ref> have had limited success on this dataset. In this paper, we first propose the Multi-Hop Pointer-Generator Model (MHPGM), a strong baseline model that uses multiple hops of bidirectional attention, self-attention, and a pointer-generator decoder to effectively read and reason within a long passage and synthesize a coherent response. Our model achieves 41.49 Rouge-L and 17.33 METEOR on the summary subtask of NarrativeQA, substantially better than the performance of previous generative models.</p><p>Next, to address the issue that understanding human-generated text and performing longdistance reasoning on it often involves intermittent access to missing hops of external commonsense (background) knowledge, we present an algorithm for selecting useful, grounded multi-hop relational knowledge paths from ConceptNet <ref type="bibr">(Speer and Havasi, 2012</ref>) via a pointwise mutual information (PMI) and term-frequency-based scoring function. We then present a novel method of inserting these selected commonsense paths between the hops of document-context reasoning within our model, via the Necessary and Optional Information Cell (NOIC), which employs a selectivelygated attention mechanism that utilizes commonsense information to effectively fill in gaps of inference. With these additions, we further improve performance on the NarrativeQA dataset, achieving 44.16 Rouge-L and 19.03 METEOR (also verified via human evaluation). We also provide manual analysis on the effectiveness of our commonsense selection algorithm.</p><p>Finally, to show the generalizability of our multi-hop reasoning and commonsense methods, we show some promising initial results via the addition of commonsense information over the baseline on <ref type="bibr">QAngaroo-WikiHop (Welbl et al., 2018)</ref>, an extractive dataset for multi-hop reasoning from a different domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Machine Reading Comprehension: MRC has long been a task used to assess a model's ability to understand and reason about language. Large scale datasets such as CNN/Daily Mail <ref type="bibr">(Hermann et al., 2015)</ref> and <ref type="bibr">SQuAD (Rajpurkar et al., 2016)</ref> have encouraged the development of many advanced, high performing attention-based neural models <ref type="bibr">(Seo et al., 2017;</ref><ref type="bibr">Dhingra et al., 2017)</ref>. Concurrently, datasets such as bAbI <ref type="bibr">(Weston et al., 2016)</ref> have focused specifically on multi-step reasoning by requiring the model to reason with disjoint pieces of information. On this task, it has been shown that iteratively updating the query representation with information from the context can effectively emulate multi-step reasoning <ref type="bibr">(Sukhbaatar et al., 2015)</ref>.</p><p>More recently, there has been an increase in multi-paragraph, multi-hop inference QA datasets such as QAngaroo <ref type="bibr">(Welbl et al., 2018)</ref> and <ref type="bibr">Narra-tiveQA (Kočiskỳ et al., 2018)</ref>. These datasets have much longer contexts than previous datasets, and answering a question often requires the synthesis of multiple discontiguous pieces of evidence. It has been shown that models designed for previous tasks <ref type="bibr">(Seo et al., 2017;</ref><ref type="bibr">Kadlec et al., 2016)</ref> have limited success on these new datasets. In our work, we expand upon Gated Attention <ref type="bibr">Network (Dhingra et al., 2017)</ref> to create a baseline model better suited for complex MRC datasets such as NarrativeQA by improving its attention and gating mechanisms, expanding its generation capabilities, and allowing access to external commonsense for connecting implicit relations.</p><p>Commonsense/Background Knowledge: Commonsense or background knowledge has been used for several tasks including opinion mining <ref type="bibr">(Cambria et al., 2010)</ref>, sentiment analysis <ref type="bibr">(Poria et al., 2015</ref><ref type="bibr">(Poria et al., , 2016</ref>, handwritten text recognition <ref type="bibr">(Wang et al., 2013)</ref>, and more recently, dialogue <ref type="bibr">(Young et al., 2018;</ref><ref type="bibr">Ghazvininejad et al., 2018)</ref>. These approaches add commonsense knowledge as relation triples or features from external databases. Recently, largescale graphical commonsense databases such as ConceptNet (Speer and Havasi, 2012) use graphical structure to express intricate relations between concepts, but effective goal-oriented graph traversal has not been extensively used in previous commonsense incorporation efforts. Knowledgebase QA is a task in which systems are asked to find answers to questions by traversing knowledge graphs <ref type="bibr">(Bollacker et al., 2008)</ref>. Knowledge path extraction has been shown to be effective at the task <ref type="bibr">(Bordes et al., 2014;</ref><ref type="bibr">Bao et al., 2016)</ref>. We apply these techniques to MRC-QA by using them to extract useful commonsense knowledge paths that fully utilize the graphical nature of databases such as ConceptNet <ref type="bibr">(Speer and Havasi, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporation of External Knowledge:</head><p>There have been several attempts at using external knowledge to boost model performance on a variety of tasks: Chen et al. <ref type="bibr">(2018)</ref> showed that adding lexical information from semantic databases such as WordNet improves performance on NLI; Xu et al. (2017) used a gated recall-LSTM mechanism to incorporate commonsense information into token representations in dialogue.</p><p>In <ref type="bibr">MRC, Weissenborn et al. (2017)</ref> integrated external background knowledge into an NLU model by using contextually-refined word embeddings which integrated information from Con-ceptNet (single-hop relations mapped to unstructured text) via a single layer bidirectional LSTM. Concurrently to our work, <ref type="bibr">Mihaylov and Frank (2018)</ref> showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention, where commonsense relations were extracted as triples. This work represented commonsense relations as keyvalue pairs and combined context representation and commonsense via a static gate.</p><p>Differing from previous works, we employ multi-hop commonsense paths (multiple connected edges within ConceptNet graph that give us information beyond a single relationship triple) to help with our MRC model. Moreover, we use this in tandem with our multi-hop reasoning architecture to incorporate different aspects of the commonsense relationship path at each hop, in order to bridge different inference gaps in the multi-hop QA task. Additionally, our model performs synthesis with its external, background knowledge as it generates, rather than extracts, its answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Hop Pointer-Generator Baseline</head><p>We first rigorously state the problem of generative QA as follows: given two sequences of input tokens: the context, X C = {w C 1 , w C 2 , . . . , w C n } and the query, X Q = {w Q 1 , w Q 2 , . . . , w Q m }, the system should generate a series of answer tokens X a = {w a 1 , w a 2 , . . . , w a p }. As outlined in previous sections, an effective generative QA model needs to be able to perform several hops of reasoning over long and complex passages. It would also need to be able to generate coherent statements to answer complex questions while having the ability to copy rare words such as specific entities from the reading context. With these in mind, we propose the Multi-Hop Pointer-Generator Model (MHPGM) baseline, a novel combination of previous works with the following major components:</p><p>• Embedding Layer: The tokens are embedded into both learned word embeddings and pretrained context-aware embeddings (ELMo <ref type="bibr">(Peters et al., 2018)</ref>). • Reasoning Layer: The embedded context is then passed through k reasoning cells, each of which iteratively updates the context representation with information from the query via BiDAF attention <ref type="bibr">(Seo et al., 2017)</ref>, emulating a single reasoning step within the multi-step reasoning process. • Self-Attention Layer: The context representation is passed through a layer of self-attention <ref type="bibr">(Cheng et al., 2016)</ref> to resolve long-term dependencies and co-reference within the context. • Pointer-Generator Decoding Layer:</p><p>A attention-pointer-generator decoder <ref type="bibr">(See et al., 2017)</ref> that attends on and potentially copies from the context is used to create the answer.</p><p>The overall model is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, and the layers are described in further detail below. Embedding layer: We embed each word from the context and question with a learned embedding space of dimension d. We also obtain contextaware embeddings for each word via the pretrained embedding from language models (ELMo) (1024 dimensions). The embedded representation for each word in the context or question, e C i or e Q i ∈ R d+1024 , is the concatenation of its learned word embedding and ELMo embedding. Reasoning layer: Our reasoning layer is composed of k reasoning cells (see <ref type="figure" target="#fig_0">Fig. 1</ref>), where each incrementally updates the context representation. The t th reasoning cell's inputs are the previous step's output ({c t−1 i } n i=1 ) and the embedded question ({e Q i } m i=1 ). It first creates step-specific context and query encodings via cell-specific bidirectional LSTMs:</p><formula xml:id="formula_0">u t = BiLSTM(c t−1 ); v t = BiLSTM(e Q )</formula><p>Then, we use bidirectional attention <ref type="bibr">(Seo et al., 2017)</ref> to emulate a hop of reasoning by focusing on relevant aspects of the context. Specifically, we first compute context-to-query attention:</p><formula xml:id="formula_1">S t ij = W t 1 u t i + W t 2 v t j + W t 3 (u t i v t j ) p t ij = exp(S t ij ) m k=1 exp(S t ik ) (c q ) t i = m j=1 p t ij v t j where W t 1 , W t 2 , W t</formula><p>3 are trainable parameters, and is elementwise multiplication. We then compute a query-to-context attention vector:</p><formula xml:id="formula_2">m t i = max 1≤j≤m S t ij ... + p sel</formula><p>Attention Distribution  </p><formula xml:id="formula_3">p t i = exp(m t i ) n j=1 exp(m t j ) q c t = n i=1 p t i u t i</formula><p>We then obtain the updated context representation:</p><formula xml:id="formula_4">c t i = [u t i ; (c q ) t i ; u t i (c q ) t i ; q c t (c q ) t i ]</formula><p>where ; is concatenation, c t is the cell's output. The initial input of the reasoning layer is the embedded context representation, i.e., c 0 = e C , and the final output of the reasoning layer is the output of the last cell, c k . Self-Attention Layer: As the final layer before answer generation, we utilize a residual static selfattention mechanism (Clark and Gardner, 2018) to help the model process long contexts with longterm dependencies. The input of this layer is the output of the last reasoning cell, c k . We first pass this representation through a fully-connected layer and then a bi-directional LSTM to obtain another representation of the context c SA . We obtain the self attention representation c : At decoding step t, the decoder receives the input x t (embedded representation of last timestep's output), the last time step's hidden state s t−1 and context vector a t−1 . The decoder computes the current hidden state s t as:</p><formula xml:id="formula_5">S SA ij = W 4 c SA i + W 5 c SA j + W 6 (c SA i c SA j ) p SA ij = exp(S SA ij ) n k=1 exp(S SA ik ) c i = n j=1 p SA ij c SA j where W 4 , W 5 ,</formula><formula xml:id="formula_6">s t = LSTM([x t ; a t−1 ], s t−1 )</formula><p>This hidden state is then used to compute a probability distribution over the generative vocabulary:</p><formula xml:id="formula_7">P gen = softmax(W gen s t + b gen )</formula><p>We employ Bahdanau attention mechanism <ref type="bibr">(Bahdanau et al., 2015)</ref> to attend over the context (c being the output of self-attention layer):</p><formula xml:id="formula_8">α i = v tanh(W c c i + W s s t + b attn )</formula><p>"What is the connection between Esther and Lady Dedlock?" "Mother and daughter." "Sir Leicester Dedlock and his wife Lady Honoria live on his estate at Chesney Wold.." "..Unknown to Sir Leicester, Lady Dedlock had a lover .. before she married and had a daughter with him.." "..Lady Dedlock believes her daughter is dead. The daughter, Esther, is in fact alive.." "..Esther sees Lady Dedlock at church and talks with her later at Chesney Wod though neither woman recognizes their connection.. </p><formula xml:id="formula_9">α i = exp(α i ) n j=1 exp(α j ) a t = n i=1α i c i</formula><p>We utilize a pointer mechanism that allows the decoder to directly copy tokens from the context based onα i . We calculate a selection distribution p sel ∈ R 2 , where p sel 1 is the probability of generating a token from P gen and p sel 2 is the probability of copying a word from the context:</p><formula xml:id="formula_10">o = σ(W a a t + W x x t + W s s t + b ptr ) p sel = softmax(o)</formula><p>Our final output distribution at timestep t is a weighted sum of the generative distribution and the copy distribution:</p><formula xml:id="formula_11">P t (w) = p sel 1 P gen (w) + p sel 2 i:w C i =wα i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Commonsense Selection and Representation</head><p>In QA tasks that require multiple hops of reasoning, the model often needs knowledge of relations not directly stated in the context to reach the correct conclusion. In the datasets we consider, manual analysis shows that external knowledge is frequently needed for inference (see <ref type="table" target="#tab_2">Table 1</ref>). Even with a large amount of training data, it is very unlikely that a model is able to learn every nuanced relation between concepts and apply the correct ones (as in <ref type="figure" target="#fig_2">Fig. 2</ref>  about a question. We remedy this issue by introducing grounded commonsense (background) information using relations between concepts from ConceptNet (Speer and Havasi, 2012) 1 that help inference by introducing useful connections between concepts in the context and question. Due to the size of the semantic network and the large amount of unnecessary information, we need an effective way of selecting relations which provides novel information while being grounded by the context-query pair. Our commonsense selection strategy is twofold: (1) collect potentially relevant concepts via a tree construction method aimed at selecting with high recall candidate reasoning paths, and (2) rank and filter these paths to ensure both the quality and variety of added information via a 3-step scoring strategy (initial node scoring, cumulative node scoring, and path selection). We will refer to <ref type="figure" target="#fig_2">Fig. 2</ref> as a running example throughout this section. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Tree Construction</head><p>Given context C and question Q, we want to construct paths grounded in the pair that emulate reasoning steps required to answer the question. In this section, we build 'prototype' paths by constructing trees rooted in concepts in the query with the following branching steps 3 to emulate multihop reasoning process. For each concept c 1 in the question, we do: Direct Interaction: In the first level, we select relations r 1 from ConceptNet that directly link c 1 to a concept within the context, c 2 ∈ C, e.g., in <ref type="figure" target="#fig_2">Fig. 2</ref>, we have lady → church, lady → mother, lady → person. Multi-Hop: We then select relations in Concept-Net r 2 that link c 2 to another concept in the context, c 3 ∈ C. This emulates a potential reason-ing hop within the context of the MRC task, e.g., church → house, mother → daughter, person → lover. Outside Knowledge: We then allow an unconstrained hop into c 3 's neighbors in ConceptNet, getting to c 4 ∈ nbh(c 3 ) via r 3 (nbh(v) is the set of nodes that can be reached from v in one hop). This emulates the gathering of useful external information to complete paths within the context, e.g., house → child, daughter → child. Context-Grounding: To ensure that the external knowledge is indeed helpful to the task, and also to explicitly link 2nd degree neighbor concepts within the context, we finish the process by grounding it again into context by connecting c 4 to c 5 ∈ C via r 4 , e.g., child → their.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Rank and Filter</head><p>This tree building process collects a large number of potentially relevant and useful paths. However, this step also introduces a large amount of noise. For example, given the question and full context (not depicted in the figure) in <ref type="figure" target="#fig_2">Fig. 2</ref>, we obtain the path "between → hard → being → cottage → country" using our tree building method, which is not relevant to our question. Therefore, to improve the precision of useful concepts, we rank these knowledge paths by their relevance and filter out noise using the following 3-step scoring method: Initial Node Scoring: We want to select paths with nodes that are important to the context, in order to provide the most useful commonsense relations. We approximate importance and saliency for concepts in the context by their termfrequency, under the heuristic that important concepts occur more frequently. Thus we score c ∈ {c 2 , c 3 , c 5 } by: score(c) = count(c)/|C|, where |C| is the context length and count() is the number of times a concept appears in the context. In <ref type="figure" target="#fig_2">Fig. 2</ref>, this ensures that concepts like daughter are scored highly due to their frequency in the context.</p><p>For c 4 , we use a special scoring function as it is an unconstrained hop into ConceptNet. We want c 4 to be a logically consistent next step in reasoning following the path of c 1 to c 3 , e.g., in <ref type="figure" target="#fig_2">Fig. 2</ref>, we see that child is a logically consistent next step after the partial path of mother → daughter. We approximate this based on the heuristic that logically consistent paths occur more frequently. Therefore, we score this node via Pointwise Mutual Information (PMI) between the partial path c 1−3 and c 4 :</p><formula xml:id="formula_12">PMI(c 4 , c 1−3 ) = log(P(c 4 , c 1−3 )/P(c 4 )P(c 1−3 )), where P(c 4 , c 1−3 ) = # of paths connecting c 1 , c 2 , c 3 , c 4 # of distinct paths of length 4 P(c 4 ) = # of nodes that can reach c 4 |ConceptNet| P(c 1−3 ) = # of paths connecting c 1 , c 2 , c 3 # of paths of length 3</formula><p>Further, it is well known that PMI has high sensitivity to low-frequency values, thus we use normalized PMI (NPMI) (Bouma, 2009):</p><formula xml:id="formula_13">score(c 4 ) = PMI(c 4 , c 1−3 )/(− log P(c 4 , c 1−3 )).</formula><p>Since the branching at each juncture represents a hop in the multi-hop reasoning process, and hops at different levels or with different parent nodes do not 'compete' with each other, we normalize each node's score against its siblings: n-score(c) = softmax siblings(c) (score(c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cumulative Node Scoring:</head><p>We want to add commonsense paths consisting of multiple hops of relevant information, thus we re-score each node based not only on its relevance and saliency but also that of its tree descendants.</p><p>We do this by computing a cumulative node score from the bottom up, where at the leaf nodes, we have c-score = n-score, and for c l not a leaf node, we have c-score(c l ) = n-score(c l ) + f (c l ) where f of a node is the average of the c-scores of its top 2 highest scoring children.</p><p>For example, given the paths lady → mother → daughter, lady → mother → married, and lady → mother → book, we start the cumulative scoring at the leaf nodes, which in this case are daughter, married, and book, where daughter and married are scored much higher than book due to their more frequent occurrences. Then, to cumulatively score mother , we would take the average score of its two highest scoring children (in this case married and daughter) and compound that with the score of mother itself. Note that the poor scoring of the irrelevant concept book does not affect the scoring of mother, which is quite high due to the concept's frequent occurrence and the relevance of its top scoring children. Path Selection: We select paths in a top-down breath-first fashion in order to add information relevant to different parts of the context. Starting at the root, we recursively take two of its children with the highest cumulative scores until we reach a leaf, selecting up to 2 4 = 16 paths. For example, if we were at node mother, this allows us to select the child node daughter and married over the child node book. These selected paths, as well as their partial sub-paths, are what we add as external information to the QA model, i.e., we add the complete path lady, AtLocation, church, Relat-edTo, house, RelatedTo, child, RelatedTo, their , but also truncated versions of the path, including lady, AtLocation, church, RelatedTo, house, Re-latedTo, child . We directly give these paths to the model as sequences of tokens. <ref type="bibr">4</ref> Overall, our sampling strategy provides the knowledge that a lady can be a mother and that mother is connected to daughter. This creates a logical connection between lady and daughter which helps highlight the importance of our second piece of evidence (see <ref type="figure" target="#fig_2">Fig. 2</ref>). Likewise, the commonsense information we extracted create a similar connection in our third piece of evidence, which states the explicit connection between daughter and Esther. We also successfully extract a more story context-centric connection, in which commonsense provides the knowledge that a lady is at the location church, which directs to another piece of evidence in the context. Additionally, this path also encodes a relation between lady and child, by way of church, which is how lady and Esther are explicitly connected in the story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Commonsense Model Incorporation</head><p>Given the list of commonsense logic paths as sequences of words:</p><formula xml:id="formula_14">X CS = {w CS 1 , w CS 2 , . . . , w CS l } where w CS i</formula><p>represents the list of tokens that make up a single path, we first embed these commonsense tokens into the learned embedding space used by the model, giving us the embedded commonsense tokens, e CS ij ∈ R d . We want to use these commonsense paths to fill in the gaps of reasoning between hops of inference. Thus, we propose Necessary and Optional Information Cell (NOIC), a variation of our base reasoning cell used in the reasoning layer that is capable of incorporating optional helpful information.</p><p>NOIC This cell is an extension to the base reasoning cell that allows the model to use commonsense information to fill in gaps of reasoning. An example of this is on the bottom left of <ref type="figure" target="#fig_0">Fig. 1</ref>, where we see that the cell first performs the operations done in the base reasoning cell and then adds optional, commonsense information.</p><p>At reasoning step t, after obtaining the output of the base reasoning cell, c t , we create a cell-specific representation for commonsense information by concatenating the embedded commonsense paths so that each path has a single vector representation, u CS i . We then project it to the same dimension as c t</p><formula xml:id="formula_15">i : v CS i = ReLU(W u CS i + b) where W and b are trainable parameters.</formula><p>We use an attention layer to model the interaction between commonsense and the context:</p><formula xml:id="formula_16">S CS ij = W CS 1 c t i + W CS 2 v CS j + W CS 3 (c t i v CS j ) p CS ij = exp(S CS ij ) l k=1 exp(S CS ij ) c CS i = l j=1 p CS ij v CS j</formula><p>Finally, we combine this commonsense-aware context representation with the original c t i via a sigmoid gate, since commonsense information is often not necessary at every step of inference:</p><formula xml:id="formula_17">z i = σ(W z [c CS i ; c t i ] + b z ) (c o ) t i = z i c t i + (1 − z i ) c CS i</formula><p>We use c o t as the output of the current reasoning step instead of c t . As we replace each base reasoning cell with NOIC, we selectively incorporate commonsense at every step of inference.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Experiment</head><p>The results of our model on both NarrativeQA and WikiHop with and without commonsense incorporation are shown in <ref type="table" target="#tab_4">Table 2</ref> and <ref type="table" target="#tab_5">Table 3</ref>. We see empirically that our model outperforms all generative models on NarrativeQA, and is competitive with the top span prediction models. Furthermore, with the NOIC commonsense integration, we were able to further improve performance (p &lt; 0.001 on all metrics 5 ), establishing a new state-of-the-art for the task.</p><p>We also see that our model performs reasonably well on WikiHop, and further achieves promising initial improvements via the addition of commonsense, hinting at the generalizability of our approaches. We speculate that the improvement is smaller on Wikihop because only approximately 11% of WikiHop data points require commonsense and because WikiHop data requires more fact-based commonsense (e.g., from Freebase <ref type="bibr">(Bollacker et al., 2008)</ref>) as opposed to semantics-based commonsense (e.g., from Con-ceptNet (Speer and Havasi, 2012)). 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Ablations</head><p>We also tested the effectiveness of each component of our architecture as well as the effective-5 Stat. significance computed using bootstrap test with 100K iterations <ref type="bibr">(Noreen, 1989;</ref><ref type="bibr">Efron and Tibshirani, 1994)</ref>. <ref type="bibr">6</ref> All results here are for the standard (non-oracle) unmasked and not-validated dataset. Welbl et al. (2018) has reported higher numbers on different data settings which are not comparable to our results. ness of adding commonsense information on the NarrativeQA validation set, with results shown in <ref type="table" target="#tab_8">Table 4</ref>. Experiment 1 and 5 are our models presented in  <ref type="bibr">(Peters et al., 2018)</ref> were also important for the model's performance and that self-attention is able to contribute significantly to performance on top of other components of the model. Finally, we see that effectively introducing external knowledge via our commonsense selection algorithm and NOIC can improve performance even further on top of our strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Commonsense Ablations</head><p>We also conducted experiments testing the effectiveness of our commonsense selection and incorporation techniques. We first tried to naively add ConceptNet information by initializing the word embeddings with the ConceptNet-trained embeddings, NumberBatch (Speer and Havasi, 2012) (we also change embedding size from 256 to 300). Then, to verify the effectiveness of our commonsense selection and grounding algorithm, we test our best model on in-domain noise by giving each context-query pair a set of random relations grounded in other context-query pairs. This should teach the model about general commonsense relations present in the domain of Narra-tiveQA but does not provide grounding that fills in specific hops of inference. We also experimented with a simpler commonsense extraction method of using a single hop from the query to the context. The results of these are shown in <ref type="table" target="#tab_9">Table 5</ref>, where we see that neither NumberBatch nor random-relationships nor single-hop common-   sense offer statistically significant improvements 7 , whereas our commonsense selection and incorporation mechanism improves performance significantly across all metrics. We also present several examples of extracted commonsense and its model attention visualization in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Human Evaluation Analysis</head><p>We also conduct human evaluation analysis on both the quality of the selected commonsense relations, as well as the performance of our final model. Commonsense Selection: We conducted manual analysis on a 50 sample subset of the NarrativeQA test set to check the effectiveness of our commonsense selection algorithm. Specifically, given a context-query pair, as well as the commonsense selected by our algorithm, we conduct two independent evaluations: (1) was any external commonsense knowledge necessary for answering the question?;</p><p>(2) were the commonsense relations provided by our algorithm relevant to the question? The result for these two evaluations as well as how they overlap with each other are shown in <ref type="table" target="#tab_11">Table 6</ref>, where we see that 50% of the cases required external commonsense knowledge, and on a majority (34%) of those cases our algorithm was able to select the correct/relevant commonsense information to fill in gaps of inference. We also see that in general, our algorithm was able to provide useful commonsense 48% of the time.</p><p>Model Performance: We also conduct human evaluation to verify that our commonsense incorporated model was indeed better than MHPGM. <ref type="bibr">7</ref> The improvement in Rouge-L and METEOR for all three ablation approaches have p ≥ 0.15 with the bootstrap test.   We randomly selected 100 examples from the Nar-rativeQA test set, along with both models' predicted answers, and for each datapoint, we asked 3 external human evaluators (fluent English speakers) to decide (without knowing which model produced each response) if one is strictly better than the other, or that they were similar in quality (bothgood or both-bad). As shown in <ref type="table" target="#tab_12">Table 7</ref>, we see that the human evaluation results are in agreement with that of the automatic evaluation metrics: our commonsense incorporation has a reasonable impact on the overall correctness of the model. The inter-annotator agreement had a Fleiss κ = 0.831, indicating 'almost-perfect' agreement between the annotators <ref type="bibr">(Landis and Koch, 1977)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present an effective reasoning-generative QA architecture that is a novel combination of previous work, which uses multiple hops of bidirectional attention and a pointer-generator decoder to effectively perform multi-hop reasoning and synthesize a coherent and correct answer. Further, we introduce an algorithm to select grounded, useful paths of commonsense knowledge to fill in the gaps of inference required for QA, as well a Necessary and Optional Information Cell (NOIC) which successfully incorporates this information during multi-hop reasoning to achieve the new state-of-the-art on NarrativeQA.  <ref type="bibr">, 2018)</ref>. NarrativeQA is a generative QA dataset where the passages are either stories or summaries of stories, and the questions ask about complex aspects of the narratives such as event timelines, characters, relations between characters, etc. Each question has two answers which are generated by human annotators and usually cannot be found in the passage directly. We focus on the summary subtask in this paper, where summaries have lengths of up to 1000 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>We also test our model on WikiHop, a fact based, multi-hop dataset. Questions in WikiHop often require a model to read several documents in order to obtain an answer. We focus on the multiple-choice part of WikiHop, where models are tasked with picking the correct response from a pool of candidates. We rank candidate responses by calculating their generation probability based on our model. As this is a multi-document QA task, we first rank the candidate documents via TF-IDF cosine distance with the question, and then take the top k documents such that their combined length is less than 1300 words.</p><p>Evaluation Metrics We evaluate NarrativeQA on the metrics proposed by its original authors: Bleu-1, <ref type="bibr">Bleu-4 (Papineni et al., 2002)</ref>, <ref type="bibr">ME-TEOR (Banerjee and Lavie, 2005)</ref> and Rouge-L <ref type="bibr">(Lin, 2004)</ref>. We also evaluate on CIDEr <ref type="bibr">(Vedantam et al., 2015)</ref> as it places emphasize on annotator consensus. For WikiHop, we evaluate on accuracy.</p><p>Training Details In training for both datasets, we minimize the negative log probability of generating the ground-truth answer with the Adam optimizer <ref type="bibr">(Kingma and Ba, 2015)</ref> with an initial learning rate of 0.001, a dropout-rate of 0.2 (dropout is applied to the input of each RNN layer) and batch size of 24. We use 256 dimensional word embeddings and a hidden size of 128 for all RNNs and k = 3 hops of multi attention. At inference time we use greedy decoding to generate the answer. For both NarrativeQA and WikiHop, we reached these parameters via tuning on the full, official validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Commonsense Extraction Examples</head><p>In Tables 8, 9, and 10 (see next page), we demonstrate extracted commonsense examples for questions that require commonsense to reach an answer. We bold words in the question and in the extracted commonsense in cases where the commonsense knowledge explicitly bridges gaps between implicitly connected words in the context or question. The relevant context is also displayed, with context words that are key to answering the question (via commonsense) marked in bold. These are then followed by a context visualization described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Commonsense Integration Visualization</head><p>We also visualize how much commonsense information is integrated into each part of the context by providing a visualization of the z i value (see end of Sec. 3.3 of main file) for i ∈ {1, 2, 3}, which is the gate value signifying how much commonsense-attention representation is used in the output context representation. In the following examples (next page), we use shades of blue to represent the average of (1 − z i ) at each word in the context (normalized within each hop), with deeper blue indicating the use of more commonsense information. As a general trend, we see that in the earlier hops, words which are near tokens that occur in both the context and commonsense paths have high activation, but the activation becomes more focused on the passage's key words w.r.t. the question, as the number of hops increase. maurya has lost her husband , and five of her sons to the sea . as the play begins nora and cathleen receive word from the priest that a body , that may be their brother michael , has washed up on shore in donegal , the island farthest north of their home island of inishmaan . bartley is planning to sail to connemara to sell a horse , and ignores maurya s pleas to stay . he leaves gracefully . maurya predicts that by nightfall she will have no living sons , and her daughters chide her for sending bartley off with an ill word . maurya goes after bartley to bless his voyage , and nora and cathleen receive clothing from the drowned corpse that confirms it is their brother . maurya returns home claiming to have seen the ghost of michael riding behind bartley and begins lamenting the loss of the men in her family to the sea , after which some villagers bring in the corpse of bartley , who has fallen off his horse into the sea and drowned . this speech of maurya s is famous in irish drama : ( raising her head and speaking as if she did not see the people around her ) they re all gone now , and there is n't anything more the sea can do to me ... . i ll have no call now to be up crying and praying when the wind breaks from the south , and you can hear the surf is in the east , and the surf is in the west , making a great stir with the two noises , and they hitting one on the other . i ll have no call now to be going down and getting holy water in the dark nights after samhain , and i wo n't care what way the sea is when the other women will be keening . ( to nora ) give me the holy water , nora ; there s a small sup still on the dresser . maurya has lost her husband , and five of her sons to the sea . as the play begins nora and cathleen receive word from the priest that a body , that may be their brother michael , has washed up on shore in donegal , the island farthest north of their home island of inishmaan . bartley is planning to sail to connemara to sell a horse , and ignores maurya s pleas to stay . he leaves gracefully . maurya predicts that by nightfall she will have no living sons , and her daughters chide her for sending bartley off with an ill word . maurya goes after bartley to bless his voyage , and nora and cathleen receive clothing from the drowned corpse that confirms it is their brother . maurya returns home claiming to have seen the ghost of michael riding behind bartley and begins lamenting the loss of the men in her family to the sea , after which some villagers bring in the corpse of bartley , who has fallen off his horse into the sea and drowned . this speech of maurya s is famous in irish drama : ( raising her head and speaking as if she did not see the people around her ) they re all gone now , and there is n't anything more the sea can do to me ... . i ll have no call now to be up crying and praying when the wind breaks from the south , and you can hear the surf is in the east , and the surf is in the west , making a great stir with the two noises , and they hitting one on the other . i ll have no call now to be going down and getting holy water in the dark nights after samhain , and i wo n't care what way the sea is when the other women will be keening . ( to nora ) give me the holy water , nora ; there s a small sup still on the dresser . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Commonsense Extraction and Visualization Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>What shore does Michael's corpse wash up on?</p><p>Context "..as the play begins nora and cathleen receive word from the priest that a body, that may be their brother michael, has washed up on shore in donegal, the island farthest north of their home island of inishmaan.." maurya has lost her husband , and five of her sons to the sea . as the play begins nora and cathleen receive word from the priest that a body , that may be their brother michael , has washed up on shore in donegal , the island farthest north of their home island of inishmaan . bartley is planning to sail to connemara to sell a horse , and ignores maurya s pleas to stay . he leaves gracefully . maurya predicts that by nightfall she will have no living sons , and her daughters chide her for sending bartley off with an ill word . maurya goes after bartley to bless his voyage , and nora and cathleen receive clothing from the drowned corpse that confirms it is their brother . maurya returns home claiming to have seen the ghost of michael riding behind bartley and begins lamenting the loss of the men in her family to the sea , after which some villagers bring in the corpse of bartley , who has fallen off his horse into the sea and drowned . this speech of maurya s is famous in irish drama : ( raising her head and speaking as if she did not see the people around her ) they re all gone now , and there is n't anything more the sea can do to me ... . i ll have no call now to be up crying and praying when the wind breaks from the south , and you can hear the surf is in the east , and the surf is in the west , making a great stir with the two noises , and they hitting one on the other . i ll have no call now to be going down and getting holy water in the dark nights after samhain , and i wo n't care what way the sea is when the other women will be keening . ( to nora ) give me the holy water , nora ; there s a small sup still on the dresser .  eight-year-old princess irene lives a lonely life in a castle in a wild , desolate , mountainous kingdom , with only her nursemaid , lootie , for company . her father , the king , is normally absent , and her mother is dead . unknown to her , the nearby mines are inhabited by a race of goblins , long banished from the kingdom and now anxious to take revenge on their human neighbors . one rainy day , the princess explores the castle and discovers a beautiful , mysterious lady , who identifies herself as irene s namesake and great-great-grandmother . the next day , princess irene persuades her nursemaid to take her outside . after dark they are chased by goblins and rescued by the young miner , curdie , whom irene befriends . at work with the rest of the miners , curdie overhears the goblins talking , and their conversation reveals to curdie the secret weakness of goblin anatomy : they have very soft , vulnerable feet . curdie sneaks into the great hall of the goblin palace to eavesdrop on their general meeting , and hears that the goblins intend to flood the mine if a certain other part of their plan should fail . he later conveys this news to his father . in the palace , princess irene injures her hand , which her great-greatgrandmother heals . a week later irene is about to see her great-great-grandmother again , but is frightened by a long-legged cat and escapes up the mountain ; whereupon the light from her great-great-grandmother s tower leads her home , where her great-great-grandmother gives irene a ring attached to a thread invisible except to herself , which thereafter connects her constantly to home . when curdie explores the goblins ' domain , he is discovered by the goblins and stamps on their feet with great success ; but when he tries to stamp on the queen s feet she is uninjured due to her stone shoes . the goblins imprison curdie , thinking he will die of starvation ; but irene s magic thread leads her to his rescue , and curdie steals one of the goblin queen s stone shoes . irene takes curdie to see her great-greatgrandmother and be introduced ; but she is only visible to irene . curdie later learns that the goblins are digging a tunnel in the mines towards the king s palace , where they plan to abduct the princess and marry her to goblin prince harelip . curdie warns the palace guards about this , but is imprisoned instead and contracts a fever through a wound in his leg , until irene s great-great-grandmother heals the wound . meanwhile , the goblins break through the palace floor and come to abduct the princess ; but curdie escapes from his prison room and stamps on the goblins ' feet . upon the goblins ' retreat , irene is believed a captive ; but curdie follows the magic thread to her refuge at his own house , and restores her to the king . when the goblins flood the mines , the water enters the palace , and curdie warns the others ; but the goblins are drowned . the king asks him to serve as a bodyguard ; but curdie refuses , saying he can not leave his mother and father , and instead accepts a new red petticoat for his mother , as a reward . eight-year-old princess irene lives a lonely life in a castle in a wild , desolate , mountainous kingdom , with only her nursemaid , lootie , for company . her father , the king , is normally absent , and her mother is dead . unknown to her , the nearby mines are inhabited by a race of goblins , long banished from the kingdom and now anxious to take revenge on their human neighbors . one rainy day , the princess explores the castle and discovers a beautiful , mysterious lady , who identifies herself as irene s namesake and great-great-grandmother . the next day , princess irene persuades her nursemaid to take her outside . after dark they are chased by goblins and rescued by the young miner , curdie , whom irene befriends . at work with the rest of the miners , curdie overhears the goblins talking , and their conversation reveals to curdie the secret weakness of goblin anatomy : they have very soft , vulnerable feet . curdie sneaks into the great hall of the goblin palace to eavesdrop on their general meeting , and hears that the goblins intend to flood the mine if a certain other part of their plan should fail . he later conveys this news to his father . in the palace , princess irene injures her hand , which her great-greatgrandmother heals . a week later irene is about to see her great-great-grandmother again , but is frightened by a long-legged cat and escapes up the mountain ; whereupon the light from her great-great-grandmother s tower leads her home , where her great-great-grandmother gives irene a ring attached to a thread invisible except to herself , which thereafter connects her constantly to home . when curdie explores the goblins ' domain , he is discovered by the goblins and stamps on their feet with great success ; but when he tries to stamp on the queen s feet she is uninjured due to her stone shoes . the goblins imprison curdie , thinking he will die of starvation ; but irene s magic thread leads her to his rescue , and curdie steals one of the goblin queen s stone shoes . irene takes curdie to see her great-greatgrandmother and be introduced ; but she is only visible to irene . curdie later learns that the goblins are digging a tunnel in the mines towards the king s palace , where they plan to abduct the princess and marry her to goblin prince harelip . curdie warns the palace guards about this , but is imprisoned instead and contracts a fever through a wound in his leg , until irene s great-great-grandmother heals the wound . meanwhile , the goblins break through the palace floor and come to abduct the princess ; but curdie escapes from his prison room and stamps on the goblins ' feet . upon the goblins ' retreat , irene is believed a captive ; but curdie follows the magic thread to her refuge at his own house , and restores her to the king . when the goblins flood the mines , the water enters the palace , and curdie warns the others ; but the goblins are drowned . the king asks him to serve as a bodyguard ; but curdie refuses , saying he can not leave his mother and father , and instead accepts a new red petticoat for his mother , as a reward . eight-year-old princess irene lives a lonely life in a castle in a wild , desolate , mountainous kingdom , with only her nursemaid , lootie , for company . her father , the king , is normally absent , and her mother is dead . unknown to her , the nearby mines are inhabited by a race of goblins , long banished from the kingdom and now anxious to take revenge on their human neighbors . one rainy day , the princess explores the castle and discovers a beautiful , mysterious lady , who identifies herself as irene s namesake and great-great-grandmother . the next day , princess irene persuades her nursemaid to take her outside . after dark they are chased by goblins and rescued by the young miner , curdie , whom irene befriends . at work with the rest of the miners , curdie overhears the goblins talking , and their conversation reveals to curdie the secret weakness of goblin anatomy : they have very soft , vulnerable feet . curdie sneaks into the great hall of the goblin palace to eavesdrop on their general meeting , and hears that the goblins intend to flood the mine if a certain other part of their plan should fail . he later conveys this news to his father . in the palace , princess irene injures her hand , which her great-greatgrandmother heals . a week later irene is about to see her great-great-grandmother again , but is frightened by a long-legged cat and escapes up the mountain ; whereupon the light from her great-great-grandmother s tower leads her home , where her great-great-grandmother gives irene a ring attached to a thread invisible except to herself , which thereafter connects her constantly to home . when curdie explores the goblins ' domain , he is discovered by the goblins and stamps on their feet with great success ; but when he tries to stamp on the queen s feet she is uninjured due to her stone shoes . the goblins imprison curdie , thinking he will die of starvation ; but irene s magic thread leads her to his rescue , and curdie steals one of the goblin queen s stone shoes . irene takes curdie to see her great-greatgrandmother and be introduced ; but she is only visible to irene . curdie later learns that the goblins are digging a tunnel in the mines towards the king s palace , where they plan to abduct the princess and marry her to goblin prince harelip . curdie warns the palace guards about this , but is imprisoned instead and contracts a fever through a wound in his leg , until irene s great-great-grandmother heals the wound . meanwhile , the goblins break through the palace floor and come to abduct the princess ; but curdie escapes from his prison room and stamps on the goblins ' feet . upon the goblins ' retreat , irene is believed a captive ; but curdie follows the magic thread to her refuge at his own house , and restores her to the king . when the goblins flood the mines , the water enters the palace , and curdie warns the others ; but the goblins are drowned . the king asks him to serve as a bodyguard ; but curdie refuses , saying he can not leave his mother and father , and instead accepts a new red petticoat for his mother , as a reward . </p><formula xml:id="formula_18">Answers the shore of donegal / donegal Extracted Commonsense up → RelatedTo → wind → Antonym → her → RelatedTo → person up → RelatedTo → north → RelatedTo → up wash → RelatedTo → up up → Antonym → down wash → RelatedTo → water → PartOf → sea → RelatedTo → fish up → RelatedTo → wind wash → RelatedTo → water → PartOf → sea shore → RelatedTo → sea wash → RelatedTo → body wash → Antonym → making up → Antonym → down → Antonym → up wash → RelatedTo → water → PartOf → sea → MadeOf → water up → RelatedTo → wind → Antonym → her wash → RelatedTo → water up → RelatedTo → south shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket → RelatedTo → horse wash → RelatedTo → clothing wash → RelatedTo → water → PartOf → sea → MadeOf → water → PartOf → sea shore → RelatedTo → sea → MadeOf → water wash → Antonym → getting up → RelatedTo → north corpse → RelatedTo → body shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain corpse → RelatedTo → body → RelatedTo → corpse corpse → RelatedTo → body → RelatedTo → water wash → HasContext → west up → RelatedTo → wind → Antonym → her → RelatedTo → person → MadeOf → water up → RelatedTo → wind → AtLocation → sea wash → RelatedTo → water → AtLocation → can shore → RelatedTo → sea → MadeOf → water → AtLocation → bucket wash → RelatedTo → will shore → RelatedTo → sea → MadeOf → water → AtLocation → fountain → RelatedTo → water</formula><formula xml:id="formula_19">species → RelatedTo → kingdom → RelatedTo → queen species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people → HasA → feet mines → FormOf → mine lives → FormOf → life mines → FormOf → mine → AtLocation → home → RelatedTo → person species → RelatedTo → kingdom → RelatedTo → queen → UsedFor → people species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person → Desires → feet mines → FormOf → mine → AtLocation → home → RelatedTo → line → RelatedTo → thread species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader → AtLocation → company species → RelatedTo → kingdom species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → leader species → RelatedTo → kingdom → DerivedFrom → king mines → FormOf → mine → AtLocation → home → RelatedTo → line species → RelatedTo → race mines → FormOf → mine → AtLocation → home species → RelatedTo → kingdom → RelatedTo → queen → RelatedTo → person species → RelatedTo → kingdom → DerivedFrom → king → RelatedTo → master → RelatedTo → young</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>What duty does ruth have to fulfill when her aunt dies?</p><p>Context "..ruth anvoy, a young american woman with a wealthy father, comes to britain to visit her widowed aunt lady coxon.." "..having made a promise to her now-deceased husband, lady coxon has for years been seeking to bestow a sum of 13,000 pounds upon a talented intellectual whose potential has been hampered by lack of money. having failed to find such a person, lady coxon tells anvoy that upon her death the money will be left to her, and she must carry on the quest.." "..anvoy, having lost nearly all her wealth, has only the 13,000 pounds from lady coxon, with a moral but not legal obligation to give it away.." "..she awards the coxon fund to saltram, who lives off it exactly as he lived off his friends, producing nothing of intellectual value.."</p><p>Answers she must give away the 13,000 pounds to an appropriate recipient. / bestow 13000 to the appropriate person Extracted Commonsense frank saltram is a man who apparently has a towering intellect , but one that manifests itself only in sparkling table-talk . he has a real and power gift to delight with his conversation , particularly when intoxicated , but other than conversation he produces nothing . saltram also recognises no obligations or duties , is ungrateful and utterly unreliable , and is apparently prone to immoral acts . he lives off others , particularly the mulvilles , who , convinced of saltram s genius and genuinely enjoying his talk , host him for months at a time . in the opinion of the unnamed narrator , saltram is not a deliberate conman ; he simply suffers from a want of dignity . the story revolves around saltram and a group of people who are fascinated by him . ruth anvoy , a young american woman with a wealthy father , comes to britain to visit her widowed aunt lady coxon . there she meets george gravener , a man with a real intellect and a future in politics , and the two become engaged . she also meets saltram , and is fascinated and impressed by his talk and intellect , though aware that he has shortcomings of character . having made a promise to her nowdeceased husband , lady coxon has for years been seeking to bestow a sum of 13,000 pounds upon a talented intellectual whose potential has been hampered by lack of money . having failed to find such a person , lady coxon tells anvoy that upon her death the money will be left to her , and she must carry on the quest . anvoy s father suffers heavy financial losses and loses most of what he has . he dies , and shortly afterwards lady coxon dies . anvoy , having lost nearly all her wealth , has only the 13,000 pounds from lady coxon , with a moral but not legal obligation to give it away . gravener urges her to keep the money , as it could be used to buy them a house once they are married . she refuses , and their relationship becomes strained . later , she entertains the idea of giving the money to saltram , who gravener despises as a fraud and not a gentleman . eventually their engagement is broken off . finally , the unnamed narrator is given a sealed letter and asked to give it to anvoy . the letter is understood to contain a denunciation of saltram s most immoral acts . the narrator must decide whether to blight saltram s prospects by delivering the letter . he is willing to do so if it will save his friend gravener s engagement with anvoy , but gravener is unable to assure him of this . eventually he does offer the letter to anvoy , but anvoy declines to read it . she awards the coxon fund to saltram , who lives off it exactly as he lived off his friends , producing nothing of intellectual value . thus the only result of the award is the mulvilles and others lose the pleasure of saltram s conversation . frank saltram is a man who apparently has a towering intellect , but one that manifests itself only in sparkling table-talk . he has a real and power gift to delight with his conversation , particularly when intoxicated , but other than conversation he produces nothing . saltram also recognises no obligations or duties , is ungrateful and utterly unreliable , and is apparently prone to immoral acts . he lives off others , particularly the mulvilles , who , convinced of saltram s genius and genuinely enjoying his talk , host him for months at a time . in the opinion of the unnamed narrator , saltram is not a deliberate conman ; he simply suffers from a want of dignity . the story revolves around saltram and a group of people who are fascinated by him . ruth anvoy , a young american woman with a wealthy father , comes to britain to visit her widowed aunt lady coxon . there she meets george gravener , a man with a real intellect and a future in politics , and the two become engaged . she also meets saltram , and is fascinated and impressed by his talk and intellect , though aware that he has shortcomings of character . having made a promise to her nowdeceased husband , lady coxon has for years been seeking to bestow a sum of 13,000 pounds upon a talented intellectual whose potential has been hampered by lack of money . having failed to find such a person , lady coxon tells anvoy that upon her death the money will be left to her , and she must carry on the quest . anvoy s father suffers heavy financial losses and loses most of what he has . he dies , and shortly afterwards lady coxon dies . anvoy , having lost nearly all her wealth , has only the 13,000 pounds from lady coxon , with a moral but not legal obligation to give it away . gravener urges her to keep the money , as it could be used to buy them a house once they are married . she refuses , and their relationship becomes strained . later , she entertains the idea of giving the money to saltram , who gravener despises as a fraud and not a gentleman . eventually their engagement is broken off . finally , the unnamed narrator is given a sealed letter and asked to give it to anvoy . the letter is understood to contain a denunciation of saltram s most immoral acts . the narrator must decide whether to blight saltram s prospects by delivering the letter . he is willing to do so if it will save his friend gravener s engagement with anvoy , but gravener is unable to assure him of this . eventually he does offer the letter to anvoy , but anvoy declines to read it . she awards the coxon fund to saltram , who lives off it exactly as he lived off his friends , producing nothing of intellectual value . thus the only result of the award is the mulvilles and others lose the pleasure of saltram s conversation . frank saltram is a man who apparently has a towering intellect , but one that manifests itself only in sparkling table-talk . he has a real and power gift to delight with his conversation , particularly when intoxicated , but other than conversation he produces nothing . saltram also recognises no obligations or duties , is ungrateful and utterly unreliable , and is apparently prone to immoral acts . he lives off others , particularly the mulvilles , who , convinced of saltram s genius and genuinely enjoying his talk , host him for months at a time . in the opinion of the unnamed narrator , saltram is not a deliberate conman ; he simply suffers from a want of dignity . the story revolves around saltram and a group of people who are fascinated by him . ruth anvoy , a young american woman with a wealthy father , comes to britain to visit her widowed aunt lady coxon . there she meets george gravener , a man with a real intellect and a future in politics , and the two become engaged . she also meets saltram , and is fascinated and impressed by his talk and intellect , though aware that he has shortcomings of character . having made a promise to her nowdeceased husband , lady coxon has for years been seeking to bestow a sum of 13,000 pounds upon a talented intellectual whose potential has been hampered by lack of money . having failed to find such a person , lady coxon tells anvoy that upon her death the money will be left to her , and she must carry on the quest . anvoy s father suffers heavy financial losses and loses most of what he has . he dies , and shortly afterwards lady coxon dies . anvoy , having lost nearly all her wealth , has only the 13,000 pounds from lady coxon , with a moral but not legal obligation to give it away . gravener urges her to keep the money , as it could be used to buy them a house once they are married . she refuses , and their relationship becomes strained . later , she entertains the idea of giving the money to saltram , who gravener despises as a fraud and not a gentleman . eventually their engagement is broken off . finally , the unnamed narrator is given a sealed letter and asked to give it to anvoy . the letter is understood to contain a denunciation of saltram s most immoral acts . the narrator must decide whether to blight saltram s prospects by delivering the letter . he is willing to do so if it will save his friend gravener s engagement with anvoy , but gravener is unable to assure him of this . eventually he does offer the letter to anvoy , but anvoy declines to read it . she awards the coxon fund to saltram , who lives off it exactly as he lived off his friends , producing nothing of intellectual value . thus the only result of the award is the mulvilles and others lose the pleasure of saltram s conversation . </p><formula xml:id="formula_20">duty → RelatedTo → moral → Antonym → immoral duty → RelatedTo → time → IsA → money duty → RelatedTo → time → IsA → money → AtLocation → church duty → DistinctFrom → off duty → RelatedTo → time → IsA → money → CapableOf → pay → bills → MotivatedByGoal → must duty → RelatedTo → time → IsA → money → AtLocation → church → RelatedTo → house duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate → RelatedTo → real her → RelatedTo → woman → RelatedTo → lady → RelatedTo → plate → Antonym → her duty → RelatedTo → moral → RelatedTo → will → RelatedTo → choose → IsA → decide duty → RelatedTo → must → RelatedTo → having → RelatedTo → estate duty → RelatedTo → obligation duty → RelatedTo → moral → RelatedTo → will → IsA → purpose her → RelatedTo → but → DistinctFrom → only → RelatedTo → child → RelatedTo → particularly her → RelatedTo → person → RelatedTo → others → RelatedTo → people her → Antonym → him → RelatedTo → he → RelatedTo → person → Desires → conversation her → RelatedTo → woman → RelatedTo → lady her → RelatedTo → woman → RelatedTo → she duty → RelatedTo → must → RelatedTo → having → RelatedTo → own → RelatedTo → having her → RelatedTo → person → DistinctFrom → man → Antonym → people her → RelatedTo → but → DistinctFrom → only → RelatedTo → child her → Antonym → him → RelatedTo → he → RelatedTo → person her → Antonym → his → RelatedTo → him → RelatedTo → person</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture for our Multi-Hop Pointer-Generator Model, and the NOIC commonsense reasoning cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and W 6 are trainable parameters. The output of the self-attention layer is generated by another layer of bidirectional LSTM. c = BiLSTM([c ; c SA ; c c SA ] Finally, we add this residually to c k to obtain the encoded context c = c k + c . Pointer-Generator Decoding Layer: Similar to the work of See et al. (2017), we use a pointergenerator model attending on (and potentially copying from) the context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Commonsense selection approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Example 1 visualized activation values of first attention hop (1 − z 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Example 1 visualized activation values of second attention hop (1 − z 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Example 1 visualized activation values of third attention hop (1 − z 3 ).QuestionWhat species lives in the nearby mines?Context "..the nearby mines are inhabited by a race of goblins.."Answers the goblins / goblins.Extracted Commonsense</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Example 2 visualized activation values of first attention hop (1 − z 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Example 2 visualized activation values of second attention hop (1 − z 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Example 2 visualized activation values of third attention hop (1 − z 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Example 3 visualized activation values of first attention hop (1 − z 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Example 3 visualized activation values of second attention hop (1 − z 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Example 3 visualized activation values of third attention hop (1 − z 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Qualitative analysis of commonsense requirements. WikiHop results are from Welbl et al. (2018); NarrativeQA results are from our manual analysis (on the validation set).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We report results on two multi-hop reasoning datasets: generative NarrativeQA (Kočiskỳ et al., 2018) (summary subtask) and extractive QAngarooWikiHop (Welbl et al., 2018). For multiple-choice WikiHop, we rank candidate responses by their generation probability. Similar to previous works (Dhingra et al., 2018), we use the non-oracle, unmasked and not-validated dataset.Lin, 2004). We also evaluate on CIDEr(Vedantam et al., 2015)  which emphasizes annotator consensus. For WikiHop, we evaluate on accuracy.More dataset, metric, and all other training details are in the supplementary.</figDesc><table><row><cell>4 Experimental Setup Datasets: Evaluation Metrics: We evaluate NarrativeQA on the metrics proposed by its original authors: Bleu-1, Bleu-4 (Papineni et al., 2002), ME-TEOR (Banerjee and Lavie, 2005) and Rouge-BLEU-1 BLEU-4 METEOR Rouge-L CIDEr 15.89 1.26 4.08 13.15 -23.20 6.39 7.77 22.26 -33.72 15.53 15.38 36.30 -36.55 19.79 17.87 41.44 -40.24 17.40 17.33 41.49 139.23 L (Model Seq2Seq (Kočiskỳ et al., 2018) ASR (Kočiskỳ et al., 2018) BiDAF  † (Kočiskỳ et al., 2018) BiAttn + MRU-LSTM  † (Tay et al., 2018) MHPGM MHPGM+ NOIC 43.63 21.07 19.03 44.16 152.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results across different metrics on the test set of NarrativeQA-summaries task. † indicates span prediction models trained on the Rouge-L retrieval oracle.</figDesc><table><row><cell>Model</cell><cell>Dev Test</cell></row><row><cell>BiDAF (Welbl et al., 2018)</cell><cell>42.1 42.9</cell></row><row><cell cols="2">Coref-GRU (Dhingra et al., 2018) 56.0 59.3</cell></row><row><cell>MHPGM</cell><cell>56.2 57.5</cell></row><row><cell>MHPGM+ NOIC</cell><cell>58.5 57.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results of our models on WikiHop dataset, measured in % accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Experiment 2 demonstrates the importance of multi-hop attention by showing that if we only allow one hop of attention (even with all other components of the model, including ELMo embeddings) the model's performance decreases by over 12 Rouge-L points. Experiment 3 and 4 demonstrate the effectiveness of other parts of our model. We see that ELMo embeddings</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Model ablations on NarrativeQA val-set.</figDesc><table><row><cell cols="2">Commonsense B-1</cell><cell>B-4</cell><cell>M</cell><cell>R</cell><cell>C</cell></row><row><cell>None</cell><cell cols="5">42.3 18.9 18.3 44.9 151.6</cell></row><row><cell>NumberBatch</cell><cell cols="5">42.6 19.6 18.6 44.4 148.1</cell></row><row><cell>Random Rel.</cell><cell cols="5">43.3 19.3 18.6 45.2 151.2</cell></row><row><cell>Single Hop</cell><cell cols="5">42.1 19.9 18.2 44.0 148.6</cell></row><row><cell>Grounded Rel.</cell><cell cols="5">45.9 21.9 20.7 48.0 166.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Commonsense ablations on NarrativeQA valset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>NarrativeQA's commonsense requirements and effectiveness of commonsense selection algorithm.</figDesc><table><row><cell>MHPGM+NOIC better</cell><cell>23%</cell></row><row><cell>MHPGM better</cell><cell>15%</cell></row><row><cell cols="2">Indistinguishable (Both-good) 41%</cell></row><row><cell>Indistinguishable (Both-bad)</cell><cell>21%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Human evaluation on the output quality of the MHPGM+NOIC vs. MHPGM in terms of correctness.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72.</figDesc><table><row><cell></cell><cell></cell><cell>the North American Chapter of the Association for Johannes Welbl, Pontus Stenetorp, and Sebastian</cell></row><row><cell></cell><cell></cell><cell>Computational Linguistics: Human Language Tech-Riedel. 2018. Constructing datasets for multi-hop</cell></row><row><cell></cell><cell></cell><cell>nologies, Volume 2 (Short Papers), volume 2, pages reading comprehension across documents. Transac-</cell></row><row><cell></cell><cell></cell><cell>42-48. tions of the Association of Computational Linguis-</cell></row><row><cell></cell><cell></cell><cell>tics, 6:287-302.</cell></row><row><cell></cell><cell></cell><cell>Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William</cell></row><row><cell></cell><cell></cell><cell>Cohen, and Ruslan Salakhutdinov. 2017. Gated-Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-attention readers for text comprehension. In Pro-der M Rush, Bart van Merriënboer, Armand Joulin, ceedings of the 55th Annual Meeting of the Associa-and Tomas Mikolov. 2016. Towards ai-complete tion for Computational Linguistics (Volume 1: Long question answering: A set of prerequisite toy tasks. Papers), volume 1, pages 1832-1846. In ICLR.</cell></row><row><cell cols="2">Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and Tiejun Zhao. 2016. Constraint-based question an-</cell><cell>Bradley Efron and Robert J Tibshirani. 1994. An intro-Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, duction to the bootstrap. CRC press. and Xiaolong Wang. 2017. Incorporating loose-</cell></row><row><cell cols="2">swering with knowledge graph. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2503-2514.</cell><cell>structured knowledge into LSTM with recall gate for Marjan Ghazvininejad, Chris Brockett, Ming-Wei conversation modeling. In International Joint Con-Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and ference on Neural Networks. Michel Galley. 2018. A knowledge-grounded neural conversation model. In AAAI. Tom Young, Erik Cambria, Iti Chaturvedi, Minlie</cell></row><row><cell></cell><cell></cell><cell>Karl Moritz Hermann, Tomas Kocisky, Edward Huang, Hao Zhou, and Subham Biswas. 2018. Aug-</cell></row><row><cell></cell><cell></cell><cell>Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-menting end-to-end dialog systems with common-</cell></row><row><cell></cell><cell></cell><cell>leyman, and Phil Blunsom. 2015. Teaching ma-sense knowledge. In Proceedings of the Thirty-</cell></row><row><cell></cell><cell></cell><cell>chines to read and comprehend. In Advances in Neu-Second AAAI Conference on Artificial Intelligence</cell></row><row><cell></cell><cell></cell><cell>ral Information Processing Systems, pages 1693-(AAAI-18).</cell></row><row><cell></cell><cell></cell><cell>1701.</cell></row><row><cell cols="2">Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embed-dings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615-620. Gerlof Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. Proceedings of GSCL, pages 31-40. Erik Cambria, Amir Hussain, Tariq Durrani, Catherine</cell><cell>A Supplemental Material Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the at-tention sum reader network. In Proceedings of the A.1 Experimental Setup 54th Annual Meeting of the Association for Compu-Datasets We test our model with and with-tational Linguistics, pages 908-918. out commonsense addition on two challenging Diederik P Kingma and Jimmy Ba. 2015. Adam: A datasets that require multi-hop reasoning and ex-method for stochastic optimization. In 3rd Interna-ternal knowledge: NarrativeQA (Kočiskỳ et al., tional Conference for Learning Representations. 2018) and QAngaroo-WikiHop (Welbl et al.</cell></row><row><cell cols="2">Havasi, Chris Eckl, and James Munro. 2010. Sen-</cell><cell>Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom,</cell></row><row><cell cols="2">tic computing for patient centered applications. In</cell><cell>Chris Dyer, Karl Moritz Hermann, Gáabor Melis,</cell></row><row><cell cols="2">Signal Processing (ICSP), 2010 IEEE 10th Interna-</cell><cell>and Edward Grefenstette. 2018. The narrativeqa</cell></row><row><cell cols="2">tional Conference on, pages 1279-1282. IEEE.</cell><cell>reading comprehension challenge. Transactions</cell></row><row><cell></cell><cell></cell><cell>of the Association of Computational Linguistics,</cell></row><row><cell cols="2">Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana</cell><cell>6:317-328.</cell></row><row><cell cols="2">Inkpen, and Si Wei. 2018. Neural natural language</cell></row><row><cell cols="2">inference models enhanced with external knowl-</cell><cell>J Richard Landis and Gary G Koch. 1977. The mea-</cell></row><row><cell cols="2">edge. In Proceedings of the 56th Annual Meeting of</cell><cell>surement of observer agreement for categorical data.</cell></row><row><cell cols="2">the Association for Computational Linguistics (Vol-</cell><cell>biometrics, pages 159-174.</cell></row><row><cell cols="2">ume 1: Long Papers), pages 2406-2417. Associa-tion for Computational Linguistics.</cell><cell>Chin-Yew Lin. 2004. Rouge: A package for auto-matic evaluation of summaries. Text Summarization</cell></row><row><cell cols="2">Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.</cell><cell>Branches Out.</cell></row><row><cell cols="2">Long short-term memory-networks for machine</cell></row><row><cell cols="2">reading. In Proceedings of the 2016 Conference on</cell><cell>Todor Mihaylov and Anette Frank. 2018. Knowledge-</cell></row><row><cell cols="2">Empirical Methods in Natural Language Process-</cell><cell>able reader: enhancing cloze-style reading compre-</cell></row><row><cell>ing, pages 551-561.</cell><cell></cell><cell>hension with external commonsense knowledge. In</cell></row><row><cell></cell><cell></cell><cell>Proceedings of the 56th Annual Meeting of the As-</cell></row><row><cell cols="2">Christopher Clark and Matt Gardner. 2018. Simple</cell><cell>sociation for Computational Linguistics (Volume 1:</cell></row><row><cell cols="2">and effective multi-paragraph reading comprehen-</cell><cell>Long Papers), pages 821-832. Association for Com-</cell></row><row><cell cols="2">sion. In Proceedings of the 56th Annual Meeting of</cell><cell>putational Linguistics.</cell></row><row><cell cols="2">the Association for Computational Linguistics (Long</cell></row><row><cell>Papers), pages 845-855.</cell><cell></cell></row><row><cell cols="2">Dirk Weissenborn, Tomáš Kočiskỳ, and Chris Dyer.</cell></row><row><cell cols="2">2017. Dynamic integration of background knowl-</cell></row><row><cell>edge in neural NLU systems.</cell><cell>arXiv preprint</cell></row><row><cell>arXiv:1706.02596.</cell><cell></cell></row></table><note>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collab- oratively created graph database for structuring hu- man knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247-1250. AcM.Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William Co- hen, and Ruslan Salakhutdinov. 2018. Neural mod- els for reasoning over multiple mentions using coref- erence. In Proceedings of the 2018 Conference of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Example 1 selected commonsense paths.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Example 2 selected commonsense paths.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Example 3 selected commonsense paths.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A semantic network where the nodes are individual concepts (words or phrases) and the edges describe directed relations between them (e.g., island, UsedFor, vacation ).2  We release all our commonsense extraction code and the extracted commonsense data at: https://github.com/ yicheng-w/CommonSenseMultiHopQA 3 If we are unable to find a relation that satisfies the condition, we keep the steps up to and including the node.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In cases where more than one relation can be used to make a hop, we pick one at random.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful comments. This work was supported by DARPA (YFA17-D17AP00022), Google Faculty Research Award, Bloomberg Data Science Research Grant, and NVidia GPU awards. The views contained in this article are those of the authors and not of the funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
