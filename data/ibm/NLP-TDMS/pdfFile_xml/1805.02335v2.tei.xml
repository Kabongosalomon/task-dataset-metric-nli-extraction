<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-03">3 Dec 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
							<email>ya.jing@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>3⋆</roleName><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-03">3 Dec 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Skeleton-based action recognition</term>
					<term>Spatial reasoning</term>
					<term>Tem- poral stack learning</term>
					<term>Clip-based incremental loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based action recognition has made great progress recently, but many problems still remain unsolved. For example, the representations of skeleton sequences captured by most of the previous methods lack spatial structure information and detailed temporal dynamics features. In this paper, we propose a novel model with spatial reasoning and temporal stack learning (SR-TSL) for skeleton-based action recognition, which consists of a spatial reasoning network (SRN) and a temporal stack learning network (TSLN). The SRN can capture the high-level spatial structural information within each frame by a residual graph neural network, while the TSLN can model the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs. During training, we propose a clip-based incremental loss to optimize the model. We perform extensive experiments on the SYSU 3D Human-Object Interaction dataset and NTU RGB+D dataset and verify the effectiveness of each network of our model. The comparison results illustrate that our approach achieves much better results than the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition is an important and challenging problem in computer vision research. It plays an important role in many applications, such as intelligent video surveillance, sports analysis and video retrieval. Human action recognition can also help robots to have a better understanding of human behaviors, thus robots can interact with people much better <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Recently, there have existed many approaches to recognize human actions, the input data type of which can be grossly divided into two categories: RGB videos <ref type="bibr" target="#b24">[25]</ref> and 3D skeleton sequences <ref type="bibr" target="#b3">[4]</ref>. For RGB videos, spatial appearance and temporal optical flow generally are applied to model the motion dynamics. However, the spatial appearance only contains 2D information that is hard to capture all the motion information, and the optical flow generally needs high computing costs. Compared to RGB videos, Johansson et al. <ref type="bibr" target="#b10">[11]</ref> have explained that 3D skeleton sequences can effectively represent the dynamics of human actions. Furthermore, the skeleton sequences can be obtained by the Microsoft Kinect <ref type="bibr" target="#b32">[33]</ref> and the advanced human pose estimation algorithms <ref type="bibr" target="#b2">[3]</ref>. Over the years, skeleton-based human action recognition has attracted more and more attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26]</ref>. In this paper, we focus on recognizing human actions from 3D skeleton sequences.</p><p>For sequential data, recurrent neural networks (RNNs) perform a strong power in learning the temporal dependencies. There has been a lot of work successfully applying RNNs for skeleton-based action recognition. Hierarchical RNN <ref type="bibr" target="#b3">[4]</ref> is proposed to learn motion representations from skeleton sequences. Shahroudy et al. <ref type="bibr" target="#b23">[24]</ref> introduce a part-aware LSTM network to further improve the performance of the LSTM framework. To model the discriminative features, a spatial-temporal attention model <ref type="bibr" target="#b25">[26]</ref> based on LSTM is proposed to focus on discriminative joints and pay different attentions to different frames. Despite the great improvement in performance, there exist two urgent problems to be solved. First, human behavior is accomplished in coordination with each part of the body. For example, walking requires legs to walk, and it also needs the swing of arms to coordinate the body balance. It is very difficult to capture the high-level spatial structural information within each frame if directly feeding the concatenation of all body joints into networks. Second, these methods utilize RNNs to directly model the overall temporal dynamics of skeleton sequences. The hidden representation of the final RNN is used to recognize the actions. For long-term sequences, the last hidden representation cannot completely contain the detailed temporal dynamics of sequences.</p><p>In this paper, we propose a novel model with spatial reasoning and temporal stack learning (SR-TSL) for this task, which can effectively solve the above challenges. <ref type="figure">Fig. 1</ref> shows the overall pipeline of our model that contains a spatial reasoning network (SRN) and a temporal stack learning network (TSLN). First, we propose a spatial reasoning network to capture the high-level spatial structural features within each frame. The body can be decomposed into different parts, e.g. two arms, two legs and one trunk. The concatenation of joints of each part is transformed into individual spatial feature with a linear layer. These individual spatial features of body parts are fed into a residual graph neural network(RGNN) to capture the high-level structural features between the different body parts, where each node corresponds to a body part. Second, we propose a temporal stack learning network to model the detailed temporal dynamics of the sequences, which consists of three skip-clip LSTMs. For a long-term sequence, it is divided into multiple clips. The short-term temporal information of each clip is modeled with an LSTM layer shared among the clips in a skipclip LSTM layer. When feeding a clip into shared LSTM, the initial hidden of <ref type="figure">Fig. 1</ref>. The overall pipeline of our model which contains a spatial reasoning network and a temporal stack learning network. In the spatial reasoning network, a residual graph neural network (RGNN) is used to capture the high-level spatial structural information between the different body parts. The temporal stack learning network can model the detailed temporal dynamics for skeleton sequence. During training, the proposed model is efficiently optimized with the clip-based incremental losses <ref type="bibr">(CIloss)</ref> shared LSTM is initialized with the sum of the final hidden state of all previous clips, which can inherit previous dynamics to maintain the dependency between clips. We propose a clip-based incremental loss to further improve the ability of stack learning. Therefore, our model can also effectively solve the problem of long-term sequence optimization. Experimental results show that the proposed SR-TSL speeds up the model convergence and improve the performance.</p><p>The main contributions of this paper are summarized as follows:</p><p>1. We propose a spatial reasoning network for each skeleton frame, which can effectively capture the high-level spatial structural information between the different body parts using a residual graph neural network. 2. We propose a temporal stack learning network to model the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs. 3. The proposed clip-based incremental loss further improves the ability of temporal stack learning, which can effectively speed up convergence and obviously improve the performance. 4. Our method obtains the state-of-the-art results on the SYSU 3D Human-Object Interaction dataset and NTU RGB+D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly review the existing literature that closely relates to the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton based action recognition</head><p>There have been amounts of work proposed for skeleton-based action recognition, which can be divided into two classes. The first class is to focus on designing handcrafted features to represent the information of skeleton motion. Wang et al. <ref type="bibr" target="#b28">[29]</ref> exploit a new feature called local occupancy pattern, which can be treated as the depth appearance of joints, and propose an actionlet ensemble model to represent each action. Hussein et al. <ref type="bibr" target="#b9">[10]</ref> use the covariance matrix for skeleton joint locations over time as a discriminative descriptor for a sequence. Vemulapalli et al. <ref type="bibr" target="#b26">[27]</ref> utilize rotations and translations to represent the 3D geometric relationships of body parts in Lie group.</p><p>The second class is to use deep neural networks to recognize human actions. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> exploit the Convolutional Neural Networks (CNNs) for skeletonbased action recognition. Recently, most of methods utilize the Recurrent Neural Networks (RNNs) for this task. Du et al. <ref type="bibr" target="#b3">[4]</ref> first propose an end-to-end hierarchical RNN for skeleton-based action recognition. Zhu et al. <ref type="bibr" target="#b33">[34]</ref> design a fully connected deep LSTM network with a regularization scheme to learn the co-occurrence features of skeleton joints. An end-to-end spatial and temporal attention model <ref type="bibr" target="#b25">[26]</ref> learns to selectively focus on discriminative joints of the skeleton within each frame of the inputs and pays different levels of attention to the outputs of different frames. Zhang et al. <ref type="bibr" target="#b31">[32]</ref> exploit a view adaptive model with LSTM architecture, which enables the network to adapt to the most suitable observation viewpoints from end to end. A two-stream RNN architecture is proposed to model both temporal dynamics and spatial configurations for skeleton-based action recognition in <ref type="bibr" target="#b27">[28]</ref>. The most similar work to ours is <ref type="bibr" target="#b15">[16]</ref> which proposes an ensemble temporal sliding LSTM (TS-LSTM) networks for skeleton-based action recognition. They utilize an ensemble of multi-term temporal sliding LSTM networks to capture short-term, medium-term, long-term temporal dependencies and even spatial skeleton pose dependency. In this paper, we design a spatial reasoning network and temporal stack learning network, which can capture the high-level spatial structural information and the detailed temporal dynamics of skeleton sequences, separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph neural networks</head><p>Recently, more and more works have used the graph neural networks (GNNs) to the graph-structured data, which can be categorized into two broad classes. The first class is to apply Convolutional Neural Networks (CNNs) to graph, which improves the traditional convolution network on graph. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> utilize the CNNs in the spectral domain relying on the graph Laplacian. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref> apply the convolution directly on the graph nodes and their neighbors, which construct the graph filters on the spatial domain. Yan et al. <ref type="bibr" target="#b30">[31]</ref> are the first to apply the graph convolutional neural networks for skeleton-based action recognition. The second class is to utilize the recurrent neural networks to every node of the graph. <ref type="bibr" target="#b22">[23]</ref> proposes to recurrently update the hidden state of each node of the graph. Li et al. <ref type="bibr" target="#b16">[17]</ref> propose a model based on Graph Neural Networks for situation recognition, which can efficiently capture joint dependencies between roles using neural networks defined on a graph. Qi et.al. <ref type="bibr" target="#b21">[22]</ref> use 3D graph neural networks for RGBD semantic segmentation. In this paper, a residual graph neural network is utilized to model the high-level spatial structural information between different body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>In this section, we briefly review the Graph Neural Networks (GNNs), the Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM), which are utilized in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Neural Network</head><p>Graph Neural Network (GNN) is introduced in [23] as a generalization of recursive neural networks, which can deal with a more general class of graphs. The GNNs can be defined as an ordered pair G = {V, E}, where V is the set of nodes and E is the set of edges. At time step t, the hidden state of the i-th (i ∈ {1, ..., |V |}) node is s t i , and the output is o t i . The set of nodes Ω v stands for the neighbors of node v.</p><p>For a GNN, the input vector of each node v ∈ V is based on the information contained in the neighborhood of node v, and the hidden state of each node is updated recurrently. At time step t, the received messages of a node are calculated with the hidden states of its neighbors. Then the received messages and previous state s t−1 i are utilized to update the hidden state s t i . Finally, the output o t i is computed with s t i . The GNN formulation at time step t is defined as follows:</p><formula xml:id="formula_0">m t i = f m {s t−1 i |î ∈ {1, ..., |Ω vi |} (1) s t i = f s m t i , s t−1 i (2) o t i = f o s t i<label>(3)</label></formula><p>where m t i is the sum of all the messages that the neighbors Ω vi send to node v i , f m is the function to compute the incoming messages, f s is the function that expresses the state of a node and f o is the function to produce the output. Similar to RNNs, these functions are the learned neural networks and are shared among different time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RNN and LSTM</head><p>Recurrent Neural Networks (RNNs) are the powerful models to capture the dependencies of sequences via cycles in the network of nodes, which are suitable for the sequence tasks. However, there exist two difficult problems of vanishing gradient and exploding gradient when the standard RNN is used for long-term sequences.</p><p>The advanced RNN architecture of Long Short-Term Memory (LSTM) is proposed by Hochreiter et al. <ref type="bibr" target="#b6">[7]</ref>. LSTM neuron contains an input gate, a forget gate, an output gate and a cell, which can promote the ability to learn long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Architecture</head><p>In this paper, we propose an effective model for skeleton-based action recognition, which contains a spatial reasoning network and a temporal stack learning network. The overall pipeline of our model is shown in <ref type="figure">Fig. 1</ref>. In this section, we will introduce these networks in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spatial Reasoning Network</head><p>Rich inherent structures of the human body that are involved in action recognition task, motivate us to design an effective architecture called spatial reasoning network to model the high-level spatial structural information within each frame. According to the general knowledge, the body can be decomposed into K parts, e.g. two arms, two legs and one trunk (shown in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>), which express the knowledge of human body configuration.</p><p>For spatial structures, the spatial reasoning network encodes the coordinate vectors via two steps (see <ref type="figure">Fig. 1</ref>) to capture the high-level spatial features of skeleton structural relationships. First, the preliminary encoding process maps the coordinate vector of each part into the individual part feature e k , k ∈ {1, ..., K} with a linear layer that is shared among different body parts. Second, all part features e k are fed into the proposed residual graph neural network (RGNN) to model the structural relationships between these body parts. <ref type="figure" target="#fig_0">Fig.  2(b)</ref> shows a RGNN with three nodes.</p><p>For a RGNN, there are K nodes that correspond to the human body parts. At time step t, each node has a relation feature vector r t k ∈ R t , where R t = {r t 1 , ..., r T K }. And r t k denotes the spatial structural relationships of the part k with other parts. We initialize the r t k with the individual part feature e k , such that r 0 k = e k . We use m t ik to denote the received message of node k from node i at time step t, where i ∈ {1, ..., K}. Furthermore, the received messages m t k of (a) (b) node k from all the neighbors Ω v k at time step t is defined as follows:</p><formula xml:id="formula_1">m t k = i∈Ωv k m t ik = i∈Ωv k W m s t−1 i + b m<label>(4)</label></formula><p>where s t−1 i is the state of node i at time step t − 1, and a shared linear layer of weights W m and biases b m will be used to compute the messages for all nodes. After aggregating the messages, updating function of the node hidden state can be defined as follows:</p><formula xml:id="formula_2">s t k = f lstm r t−1 k , m t k , s t−1 k<label>(5)</label></formula><p>where f lstm (·) denotes the LSTM cell function. Then, we calculate the relation representation r t k at time step t via:</p><formula xml:id="formula_3">r t k = r t−1 k + s t k<label>(6)</label></formula><p>The residual design of Eqn.6 aims to add the relationship features between each part based on the individual part features, so that the representations contain the fusion of both features. After the RGNN is updated T times, we extract node-level output as the spatial structural relationships r T k of each part within each frame. Finally, the high-level spatial structural information q of human body for a frame can be computed as follows:</p><formula xml:id="formula_4">r T = concat [r T 1 , r T 2 , ..., r T k ] , ∀k ∈ K (7) q = f r r T (8)</formula><p>where f r (·) is a linear layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Temporal Stack Learning Network</head><p>To further exploit the discriminative features of various actions, the proposed temporal stack learning network further focus on modeling detailed temporal dynamics. For a skeleton sequence, it has rich and detailed temporal dynamics in the short-term clips. To capture the detailed temporal information, the longterm sequence can be decomposed into multiple continuous clips. In a skeleton sequence, it consists of N frames. The sequence is divided into M clips at intervals of d frames. The high-level spatial structural features {Q 1 , Q 2 , ..., Q M } of the skeleton sequence can be extracted from the spatial reasoning network. Q m = {q md+1 , q md+2 , ..., q (m+1)d } is the set of features of clip m, and q n denotes the high-level spatial structural features of the skeleton frame n, n ∈ {1, ..., N }. Our proposed temporal stack learning network is a two stream network: position network and velocity network (see <ref type="figure">Fig. 1</ref>). The two networks have the same architecture, which is composed of three skip-clip LSTM layers (shown in <ref type="figure" target="#fig_1">Fig. 3</ref>). The inputs of position network are the high-level spatial structural features {Q 1 , Q 2 , ..., Q M }. The inputs of velocity network are the temporal differences {V 1 , V 2 , ..., V M } of the spatial features between two consecutive frames, where V m = {v md+1 , v md+2 , ..., v (m+1)d }. v n = q n − q n−1 denotes the temporal difference of high-level spatial features for the skeleton frame n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skip-Clip LSTM Layer</head><p>In the skip-clip LSTM layer, there is an LSTM layer shared among the continuous clips (see <ref type="figure" target="#fig_1">Fig. 3</ref>). For the position network, the spatial features of continuous skeleton frames in the clip m will be fed into the shared LSTM to capture the short-term temporal dynamics in the first skip-clip LSTM layers:</p><formula xml:id="formula_5">h ′ m = f LST M (Q m ) = f LST M {q md+1 , q md+2 , ..., q (m+1)d }<label>(9)</label></formula><p>where h ′ m is the last hidden state of shared LSTM for the clip m, f LST M (·) denotes the shared LSTM in the skip-clip LSTM layer.</p><p>Note that the inputs of LSTM cell between the first skip-clip LSTM layer and the other layers are different (see <ref type="figure" target="#fig_1">Fig. 3</ref>). In order to gain more dependency between two adjacent frames, the input x l t of LSTM cell for the l (l ≥ 2) layer at time step t is defined as follows:</p><formula xml:id="formula_6">x l t = concat h l−1 t−1 , h l−1 t<label>(10)</label></formula><p>where h l−1 t is the hidden state of the l − 1 LSTM layer at time step t. Then the representation of clip dynamics can be calculated as follows: where H m−1 and H m denote the representations of clip m − 1 and m, respectively. The representation H m is to aggregate all the detailed temporal dynamics of the m-th clip and all previous clips to represent the long-term sequence. When feeding the clip m into the shared LSTM layer, we initialize the initial hidden state h 0 m of the shared LSTM with the H m−1 , such that h 0 m = H m−1 , which can inherit previous dynamics to learn the short-term dynamics of the m-th clip to maintain the dependency between clips.</p><formula xml:id="formula_7">H m = H m−1 + h ′ m = m i=1 h ′ i (11)</formula><p>The skip-clip LSTM layer can capture the temporal dynamics of the shortterm clip based on the temporal information of previous clips. And the larger m is, the richer temporal dynamics H m contains.</p><p>Learning the Classier Finally, two linear layers are used to compute the scores for C classes:</p><formula xml:id="formula_8">O m = F o (H m )<label>(12)</label></formula><p>where O m is the score of clip m and O m = (o m1 , o m2 , ..., o mC ), F o denotes the two linear layers. And the output is fed to a softmax classifier to predict the probability being the i th class:</p><formula xml:id="formula_9">y mi = e omi C j=1 e omj , i = 1, ..., C<label>(13)</label></formula><p>whereŷ mi indicates the probability that the clip m is predicted as the i th class. Andŷ m = (ŷ m1 , ...,ŷ mC ) denotes the probability vector of clip m. In order to optimize the model, we propose the clip based incremental losses for a skeleton sequence: <ref type="bibr" target="#b15">(16)</ref> where y = (y 1 , ..., y C ) denotes the groundtruth label. The richer temporal information the clip contains, the greater the coefficient m M is. The clip-based incremental loss will promote the ability of modeling the detailed temporal dynamics for long-term skeleton sequences. Finally, the training loss of our model is defined as follows:</p><formula xml:id="formula_10">L p = − M m=1 m M C i=1 y i logŷ p mi (14) L v = − M m=1 m M C i=1 y i logŷ v mi (15) L s = − M m=1 m M C i=1 y i logŷ s mi</formula><formula xml:id="formula_11">L = L p + L v + L s<label>(17)</label></formula><p>Due to the mechanisms of skip-clip LSTM (see the Eqn.11), the representation H s M of clip M aggregates all the detailed temporal dynamics of the continuous clips from the position sequences and velocity sequences. In the testing process, we only use the probability vectorŷ s M to predict the class of the skeleton sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To verify the effectiveness of our proposed model for skeleton-based action recognition, we perform extensive experiments on the NTU RGB+D dataset <ref type="bibr" target="#b23">[24]</ref> and the SYSU 3D Human-Object Interaction dataset <ref type="bibr" target="#b7">[8]</ref>. We also analyze the performance of our model with several variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU RGB+D Dataset (NTU)</head><p>This is the current largest action recognition dataset with joints annotations that are collected by Microsoft Kinect v2. It has 56880 video samples and contains 60 action classes in total. These actions are performed by 40 distinct subjects. It is recorded with three cameras simultaneously in different horizontal views. The joints annotations consist of 3D locations of 25 major body joints. <ref type="bibr" target="#b23">[24]</ref> defines two standard evaluation protocols for this dataset: Cross-Subject and Cross-View. For Cross-Subject evaluation, the 40 subjects are split into training and testing groups. Each group consists of 20 subjects. For Cross-View evaluation, all the samples of camera 2 and 3 are used for training while the samples of camera 1 are used for testing.</p><p>SYSU 3D Human-Object Interaction dataset (SYSU) This dataset contains 480 video samples in 12 action classes. These actions are performed by 40 subjects. There are 20 joints for each subject in the 3D skeleton sequences. There are two standard evaluation protocols <ref type="bibr" target="#b7">[8]</ref> for this dataset. In the first setting (setting-1), for each activity class, half of the samples are used for training and the rest for testing. In the second setting (setting-2), half of subjects are used to train model and the rest for testing. For each setting, there is 30-fold cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>In all our experiments, we set the hidden state dimension of RGNN to 256. For the NTU dataset, the human body is decomposed into K = 8 parts: two arms, two hands, two legs, one trunk and one head. For the SYSU dataset, there are K = 5 parts: two arms, two legs, and one trunk. We set the length N = 100 of skeleton sequences for the two datasets. The neuron size of LSTM cell in the skip-clip LSTM layer is 512. The learning rate, initiated with 0.0001, is reduced by multiplying it by 0.1 every 30 epochs. The batch sizes for the NTU dataset and the SYSU dataset are 64 and 10, respectively. The network is optimized using the ADAM optimizer <ref type="bibr" target="#b13">[14]</ref>. Dropout with a probability of 0.5 is utilized to alleviate overfitting during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>We compare the performance of our proposed model against several state-ofthe-art approaches on the NTU dataset and SYSU dataset in <ref type="table" target="#tab_0">Table 1 and Table  2</ref>. These methods for skeleton-based action recognition can be divided into two categories: CNN-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref> and LSTM-based methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, we can see that our proposed model achieves the best performances of 84.8% and 92.4% on the current largest NTU dataset. Our performances significantly outperform the state-of-the-art CNN-based method <ref type="bibr" target="#b30">[31]</ref> by about 3.3% and 4.1% for cross-subject evaluation and cross-view evaluation, respectively. Our model belongs to the LSTM-based methods. Compared with VA-LSTM <ref type="bibr" target="#b31">[32]</ref> that is the current best LSTM-based method for action recognition, our results are about 5.4% and 4.8% better than VA-LSTM on the NTU dataset. Ensemble TS-LSTM <ref type="bibr" target="#b15">[16]</ref> is the most similar work to ours. The results of our model outperform by 10.2% and 11.1% compared with <ref type="bibr" target="#b15">[16]</ref> in cross-subject evaluation and cross-view evaluation, respectively. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our proposed model achieves the best performances of 80.7% and 81.9% on SYSU dataset, which significantly outperforms the state-of-the-art approach <ref type="bibr" target="#b31">[32]</ref> by about 3.8% and 4.4% for setting-1 and setting-2, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Analysis</head><p>We analyze the proposed model by comparing it with several baselines. The comparison results demonstrate the effectiveness of our model. There are two key ingredients in the proposed model: spatial reasoning network (SRN) and temporal stack learning network (TSLN). To analyze the role of each component, we compare our model with several combinations of these components. Each variant is evaluated on NTU dataset. FC+LSTM For this model, the coordinate vectors of each body part are encoded with the linear layer and three LSTM layers are used to model the sequence dynamics. It is also a two stream network to learn the temporal dynamics from position and velocity.</p><p>SRN+LSTM Compared with FC+LSTM, this model uses spatial reasoning network to capture the high-level spatial structural features of skeleton sequences within each frame.</p><p>FC+TSLN Compared with FC+LSTM, the temporal stack learning network replaces three LSTM layers to learn the detailed sequence dynamics for skeleton sequences.</p><p>SR-TSL (Position) Compared with our proposed model, the temporal stack learning network of this model only contains the position network.</p><p>SR-TSL (Velocity) Compared with our proposed model, the temporal stack learning network of this model only contains the velocity network.</p><p>SR-TSL It denotes our proposed model. <ref type="table" target="#tab_2">Table 3</ref> shows the comparison results of the variants and our proposed model on NTU and SYSU dataset. We can observe that our model can obviously increase the performances on both datasets. And the increased performances showed in <ref type="table" target="#tab_2">Table 3</ref> illustrate that the spatial reasoning network and temporal stack learning network are effective for the skeleton based action recognition, especially the temporal stack learning network. Furthermore, the two stream architecture of temporal stack learning network is efficient to learn the temporal dynamics from the velocity sequence and position sequence. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the accuracy of the baselines and our model on the testing set of NTU RGB+D dataset during learning phase. We can see that our proposed model can speed up convergence and obviously improve the performance. We also show the process of temporal stack learning in <ref type="figure" target="#fig_4">Fig. 5</ref>. With the increase of m, the much richer temporal information is contained in the representation of a sequence. And the network can consider more temporal dynamics of the details to recognize human action, so as to improve the accuracy. The above results illustrate the proposed SR-TSL can effectively speed up convergence and obviously improve the performance.</p><p>We also discuss the effect of two important hyper-parameters: the time step T of the RGNN and the length d of clips. The comparison results are shown in <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref>. For the time step T , we can find that the performance increases by a small amount when increasing T , and saturates soon. We think that the high-level spatial structural features between a small number of body parts can be learned quickly. For the length d of clips, with the increase of d, the performance is significantly improved and then saturated. The reason of  saturation is that learning short-term dynamic does not require too many frames. The above experimental results illustrate that our proposed model is effective for skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a novel model with spatial reasoning and temporal stack learning for long-term skeleton based action recognition, which achieves much better results than the state-of-the-art methods. The spatial reasoning network can capture the high-level spatial structural information within each frame, while the temporal stack learning network can model the detailed temporal dynamics of skeleton sequences. We also propose a clip-based incremental loss to further improve the ability of stack learning, which provides an effective way to solve long-term sequence optimization. With extensive experiments on the current largest NTU RGB+D dataset and SYSU dataset, we verify the effectiveness of our model for the skeleton based action recognition. In the future, we will further analyze the error samples to improve the model, and consider more contextual information, such as interactions, to aid action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of residual graph neural network (RGNN). (a) illustrates five human pose parts and a corresponding RGNN. (b) shows the principle of a RGNN with three nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of three skip-clip LSTM layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Our proposed temporal stack learning network is a two stream network, so the clip dynamic representations (H p m , H v m and H s m ) of three modes will be captured. H p m and H v m denote the dynamic representations extracted from the position and velocity for the clip m, respectively. And H s m is the sum of H p m and H v m . The probability vectors (ŷ p m ,ŷ v m andŷ s m ) can be predicted from the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The accuracy of the baselines and our model on the testing set of NTU RGB+D dataset during learning phase. (a) shows the comparison results for cross-subject evaluation, and (b) is for cross-view evaluation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The accuracy of the increasing clips on the testing set of NTU RGB+D dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The comparison results on NTU RGB+D dataset with Cross-Subject and Cross-View settings in accuracy (%)</figDesc><table><row><cell>Methods</cell><cell>Cross-Subject</cell><cell>Cross-View</cell></row><row><cell>HBRNN-L [4] (2015)</cell><cell>59.1</cell><cell>64.0</cell></row><row><cell>Part-aware LSTM [24] (2016)</cell><cell>62.9</cell><cell>70.3</cell></row><row><cell>Trust Gate ST-LSTM [18] (2016)</cell><cell>69.2</cell><cell>77.7</cell></row><row><cell>Two-stream RNN [28] (2017)</cell><cell>71.3</cell><cell>79.5</cell></row><row><cell>STA-LSTM [26] (2017)</cell><cell>73.4</cell><cell>81.2</cell></row><row><cell>Ensemble TS-LSTM [16] (2017)</cell><cell>74.6</cell><cell>81.3</cell></row><row><cell>Visualization CNN [19] (2017)</cell><cell>76.0</cell><cell>82.6</cell></row><row><cell>VA-LSTM [32] (2017)</cell><cell>79.4</cell><cell>87.6</cell></row><row><cell>ST-GCN [31] (2018)</cell><cell>81.5</cell><cell>88.3</cell></row><row><cell>SR-TSL (Ours)</cell><cell>84.8</cell><cell>92.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The comparison results on SYSU dataset in accuracy (%)</figDesc><table><row><cell>Methods</cell><cell>Setting-1</cell><cell>Setting-2</cell></row><row><cell>LAFF [9] (2016)</cell><cell>-</cell><cell>54.2</cell></row><row><cell>Dynamic Skeletons [8] (2015)</cell><cell>75.5</cell><cell>76.9</cell></row><row><cell>VA-LSTM [32] (2017)</cell><cell>76.9</cell><cell>77.5</cell></row><row><cell>SR-TSL (Ours)</cell><cell>80.7</cell><cell>81.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The comparison results on NTU and SYSU dataset in accuracy (%). We compare the performances of several variants and our proposed model to verify the effectiveness of our model</figDesc><table><row><cell>Methods</cell><cell>NTU Cross-Subject</cell><cell>Cross-View</cell><cell cols="2">SYSU Setting-1 Setting-2</cell></row><row><cell>FC + LSTM</cell><cell>77.0</cell><cell>84.7</cell><cell>39.9</cell><cell>40.7</cell></row><row><cell>SRN + LSTM</cell><cell>78.7</cell><cell>87.3</cell><cell>42.1</cell><cell>44.4</cell></row><row><cell>FC + TSLN</cell><cell>83.8</cell><cell>91.6</cell><cell>77.3</cell><cell>77.4</cell></row><row><cell>SR-TSL(Position)</cell><cell>78.8</cell><cell>88.2</cell><cell>77.1</cell><cell>76.9</cell></row><row><cell>SR-TSL(Velocity)</cell><cell>82.2</cell><cell>90.6</cell><cell>71.7</cell><cell>71.8</cell></row><row><cell>SR-TSL (Ours)</cell><cell>84.8</cell><cell>92.4</cell><cell>80.7</cell><cell>81.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>The comparison results on NTU dataset in accuracy (%). We compare several models that have different time steps for the RGNN to show the improvements achieved at every step The comparison results on NTU dataset in accuracy (%). We compare the performances of several proposed models that have different the length d of clips</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>TSLN</cell><cell cols="2">Cross-Subject Cross-View</cell></row><row><cell cols="3">RGNN Cross-Subject Cross-View</cell><cell>d = 2</cell><cell>81.6</cell><cell>90.6</cell></row><row><cell>T = 1</cell><cell>84.1</cell><cell>92.0</cell><cell>d = 4</cell><cell>84.1</cell><cell>91.4</cell></row><row><cell>T = 2</cell><cell>84.4</cell><cell>92.2</cell><cell>d = 6</cell><cell>84.5</cell><cell>92.4</cell></row><row><cell>T = 3</cell><cell>84.5</cell><cell>92.4</cell><cell>d = 8</cell><cell>84.5</cell><cell>92.3</cell></row><row><cell>T = 4</cell><cell>84.7</cell><cell>92.3</cell><cell>d = 10</cell><cell>84.8</cell><cell>92.1</cell></row><row><cell>T = 5</cell><cell>84.8</cell><cell>92.3</cell><cell>d = 15</cell><cell>84.7</cell><cell>92.2</cell></row><row><cell>T = 6</cell><cell>84.7</cell><cell>92.2</cell><cell>d = 20</cell><cell>84.4</cell><cell>92.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACM Computing Surveys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Human activity recognition from 3d data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Pattern Recognition Letters</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Real-time rgb-d activity prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B W Z A S Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A survey of vision-based methods for action representation, segmentation and recognition. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaoou Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
