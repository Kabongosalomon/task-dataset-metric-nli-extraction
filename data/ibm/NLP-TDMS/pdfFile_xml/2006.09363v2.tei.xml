<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building One-Shot Semi-supervised (BOSS) Learning up to Fully Supervised Performance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
							<email>leslie.smith@nrl.navy.mil</email>
							<affiliation key="aff0">
								<orgName type="institution">US Naval Research Laboratory Washington</orgName>
								<address>
									<region>DC</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Adam Conovaloff NRC Postdoctoral Fellow US Naval Research Laboratory Washington</orgName>
								<address>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Building One-Shot Semi-supervised (BOSS) Learning up to Fully Supervised Performance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reaching the performance of fully supervised learning with unlabeled data and only labeling one sample per class might be ideal for deep learning applications. We demonstrate for the first time the potential for building one-shot semi-supervised (BOSS) learning on Cifar-10 and SVHN up to attain test accuracies that are comparable to fully supervised learning. Our method combines class prototype refining, class balancing, and self-training. A good prototype choice is essential and we propose a technique for obtaining iconic examples. In addition, we demonstrate that class balancing methods substantially improve accuracy results in semi-supervised learning to levels that allow self-training to reach the level of fully supervised learning performance. Rigorous empirical evaluations provide evidence that labeling large datasets is not necessary for training deep neural networks. We made our code available at https://github.com/lnsmith54/BOSS to facilitate replication and for use with future real-world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, deep learning has achieved state-of-theart performance for computer vision tasks such as image classification. However, a major barrier to the wider-spread adoption of deep neural networks for new applications is that training state-of-the-art deep networks typically requires thousands to millions of labeled samples to perform at high levels of accuracy and to generalize well.</p><p>Unfortunately, manual labeling is labor intensive and might not be practical if labeling the data requires specialized expertise, such as in medical, defense, and scientific applications. In typical real-world scenarios for deep learning, one often has access to large amounts of unlabeled data but lacks the time or expertise to label the required mas-sive numbers needed for training, validation, and testing. An ideal solution might be to achieve performance levels that are equivalent to fully supervised trained networks with only one manually labeled image per class.</p><p>In this paper we investigate the potential for building one-shot semi-supervised (BOSS) learning up to achieve comparable performance as fully supervised training. To date, one-shot semi-supervised learning has been little studied and viewed as difficult. We build on the recent observation that one-shot semi-supervised learning is plagued by class imbalance problems <ref type="bibr" target="#b20">[21]</ref>. In our context, class imbalance refers to a trained network with near 100% accuracy on a subset of classes and poor performance on other classes. However, we are the first to apply data imbalance methods to unlabeled data.</p><p>Specifically, we demonstrate that good prototypes are crucial for successful semi-supervised learning and propose a prototype replacement method for the poorly performing classes. Also, we make use of the state-of-the-art in semisupervised learning methods (i.e., FixMatch <ref type="bibr" target="#b22">[23]</ref>) in our experiments. To combat class imbalance, we tested several variations of methods found in the literature for data imbalance problems <ref type="bibr" target="#b11">[12]</ref>, which refers to the situation where the number of training samples per class vary substantially. We are the first to demonstrate that these methods significantly boost the performance of one-shot semi-supervised learning. Combining these methods with self-training <ref type="bibr" target="#b18">[19]</ref> makes it possible on Cifar-10 and SVHN to attain comparable performance as fully supervised trained deep networks.</p><p>Our contributions are:</p><p>1. We rigorously demonstrate for the first time the potential for one-shot semi-supervised learning to reach test accuracies with Cifar-10 and SVHN that are comparable to fully supervised learning.</p><p>2. We propose the concept of class balancing on unlabeled data and investigate their value of for one-shot semi-supervised learning. We introduce a novel measure of minority and majority classes and propose four class balancing methods that improve the performance of semi-supervised learning.</p><p>3. We investigate the causes for poor performance and hyper-parameter sensitivity. We hypothesis two causes and demonstrate solutions that improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semi-supervised learning: Semi-supervised learning is a hybrid between supervised and unsupervised learning, which combines the benefits of both and is better suited to real-world scenarios where unlabeled data is abundant. As with supervised learning, semi-supervised learning defines a task (i.e., classification) from labeled data but typically it requires much fewer labeled samples. In addition, semi-supervised learning leverages feature learning from unlabeled data to avoid overfitting the limited labeled samples. Semi-supervised learning is a large and mature field and there are several surveys and books on semi-supervised learning methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref> for the interested reader. In this Section we mention only the most relevant of recent methods.</p><p>Recently there have been a series of papers on semisupervised learning from Google Reseach, including Mix-Match <ref type="bibr" target="#b3">[4]</ref> , ReMixMatch <ref type="bibr" target="#b2">[3]</ref>, and FixMatch <ref type="bibr" target="#b22">[23]</ref>. Mix-Match combines consistency regularization with data augmentation <ref type="bibr" target="#b19">[20]</ref>, entropy minimization (i.e., sharpening) <ref type="bibr" target="#b9">[10]</ref>, and mixup <ref type="bibr" target="#b34">[35]</ref>. ReMixMatch improved on MixMatch by incorporating distribution alignment and augmentation anchors. Augmentation anchors are similar to pseudolabeling. FixMatch is the most recent and demonstrated state-of-the-art semi-supervised learning performance. In addition, the FixMatch paper has a discussion on one-shot semi-supervised learning with Cifar-10.</p><p>The FixMatch algorithm <ref type="bibr" target="#b22">[23]</ref> is primarily a combination of consistency regularization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref> and pseudo-labeling <ref type="bibr" target="#b15">[16]</ref>. Consistency regularization utilizes unlabeled data by relying on the assumption that the model should output the same predictions when fed perturbed versions as on the original image. Consistency regularization has recently become a popular technique in unsupervised, self-supervised, and semi-supervised learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>. Several researchers have observed that strong data augmentation should not be used when infering pseudo-labels for the unlabeled data but should be employed for consistency regularization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>. Pseudo-labeling is based on the idea that one can use the model to obtain artificial labels for unlabeled data by retaining pseudo-labels for samples whose probability are above a predefined threshold.</p><p>A recent survey of semi-supervised learning <ref type="bibr" target="#b26">[27]</ref> provides a taxonomy of classification algorithms. One of the methods in semi-supervised learning is self-training iterations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19]</ref> where a classifier is iteratively trained on labeled data plus high confidence pseudo labeled data from previous iterations. In our experiments we found that selftraining provided a final boost to make the performance comparable to supervised training with the full labeled training dataset.</p><p>Class imbalance: Smith and Conovaloff <ref type="bibr" target="#b20">[21]</ref> demonstrated that in one-shot semi-supervised learning there are large variation in class performances, with some classes achieving near 100% test accuracies while other classes near 0% accuracies. That is, strong classes starve the weak classes, which is analogous to the class imbalance problem <ref type="bibr" target="#b11">[12]</ref>. This observation suggests an opportunity to improve the overall performance by actively improving the performance of the weak classes.</p><p>We borrowed techniques from the literature on training with imbalanced data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24]</ref> (i.e., some classes having many more training samples than other classes) to experiment with several methods for improving the performance of the weak classes. However, with unlabeled data, labels to define the ground truth as to minority and majority classes do not exist. In this paper, we propose using the pseudolabels as a surrogate to the ground truth for example class counting. Our experiments demonstrate that combining the counting of the pseudo-labels and methods for handling data imbalance substantially improves performance. Methods for handling class imbalance can be grouped into two categories: data-level and algorithm-level methods. Datalevel techniques <ref type="bibr" target="#b29">[30]</ref> reduce the level of imbalance by undersampling the majority classes and oversampling the minority classes. Algorithm-level techniques <ref type="bibr" target="#b23">[24]</ref> are commonly implemented with smaller loss factor weights for the training samples belonging to the majority classes and larger weights for the training samples belonging to the minority classes. In our experiments we tested variations of both types of methods and a hybrid of the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-learning:</head><p>Our scenario superficially bears similarity to few-shot meta learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref>, which is a highly active area of research. The majority of the work in this area relies on a large labeled dataset with similar data statistics but this can be an onerous requirement for new applications. While there are some recent efforts in unsupervised pretraining for few-shot meta learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>, our experiments with these methods demonstrated their inability to adequately perform in one-shot learning to bootstrap our process. Specifically, unsupervised one-shot learning with only five classes obtained a test accuracy of about 50% on high confidence samples and the accuracy dropped sharply when increasing the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BOSS Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">FixMatch</head><p>Since we build on FixMatch <ref type="bibr" target="#b22">[23]</ref>, we briefly describe the algorithm and adopt the formalism used in the original paper. For an N-class classification problem, let us define is a hyperparameter that determines the ratio of U to χ. Let p m (y|x) be the predicted class distribution produced by the model for input x b . We denote the cross-entropy between two probability distributions p and q as H(p, q).</p><formula xml:id="formula_0">χ = {(x b , y b ) : b ∈ (1, ..., B)}</formula><p>The loss function for FixMatch consists two terms: a supervised loss L s applied to labeled data and an unsupervised loss L u for the unlabeled data. L s is the cross-entropy loss on weakly augmented labeled examples:</p><formula xml:id="formula_1">L s = 1 B B b=1 H(y b , p m (y|α(x b ))) (1) where α(x b ) represent weak data augmentation on labeled sample x b .</formula><p>For the unsupervised loss, the algorithm computes the label based on weakly augmented versions of the image as q b = p m (y|α(u b )). It is essential that the label is computed on weakly augmented versions of the unlabeled training samples and not on strongly augmented versions. The pseudo-label is computed asq b = arg max(q b ) and the unlabeled loss is given as:</p><formula xml:id="formula_2">L u = 1 µ µ b=1 1(max(q b ) ≥ τ )H(q b , p m (y|A(u b ))) (2)</formula><p>where A(u b ) represents applying strong augmentation to sample u b and τ is a scalar confidence threshold that is used to include only high confidence terms. The total loss is given by L = L s + λ u L u where λ u is a scalar hyperparameter. Additional details on the FixMatch algorithm are available in the paper <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prototype refining</head><p>Previous work by Sohn, et al.</p><p>[23] on one-shot semisupervised learning relied on the dataset labels to randomly choose an example for each class. The authors demonstrated that the choice of these samples significantly affected the performance of their algorithm. Specifically, they ordered the CIFAR-10 training data by how representative they were of their class by utilizing fully supervised trained models and found that using more prototypical examples achieved a median accuracy of 78% while the use of poorly representative samples failed to converge at all. The authors acknowledged that their method for finding prototypes was not practical. In contrast, we now present a practical approach for choosing an iconic prototype for each class.</p><p>In real-world scenarios, one's data is initially all unlabeled but it is not overly burdensome for an expert to manually sift through some of their dataset to find one iconic example of each class. In choosing iconic images of each class, the labeler's goal is to pick images that represent the class objects well, while minimizing the amount of background distractors in the image. While the labeler is choosing the most iconic examples to be class prototypes for oneshot training of the network, it is beneficial to designate the less representative examples as part of a validation or test dataset. In our own experiments with labeled datasets Cifar-10 and SVHN, we did not rely on the training labels but reviewed a small fraction of the training data to manually choose class prototypes.</p><p>In addition, we also propose a simple iterative technique for improving the choice of prototypes because good prototypes are important to good performance. After choosing prototypes, the next step is to make a training run and examine the class accuracies. For any class with poor accuracy relative to the other classes, it is likely that a better prototype can be chosen. We recommend returning to the unlabeled dataset to find replacement prototypes for only the poorly performing classes. In our experiments we found doing this even once to be beneficial.</p><p>One might argue that prototype refining is as much work as labeling several examples per class and using many training samples will make it easier to train the model. From only a practical perspective, labeling five or ten examples per class is not substantially more effort relative to labeling only one iconic example per class and prototype refining. While in practice one may want to start with more than one example for ease of training, there are scientific, educational, and algorithmic benefits to studying one-shot semi-supervised learning, which we discuss in our supplemental materials. Also, non-representative examples can be included in a labeled test or validation dataset for use in evaluating the quality of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Class balancing</head><p>We believe a class imbalance problem is an important factor in training neural networks, not only in one-shot semi-supervised learning but also a factor for small to midsized datasets. It is typical that a network with random weights usually outputs a single class label for every sample (i.e., randomly initialized networks do not generate random predictions). Hence, all networks start their training with elements of the class imbalance problem but the presence of large, balanced training data allows the network to overcome this problem. Since class imbalance is always present when training deep networks, class balancing meth-ods might always be valuable, particularly when training on one-shot, few-shot, or small labeled datasets, and we leave further investigations of this for future work.</p><p>Unlike the data imbalance domain, the ground truth imbalance proportions are unknown with unlabeled datasets. Our innovation here is to use the model generated pseudolabels as a surrogate for class counting and estimating class imbalance ratios (i.e., determining majority and minority classes ). Specifically, as the algorithm computes the pseudo-labels for all of the unlabeled training samples, it counts the number that fall within each class, which we designate as C = {c n : n ∈ (1, ..., N )} where N is the number of classes. We assume a similar number of unlabeled samples in each class so the number of pseudo-labels in each class should also be similar.</p><p>Our first class balancing method is based on oversampling minority classes. Our algorithm reduces the pseudolabeling thresholds for minority classes to include more examples of the minority classes in the training. Formally, in pseudo-labeling the following unsupervised loss function is used for the unlabeled data in place of Equation 2:</p><formula xml:id="formula_3">L u = 1 µ µ b=1 1(max(q b ) ≥ τ n )H(q b , q b ) (3) where q b = p m (y|A(u b )),q b = arg max(q b )</formula><p>, and τ n is the class dependent threshold for inclusion in the unlabeled loss L u . We define the class dependent thresholds as:</p><formula xml:id="formula_4">τ n = τ − ∆(1 − c n max(C) )<label>(4)</label></formula><p>where c n is the number of pseudo-labeled in class n, max(C) is the maximum count of all the classes, and ∆ is a scalar hyper-parameter (τ &gt; ∆ &gt; 0) guiding how much to lower the threshold for minority classes. Hence, the most frequent class will use a threshold of τ while minority classes will use lower thresholds, down to τ − ∆. The next two class balancing methods are variations on loss function class weightings. In the FixMatch algorithm, all unlabeled samples above the threshold are included in <ref type="figure">Equation 3</ref> with the same weight. Instead, our second class balancing algorithm becomes:</p><formula xml:id="formula_5">L u = 1 Zµ µ b=1 1(max(q b ≥ τ n ))H(q b , q b )/c n (5)</formula><p>where the loss terms are divided by c n and Z is a normalizing factor that makes L u the same magnitude as without this weighting scheme (this allows the unlabeled loss weighting λ u to remain the same).</p><p>Our third class balancing algorithm is identical to the previous method except it uses an alternate class countĉ u in Equation 4. We defineĉ u using only the high confidence pseudo-labeled samples (i.e., samples that are above the threshold). The intuition of this third method is that each of the classes should contribute equally to the loss L u (i.e., each sample's loss is divided by the number of samples of that class included in L u ). In practice, this method's weights might be an order of magnitude larger than the previous method's weights, which might contribute to training instability, so we compare both methods in Section 4.2.</p><p>Our fourth class balancing algorithm is a hybrid of the data and algorithmic methods. Specifically, it is a combination of our class balancing methods 1 and 3. Our experiments with this hybrid method demonstrates the benefits of combining the class balancing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Self-training iterations</head><p>Labeled and unlabeled data play different roles in semisupervised learning. Here we propose self-training iterations where the pseudo-labels of the highest confidence unlabeled training samples are combined with labeled samples in a new iteration. Increasing the number of labeled samples per class improves performance, and substantially reduces training instability and performance variability. Although some of these pseudo-labels might be wrong, we rely on the observation that the training of deep networks are robust to small amounts of labeling noise (i.e., labeling noise of less than 10% does not harm the trained network's performance <ref type="bibr" target="#b0">[1]</ref>). Hence, we aimed to achieve a 90% accuracy from semi-supervised learning with the class balancing methods.</p><p>Self-training in BOSS adds to the testing stage a computation of the model predictions on all of the unlabeled training data. These are sorted from the the highest prediction probabilities down and the dataset is saved. After the original training run, the labeled data can be combined with a number of the highest prediction samples from each class and a subsequent self-training iteration run can use the larger labeled dataset for retraining a new network. We experimented with labeling 5, 10, 20, and 40 of the top predictions per class and the results are reported in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this Section we demonstrate that the BOSS algorithms can achieve comparable performance with fully-supervised training of Cifar-10 <ref type="bibr" target="#b13">[14]</ref> and SVHN <ref type="bibr" target="#b17">[18]</ref>. We compare our results to FixMatch 1 <ref type="bibr" target="#b22">[23]</ref> and demonstrate the value of our approach. Our experiments use a Wide ResNet-28-2 <ref type="bibr" target="#b32">[33]</ref> that matches the FixMatch reported results and we used the same cosine learning rate schedule described by Sohn, et al. <ref type="bibr" target="#b22">[23]</ref>. We repeated our experiments with a ShakeNet model <ref type="bibr" target="#b8">[9]</ref> and obtained similar result that lead to the same insights and conclusions. Our hyper-parameters Set airplane auto bird cat deer dog frog horse ship truck Mean <ref type="table" target="#tab_1">1  29  98  71  89  97  16  98  97  97  97  79  2  28  99  70  43  97  89  98  97  98  0  72  3  96  98  63  20  97  96  98  87  98  97  86  4  29  98  65  10  96  32  98  97  97  96  72  5  28  97  70  46  96  48  53  76  96  97  72  6  80  98  71  52  97  92  98  87  98  97  82  7  28  99  75  54  95  86  95  86  96  94  83   Table 1</ref>. Class accuracies. One-shot semi-supervised average (of 2 runs) class accuracies for Cifar-10 test data with the FixMatch model, that was trained on sets of manually chosen prototypes for each class. Prototype set 6 was modified from set 2 and prototype set 7 was modified from set 4 (i.e., prototype refining).</p><p>were in a small range and the specifics are provided in the supplementary materials. For data and data augmentation, we used the default augmentation in FixMatch but additional experiments (not shown) did show that using RandAugment <ref type="bibr" target="#b5">[6]</ref> for strong data augmentation provides a slight improvement. Our runs with fully supervised learning of the Wide ResNet-28-2 model produced a test accuracy of 94.9 ± 0.3% for Cifar-10 <ref type="bibr" target="#b13">[14]</ref> and test accuracy of 98.26 ± 0.04% for SVHN <ref type="bibr" target="#b17">[18]</ref>, which we use for our basis of comparison. We made our code available at https://github.com/lnsmith54/BOSS to facilitate replication and for use with future real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Choosing prototypes and prototype refining</head><p>For our experiments with Cifar-10, we manually reviewed the first few hundred images and choose five sets of prototypes that we will refer to as class prototype sets 1 to 5. However, the practioner need only create one set of class prototypes and can perform prototype refining, as we describe below. <ref type="table">Table 1</ref> presents the averaged (over two runs) test accuracies for each class, computed from FixMatch on the Cifar-10 test dataset for each of the prototype sets 1 to 5. This Table illustrates that a good choice of prototypes (i.e., set=3) can lead to good performance in most of the classes, which enables a good overall performance. <ref type="table">Table 1</ref> also shows that for other sets the class accuracies can be quite high for some classes while low for other classes. Hence, the poor performance of some classes implies that the choice of prototypes for these classes in those sets can be improved. In prototype refining, one simply reviews the class accuracies to find which prototypes should be replaced.</p><p>We demonstrate prototype refining with two examples. The airplane and truck class accuracies in set 2 are poor so we replaced these two prototypes and name this set 6. In set 4, the cat and dog classes are performing poorly so we replaced these two prototypes and name this set 7. <ref type="table">Table 1</ref> shows the class accuracies for sets 6 and 7 and these results are better than the original sets; that is, prototype refining of these two sets raised the overall test accuracies from 72% up to 82-83%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Class balancing</head><p>In this Section we report the results from FixMatch and demonstrate substantial improvements with the class balancing methods in BOSS. <ref type="table">Table 2</ref> presents our main results, which illustrates the benefits from prototype refining, class balancing, and one self-training iteration. The first five rows in the table list the results for the five sets of class prototypes (i.e., 1 prototype per class) for Cifar-10. Rows for sets 6 and 7 provides the results for prototype refining of the original sets 2 and 4, respectively. The FixMatch column shows results (i.e., average and standard deviation over four runs) for the original FixMatch code on the prototype sets. The number within brackets <ref type="bibr">[.]</ref> are results from a PyTorch reimplementation of FixMatch, that we discuss below.</p><p>The next four columns presents the BOSS results with class balancing methods. As described in Section 3.3, class balance method 1 represents oversampling of minority classes, balance methods 2 and 3 are two forms of classbased loss weightings, and balance = 4 is a hybrid that combines balance methods 1 and 3. The use of class balancing significantly improves on the original FixMatch results, with increases of up to 20 absolute percentage points. Generally, the hybrid class balance method 4 is best, except when instabilities hurt the performance. The performance is generally in the 90% range with good performance across all the classes, which enables the self-training iteration to bump the accuracies to be comparable to the test accuracy from supervised training on the full labeled training dataset. <ref type="table">Table 2</ref> indicates that good class prototypes (i.e., sets 3, 6, and 7) result in test accuracies near 90% and low variance between runs. However, when some of the class prototypes are inferior, some of the of the training runs exhibit instabilities that cause lower averaged accuracies and higher variance. We provide a discussion in Section 4.5 on the cause of these instabilities and on how to improve these results.</p><p>PyTorch version: We have taken advantage of a Py-BOSS balance method Torch reimplementation 2 of the original TensorFlow version of the FixMatch code to test our proposed BOSS methods in PyTorch. <ref type="table">Table 2</ref> reports the best test accuracies for the PyTorch version in the brackets <ref type="bibr">[.]</ref>. It is clear to us that the researcher who reimplemented FixMatch in PyTorch took care to replicate FixMatch. In training with 4 labeled samples per class, his code obtained a test accuracy of 89 ± 5% for Cifar-10, compared to results of 87 ± 3% reported in the paper. However, it is also clear from our experiments and <ref type="table">Table 2</ref> that there are substantial differences between the TensorFlow and PyTorch versions when comparing one-shot semi-supervised learning. We suggest that the sensitivity of one-shot semi-supervised learning reveals even minor differences that are invisible in fully supervised learning.</p><p>The PyTorch implementation results shown in <ref type="table">Table 2</ref> also shows that the class balancing methods improve the test accuracy over FixMatch. In particular, class balance method 1 (i.e., oversampling) appears to improve the test accuracy more than the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Self-training iterations</head><p>The final four columns of <ref type="table">Table 2</ref> list the results of performing one self-training iteration. The self-training was initialized with the original single labeled sample per class, plus the most confident pseudo-labeled examples from the BOSS training run that is highlighted in bold. For example, the '+5' columns means that five pseudo-labeled examples per class were combined with the original labeled prototypes to make a set with a total of 60 labeled examples. These self-training results demonstrate that one-shot semisupervised learning can reach comparable performance to the results from fully supervised training (i.e., 94.9%), often with adding as few as 5 samples per class. However, we expect that in practice, self-training by adding more samples per class will prove more reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">SVHN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVHN is obtained from house numbers in Google Street</head><p>View images and is used for recognizing digits (i.e., 0 -9) in natural scene images. Visual review of the images show that the training samples are of poor quality (i.e., blurry) and often contain distractors (i.e., multiple digits in an image). Because of the quality issue, we needed to review several hundred unlabled training samples in order to find four class prototype sets that are reported in <ref type="table" target="#tab_1">Table 3</ref>.</p><p>Even though the SVHN training images are of poorer quality than the Cifar-10 training images, one-shot semisupervised learning with FixMatch on sets of prototypes produced higher test accuracies than with Cifar-10. <ref type="table" target="#tab_1">Table  3</ref> presents equivalent results for the SVHN dataset as those results that were reported in <ref type="table">Table 2</ref> for Cifar-10. Since the results for FixMatch are all above 89%, we did not perform prototype refining on any of these sets. However, here too the class balancing methods increase the test accuracies above the FixMatch results. With these four class pro-  totype sets, class balance method 1 produces the best results. The test accuracies from balance method 1 are approximately 1% lower than the fully supervised results of 98.26 ± 0.04%. The improvements from self-training were small and the best results fell about 0.5% below the results of of fully supervised training. We believe the differences between Cifar-10 and SVHN are related to the natures of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Investigation of training instabilities</head><p>In our experiments we observed high sensitivity of oneshot semi-supervised learning performance to the choices for the hyper-parameters and the class prototypes sets. That is, we observed that good choices for the prototypes and prototype refining significantly reduced the instabilities and the variability of the results (i.e., few instabilities were encountered for Cifar-10 prototype sets 3, 6, and 7 so the final accuracies were higher and the standard deviations of the results were lower). In sets where the performance was inferior, there was always at least one class that performed poorly. In addition, we found that the hyper-parameter values made a significant difference.</p><p>We investigated the cases of poor performance and discovered that there were two different situations. <ref type="figure" target="#fig_1">Figure 1</ref> provides examples of test accuracies during the training for both situations. The blue curve is the test accuracy where in one training run the network learns a final test accuracy of 77% We hypothesize that in this situation the network can get stuck in a poor local minimum that is due to poor prototype choices and can be improved with prototype refining or by hyper-parameter fine tuning. The red curve in <ref type="figure" target="#fig_1">Figure  1</ref> is an example of the other case and here the training is dominated by instabilities (i.e., where the model suddenly diverges during training) and the final test accuracy is 65%. Interestingly, we found that it is important when tuning the hyper-parameters to identify which scenario is occurring.</p><p>Our experiments with training instabilities (i.e., the red curve) implied that they can be caused by too much class balancing. We hypothesize that when the model struggles to classify some of the classes, the class balancing methods can force the pseudo-labeling to mislabel samples in order to have the appearance of class balance. In these cases, it is better to reduce the amount of class balancing by using a smaller value for ∆ for class balance methods 1 and 4, and using a smaller value for λ u for class balance methods 2 and 3. In addition, we observed that decreasing weight decay (WD) and the learning rate (LR) improves performance when there were instabilities.</p><p>On the other hand, if the inferior performance is due to poor local minimum (i.e., the blue curve), one can either improve the class prototypes (i.e., prototype refining) or increase the amount of class balancing. This is the opposite of what should do for instabilities; that is, one can use a larger value for ∆ for class balance methods 1 and 4, use a larger value for λ u for class balance methods 2 and 3, or increase weight decay (WD) and the learning rate (LR). We also observed that it helps to increase τ if there are instabilities and to decrease τ in the poor local minimum situation. <ref type="table" target="#tab_2">Table 4</ref> demonstrates how to improve the results presented in <ref type="table">Table 2</ref> (for consistency we used the same hyperparameter values for all of the class balance runs shown in <ref type="table">Table 2</ref>).  We list the class prototype set (Set), the BOSS class balancing method (Balance), weight decay (WD), initial learning rate (LR), the change in the confidence threshold for minority classes (∆), the unlabeled loss multiplicative factor (λ u ), the confidence threshold (τ ), and the final test accuracy. Furthermore, we provide a short description that indicates if the training curve displays instabilities (i.e., the red curve in <ref type="figure" target="#fig_1">Figure 1</ref>) or a poor local minimum (i.e., the blue curve). Or the description points out the hyper-parameters that were tuned to improve the performance. The examples in <ref type="table" target="#tab_2">Table 4</ref> show improved results for both the problem of instability and for poor local minimums. The examples include modifying ∆, weight decay, learning rate, and τ . In most cases the final accuracies are improved substantially with small changes in the hyper-parameter values. This demonstrates the sensitivity of one-shot semisupervised learning to hyper-parameter values.</p><p>While this sensitivity can be challenging in practice, we note that this sensitivity can also lead to new opportunities. For example, often researchers propose new network architectures, loss functions, and optimization functions that are tested in the fully supervised regime where small performance gains are used to claim a new stateof-the-art. If these algorithms were instead tested in oneshot semi-supervised learning, more substantial differences in performance would better differentiate methods. Along these lines, we also advocate the use of one-shot semisupervised learning with AutoML and neural architecture search (NAS) <ref type="bibr" target="#b6">[7]</ref> to find optimal hyper-parameters and architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>The BOSS methodology relies on simple concepts: choosing iconic training samples with minimal background distractors, employing class balancing techniques, and selftraining with the highest confidence pseudo-labeled samples. Our experiments in Section 4 demonstrate the potential of training a network with only one sample per class and we have confirmed the importance of class balancing methods. While our methods have limitations (as discussed in the supplemental materials), this paper breaks new ground in one-shot semi-supervised learning and attains high performance. BOSS brings one-shot and fewshot semi-supervised learning closer to reality.</p><p>We proposed the novel concept of class balancing on unlabeled data. We introduced a novel way to measure class imbalance with unlabeled data and proposed four class balancing methods that improve the performance of semisupervised learning. In addition, we investigated hyperparameter sensitivity and the causes for weak performance (i.e., training instabilities), where we proposed two opposite sets of solutions.</p><p>Our work provides researchers with the following observations and insights:</p><p>1. There is evidence that labeling a large number of samples might not be necessary for training deep neural networks to high levels of performance.</p><p>2. All networks have a class imbalance problem to some degree. Examining class accuracies relative to each other provides insights into the network's training.</p><p>3. Each training sample can affect the training. Oneshot semi-supervised learning provides a mechanism to study the atomic impact of a single sample. This opens up the opportunity to investigate the factors in a sample that help or hurt training performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplemental Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Broader Impact</head><p>It is widely accepted that large labeled datasets are an essential component of training deep neural networks, either directly for training or indirectly via transfer learning. To the best of our knowledge, this paper is the first to demonstrate performance comparable to fully supervised learning with one-shot semi-supervised learning. Eliminating the burden of labeling massive amounts of training data creates great potential for new neural network applications that attain high performance, which is especially important when labeling requires expertise. Hence, the societal impact will be to make deep learning applications even more widespread.</p><p>From a scientific perspective, one-shot semi-supervised learning provides important insights on the intricacies of training deep neural networks. The effect of changing just one training image can significantly impact the final performance. Unlike fully supervised learning that commonly deals with the training of large datasets, this method provides a technique to gain information about the impact of a single labeled sample in training. In addition, we anticipate that further investigation into the instability issues of one-shot semi-supervised learning will lead to new understandings of training neural network.</p><p>Furthermore, the experience of training highly sensitive networks provides an educational experience on hyperparameter tuning that carries over to easier training situa-tions. In order to achieve convergence with one-shot semisupervised learning, one must learn how to tune the hyperparameters and architecture well. Similarly, we believe that utilizing one-shot semi-supervised learning with automatic methods such as AutoML and neural architecture search (NAS) will lead to better choices for hyper-parameters and architectures.</p><p>Limitations: While our work has taken valuable steps towards making one or few-shot semi-supervised learning possible for applications, a large gap still remains before this can be realized in practice, especially due to issues with stability during training and hyper-parameter sensitivity. The sensitivity of the results to choices of the hyperparameters makes one-shot semi-supervised learning difficult to use in real-world applications. While there is a wide range of valuable applications (e.g., medical) that could benefit from semi-supervised learning, the testing of these applications is beyond the scope of this work.</p><p>While we attempted to provide a thorough investigation, there are a number of limitations in our work and several factors that we did not have sufficient time to explore. Our implementation was built on state-of-the-art Fix-Match algorithm but the ideas presented here should carry over to other semi-supervised learning methods, such as the π model, temporal ensembling <ref type="bibr" target="#b14">[15]</ref>, VAT <ref type="bibr" target="#b16">[17]</ref>, ICT <ref type="bibr" target="#b27">[28]</ref>, UDA <ref type="bibr" target="#b30">[31]</ref>, S 4 L [34], MixMatch <ref type="bibr" target="#b3">[4]</ref>, and Mean Teachers <ref type="bibr" target="#b24">[25]</ref> but this was not tested because none of these other methods have demonstrated results with less than 25 labeled examples per class. The model used in our experiments was a Wide ResNet-28-2 and the experiments were replicated with ShakeNet <ref type="bibr" target="#b8">[9]</ref> with analogous results, indicating that our conclusions and insights are independent of model architecture.</p><p>In addition, we made use of labeled test data to demonstrate the performance of BOSS. In practical settings, one has a large unlabeled dataset and one wishes to avoid burdensome manual labeling. However, the samples in the test dataset are less important than the choices for the class prototypes, so a small test dataset can be quickly created from the "discards" when searching for iconic prototypes. A small test dataset is useful for prototype refinement (i.e., deciding which class prototypes to replace) and it provides the practitioner with useful feedback on the system's performance with a little additional effort. But even without any test data, one can utilize the pseudo-labeled class counts to decide which class prototypes should be replaced.</p><p>Furthermore, there are several assumptions that might not hold true in a practical setting. First of all, there is an implicit assumption that the unlabeled dataset is class balanced; that is, it contains the same number of samples of each class. In practical situations with large amounts of unlabeled data, this assumption is unlikely to be true. In cases where the number of unlabeled samples belonging to each  <ref type="table">Table 6</ref>. Test accuracies for class prototype set 2 for two hyper-parameter settings. The hyper-parameters are weight decay (WD), learning rate (LR), batch size (BS), and the ratio of the unlabeled to labeled data (ru).</p><p>class can be estimated, it is possible to adapt the class balancing methods. When the number of unlabeled samples belonging to each class is unknown, it is possible to create a small validation set in a similar manner as described above for creating a test set and utilize the validation set as a measure of class balance.</p><p>In addition, we also assume in our experiments that all of the unlabeled samples belong to one of the known classes. In practical settings, the unlabeled dataset might contain samples that don't belong to any of the prototype classes. We did not test the situation where we use only a subset of the classes in the training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Hyper-parameters</head><p>For FixMatch we used the default hyper-parameters that were specified in Sohn, et al. <ref type="bibr" target="#b22">[23]</ref>. However, in our initial experiments with the class balance methods, we found that these hyper-parameters performed poorly. Therefore, we used a different set of hyper-parameter values for FixMatch and for the BOSS methods. <ref type="table" target="#tab_4">Table 5</ref> contains the hyper-parameter values used for the results reported in our paper. Additional hyper-parameter settings that were consistent over all the runs include setting kimgs = 32768 (i.e., the number of training images) and λ u = 1 (i.e., the unlabeled loss multiplicative factor). Furthermore, we set the augment input parameter to 'd.d.d', which is the default data augmentation for the labeled and unlabeled data. Our early experiments with setting the augment input parameter to 'd.d.rac' produces small improvements so we subsequently used the default values. The balance column reflects the class balancing method used (balance = 0 corresponds to FixMatch, which does not use any class balancing method). The remaining columns specify the weight decay, learning rate, batch size, momentum, ra-tio of unlabeled to labeled data, confidence threshold, and change in the confidence threshold for minority classes. Details of these last three hyper-parameters are provided in the main text.</p><p>Specifically, we found that increasing the ratio of unlabeled to labeled data (from 7 to 9), weight decay (from 5 × 10 −4 to 8 × 10 −4 ) and the learning rates (from 0.03 to 0.06) improved performance. We also found that decreasing the confidence threshold from 0.95 to 0.9 improved performance but for class balancing methods 1 and 4, we left the confidence threshold at 0.95 because the class-based thresholds were lowered by these class balancing methods. We also discovered that a smaller batch-size improved performance and chose a batch size of 30 that was a multiple of the number of classes. Our experiments with momentum found a small improvement with values between 0.85 and 0.9 and settled on using 0.88 for our experiments.</p><p>As mentioned above, we tried to use the same hyperparameters for both FixMatch and for the class balancing methods but this proved to provide an unfair comparison to one or the other. <ref type="table">Table 6</ref> illustrates this. This <ref type="table">Table provides</ref> the averaged test accuracies for class prototype set 2 for the default and another choice of weight decay (WD), learning rate (LR), batch size (BS), and the ratio of the unlabeled to labeled data (r u ). The results for the BOSS methods improve significantly by tuning the hyper-parameters but the performance of FixMatch is reduced substantially. So we used the default set of hyper-parameters for FixMatch and another set of hyper-parameter values for the class balance methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>as a batch of B labeled examples, where x b are the training examples and y b are their labels. We also define U = {u b : b ∈ (1, ..., µ)} as a batch of µ unlabeled examples where µ = r u B and r u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>An example of training to a poor local minimum (blue) and training with instabilities (red). Both end with poor test accuracies but for different reasons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>± 3 97.4 ± .2 96.4 ± .9 95.7 ± 1.6 96.8 ± .1 97.9 97.9 97.9 97.8 2 91.5 ± 3 97.4 ± .1 97.1 ± .1 97.1 ± .1 95.6 ± .1 94.1 97.9 97.6 97.7 3 93.9 ± .1 97.3 ± .3 97.2 ± .2 92 ± 7 91.3 ± .3 97.8 97.9 97.8 97.9 489.2 ± 12 96.5 ± .6 90 ± 10 89 ± 11 83 ± 16 97.6 96.7 97.0 98.0 SVHN. BOSS methods are compared using four sets of class prototypes (i.e., 1 prototype per class) for SVHN. The FixMatch column shows results for the original FixMatch code on the prototype sets. The next four columns gives the accuracy results for the class balance methods Results are an average of test accuracies for four runs. The self-training iteration was performed on the results from the class balancing shown in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">BOSS balance method</cell><cell></cell><cell></cell><cell>self-training</cell></row><row><cell cols="2">set FixMatch</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>+5</cell><cell>+10 +20 +40</cell></row><row><cell>1</cell><cell>95.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>contains results of hyper-parameter fine tuning where we reported earlier test accuracies below 85%.</figDesc><table><row><cell cols="2">Set Balance</cell><cell>Description</cell><cell>WD</cell><cell>LR</cell><cell>∆</cell><cell>λ u</cell><cell>τ</cell><cell>Accuracy (%)</cell></row><row><cell>1</cell><cell>3</cell><cell>Instabilities</cell><cell cols="2">8 × 10 −4 0.06</cell><cell>0</cell><cell>1</cell><cell>0.9</cell><cell>84 ± 6</cell></row><row><cell>1</cell><cell>3</cell><cell>Decrease λ u</cell><cell cols="2">8 × 10 −4 0.06</cell><cell>0</cell><cell>0.5</cell><cell>0.9</cell><cell>87 ± 1</cell></row><row><cell>2</cell><cell>4</cell><cell>Instabilities</cell><cell cols="3">8 × 10 −4 0.06 0.25</cell><cell>1</cell><cell>0.95</cell><cell>80 ± 14</cell></row><row><cell>2</cell><cell>4</cell><cell cols="4">Decrease ∆, WD, LR 6 × 10 −4 0.04 0.1</cell><cell>1</cell><cell>0.95</cell><cell>94.5 ± 0.1</cell></row><row><cell>4</cell><cell>1</cell><cell>Local min</cell><cell cols="3">8 × 10 −4 0.06 0.25</cell><cell>1</cell><cell>0.9</cell><cell>77.5 ± 0.1</cell></row><row><cell>4</cell><cell>1</cell><cell>Increase ∆, τ</cell><cell cols="3">8 × 10 −4 0.06 0.3</cell><cell>1</cell><cell>0.95</cell><cell>93.2 ± 0.2</cell></row><row><cell>4</cell><cell>2</cell><cell>Local min</cell><cell cols="2">8 × 10 −4 0.06</cell><cell>0</cell><cell>1</cell><cell>0.9</cell><cell>81 ± 6</cell></row><row><cell>4</cell><cell>2</cell><cell>Increase λ u</cell><cell cols="2">8 × 10 −4 0.06</cell><cell>0</cell><cell>2</cell><cell>0.9</cell><cell>92 ± 2</cell></row><row><cell>4</cell><cell>3</cell><cell>Local min</cell><cell cols="2">8 × 10 −4 0.06</cell><cell>0</cell><cell>1</cell><cell>0.9</cell><cell>81 ± 8</cell></row><row><cell>4</cell><cell>3</cell><cell>Increase λ u</cell><cell cols="2">8 × 10 −4 0.06</cell><cell>0</cell><cell>2</cell><cell>0.9</cell><cell>88 ± 3</cell></row><row><cell>5</cell><cell>1</cell><cell>Instabilities</cell><cell cols="3">8 × 10 −4 0.06 0.25</cell><cell>1</cell><cell>0.95</cell><cell>86 ± 7</cell></row><row><cell>5</cell><cell>1</cell><cell>Decrease ∆</cell><cell cols="3">8 × 10 −4 0.06 0.1</cell><cell>1</cell><cell>0.95</cell><cell>90.7 ± 0.1</cell></row><row><cell>5</cell><cell>2</cell><cell>Instabilities</cell><cell cols="2">8 × 10 −4 0.06</cell><cell>0</cell><cell>1</cell><cell>0.9</cell><cell>89 ± 6</cell></row><row><cell>5</cell><cell>2</cell><cell>Decrease λ u</cell><cell cols="2">8 × 10 −4 0.06</cell><cell>0</cell><cell cols="2">0.75 0.9</cell><cell>91.7 ± 1</cell></row><row><cell>5</cell><cell>3</cell><cell>Instabilities</cell><cell cols="2">8 × 10 −4 0.06</cell><cell>0</cell><cell>1</cell><cell>0.9</cell><cell>83 ± 10</cell></row><row><cell>5</cell><cell>3</cell><cell>Decrease WD, LR</cell><cell cols="2">6 × 10 −4 0.04</cell><cell>0</cell><cell>1</cell><cell>0.9</cell><cell>93.5 ± 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Illustration of the sensitivity to the hyper-parameters WD, LR, ∆, λu and τ . See the text for guidance on how to tune these hyper-parameters for situations with inferior performance due to instabilities or local minimums.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Hyper-parameter values for each of the various steps in the training.</figDesc><table><row><cell>Method</cell><cell cols="6">balance weight decay LR Batch Momentum r u</cell><cell>τ</cell><cell>∆</cell></row><row><cell>FixMatch</cell><cell>0</cell><cell>5 × 10 −4</cell><cell>0.03</cell><cell>64</cell><cell>0.88</cell><cell cols="2">7 0.95</cell><cell>0</cell></row><row><cell>Cifar training</cell><cell>1, 4</cell><cell>8 × 10 −4</cell><cell>0.06</cell><cell>30</cell><cell>0.88</cell><cell cols="3">9 0.95 0.25</cell></row><row><cell>Cifar training</cell><cell>2, 3</cell><cell>8 × 10 −4</cell><cell>0.06</cell><cell>30</cell><cell>0.88</cell><cell>9</cell><cell>0.9</cell><cell>0</cell></row><row><cell>Self-training</cell><cell>4</cell><cell>5 × 10 −4</cell><cell>0.03</cell><cell>64</cell><cell>0.88</cell><cell cols="3">7 0.95 0.25</cell></row><row><cell>SVHN training</cell><cell>1, 4</cell><cell>6 × 10 −4</cell><cell>0.04</cell><cell>32</cell><cell>0.85</cell><cell cols="3">7 0.95 0.25</cell></row><row><cell>SVHN training</cell><cell>2, 3</cell><cell>6 × 10 −4</cell><cell>0.04</cell><cell>32</cell><cell>0.85</cell><cell>7</cell><cell>0.9</cell><cell>0</cell></row><row><cell>Self-training</cell><cell>0</cell><cell>6 × 10 −4</cell><cell>0.04</cell><cell>32</cell><cell>0.85</cell><cell cols="3">7 0.95 0.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BOSS balance method</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">WD/LR/BS/r u</cell><cell>FixMatch</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell>4</cell><cell></cell></row><row><cell cols="2">5 × 10 −4 /0.03/64/7</cell><cell>74 ± 5</cell><cell>34 ± 2</cell><cell cols="4">44 ± 7 40 ± 2 31.5 ± 0.5</cell><cell></cell></row><row><cell cols="2">8 × 10 −4 /0.06/30/9</cell><cell>47 ± 8</cell><cell cols="3">93 ± 0.7 90 ± 2 84 ± 13</cell><cell cols="2">78 ± 20</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">With appreciation, we acknowledge the use of the code kindly provided by the authors at https : / / github . com / googleresearch/fixmatch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">With appreciation, we acknowledge the use of the code provided at https://github.com/CoinCheung/fixmatch</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this Section we describe the changes we made to the original FixMatch codes and provide guidance on how to replicate our experiments. This Section relies on the reader being familiar with the TensorFlow version at https:// github.com/google-research/fixmatch and the PyTorch version located at https://github.com/ CoinCheung/fixmatch. We provide a copy of our codes as part of our Supplemental Materials.</p><p>Modifications to the original TensorFlow version of the FixMatch code were localized. In the TensorFlow version, the primary changes were made to fixmatch.py. This includes the implementation of the four class balancing methods. In support of these methods, the code for computing the number of pseudo-labels in each class was implemented. Also, a few new input parameters were added to this file that are related to the class balancing methods. Specifically, we added the input parameter "balance" to specify the class balancing method (balance=0 acts the same as the original Fix-Match code) and "delT" (i.e., ∆) as the amount that balance method 1 can reduce the threshold. Modifications were also made to cta/lib/train.py to compute test accuracies for each class, keep track of the best test accuracy, and output the sorted pseudo-labels for the unlabeled training data. In addition, changes to libml/data.py and libml/augment.py were required in order to accept the new prototype versions of the labeled datasets.</p><p>In addition to the code, the TensorFlow FixMatch version required several other steps that are supported by code in the scripts folder. Instructions for creating the necessary dataset files are located on the website at https: / / github . com / google -research / fixmatch. These instructions use programs in the scripts folder that needed to be modified in order to create the dataset files needed for the prototype sets and for self-training.</p><p>We named the prototype datasets with a 'p' at the end to distinguish them from the original datasets.</p><p>That is, 'cifar10' became 'cifar10p' and 'svhn' became 'svhnp'.</p><p>Therefore, it was necessary to create scripts/cifar10 prototypes.py and scripts/svhn prototypes.py to generate the labeled training data files. We note that to be consistent with the Tensor-Flow FixMatch, we used 'seed' as the input parameter to represent different prototype sets. It is also necessary to copy the unlabeled training and labeled training files from the cifar10/svhn file names to the cifar10p/svhnp file names and we provide shell scripts to do so.</p><p>Self-training is performed as a separate step from the first training run. The training run will have created three files containing the pseudo-labels for the unlabeled training data sorted from the most confident predictions down. The three files are the pseudo-labels, the confidences, and the true labels (used only for debug pur-poses). The programs scripts/cifar10 iteration.py and scripts/svhn iteration.py are provided to combine the highest confidence pseudo-labeled examples with the labeled class prototypes and create the necessary files for the selftraining run. We provide shell scripts as a template for how this is done. Once these files are created, the self-training iteration can be run.</p><p>Most of our experiments were run on a SuperMicro Su-perServer with Tesla V100 GPUs. We discovered that it was important to run our experiments on only 1 GPU and all our runs using multiple GPUs performed poorly.</p><p>Modifications to the PyTorch version of the FixMatch code were simpler than for the TensorFlow code. However, the execution of this code ran almost three times longer, which greatly reduced the number of experiments we could run due to constraints on computational resources. The primary modifications for class balancing were added to label guessor.py. Secondary modification were made to the main program in train.py to add the class balancing input parameters and arguments for the call to label guessor. In addition, cifar.py was modified to use the class prototypes instead of random examples. It was not necessary to create class prototype files as it was with the TensorFlow version. We did not have sufficient time to test self-training with the PyTorch version.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Image classification with deep learning in the presence of noisy labels: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Görkem</forename><surname>Algan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilkay</forename><surname>Ulusoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05170</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Assume, augment and learn: Unsupervised few-shot meta-learning via random labels and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09884</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised learning (chapelle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical data augmentation with no separate search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Justin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV/MOTION</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Empirical perspectives on one-shot semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conovaloff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04141</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cost-sensitive boosting for classification of imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3358" to="3378" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study. Knowledge and Information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="245" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jesper E Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiclass imbalance problems: Analysis and potential solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1119" to="1130" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<title level="m">Self-training with noisy student improves imagenet classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Introduction to semisupervised learning. Synthesis lectures on artificial intelligence and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
