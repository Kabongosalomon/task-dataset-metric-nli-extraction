<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Diverse Stochastic Human-Action Generators by Learning Smooth Latent Transitions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Duke University</orgName>
								<address>
									<addrLine>1 {zhenyiwa, pingyu</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
							<email>changyou@buffalo.edu2ryzhang@cs.duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Diverse Stochastic Human-Action Generators by Learning Smooth Latent Transitions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-motion generation is a long-standing challenging task due to the requirement of accurately modeling complex and diverse dynamic patterns. Most existing methods adopt sequence models such as RNN to directly model transitions in the original action space. Due to high dimensionality and potential noise, such modeling of action transitions is particularly challenging. In this paper, we focus on skeleton-based action generation and propose to model smooth and diverse transitions on a latent space of action sequences with much lower dimensionality. Conditioned on a latent sequence, actions are generated by a frame-wise decoder shared by all latent action-poses. Specifically, an implicit RNN is defined to model smooth latent sequences, whose randomness (diversity) is controlled by noise from the input. Different from standard action-prediction methods, our model can generate action sequences from pure noise without any conditional action poses. Remarkably, it can also generate unseen actions from mixed classes during training. Our model is learned with a bi-directional generative-adversarial-net framework, which not only can generate diverse action sequences of a particular class or mix classes, but also learns to classify action sequences within the same model. Experimental results show the superiority of our method in both diverse action-sequence generation and classification, relative to existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Human-action generation is an important task for modeling dynamic behavior of human activities, with vast real applications such as video synthesis <ref type="bibr" target="#b44">(Wang et al. 2018a</ref>), action classification <ref type="bibr" target="#b21">(Kim and Reiter 2017;</ref><ref type="bibr" target="#b19">Ke et al. 2017;</ref><ref type="bibr" target="#b51">Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b29">Liu et al. 2016;</ref><ref type="bibr" target="#b7">Du, Wang, and Wang 2015)</ref> and action prediction <ref type="bibr" target="#b31">(Martinez, Black, and Romero 2017;</ref><ref type="bibr" target="#b45">Wang et al. 2018b;</ref><ref type="bibr" target="#b1">Barsoum, Kender, and Liu 2017)</ref>. Directly generating human actions from scratch is particularly challenging due to the complexity and high-dimensionality of natural scenes. One promising workaround is to first generate easier-to-dealwith skeleton-based action sequences, based on which natural sequence are then rendered. This paper thus focuses on skeleton-based action-sequence generation.</p><p>Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  <ref type="figure">Figure 1</ref>: Generating sentences from pure noise, our model can learn smooth latent-frame transitions via an implicit LSTM-based RNN, which are then decoded to an action sequence via a shared decoder. The action sequence endows a flexible implicit distribution induced by the input noise. Our model not only can generate actions whose class are seen during training (e.g., throw, kick), but also can generate actions of unseen mixed classes (e.g., throw+kick).</p><p>Skeleton-based human action generation can be categorized into action synthesis (also referred to generation) <ref type="bibr" target="#b23">(Kovar, Gleicher, and Pighin 2002)</ref> and prediction. Action synthesis refers to synthesizing a whole action sequence from scratch, with controllable label information; whereas action prediction refers to predicting remaining action-poses given a portion of seed frames. These two tasks are closely related, e.g., the latter can be considered as a conditional variant of the former. In general, however, action synthesis is considered more challenging due to little input information available. Existing action-prediction methods can be categorized into deterministic <ref type="bibr" target="#b31">(Martinez, Black, and Romero 2017;</ref><ref type="bibr" target="#b18">Judith BÃ¼tepage 2017;</ref><ref type="bibr" target="#b45">Wang et al. 2018b;</ref><ref type="bibr" target="#b15">Harvey and Pal 2018)</ref> and stochastic <ref type="bibr" target="#b1">(Barsoum, Kender, and Liu 2017;</ref><ref type="bibr" target="#b24">Kundu, Gor, and Babu 2019;</ref><ref type="bibr" target="#b14">Habibie et al. 2017)</ref> approaches. Predicted action sequences in deterministic approaches are not associated with randomness; thus, there is no variance once input sub-sequences are given. By contrast, stochastic approaches can induce probability distributions over predicted sequences. In most cases, stochastic (probabilistic) approaches are preferable as they allow one to generate dif-arXiv:1912.10150v1 [cs.CV] 21 Dec 2019 ferent action sequences conditioned on the same context.</p><p>For diverse action generation, models are required to be stochastic so that the synthesis process can be considered as drawing samples from an action-sequence probability spaces. As a result, one approach for action synthesis is to learn a stochastic generative model, which induces probability distributions over the space of action sequences from which we can easily sample. Once such a model is learned, new actions can be generated by merely sampling from the generative model.</p><p>Among various deep generative models, the generative adversarial network (GAN) <ref type="bibr" target="#b10">(Goodfellow et al. 2014</ref>) is one of the state-of-the-art methods, with applications on various tasks such as image generation <ref type="bibr" target="#b30">(Ma et al. 2017)</ref>, characters creation <ref type="bibr" target="#b17">(Jin et al. 2017)</ref>, video generation <ref type="bibr" target="#b42">(Vondrick, Pirsiavash, and Torralba 2016)</ref> and prediction <ref type="bibr" target="#b5">(Denton and Birodkar 2017;</ref><ref type="bibr" target="#b25">Lee et al. 2018;</ref><ref type="bibr" target="#b28">Liang et al. 2017)</ref>. However, most existing GAN-based methods for action generation consider directly learning frame transitions on the original action space. In other words, these works define action generators with recurrent neural networks (RNNs) that directly produce action sequences <ref type="bibr" target="#b28">(Liang et al. 2017;</ref><ref type="bibr" target="#b14">Habibie et al. 2017;</ref><ref type="bibr" target="#b25">Lee et al. 2018;</ref><ref type="bibr" target="#b46">Wang, Chai, and Xia 2018)</ref>. However, these models are usually difficult to train due to the complexity and high-dimensionality of the action space.</p><p>In this paper, we overcome this issue by breaking the generator into two components: a smooth-latent-transition component (SLTC) and a global skeleton-decoder component (GSDC). <ref type="figure">Figure 1</ref> illustrates the key features of and some results from our model. Specifically,</p><p>â¢ The SLTC is responsible for generating smooth latent frames, each of which corresponds to the latent representation of a generated action-pose. The SLTC is modeled by an implicit LSTM, which takes a sequence of independent noise plus a one-hot class vector as input, and outputs a latent-frame sequence. Our method inherits the advantage of RNNs, which could generate diverse length sequences but on a much lower-dimensional latent space, an advantage over existing methods such as <ref type="bibr" target="#b3">(Cai et al. 2018</ref>).</p><p>â¢ The GSDC is responsible for decoding each latent frame to an output action pose, via a shared (global) decoder implemented by a deep neural network <ref type="bibr">(DNN)</ref>. Note that at this stage, only a mapping from a single latent frame to an action-pose needs to be learned, i.e., no sequence modeling is needed, making generation much simpler.</p><p>Our model is learned by adopting the bi-directional GAN framework <ref type="bibr" target="#b6">Donahue, KrÃ¤henbÃ¼hl, and Darrell 2017)</ref>, consisting of a stochastic action generator, an action classifier and an action discriminator. These three networks compete with each other adversarially. At equilibrium, the generator can learn to generate diverse action sequences that match the training data. In addition, the classifier is able to learn to classify both real and synthesized action sequences. Our contributions are summarized as follows:</p><p>â¢ We propose a novel stochastic action sequence generator architecture, which benefits from an ability to learn smooth latent transitions. The proposed architecture eases the training of the RNN-based generator for sequence generation, and at the same time can learn to generate much higher-quality and diverse actions. â¢ We propose to learn an action-sequence classifier simultaneously within the bi-directional GAN framework, achieving both action generation and classification. â¢ Extensive experiments are conducted, demonstrating the superiority of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Skeleton-based action prediction has been studied for years. One of the most popular methods for human motion prediction (conditioned on a portion of seed action-poses) is based on recurrent neural networks <ref type="bibr" target="#b31">(Martinez, Black, and Romero 2017;</ref><ref type="bibr" target="#b45">Wang et al. 2018b;</ref><ref type="bibr" target="#b24">Kundu, Gor, and Babu 2019)</ref>. For skeleton-based action generation, switching linear models <ref type="bibr" target="#b35">(PavloviÄ, Rehg, and MacCormick 2001;</ref><ref type="bibr" target="#b2">Bissacco 2005;</ref><ref type="bibr" target="#b33">Oh et al. 2005</ref>) were proposed to model stochastic dynamics of human motions. However, it is difficult to select a suitable number of switching states for best modeling. Furthermore, it usually requires a large amount of training data due to the large model size. Restricted Boltzmann Machine (RBM) also has been applied for motion generation <ref type="bibr" target="#b40">(Taylor, Hinton, and Roweis 2007;</ref><ref type="bibr" target="#b38">Sutskever et al. 2008;</ref><ref type="bibr" target="#b39">Taylor and Hinton 2009</ref>). However, inference for RBM is known to be particularly challenging. Gaussian-process latent variable models <ref type="bibr" target="#b48">(Wang, Fleet, and Hertzmann 2008;</ref><ref type="bibr">Urtasun et al. 2008</ref>) and its variants <ref type="bibr" target="#b47">(Wang, Fleet, and Hertzmann 2007)</ref> have been applied for this task. One problem with such methods, however, is that they are not scalable enough to deal with large-scale data. For deep-learning-based methods, RNNs are probably one of the most successful models <ref type="bibr" target="#b9">(Fragkiadaki et al. 2015)</ref>. However, most existing models assume output distributions as Gaussian or Gaussian mixture. Different from our implicit representation, these methods are not expressive enough to capture the diversity of human actions.</p><p>In contrast to action prediction, limited work has been done for diverse action generation, apart from some preliminary work. Specifically, the motion graph approach <ref type="bibr" target="#b32">(Min and Chai 2012)</ref> needs to extract motion primitives from prerecorded data; the diversity and quality of action will be restricted by way of defining the primitives and transitions between the primitives. Variational autoencoder and GAN have also been applied in <ref type="bibr" target="#b14">(Habibie et al. 2017;</ref><ref type="bibr" target="#b46">Wang, Chai, and Xia 2018;</ref><ref type="bibr" target="#b20">Kiasari, Moirangthem, and Lee 2018)</ref> for motion generation. However, these methods directly learn motion transitions with an RNN, and the error of current frame will be accumulate into the next frame, thus making it inapplicable to generate long action sequences, especially for aperiodic motions such as eating and drinking.</p><p>Another distinction between our model and existing methods is that the latter typically require some seed action-frames as input to a generator <ref type="bibr" target="#b50">(Xue et al. 2016;</ref><ref type="bibr" target="#b1">Barsoum, Kender, and Liu 2017;</ref><ref type="bibr" target="#b46">Wang, Chai, and Xia 2018;</ref><ref type="bibr" target="#b20">Kiasari, Moirangthem, and Lee 2018)</ref>, which is learned based on the GAN framework; whereas our model is designed to generate action sequence from scratch, and learned based on the bi-direction GAN framework in order to achieve simultaneous action generation and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Proposed Model</head><p>We first illustrate our whole model in <ref type="figure" target="#fig_1">Figure 2</ref>, followed by detailed descriptions of specific components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Setup and Challenges</head><p>Our training dataset is represented as</p><formula xml:id="formula_0">X {(x (i) 1 , Â· Â· Â· , x (i) Ti , y (i) )} i , where x (i) t</formula><p>â R d represents one action-pose of dimension d; T i is the length of the sequence; and y (i) is the corresponding one-hot label vector of the sequence * . Our basic goal is to train a stochastic sequence generator G Î¸ using a DNN parametrized by Î¸. Hence, given a label y and a sequence of random noises Î¾ (specified latter), G Î¸ is supposed to generate a new action sequence following</p><formula xml:id="formula_1">(x 1 , Â· Â· Â· ,x T ) = G Î¸ (y, Î¾) ,<label>(1)</label></formula><p>where T is the length of the sequence that can be specified flexibly in G Î¸ .</p><p>Remark 1 Similar to implicit generative models such as GAN, we call the generator form (1) an implicit generator, in the sense that the generated sequence (x 1 , Â· Â· Â· ,x T ) is a random sequence endowing an implicit probability distribution with an unknown density function induced by the random noise Î¾. A traditional way of modeling action sequences usually defines G Î¸ as an RNN, which typically defines a Gaussian distribution (explicit) forx t , referred to as explicit modeling. An implicit distribution is typically much more flexible than explicit distributions as the density is not restricted to a particular distribution class. * Our model can also be applied to data without labels by simply removing y (i) from the generator. We focus on the one with labels.</p><p>Challenges There are a few challenges. The first one relates to how to define an expressive-enough generator for diverse action generation. We adopt an implicit model of action sequences without explicit form assumption; Thus, the generator benefits from better representation power to generate more sophisticated, higher-quality and diverse action sequences. The second challenge is to find an appropriate generator structure. One straightforward way is to define G Î¸ as an RNN that outputs action sequences directly, similar to <ref type="bibr" target="#b14">(Habibie et al. 2017;</ref><ref type="bibr" target="#b46">Wang, Chai, and Xia 2018)</ref>. However, it is well known that an RNN with high-dimensional outputs is challenging to train <ref type="bibr" target="#b34">(Pascanu, Mikolov, and Bengio 2013)</ref>. In recent years, attention <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref> and the Transformer model <ref type="bibr" target="#b41">(Vaswani et al. 2017)</ref> have been developed to enhance/replace RNN-based models. Attention and Transformer are used for addressing the long-term dependency problem in seq2seq-based models. They are not directly applicable to our setting because our model is not a simple seq2seq model, as our inputs are purely random noise. To this end, we propose a novel generator structure, where smooth latent-action transitions are first inferred via an RNN, which are then fed into a shared frame-wise decoder (non-sequential) to map all latent poses to their corresponding action-poses. The detailed structure of the generator is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> and described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Action-Sequence Generator</head><p>Our action-sequence generator consists of two components, SLTC and GSDC. The detailed structure of the generator is illustrated as G Î¸ in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Learning smooth latent transitions Instead of directly modeling sequential transitions in the action space, we propose to model them in a latent action-sequence space. To this end, we decompose G Î¸ as compositions of an implicit LSTM and a shared frame-wise decoder. The LSTM generator (a.k.a.SLTC) models smooth latent action transitions, and the shared decoder (a.k.a.GSDC) models frame-wise mapping from latent space to action space. Specifically, denote h t to be the latent representation of an action-pose x t . We define (h 1 , Â· Â· Â· , h T ) to be outputs of an implicit LSTM, written as:</p><formula xml:id="formula_2">(h 1 , Â· Â· Â· , h T ) = LSTM (Î¾ 1 , Â· Â· Â· , Î¾ T , y; Î¸ 1 ) ,<label>(2)</label></formula><p>where (Î¾ t , y) is the input of the LSTM at time t (the noise Î¾ t 's are independent of each other for all t); and Î¸ 1 â Î¸ is the parameter of the LSTM network. We called (2) implicit LSTM because its input consists of independent noise Î¾ t at each time in both training and testing (generation) stages (please see the generator graph in <ref type="figure" target="#fig_1">Figure 2</ref>). This generator is different from standard LSTM where the output of previous time will be used as input of current time in the testing stage.</p><p>In addition, the noise in the input would induce a much more flexible implicit distribution on h t ; whereas standard LSTM defines an explicit distribution such as Gaussian, restricting the representation power. Another advantage of adopting an implicit LSTM as a latent-frame generator is that the length of a generated action sequence could be induced from the latent space instead of the action space. In general, the dimension of h t is much smaller than action poses, making the training of the LSTM easier. Finally, modeling latent representations with an LSTM also allows latent transitions to be smooth, that is the desired property of action sequence generation.</p><p>To further ease the training of LSTM, we propose a variant whose outputs are defined as the residual latent sequences. That is, instead of modeling as in <ref type="formula" target="#formula_2">(2)</ref>, we define the following generating process:</p><formula xml:id="formula_3">(v 1 , Â· Â· Â· , v T ) = LSTM (Î¾ 1 , Â· Â· Â· , Î¾ T , y; Î¸ 1 ) h t+1 = h t + v t .<label>(3)</label></formula><p>The shared frame-wise decoder The second component, GSDC, is a shared frame-wise decoder mapping one latent frame h t to the corresponding action pose x t . Specifically, given h t from SLTC, we have, for all t,</p><formula xml:id="formula_4">x t = Dec(h t , y; Î¸ 2 ) ,</formula><p>where Dec(Â·, Â·; Î¸ 2 ) represents a decoder implemented as any DNN with parameter Î¸ 2 â Î¸, mapping an input latent frame h t to an output action posex t . In experiments, we use a simple MLP structure for Dec.</p><p>The whole action sequence generator Stacking the above two components constitutes our implicit action generator. To further enforce smooth transitions, we penalize the generated latent action poses by the changes of consecutive frames, i.e., with the following regularizer, similar to <ref type="bibr" target="#b3">(Cai et al. 2018)</ref>:</p><formula xml:id="formula_5">â¦({ht}, {xt}) T t=2 Ï1 ht â htâ1 2 + Ï2 x t âxtâ1 2<label>(4)</label></formula><p>where Ï 1 and Ï 2 control the relative importance of the corresponding regularizer term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action-Sequence Classifier</head><p>Modeling action-sequence generation and classification simultaneously enables information sharing between the generator and classifier, thus it is expected to be able to boost model performance. As a result, we define a classifier C with a bi-directional LSTM <ref type="bibr" target="#b36">(Schuster and Paliwal 1997)</ref>, whose outputs are further appended with a fully connected layer and a softmax layer for classification. The purpose of adopting the bi-directional LSTM is to effectively model framewise relation from two directions, which has been shown more effective than single direction modeling in sequence models <ref type="bibr" target="#b16">(Huang, Xu, and Yu 2015;</ref><ref type="bibr">Sundermeyer et al. 2014;</ref><ref type="bibr" target="#b11">Graves 2012)</ref>. Please refer to C Ï Ï Ï in <ref type="figure" target="#fig_1">Figure 2</ref> for a detailed structure of our sequence classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Discriminator and Model Training</head><p>Bi-directional GAN based training The proposed actionsequence generator and classifier constitute a pair of networks that can translate between each other, i.e., inverting the classifier achieves the same goal of the generator. To train these two networks effectively, we borrow ideas from bi-directional GAN, and define a discriminator to play adversarial games with the generator and classifier. Specifically, the action-label pairs come from two sources: one starts from a random label and then generates an action sequence via the generator  G Î¸ ; the other starts from a randomly-sampled training action sequence and then generates a label via the classifier C Ï Ï Ï . Let q(y) be a prior distribution over labels â  ; q Î¸ (x| y) be the implicit distribution induced by the generator; p(x) be the empirical action-sequence distribution of the training data; and p Ï Ï Ï (á»¹| x) be the conditional label distribution induced by the classifier given a sequence x. Our model updates the generator G Î¸ , the classifier C Ï Ï Ï , and the discriminator D Ï Ï Ï alternatively by following the GAN training procedure. Similar to the classifier, the discriminator is also defined by a bidirectional LSTM. The bi-directional GAN is trained to match the joint distributions q(y)q Î¸ (x|y) and p(x)p Ï Ï Ï (á»¹| x), via the following min-max game:</p><formula xml:id="formula_6">min G Î¸ ,C Ï Ï Ï max D Ï Ï Ï V (G Î¸ , C Ï Ï Ï , D Ï Ï Ï ) = E xâ¼p(x),á»¹â¼p Ï Ï Ï (Â·| x) [log D Ï Ï Ï (x,á»¹)] + E yâ¼q(y),xâ¼q Î¸ (Â·| y) [log(1 â D Ï Ï Ï (x, y))] .<label>(5)</label></formula><p>In addition, motivated by CycleGAN ) and ALICE <ref type="bibr" target="#b17">(Li et al. 2017)</ref>, a cycle-consistency loss is introduced:</p><formula xml:id="formula_7">L c H(C Ï Ï Ï (G Î¸ (y, Î¾)), y) ,<label>(6)</label></formula><p>where H(Â·, Â·) denotes the cross entropy between two distributions. Combining (4), (5) and (6) constitutes the final loss of our model.</p><p>Shared frame-wise decoder pretraining It is useful to pretrain the shared frame-wise decoder with the training data. â  We adopt a uniform distribution in our method. <ref type="figure">Figure 5</ref>: Diversity of generated action sequences.</p><p>To this end, we use the conditioned WGAN-GP model <ref type="bibr" target="#b13">(Gulrajani et al. 2017)</ref> to train a generator to generate independent action poses from a given label. The generator, denoted as G Î¸2 (Â·), corresponds to the shared decoder in our model. To match the input with our frame decoder, we replace the original input h t with a random sample from a simple distribution p h (Â·), e.g., the standard Gaussian distribution. The discriminator, denoted asD(Â·), is an auxiliary network to be discarded after pretraining. The objective function is defined as:</p><formula xml:id="formula_8">min G Î¸ 2 max DV (D,á¸  Î¸2 ) = E xâ¼pdata [D(x)]â (7) E hâ¼p h (h) [D(á¸  Î¸2 (h))] + Î»E xâ¼pdata [( xD (x) 2 â 1) 2 ]</formula><p>where p data denotes the frame distribution of training data; and Î» controls the magnitude of the gradient penalty to enforce the Lipschitz constraint.</p><p>Finally, the whole training procedure of our model is described in Algorithm (See Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate our proposed model for diverse action generation on two datasets, in terms of both action-sequence quality and diversity. We also conduct extensive human evaluation for the results generated by different models. Ablation study, implementation details and more results are provided in the Appendix. Code is also made available â¡ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets &amp; Baselines</head><p>Datasets We adopt the human-3.6m dataset (Catalin Ionescu and Sminchisescu 2014) and the NTU dataset ). The human-3.6m is a large scale dataset for human activity recognition and analysis. Following <ref type="bibr" target="#b3">(Cai et al. 2018)</ref>, we subsample the video frames to 16 fps to obtain more significant action variations. Our model is trained on 10 classes of actions, including Directions, Discussion, Eating, Greeting, Phoning, Posing, Sitting, SittingDown, Walking, and Smoking.</p><p>The NTU RGB+D is a large action dataset collected with Microsoft Kinect v.2 cameras . For our purpose, we only use the 2D skeleton locations of 25 major body joints in the corresponding depth/IR frame data. Similar to the human3.6m dataset, we also sample 10 action classes for training and testing, including drinking water, â¡ https://github.com/zheshiyige/Learning-Diverse-Stochastic-Human-Action-Generators-by-Learning-Smooth-Latent-Transitions throw, sitting down, wear jacket, standing up, hand waving, kicking something, jump up, make a phone call and cross hands in front. We follow the evaluation protocols of previous literature on this dataset to adopt cross-subject and cross-view recognition accuracy. For the cross-subject evaluation, sequences for training (20 subjects) and testing (20 subjects) come from different subjects. For the cross-view evaluation, the training dataset consists of action sequences collected by two cameras, and the test dataset consists of the remaining data. After splitting and cleaning missing or incomplete sequences, there are 2260 and 1070 action sequences for training and testing, respectively, for cross-subject evaluation; and there are 2213 and 1117 action sequences for training and testing, respectively, for cross-view evaluation.</p><p>Baselines Generating action sequences from scratch is a relatively less explored field. The most related models to ours we found are the recently proposed generative models EPVA-adv in <ref type="bibr" target="#b49">(Wichers et al. 2018)</ref>, action generator trained with VAE <ref type="bibr" target="#b14">(Habibie et al. 2017)</ref> as well as the model proposed in <ref type="bibr" target="#b3">(Cai et al. 2018)</ref> for the human-action generation. In the experiments, we will compare our model with these three, as well as other specific baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Results</head><p>Latent frame transitions To show the effectiveness of our proposed latent-transition mechanism, we visualize the learned latent representations for selected classes.</p><p>Latent trajectories of different classes on the Human-3.6 dataset are plotted in <ref type="figure" target="#fig_2">Figure 3</ref>, with a latent-frame dimension of 2. More results with higher dimensionalities are provided in the Appendix. It is interesting to observe that for the 2dimensional-latent-space case, some latent trajectories intercept with each other. This is reasonable because action poses in different action categories might be similar, e.g., smoking (green) and eating (blue) in <ref type="figure" target="#fig_2">Figure 3 (left)</ref>.</p><p>To demonstrate the diversity of the learned action generator, we plot multiple latent trajectories for selected classes in <ref type="figure" target="#fig_3">Figure 4</ref>, all starting from the same initial point. It is clear that as time goes on, the generated latent frames become more diverse, a distinct property lacking in deterministic generators in most action-prediction models such as <ref type="bibr" target="#b31">(Martinez, Black, and Romero 2017;</ref><ref type="bibr" target="#b45">Wang et al. 2018b)</ref>.</p><p>To better illustrate the diversity of the action sequences, we compare our model with the three recently proposed models <ref type="bibr" target="#b14">(Habibie et al. 2017;</ref><ref type="bibr" target="#b3">Cai et al. 2018;</ref><ref type="bibr" target="#b49">Wichers et al. 2018)</ref>. The mean and variance of each action pose along time for a set of trajectories are plotted in <ref type="figure">Figure 5</ref>. It is remarkable to find that trajectories from our model are much more diverse than the baselines. For a quantitative comparison, the standard derivations for different action classes are listed in the Appendix. More diverse action sequences generation results are provided in the Appendix. All these results indicate the superiority of our model in generating diverse action sequences.</p><p>Quality of generated action sequences We adopt two metrics to measure the quality of the generated actions: . <ref type="table">Table 1</ref>: Comparisons of our model with <ref type="bibr" target="#b14">(Habibie et al. 2017;</ref><ref type="bibr" target="#b3">Cai et al. 2018;</ref><ref type="bibr" target="#b49">Wichers et al. 2018)</ref>   action-classification accuracy and the maximum mean discrepancy (MMD) between generated and real action sequences. The latter metric is adapted from measuring the quality of generated images of GAN-based model, and has been used in <ref type="bibr" target="#b43">(Walker et al. 2017)</ref> for measuring the quality of action sequences. For action classification, we adopt the trained classifier from our model to classify both real (testing) actions and a set of randomly generated actions from our model. A good generator should generate action sequences that endow similar or better classification accuracies than real data. We compare our model with a baseline, which uses the same classifier structure but is trained independently on the training data. For every action class, we randomly sample 100 sequences for testing. The classification accuracies are shown in <ref type="table" target="#tab_1">Table 2</ref>. It is seen that our model achieves comparable performance on real and generated action sequences, which outperforms the baseline model to a large margin in general. In some cases, the accuracy on generated action sequence is higher than that of real data is because both the real data and the generated data are involved in the training of classifier under the bidirectional GAN framework (Donahue, KrÃ¤henbÃ¼hl, and <ref type="bibr" target="#b6">Darrell 2017)</ref>. The cross-subject and cross-view classification accuracies across different classes are 0.824 and 0.885 for our model respectively.</p><p>The MMD measures the discrepancy of two distributions based on their samples (the generated and real action sequences in our case). Since our data are represented as sequences, we proposed two sequence-level MMD metrics (more details in Appendix ). Following <ref type="bibr" target="#b43">(Walker et al. 2017)</ref>, we vary the bandwidth from 10 â4 to 10 9 and report the maximum value computed. We compared our model with <ref type="bibr" target="#b14">(Habibie et al. 2017;</ref><ref type="bibr" target="#b3">Cai et al. 2018;</ref><ref type="bibr" target="#b49">Wichers et al. 2018)</ref>. <ref type="bibr" target="#b49">(Wichers et al. 2018</ref>) contains three models: E2E, EPVA and EPVAadv. These models require a few seed frames as inputs to the generator. To adapt these model to our setting, we define two variants: 1) given labels and only the first frame as seed input to their models; 2) given only labels but no frames as seed input. The results are reported in <ref type="table">Table 1</ref>. It is interesting to see that without the need of a seed action-pose, our model obtains much lower MMD scores than the baselines with no seed action-poses. <ref type="figure" target="#fig_4">Figure 6</ref> further plots some examples of generated action sequences on the two datasets, which further demonstrates the high quality of our generated actions.</p><p>Novel action generation by mixing action classes Another distinct feature of our model is its ability to generate novel action sequences by specifying the input class variable y. One interesting way for this is to mix several action classes such that the elements satisfy y k â¥ 0 and k y k = 1. When feeding such a soft-mixed label into our model, due to the smoothness of the learned latent space, one would expect the generated sequence contains all features from the mixing classes. To demonstrate this, we consider mixing two classes. <ref type="figure" target="#fig_5">Figure 7</ref> plots the latent trajectories for different mixing coefficients. As expected, the trajectories of mixing classes smoothly interpolate between the original trajectories. For better visualization, let the first two classes correspond to walking and phoning. We set y (0.5, 0.5, 0, Â· Â· Â· , 0) and generate the corresponding action sequences by feeding it into the generator. The generated sequences are shown in <ref type="figure">Figure 8</ref>. It is interesting to see that the sequence with mixing classes indeed contains actions with both hands and legs, which correspond to walking and phoning, respectively. <ref type="figure">Figure 8</ref>: Novel mixed action sequences generated on human3.6 dataset. First row: generated sequence of "Walking"; Second row: generated sequence of "Phoning"; Third row: generated sequence of mixed "Walking" + "Phoning".  Human evaluation We run perceptual studies on Amazon Mechanical Turk (AMT) to assess the realism of generated actions. There were 120 participants in this test for three-round evaluations. Every worker was assigned some collection of evaluation tasks, each of which consists of four videos generated by one of the four models, including <ref type="bibr" target="#b14">(Habibie et al. 2017;</ref><ref type="bibr" target="#b49">Wichers et al. 2018;</ref><ref type="bibr" target="#b3">Cai et al. 2018</ref>) and ours. The worker was asked to evaluate each group of videos with a scale from 1 to 5. The higher the score, the more realistic of an action. <ref type="table" target="#tab_2">Table 3</ref> summarizes the results, which clearly shows the superiority of our method over others. Scoring standard and detailed experiment design are provided in the Appendix.</p><p>Ablation study We conduct extensive ablation study to better understand each component of our model, including smoothness term, cycle consistency loss, and residual latent sequence prediction. More details are described in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We propose a new framework for stochastic diverse action generation, which induces flexible implicit distributions on generated action sequences. Different from existing actionprediction methods, our model does not require conditional action poses in the generation process, although it can be easily generalized to this setting. Within the core is a latentaction generator that learns smooth latent transitions, which are then fed to a shared decoder to generate final action sequences. Our model is formulated within the bi-directional GAN framework, which contains a sequence classifier that simultaneously learns action classification. Experiments are conducted on two accessible datasets, demonstrating the effectiveness of the proposed model, and obtaining better results compared to related baseline models.</p><formula xml:id="formula_9">1 m(m â 1) Î£ m i=1 Î£ m j =i k(x i , x j ) + 1 n(n â 1) Î£ n i=1 Î£ n j =i k(y i , y j ) â 2 mn Î£ m i=1 Î£ n j=1 k(x i , y j )</formula><p>with k(Â·, Â·) the Gaussian kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use Adam optimization algorithm <ref type="bibr" target="#b22">(Kingma and Ba 2015)</ref> for learning the whole network parameters. The weight of gradient penalty Î» for the WGAN-GP is set to be 10. The learning rate for optimizing the generator and discriminator loss of the WGAN-GP is set to be 0.001. The learning rate is set to be 0.0001 for optimizing the generator and discriminator loss of our sequence generation model. The weight Î³ for the cycle consistency loss is set to be 0.1 and the weight Ï 1 and Ï 2 for regularization loss are set to be 0.05 and 0.00005 respectively. The number of hidden units for the fully connected layers is set to 1024 for the LSTM discriminator and classifier. The number of hidden units for the LSTM generator is set to be 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Experiment Results</head><p>The novel actions mixed with the two actions "Throw" and "Kick" trained on NTU RGB+D dataset are shown in <ref type="figure" target="#fig_5">Figure 17</ref>. It is also interesting to see that the sequence with mixing classes indeed contains actions with both hands and legs, which correspond to "Throw" and "Kick", respectively. Diverse generated latent space with different latent dimensions are shown in <ref type="figure">Figure 15</ref>. <ref type="figure">Figure 10 and 11</ref> shows other examples of generated action sequences on the two datasets. <ref type="table" target="#tab_3">Table 4</ref> shows the standard deviation of the distance of the generated action sequences to the mean action for each action class. <ref type="figure" target="#fig_1">Figure 12</ref>, 13, 14 shows more diverse action generation results on human3.6 dataset.</p><p>For latent dimensions larger than 2, the latent trajectories from different classes are found to be separated quite well. <ref type="figure" target="#fig_4">Figure 16</ref> shows more visualization results of latent space in higher dimension. Apart from the impact of tSNE, we suspect in a higher latent space, our model is flexible enough to separate trajectories of different classes, as the input of the generator contains label information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We run ablation experiments over the components of our model to better understand the effects of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of smooth regularizer:</head><p>We remove the regularizer for consecutive frames. <ref type="table" target="#tab_4">Table 5</ref> shows the ablation study for smoothness term in the loss function. "no smoothness" means that we remove both the regularization term on the latent and action space. "only action" means that we only add regularizer on the action space. "only latent" means that we only add regularizer on the latent space. "latent and action" means that we add regularizer on both action and latent space.</p><p>The results show that the regularization of both latent space and real action space is better than only regularize the latent or real action space or no regularization at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of cycle consistency loss:</head><p>We add or remove the classifier in the model, i.e., add or remove the cycle consistency loss. <ref type="table" target="#tab_1">Table 2</ref> shows the results of the ablation study for the cycle consistency loss, which is shown in Equation <ref type="formula" target="#formula_7">(6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of transition residual:</head><p>We study the comparison of predicting the latent transition residual and predicting the latent transition directly. <ref type="table" target="#tab_5">Table 6</ref> shows the results of the ablation study for latent transition residual. "direct latent" means that we use LSTM to predict the latent transition   directly. "residual latent" means that we use LSTM to predict the residual of latent transition as in Equation <ref type="formula" target="#formula_3">(3)</ref>. <ref type="figure">Figure 9</ref> shows experiment design for Human Evaluation. At each page, we give five scoring standard with word descriptions as well as video examples to make sure workers share the same grading standard. After that, we provide four sample videos from each model for the blind test.  Figure 17: Novel mixed action sequences generated on NTU RGBD dataset. First row: generated sequence of "Throw"; Second row: generated sequence of "Kick"; Third row: generated sequence of mixed "Throw" + "Kick".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed action-generation model (top), with detailed structures of action sequence generator (G Î¸ ), discriminator (D Ï Ï Ï ). The classifier (C Ï Ï Ï ) is the same as D Ï Ï Ï except that it outputs a class label instead of a binary value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Latent space with dimension = 2. The trajectories intercept with each other due to some similar frames in different action sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Action diversity of generated latent trajectories. Left: Greeting; Right: Posing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Randomly selected action sequences generated on human3.6 dataset ( First row: Smoking) and NTU RGBD dataset (Last row: Drinking)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Latent space of mixed classes of actions with different mixing proportions p 1 and p 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 11 :Figure 15 :</head><label>91115</label><figDesc>Human evaluation screenshot for Amazon Mechanical Turk. Several action sequences generated by training on NTU RGBD dataset Action diversity of generated latent space with different latent dimensions. The latent space dimension of first row, second row, third row is 2, 6 and 12 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 16 :</head><label>16</label><figDesc>Latent space with different dimensions. Left: dim = 6; Right: dim = 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Cross-view and cross-subject evaluation of classification accuracies on NTU-RGB dataset. Baseline_T means the independently trained classifier testing on real data, and Baseline_G means the independently trained classifier testing on generated sequences. Ours_T means our model testing on real data, and Ours_G means our model testing on generated sequences.</figDesc><table><row><cell>Split</cell><cell>Data</cell><cell cols="10">drinking throw sitting down wear jacket standing up hand waving kick jump phoning cross hands</cell></row><row><cell></cell><cell>Baseline_T</cell><cell>0.71</cell><cell>0.90</cell><cell>0.92</cell><cell>0.94</cell><cell>0.93</cell><cell>0.73</cell><cell cols="2">0.85 0.76</cell><cell>0.87</cell><cell>0.92</cell></row><row><cell>cross-view</cell><cell>Baseline_G</cell><cell>0.65</cell><cell>0.21</cell><cell>0.25</cell><cell>0.38</cell><cell>0.75</cell><cell>0.61</cell><cell cols="2">0.66 0.46</cell><cell>0.75</cell><cell>0.13</cell></row><row><cell></cell><cell>Ours_T</cell><cell>0.82</cell><cell>0.92</cell><cell>0.94</cell><cell>0.96</cell><cell>0.95</cell><cell>0.76</cell><cell cols="2">0.86 0.93</cell><cell>0.84</cell><cell>0.88</cell></row><row><cell></cell><cell>Ours_G</cell><cell>0.87</cell><cell>0.94</cell><cell>0.98</cell><cell>0.93</cell><cell>0.95</cell><cell>0.81</cell><cell cols="2">0.97 0.79</cell><cell>0.91</cell><cell>0.82</cell></row><row><cell></cell><cell>Baseline_T</cell><cell>0.76</cell><cell>0.80</cell><cell>0.90</cell><cell>0.90</cell><cell>0.92</cell><cell>0.73</cell><cell>0.6</cell><cell>0.67</cell><cell>0.86</cell><cell>0.74</cell></row><row><cell>cross-sub</cell><cell>Baseline_G</cell><cell>0.60</cell><cell>0.15</cell><cell>0.12</cell><cell>0.57</cell><cell>0.74</cell><cell>0.67</cell><cell cols="2">0.34 0.26</cell><cell>0.59</cell><cell>0.21</cell></row><row><cell></cell><cell>Ours_T</cell><cell>0.82</cell><cell>0.65</cell><cell>0.93</cell><cell>0.95</cell><cell>0.94</cell><cell>0.83</cell><cell cols="2">0.74 0.69</cell><cell>0.82</cell><cell>0.81</cell></row><row><cell></cell><cell>Ours_G</cell><cell>0.83</cell><cell>0.86</cell><cell>0.92</cell><cell>0.91</cell><cell>0.90</cell><cell>0.87</cell><cell cols="2">0.86 0.82</cell><cell>0.81</cell><cell>0.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Human evaluations.</figDesc><table><row><cell>Model</cell><cell>Average Score</cell></row><row><cell>(Habibie et al. 2017)</cell><cell>2.445</cell></row><row><cell>(Wichers et al. 2018)</cell><cell>2.387</cell></row><row><cell>(Cai et al. 2018)</cell><cell>2.847</cell></row><row><cell>Ours</cell><cell>3.378</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Standard derivation of generated action sequences Data Directions Discussion Eating Greeting Phoning Posing Sitting SittingDown Smoking Walking<ref type="bibr" target="#b14">(Habibie et al. 2017)</ref> </figDesc><table><row><cell></cell><cell>9.10</cell><cell>13.18</cell><cell>12.44</cell><cell>7.12</cell><cell>11.33</cell><cell>8.11</cell><cell>9.38</cell><cell>10.45</cell><cell>9.47</cell><cell>8.50</cell></row><row><cell>(Cai et al. 2018)</cell><cell>5.26</cell><cell>13.57</cell><cell cols="2">13.26 11.19</cell><cell cols="3">10.49 19.37 9.93</cell><cell>12.58</cell><cell>8.00</cell><cell>8.21</cell></row><row><cell>EPVA-adv (Wichers et al. 2018)</cell><cell>7.42</cell><cell>3.77</cell><cell>18.98</cell><cell>9.37</cell><cell>2.22</cell><cell>1.87</cell><cell>2.48</cell><cell>2.83</cell><cell>5.01</cell><cell>5.93</cell></row><row><cell>Ours</cell><cell>17.74</cell><cell>21.70</cell><cell cols="2">30.45 67.92</cell><cell cols="3">20.53 22.73 14.08</cell><cell>33.62</cell><cell>16.50</cell><cell>12.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of the smoothness term in our model in terms of Maximum Mean Discrepancy. The lower the better.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Smoothness term ablation study</cell><cell></cell></row><row><cell></cell><cell cols="4">no smoothness only action only latent latent and action</cell></row><row><cell>MMD avg</cell><cell>0.798</cell><cell>0.396</cell><cell>0.214</cell><cell>0.195</cell></row><row><cell>MMD seq</cell><cell>1.254</cell><cell>0.566</cell><cell>0.259</cell><cell>0.218</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of residual latent transition prediction in terms of Maximum Mean Discrepancy. The lower the better.</figDesc><table><row><cell></cell><cell cols="2">residual latent prediction ablation study</cell></row><row><cell></cell><cell>direct latent</cell><cell>residual latent</cell></row><row><cell>MMD avg</cell><cell>0.975</cell><cell>0.195</cell></row><row><cell>MMD seq</cell><cell>0.829</cell><cell>0.218</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Algorithm 1 Stochastic action generation via learning smooth latent transitions. Require: Generator G Î¸ ; Discriminator D Ï Ï Ï ; classifier C Ï Ï Ï .</p><p>Training data {(x (i) , y (i) )}. Number of updating steps k for discriminator; the weight Ï 1 and Ï 2 for the regularization loss; and the weight Î³ for the cycle consistency loss.</p><p>Pretrain the shared frame-wise decoder.</p><p>Update the discriminator by stochastic gradient ascent:</p><p>end for Sample minibatch of m noise samples {Î¾ 1 , Â· Â· Â· , Î¾ m }.</p><p>Update the generator and classifier parameters by stochastic gradient descent:</p><p>where X j seq and Y j seq denote generated and real test sequences of action class j, respectively; X j i and Y j i denote the i th frame of the generated and real test sequences of action class j, respectively; and MMD u [F, X, Y ] is defined as an unbiased MMD estimator <ref type="bibr" target="#b12">(Gretton et al. 2012)</ref>:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hp-gan: Probabilistic 3d human motion prediction via gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling and learning contact dynamics in human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep video generation, prediction and completion of human action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A recurrent variational autoencoder for human motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yearsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent transition networks for character locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards the automatic anime characters creation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>BÃ¼tepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K H K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Human action generation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kiasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Moirangthem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BNMW CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bihmp-gan: Bidirectional 3d human motion prediction gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Stochastic adversarial video prediction</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Alice: Towards understanding adversarial learning for joint distribution matching</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dual motion gan for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Motiongraphs++: A compact generative model for semantic motion analysis and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning and inference in parametric switching linear dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning switching linear models of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>PavloviÄ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSP</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. Sundermeyer, M</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ntu rgb+d: A large scale dataset for 3d human activity analysis. In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The recurrent temporal restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Factored conditional restricted boltzmann machines for modeling motion style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">;</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>PopoviÄ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Urtasun, R</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial geometry-aware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Combining recurrent neural networks and adversarial training for human motion modelling, synthesis and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1806.08666.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multifactor gaussian process models for style-content separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spatial temporal gcns for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
