<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Multi-Center Learning for Face Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengliang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Hao</surname></persName>
							<email>haoyangyang2014@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Software Engineering</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Multi-Center Learning for Face Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Multi-Center Learning</term>
					<term>Model Assembling</term>
					<term>Face Alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial landmarks are highly correlated with each other since a certain landmark can be estimated by its neighboring landmarks. Most of the existing deep learning methods only use one fully-connected layer called shape prediction layer to estimate the locations of facial landmarks. In this paper, we propose a novel deep learning framework named Multi-Center Learning with multiple shape prediction layers for face alignment. In particular, each shape prediction layer emphasizes on the detection of a certain cluster of semantically relevant landmarks respectively. Challenging landmarks are focused firstly, and each cluster of landmarks is further optimized respectively. Moreover, to reduce the model complexity, we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer. Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real-time performance. The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension.</p><p>Index Terms-Multi-Center Learning, Model Assembling, Face Alignment * Corresponding author.</p><p>(a) Chin is occluded.</p><p>(b) Right contour is invisible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Face alignment refers to detecting facial landmarks such as eye centers, nose tip, and mouth corners. It is the preprocessor stage of many face analysis tasks like face animation <ref type="bibr" target="#b0">[1]</ref>, face beautification <ref type="bibr" target="#b1">[2]</ref>, and face recognition <ref type="bibr" target="#b2">[3]</ref>. A robust and accurate face alignment is still challenging in unconstrained scenarios, owing to severe occlusions and large appearance variations. Most conventional methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> only use low-level handcrafted features and are not based on the prevailing deep neural networks, which limits their capacity to represent highly complex faces.</p><p>Recently, several methods use deep networks to estimate shapes from input faces. Sun et al. <ref type="bibr" target="#b7">[8]</ref>, Zhou et al. <ref type="bibr" target="#b8">[9]</ref>, and Zhang et al. <ref type="bibr" target="#b9">[10]</ref> employed cascaded deep networks to refine predicted shapes successively. Due to the use of multiple networks, these methods have high model complexity with complicated training processes. Taking this into account, Zhang et al. <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> proposed a Tasks-Constrained Deep Convolutional Network (TCDCN), which uses only one deep network with excellent performance. However, it needs extra labels of facial attributes for training samples, which limits its universality.</p><p>Each facial landmark is not isolated but highly correlated with adjacent landmarks. As shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, facial landmarks along the chin are all occluded, and landmarks around the mouth are partially occluded. <ref type="figure" target="#fig_0">Fig. 1(b)</ref> shows that landmarks on the right side of face are almost invisible. Therefore, landmarks in the same local face region have similar properties including occlusion and visibility. It is observed that the nose can be localized roughly with the locations of eyes and mouth. There are also structural correlations among different facial parts. Motivated by this fact, facial landmarks are divided into several clusters based on their semantic relevance.</p><p>In this work 1 , we propose a novel deep learning framework named Multi-Center Learning (MCL) to exploit the strong correlations among landmarks. In particular, our network uses multiple shape prediction layers to predict the locations of landmarks, and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively. By weighting the loss of each landmark, challenging landmarks are focused firstly, and each cluster of landmarks is further optimized respectively. Moreover, to decrease the model complexity, we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer. The entire framework reinforces the learning process of each landmark with a low model complexity.</p><p>The main contributions of this study can be summarized as follows:</p><p>• We propose a novel multi-center learning framework for exploiting the strong correlations among landmarks. <ref type="bibr">•</ref> We propose a model assembling method which ensures a low model complexity. <ref type="bibr">•</ref> Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real-time performance.</p><p>The remainder of this paper is structured as below. We discuss related works in the next section. In Section III, we illuminate the structure of our network and the learning algorithm. Extensive experiments are carried out in Section IV. Section V concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>We review researches from three aspects related to our method: conventional face alignment, unconstrained face alignment, face alignment via deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Conventional Face Alignment</head><p>Conventional face alignment methods can be classified as two categories: template fitting and regression-based.</p><p>Template fitting methods match faces by constructing shape templates. Cootes et al. <ref type="bibr" target="#b14">[15]</ref> proposed a typical template fitting method named Active Appearance Model (AAM), which minimizes the texture residual to estimate the shape. Asthana et al. <ref type="bibr" target="#b15">[16]</ref> used regression techniques to learn functions from response maps to shapes, in which the response map has stronger robustness and generalization ability than texture based features of AAM. Pedersoli et al. <ref type="bibr" target="#b16">[17]</ref> developed the mixture of trees of parts method by extending the mixtures from trees to graphs, and learned a deformable detector to align its parts to faces. However, these templates are not complete enough to cover complex variations, which are difficult to be generalized to unseen faces.</p><p>Regression-based methods predict the locations of facial landmarks by learning a regression function from face features to shapes. Cao et al. <ref type="bibr" target="#b3">[4]</ref> proposed an Explicit Shape Regression (ESR) method to predict the shape increment with pixeldifference features. Xiong et al. <ref type="bibr" target="#b4">[5]</ref> proposed a Supervised Descent Method (SDM) to detect landmarks by solving the nonlinear least squares problem, with Scale-Invariant Feature Transform (SIFT) <ref type="bibr" target="#b17">[18]</ref> features and linear regressors being applied. Ren et al. <ref type="bibr" target="#b6">[7]</ref> used a locality principle to extract a set of Local Binary Features (LBF), in which a linear regression is utilized for localizing landmarks. Lee et al. <ref type="bibr" target="#b18">[19]</ref> employs Cascade Gaussian Process Regression Trees (cGPRT) with shape-indexed difference of Gaussian features to achieve face alignment. It has a better generalization ability than cascade regression trees, and shows strong robustness against geometric variations of faces. Most of these methods give an initial shape and refine the shape in an iterative manner, where the final solutions are prone to getting trapped in a local optimum with a poor initialization. In contrast, our method uses a deep neural network to regress from raw face patches to the locations of landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unconstrained Face Alignment</head><p>Large pose variations and severe occlusions are major challenges in unconstrained environments. Unconstrained face alignment methods are based on 3D models or deal with occlusions explicitly.</p><p>Many methods utilize 3D shape models to solve largepose face alignment. Nair et al. <ref type="bibr" target="#b19">[20]</ref> refined the fit of a 3D point distribution model to perform landmark detection. Yu et al. <ref type="bibr" target="#b20">[21]</ref> used a cascaded deformable shape model to detect landmarks of large-pose faces. Cao et al. <ref type="bibr" target="#b0">[1]</ref> employed a displaced dynamic expression regression to estimate the 3D face shape and 2D facial landmarks. The predicted 2D landmarks are used to adjust the model parameters to better fit the current user. Jeni et al. <ref type="bibr" target="#b21">[22]</ref> proposed a 3D cascade regression method to implement 3D face alignment, which can maintain the pose invariance of facial landmarks within the range of around 60 degrees.</p><p>There are several occlusion-free face alignment methods. Burgos-Artizzu et al. <ref type="bibr" target="#b5">[6]</ref> developed a Robust Cascaded Pose Regression (RCPR) method to detect occlusions explicitly, and uses shape-indexed features to regress the shape increment. Yu et al. <ref type="bibr" target="#b22">[23]</ref> utilizes a Bayesian model to merge the estimation results from multiple regressors, in which each regressor is trained to localize facial landmarks with a specific pre-defined facial part being occluded. Wu et al. <ref type="bibr" target="#b23">[24]</ref> proposed a Robust Facial Landmark Detection (RFLD) method, which uses a robust cascaded regressor to handle complex occlusions and large head poses. To improve the performance of occlusion estimation, landmark visibility probabilities are estimated with an explicit occlusion constraint. Different from these methods, our method is not based on 3D models and does not process occlusions explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face Alignment via Deep Learning</head><p>Deep learning methods can be divided into two classes: single network based and multiple networks based.</p><p>Sun et al. <ref type="bibr" target="#b7">[8]</ref> estimated the locations of 5 facial landmarks using Cascaded Convolutional Neural Networks (Cascaded CNN), in which each level computes averaged estimated shape and the shape is refined level by level. Zhou et al. <ref type="bibr" target="#b8">[9]</ref> used multi-level deep networks to detect facial landmarks from coarse to fine. Similarly, Zhang et al. <ref type="bibr" target="#b9">[10]</ref> proposed Coarse-to-Fine Auto-encoder Networks (CFAN). These methods all use multi-stage deep networks to localize landmarks in a coarse-tofine manner. Instead of using cascaded networks, Honari et al. <ref type="bibr" target="#b24">[25]</ref> proposed Recombinator Networks (RecNet) for learning coarse-to-fine feature aggregation with multi-scale input maps, where each branch extracts features based on current maps and the feature maps of coarser branches.</p><p>A few methods employ a single network to solve the face alignment problem. Shao et al. <ref type="bibr" target="#b25">[26]</ref> proposed a Coarse-to-Fine Training (CFT) method to learn the mapping from input face patches to estimated shapes, which searches the solutions smoothly by adjusting the relative weights between principal landmarks and elaborate landmarks. Zhang et al. <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> used the TCDCN with auxiliary facial attribute recognition to predict correlative facial properties like expression and pose, which improves the performance of face alignment. Xiao et al. <ref type="bibr" target="#b26">[27]</ref> proposed a Recurrent Attentive-Refinement (RAR) network for face alignment under unconstrained conditions, where shape-indexed deep features and temporal information are taken as inputs and shape predictions are recurrently revised. Compared to these methods, our method uses only one network and is independent of additional facial attributes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTI-CENTER LEARNING FOR FACE ALIGNMENT A. Network Architecture</head><p>The architecture of our network MCL is illustrated in <ref type="figure" target="#fig_1">Fig.  2</ref>. MCL contains three max-pooling layers, each of which follows a stack of two convolutional layers proposed by VGGNet <ref type="bibr" target="#b27">[28]</ref>. In the fourth stack of convolutional layers, we use a convolutional layer with D feature maps above two convolutional layers. We perform Batch Normalization (BN) <ref type="bibr" target="#b28">[29]</ref> and Rectified Linear Unit (ReLU) <ref type="bibr" target="#b29">[30]</ref> after each convolution to accelerate the convergence of our network. Most of the existing deep learning methods such as TCDCN <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> use the fully-connected layer to extract features, which is apt to overfit and hamper the generalization ability of the network. To sidestep these problems, we operate Global Average Pooling <ref type="bibr" target="#b13">[14]</ref> on the last convolutional layer to extract a high-level feature representation x, which computes the average of each feature map. With this improvement, our MCL acquires a higher representation power with fewer parameters.</p><p>Face alignment can be regarded as a nonlinear regression problem, which transforms appearance to shape. A transformation Φ S (·) is used for modeling this highly nonlinear function, which extracts the feature x from the input face image I, formulated as</p><formula xml:id="formula_0">x = Φ S (I),<label>(1)</label></formula><formula xml:id="formula_1">where x = (x 0 , x 1 , · · · , x D ) T ∈ R (D+1)×1 , x 0 = 1</formula><p>corresponds to the bias, and Φ S (·) is a composite function of operations including convolution, BN, ReLU, and pooling. Traditionally, only one shape prediction layer is used, which limits the performance. In contrast, our MCL uses multiple shape prediction layers, each of which emphasizes on the detection of a certain cluster of landmarks. The first several layers are shared by multiple shape prediction layers, which are called shared layers forming the composite function Φ S (·). For the i-th shape prediction layer, i = 1, · · · , m, a weight matrix W i = (w i 1 , w i 2 , · · · , w i 2n ) ∈ R (D+1)×2n is used to connect the feature x, where m and n are the number of shape prediction layers and landmarks, respectively. The reason why we train each shape prediction layer to predict n landmarks instead of one cluster of landmarks is that different facial parts have correlations, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>To decrease the model complexity, we use a model assembling function Φ a (·) to integrate multiple shape prediction layers into one shape prediction layer, which is formulated as</p><formula xml:id="formula_2">W a = Φ a (W 1 , · · · , W m ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">W a = (w a 1 , w a 2 , · · · , w a 2n ) ∈ R (D+1)×2n is the assem- bled weight matrix. Specifically, w a 2j−1 = w i 2j−1 , w a 2j = w i 2j , j ∈ P i , i = 1, · · · , m, where P i is the i-th cluster of indexes of landmarks. The final predictionŷ = (ŷ 1 ,ŷ 2 , · · · ,ŷ 2n ) is defined asŷ = W aT x,<label>(3)</label></formula><p>whereŷ 2j−1 andŷ 2j denote the predicted x-coordinate and y-coordinate of the j-th landmark respectively. Compared to other typical convolutional networks like VG-GNet <ref type="bibr" target="#b27">[28]</ref>, GoogLe-Net <ref type="bibr" target="#b30">[31]</ref>, and ResNet <ref type="bibr" target="#b31">[32]</ref>, our network MCL is substantially smaller and shallower. We believe that such a concise structure is efficient for estimating the locations of facial landmarks. Firstly, face alignment aims to regress coordinates of fewer than 100 facial landmarks generally, which demands much lower model complexity than visual recognition problems with more than 1, 000 classes. Secondly, a very deep network may fail to work well for landmark detection owing to the reduction of spatial information layer by layer. Other visual localization tasks, like face detection, usually use multiple cascaded shallow networks rather than a single very deep network. Finally, common face alignment benchmarks only contain thousands of training images. A simple network is not easy to overfit given a small amount of raw training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Algorithm</head><p>The overview of our learning algorithm is shown in Algorithm 1. Ω t and Ω v are the training set and the validation Algorithm 1 Multi-Center Learning Algorithm. Input: A network MCL, Ω t , Ω v , initialized Θ. Output: Θ. 1: Pre-train shared layers and one shape prediction layer until convergence; 2: Fix the parameters of the first six convolutional layers and fine-tune subsequent layers until convergence; <ref type="bibr">3:</ref> Fine-tune all the layers until convergence; <ref type="bibr">4:</ref> for i = 1 to m do 5:</p><p>Fix Θ S and fine-tune the i-th shape prediction layer until convergence; 6: end for 7: Θ = Θ S ∪ W a ; 8: Return Θ.</p><p>set respectively. Θ is the set of parameters including weights and biases of our network MCL, which is updated using Mini-Batch Stochastic Gradient Descent (SGD) <ref type="bibr" target="#b32">[33]</ref> at each iteration. The face alignment loss is defined as</p><formula xml:id="formula_4">E = n j=1 u j [(y 2j−1 −ŷ 2j−1 ) 2 + (y 2j −ŷ 2j ) 2 ]/(2d 2 ), (4)</formula><p>where u j is the weight of the j-th landmark, y 2j−1 and y 2j denote the ground-truth x-coordinate and y-coordinate of the j-th landmark respectively, and d is the ground truth interocular distance between the eye centers.</p><p>Inter-ocular distance normalization provides fair comparisons among faces with different size, and reduces the magnitude of loss to speed up the learning process. During training, a too high learning rate may cause the missing of optimum so far as to the divergence of network, and a too low learning rate may lead to falling into a local optimum. We employ a low initial learning rate to avoid the divergence, and increase the learning rate when the loss is reduced significantly and continue the training procedure.</p><p>1) Pre-Training and Weighting Fine-Tuning: In Step 1, a basic model (BM) with one shape prediction layer is pretrained to learn a good initial solution. In Eq. 4, u j = 1 for all j. The average alignment error of each landmark of BM on Ω v are b 1 , · · · , b n respectively, which are averaged over all the images. The landmarks with larger errors than remaining landmarks are treated as challenging landmarks.</p><p>In Steps 2 and 3, we focus on the detection of challenging landmarks by assigning them larger weights. The weight of the j-th landmark is proportional to its alignment error as</p><formula xml:id="formula_5">u j = n b j / n j=1 b j .<label>(5)</label></formula><p>Instead of fine-tuning all the layers from BM directly, we use two steps to search the solution smoothly.</p><p>Step 2 searches the solution without deviating from BM overly.</p><p>Step 3 searches the solution within a larger range on the basis of the previous step. This stage is named weighting fine-tuning, which learns a weighting model (WM) with higher localization accuracy of challenging landmarks.</p><p>2) Multi-Center Fine-Tuning and Model Assembling: The face is partitioned into seven parts according to its semantic structure: left eye, right eye, nose, mouth, left contour, chin, and right contour. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, different labeling patterns of 5, 29, and 68 facial landmarks are partitioned into 4, 5, and 7 clusters respectively. For the i-th shape prediction layer, the i-th cluster of landmarks are treated as the optimized center, and the set of indexes of remaining landmarks is denoted as Q i .  From Steps 4 to 6, the parameters of shared layers Θ S are fixed, and each shape prediction layer is initialized with the parameters of the shape prediction layer of WM. When finetuning the i-th shape prediction layer, the weights of landmarks in P i and Q i are defined as</p><formula xml:id="formula_6">u P i = αu Q i ,<label>(6)</label></formula><p>where α 1 is a coefficient to make the i-th shape prediction layer emphasize on the detection of the i-th cluster of landmarks. The constraint between u P i and u Q i is formulated as</p><formula xml:id="formula_7">u P i |P i | + u Q i (n − |P i |) = n,<label>(7)</label></formula><p>where | · | refers to the number of elements in a cluster. With Eqs. 6 and 7, the solved weights are formulated as</p><formula xml:id="formula_8">u P i = αn/[(α − 1)|P i | + n],</formula><formula xml:id="formula_9">u Q i = n/[(α − 1)|P i | + n].<label>(8)</label></formula><p>The average alignment error of each landmark of WM on Ω v are w 1 , · · · , w n respectively. Similar to Eq. 5, the weight of the j-th landmark is</p><formula xml:id="formula_10">u j = u P i |P i | · w j / j∈P i w j , j ∈ P i , u Q i (n − |P i |) · w j / j∈Q i w j , j ∈ Q i .<label>(9)</label></formula><p>Although the landmarks in P i are mainly optimized, remaining landmarks are still considered with very small weights rather than zero. This is beneficial for utilizing implicit structural correlations of different facial parts and searching the solutions smoothly. This stage is called multi-center finetuning which learns multiple shape prediction layers. In Step 7, multiple shape prediction layers are assembled into one shape prediction layer by Eq. 2. With this model assembling stage, our method learns an assembling model (AM).</p><p>There is no increase of model complexity in the assembling process, so AM has a low computational cost. It improves the detection precision of each facial landmark by integrating the advantage of each shape prediction layer.</p><p>3) Analysis of Model Learning: To investigate the influence from the weights of landmarks on learning procedure, we calculate the derivative of Eq. 4 with respect toŷ k :</p><formula xml:id="formula_11">∂E ∂ŷ k = u j (ŷ k − y k )/d 2 ,<label>(10)</label></formula><p>where k ∈ {2j − 1, 2j}, j = 1, · · · , n. During the learning process, the assembled weight matrix W a in Eq. 3 is updated by SGD. Specifically,</p><formula xml:id="formula_12">w a k = w a k − η ∂E ∂w a k = w a k − η ∂E ∂ŷ k ∂ŷ k ∂w a k = w a k − η ∂E ∂ŷ k x. In summary, W a is updated as w a k = w a k − ηu j (ŷ k − y k )x/d 2 ,<label>(11)</label></formula><p>where η is the learning rate. If the j-th landmark is given a larger weight, its corresponding parameters will be updated with a larger step towards the optimal solution. Therefore, weighting the loss of each landmark ensures that the landmarks with larger weights are mainly optimized. Our method first uses the weighting fine-tuning stage to optimize challenging landmarks, and further uses the multi-center fine-tuning stage to optimize each cluster of landmarks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Datasets and Settings</head><p>1) Datasets: There are three challenging benchmarks AFLW <ref type="bibr" target="#b33">[34]</ref>, COFW <ref type="bibr" target="#b5">[6]</ref>, and IBUG <ref type="bibr" target="#b34">[35]</ref>, which are used for evaluating face alignment with severe occlusion and large variations of pose, expression, and illumination. The provided face bounding boxes are employed to crop face patches during testing.</p><p>• AFLW <ref type="bibr" target="#b33">[34]</ref> contains 25, 993 faces under real-world conditions gathered from Flickr. Compared with other datasets like MUCT <ref type="bibr" target="#b35">[36]</ref> and LFPW <ref type="bibr" target="#b36">[37]</ref>, AFLW exhibits larger pose variations and extreme partial occlusions.</p><p>Following the settings of <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>, 2, 995 images are used for testing, and 10, 000 images annotated with 5 landmarks are used for training, which includes 4, 151 LFW <ref type="bibr" target="#b37">[38]</ref> images and 5, 849 web images. • COFW <ref type="bibr" target="#b5">[6]</ref> is an occluded face dataset in the wild, in which the faces are designed with severe occlusions using accessories and interactions with objects. It contains 1, 007 images annotated with 29 landmarks. The training set includes 845 LFPW faces and 500 COFW faces, and the testing set includes remaining 507 COFW faces. • IBUG <ref type="bibr" target="#b34">[35]</ref> contains 135 testing images which present large variations in pose, expression, illumination, and occlusion. The training set consists of AFW <ref type="bibr" target="#b38">[39]</ref>, the training set of LFPW, and the training set of Helen <ref type="bibr" target="#b39">[40]</ref>, which are from 300-W <ref type="bibr" target="#b34">[35]</ref> with 3, 148 images labeled with 68 landmarks. 2) Implementation Details: We enhance the diversity of raw training data on account of their limited variation patterns, using five steps: rotation, uniform scaling, translation, horizontal flip, and JPEG compression. In particular, for each training face, we firstly perform multiple rotations, and attain a tight face bounding box covering the ground truth locations of landmarks of each rotated result respectively. Uniform scaling and translation with different extents on face bounding boxes are further conducted, in which each newly generated face bounding box is used to crop the face. Finally training samples are augmented through horizontal flip and JPEG compression. It is beneficial for avoiding overfitting and improving the robustness of learned models by covering various patterns.</p><p>We train our MCL using an open source deep learning framework Caffe <ref type="bibr" target="#b40">[41]</ref>. The input face patch is a 50 × 50 grayscale image, and each pixel value is normalized to [−1, 1) by subtracting 128 and multiplying 0.0078125. A more complex model is needed for a labeling pattern with more facial landmarks, so D is set to be 512/512/1, 024 for 5/29/68 facial landmarks. The type of solver is SGD with a mini-batch size of 64, a momentum of 0.9, and a weight decay of 0.0005. The maximum learning iterations of pre-training and each finetuning step are 18×10 4 and 6×10 4 respectively, and the initial learning rates of pre-training and each fine-tuning step are 0.02 and 0.001 respectively. Note that the initial learning rate of fine-tuning should be low to preserve some representational structures learned in the pre-training stage and avoid missing good intermediate solutions. The learning rate is multiplied by a factor of 0.3 at every 3 × 10 4 iterations, and the remaining parameter α is set to be 125.</p><p>3) Evaluation Metric: Similar to previous methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we report the inter-ocular distance normalized mean error, and treat the mean error larger than 10% as a failure. To conduct a more comprehensive comparison, the cumulative errors distribution (CED) curves are plotted. To measure the time efficiency, the average running speed (Frame per Second, FPS) on a single core i5-6200U 2.3GHz CPU is also reported. A single image is fed into the model at a time when computing the speed. In other words, we evaluate methods on four popular metrics: mean error (%), failure rate (%), CED curves, and average running speed. In the next sections, % in all the results are omitted for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-Art Methods</head><p>We compare our work MCL against state-of-the-art methods including ESR <ref type="bibr" target="#b3">[4]</ref>, SDM <ref type="bibr" target="#b4">[5]</ref>, Cascaded CNN <ref type="bibr" target="#b7">[8]</ref>, RCPR <ref type="bibr" target="#b5">[6]</ref>, CFAN <ref type="bibr" target="#b9">[10]</ref>, LBF <ref type="bibr" target="#b6">[7]</ref>, cGPRT <ref type="bibr" target="#b18">[19]</ref>, CFSS <ref type="bibr" target="#b41">[42]</ref>, TCDCN <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, ALR <ref type="bibr" target="#b42">[43]</ref>, CFT <ref type="bibr" target="#b25">[26]</ref>, RFLD <ref type="bibr" target="#b23">[24]</ref>, RecNet <ref type="bibr" target="#b24">[25]</ref>, RAR <ref type="bibr" target="#b26">[27]</ref>, and FLD+PDE <ref type="bibr" target="#b43">[44]</ref>. All the methods are evaluated on testing images using the face bounding boxes provided by benchmarks. In addition to given training images, TCDCN uses outside training data labeled with facial attributes. RAR augments training images with occlusions incurred by outside natural objects like sunglasses, phones, and hands. FLD+PDE performs facial landmark detection, pose and deformation estimation simultaneously, in which the training data of pose and deformation estimation are used. Other methods including our MCL only utilize given training images from the benchmarks. <ref type="table" target="#tab_1">Table I</ref> reports the results of our method and previous works on three benchmarks. Our method MCL outperforms most of the state-of-the-art methods, especially on AFLW dataset where a relative error reduction of 3.93% is achieved compared to RecNet. Cascaded CNN estimates the location of each <ref type="bibr" target="#b1">2</ref> The result is acquired by running the code at https://github.com/seetaface/SeetaFaceEngine/tree/master/FaceAlignment.   We compare with other methods on several challenging images from AFLW and COFW respectively in <ref type="figure" target="#fig_5">Fig. 4</ref>. Our method MCL indicates higher accuracy in the details than previous works. More examples on challenging IBUG are presented in <ref type="figure" target="#fig_6">Fig. 5</ref>. MCL demonstrates a superior capability of handling severe occlusions and complex variations of pose, expression, illumination. The CED curves of MCL and several state-of-the-art methods are shown in <ref type="figure" target="#fig_7">Fig. 6</ref>. It is observed that MCL achieves competitive performance on all three benchmarks. The average running speed of deep learning methods for detecting 68 facial landmarks are presented in <ref type="table" target="#tab_1">Table II</ref>. Except for the methods tested on the i5-6200U 2.3GHz CPU, other methods are reported with the results in the original papers. Since CFAN utilizes multiple networks, it costs more running time. RAR achieves only 4 FPS on a Titan-Z GPU, which cannot be applied to practical scenarios. Both TCDCN and our method MCL are based on only one network, so they show higher speed. Our method only takes 17.5 ms per face on a single core i5-6200U 2.3GHz CPU. This profits from low model complexity and computational costs of our network. It can be concluded that our method is able to be extended to real-time facial landmark tracking in unconstrained environments. C. Ablation Study 1) Global Average Pooling vs. Full Connection: Based on the previous version of our work <ref type="bibr" target="#b12">[13]</ref>, the last maxpooling layer and the D-dimensional fully-connected layer are replaced with a convolutional layer and a Global Average Pooling layer <ref type="bibr" target="#b13">[14]</ref>. The results of the mean error of BM and the previous version (pre-BM) <ref type="bibr" target="#b12">[13]</ref> are shown in <ref type="table" target="#tab_1">Table  III</ref>. It can be seen that BM performs better on IBUG and  COFW but worse on AFLW than pre-BM. It demonstrates that Global Average Pooling is more advantageous for more complex problems with more facial landmarks. There are higher requirements for learned features when localizing more facial landmarks. For simple problems especially for localizing 5 landmarks of AFLW, a plain network with full connection is more prone to being trained.</p><p>The difference between pre-BM and BM is the structure of learning the feature x. The number of parameters for this part of pre-BM and BM are (4 × 4 × 128 + 1)D = 2, 049D and (3 × 3 × 128 + 1)D + 2D + 2D = 1, 157D respectively, where the three terms for BM correspond to the convolution, the expectation and variance of BN <ref type="bibr" target="#b28">[29]</ref>, and the scaling and shifting of BN. Therefore, BM has a stronger feature learning ability with fewer parameters than pre-BM. 2) Robustness of Weighting: To verify the robustness of weighting, random perturbations are added to the weights of landmarks. In particular, we plus a perturbation δ to the weight of each of random n/2 landmarks and minus δ to the weight of each of remaining n − n/2 landmarks, where · refers to rounding down to the nearest integer. <ref type="figure">Fig. 7</ref> shows the variations of mean error of WM with the increase of δ. When δ is 0.4, WM can still achieves good performance. Therefore, weighting the loss of each landmark is robust to random perturbations. Even if different weights are obtained, the results will not be affected as long as the relative sizes of weights are identical. 3) Analysis of Shape Prediction Layers: Our method learns each shape prediction layer respectively with a certain cluster of landmarks being emphasized. The results of WM and two shape prediction layers with respect to the left eye and the right eye on IBUG benchmark are shown in <ref type="table" target="#tab_1">Table IV</ref>. Compared to WM, the left eye model and the right eye model both reduce the alignment errors of their corresponding clusters. As a result, the assembled AM can improve the detection accuracy of landmarks of the left eye and the right eye on the basis of WM.</p><p>Note that the two models also improve the localization precision of other clusters. Taking the left eye model as an example, it additionally reduces the errors of landmarks of right eye, mouth, and chin, which is due to the correlations among different facial parts. Moreover, for the right eye cluster, the right eye model improves the accuracy more significantly than the left eye model. It can be concluded that each shape prediction layer emphasizes on the corresponding cluster respectively. 4) Integration of Weighting Fine-Tuning and Multi-Center Fine-Tuning: Here we validate the effectiveness of weighting fine-tuning by removing the weighting fine-tuning stage to learn a Simplified AM from BM. <ref type="table" target="#tab_5">Table V</ref> presents the results of mean error of Simplified AM and AM respectively on COFW and IBUG. Note that Simplified AM has already acquired good results, which verifies the effectiveness of the multicenter fine-tuning stage. The accuracy of AM is superior to that of Simplified AM especially on challenging IBUG, which is attributed to the integration of two stages. A Weighting Simplified AM from Simplified AM using the weighting finetuning stage is also learned, whose results are shown in <ref type="table" target="#tab_5">Table  V</ref>. It can be seen that Weighting Simplified AM improves slightly on COFW but fails to search a better solution on IBUG. Therefore, we choose to use the multi-center finetuning stage after the weighting fine-tuning stage.  <ref type="table" target="#tab_1">Table VI</ref> summarizes the results of mean error and failure rate of BM, WM, and AM. It can be observed that AM has higher accuracy and stronger robustness than BM and WM. <ref type="figure" target="#fig_9">Fig. 8</ref> depicts the enhancement from WM to AM for several examples of COFW. The localization accuracy of facial landmarks from each cluster is improved in the details. It is because each shape prediction layer increases the detection precision of corresponding cluster respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MCL for Partially Occluded Faces</head><p>The correlations among different facial parts are very useful for face alignment especially for partially occluded faces. To investigate the influence of occlusions, we directly use trained WM and AM without any additional processing for partially occluded faces. Randomly 30% testing faces from COFW are processed with left eyes being occluded, where the tight bounding box covering landmarks of left eye is filled with gray color, as shown in <ref type="figure" target="#fig_10">Fig. 9</ref>.  <ref type="table" target="#tab_1">Table VII</ref> shows the mean error results for the left eye cluster and other clusters of WM and AM on COFW benchmark, where "with (w/) occlusion (occlu.)" denotes that left eyes of the testing faces are processed with handcrafted occlusions as illustrated in <ref type="figure" target="#fig_10">Fig. 9</ref>, and "without (w/o) occlu." denotes that the testing faces are kept unchanged. Note that our method does not process occlusions explicitly, in which the training data is not performed handcrafted occlusions. After processing testing faces with occlusions, the mean error results of both WM and AM increase. Besides the results of landmarks from the left eye cluster, the results of remaining landmarks from other clusters become worse slightly. This is because different facial parts have correlations and the occlusions of the left eye influences results of other facial parts. Note that WM and AM still perform well on occluded left eyes with the mean error of 6.60 and 6.50 respectively, due to the following reasons. First, WM weights each landmark proportional to its alignment error, which exploits correlations among landmarks. Second, AM uses an independent shape prediction layer focusing on a certain cluster of landmarks with small weights u j &gt; 0, j ∈ Q i in Eq. 9 for remaining landmarks, respectively, where correlations among landmarks are further exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Weighting Fine-Tuning for State-of-the-Art Frameworks</head><p>Most recently, there are a few well-designed and welltrained deep learning frameworks advancing the performance of face alignment, in which DAN <ref type="bibr" target="#b44">[45]</ref> is a typical work. DAN uses cascaded deep neural networks to refine the localization accuracy of landmarks iteratively, where the entire face image and the landmark heatmap generated from the previous stage are used in each stage. To evaluate the effectiveness of our method extended to state-of-the-art frameworks, we conduct experiments with our proposed weighting fine-tuning being applied to DAN. In particular, each stage of DAN is first pre-trained and further weighting fine-tuned, where DAN with weighting fine-tuning is named DAN-WM. Note that the results of retrained DAN (re-DAN) using the published code <ref type="bibr" target="#b44">[45]</ref> are slightly worse than reported results of DAN <ref type="bibr" target="#b44">[45]</ref>. For a fair comparison, the results of mean error of DAN, re-DAN, and DAN-WM on IBUG benchmark are all shown in <ref type="table" target="#tab_1">Table VIII</ref>. It can be seen that the mean error of re-DAN is reduced from 7.97 to 7.81 after using our proposed weighting fine-tuning. Note that our method uses only a single neural network, which has a concise structure with low model complexity. Our network can be replaced with a more powerful one such as cascaded deep neural networks, which could further improve the performance of face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have developed a novel multi-center learning framework with multiple shape prediction layers for face alignment. The structure of multiple shape prediction layers is beneficial for reinforcing the learning process of each cluster of landmarks. In addition, we have proposed the model assembling method to integrate multiple shape prediction layers into one shape prediction layer so as to ensure a low model complexity. Extensive experiments have demonstrated the effectiveness of our method including handling complex occlusions and appearance variations. First, each component of our framework including Global Average Pooling, multiple shape prediction layers, weighting fine-tuning, and multicenter fine-tuning contributes to face alignment. Second, our proposed neural network and model assembling method allow real-time performance. Third, we have extended our method for detecting partially occluded faces and integrating with state-of-the-art frameworks, and have shown that our method exploits correlations among landmarks and can further improve the performance of state-of-the-art frameworks. The proposed framework is also promising to be applied for other face analysis tasks and multi-label problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Examples of unconstrained face images with partial occlusions and large pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of our network MCL. The expression k 1 × k 2 × k 3 attached to each layer denotes the height, width, and channel respectively. Every two convolutional layers possess the same expression. The expression k 4 × k 5 /k 6 /k 7 denotes the height, width, stride, and padding of filters respectively. The same type of layers use the identical filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Partitions of facial landmarks for different labeling patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Results of Cascaded CNN, ALR, and MCL on AFLW.(b) Results of RCPR, CFT, and MCL on COFW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Face alignment results of state-of-the-art methods and our method MCL on challenging faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of LBF, CFSS, and MCL on challenging IBUG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of CED curves with previous methods on three benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>9 IBUGFig. 7 .</head><label>97</label><figDesc>Mean error of WM on AFLW and IBUG with different δ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Examples of improvement for different facial landmarks from WM to AM on COFW dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Example faces from COFW with left eyes occluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF RESULTS OF MEAN ERROR WITH STATE-OF-THE-ART METHODS. SEVERAL METHODS DID NOT SHARE THEIR RESULTS OR CODE ON SOME BENCHMARKS, SO WE USE RESULTS MARKED WITH "*" FROM<ref type="bibr" target="#b10">[11]</ref>,<ref type="bibr" target="#b11">[12]</ref>. the second and third level, and every two networks are used to detect one landmark. It is difficult to be extended to dense landmarks owing to the explosion of the number of networks. TCDCN relies on outside training data for auxiliary facial attribute recognition, which limits the universality. It can be seen that MCL outperforms Cascaded CNN and TCDCN on all the benchmarks. Moreover, MCL is robust to occlusions with the performance on par with RFLD, benefiting from utilizing semantical correlations among different landmarks. RecNet and RAR show significant results, but their models are very complex with high computational costs.</figDesc><table><row><cell>Method</cell><cell>AFLW 5 landmarks</cell><cell>COFW 29 landmarks</cell><cell>IBUG 68 landmarks</cell></row><row><cell>ESR [4]</cell><cell>12.4*</cell><cell>11.2*</cell><cell>17.00*</cell></row><row><cell>SDM [5]</cell><cell>8.5*</cell><cell>11.14*</cell><cell>15.40*</cell></row><row><cell>Cascaded CNN [8]</cell><cell>8.72</cell><cell>-</cell><cell>-</cell></row><row><cell>RCPR [6]</cell><cell>11.6*</cell><cell>8.5</cell><cell>17.26*</cell></row><row><cell>CFAN [10]</cell><cell>7.83 2</cell><cell>-</cell><cell>16.78*</cell></row><row><cell>LBF [7]</cell><cell>-</cell><cell>-</cell><cell>11.98</cell></row><row><cell>cGPRT [19]</cell><cell>-</cell><cell>-</cell><cell>11.03</cell></row><row><cell>CFSS [42]</cell><cell>-</cell><cell>-</cell><cell>9.98</cell></row><row><cell>TCDCN [11], [12]</cell><cell>8.0</cell><cell>8.05</cell><cell>8.60</cell></row><row><cell>ALR [43]</cell><cell>7.42</cell><cell>-</cell><cell>-</cell></row><row><cell>CFT [26]</cell><cell>-</cell><cell>6.33</cell><cell>10.06</cell></row><row><cell>RFLD [24]</cell><cell>-</cell><cell>5.93</cell><cell>-</cell></row><row><cell>RecNet [25]</cell><cell>5.60</cell><cell>-</cell><cell>8.44</cell></row><row><cell>RAR [27]</cell><cell>7.23</cell><cell>6.03</cell><cell>8.35</cell></row><row><cell>FLD+PDE [44]</cell><cell>-</cell><cell>6.40</cell><cell>-</cell></row><row><cell>MCL</cell><cell>5.38</cell><cell>6.00</cell><cell>8.51</cell></row><row><cell cols="2">landmark separately in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II AVERAGE</head><label>II</label><figDesc>RUNNING SPEED OF DEEP LEARNING METHODS. THE TIME OF THE FACE DETECTION IS EXCLUDED.</figDesc><table><row><cell>Method</cell><cell>Speed (FPS)</cell><cell>Platform</cell></row><row><cell>CFAN [10]</cell><cell>43</cell><cell>i7-3770 3.4 GHz CPU</cell></row><row><cell>TCDCN [12]</cell><cell>50</cell><cell>i5-6200U 2.3GHz CPU</cell></row><row><cell>CFT [26]</cell><cell>31</cell><cell>i5-6200U 2.3GHz CPU</cell></row><row><cell>RAR [27]</cell><cell>4</cell><cell>Titan-Z GPU</cell></row><row><cell>MCL</cell><cell>57</cell><cell>i5-6200U 2.3GHz CPU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF MEAN ERROR OF PRE-BM AND BM ON THREE BENCHMARKS.</figDesc><table><row><cell>Method</cell><cell>AFLW 5 landmarks</cell><cell>COFW 29 landmarks</cell><cell>IBUG 68 landmarks</cell></row><row><cell>pre-BM [13]</cell><cell>5.61</cell><cell>6.40</cell><cell>9.23</cell></row><row><cell>BM</cell><cell>5.67</cell><cell>6.25</cell><cell>8.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF MEAN ERROR OF LANDMARKS OF EACH CLUSTER ON IBUG.</figDesc><table><row><cell>Cluster</cell><cell>WM</cell><cell>Left Eye Model</cell><cell>Right Eye Model</cell></row><row><cell>Left Eye</cell><cell>8.09</cell><cell>7.92</cell><cell>8.10</cell></row><row><cell>Right Eye</cell><cell>7.73</cell><cell>7.55</cell><cell>7.30</cell></row><row><cell>Nose</cell><cell>6.19</cell><cell>6.42</cell><cell>6.59</cell></row><row><cell>Mouth</cell><cell>6.92</cell><cell>6.80</cell><cell>7.08</cell></row><row><cell>Left Contour</cell><cell>12.66</cell><cell>12.83</cell><cell>12.74</cell></row><row><cell>Chin</cell><cell>13.55</cell><cell>13.50</cell><cell>13.45</cell></row><row><cell>Right Contour</cell><cell>13.38</cell><cell>13.47</cell><cell>13.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V RESULTS</head><label>V</label><figDesc>OF MEAN ERROR OF SIMPLIFIED AM, AM, AND WEIGHTING SIMPLIFIED AM. 29 LANDMARKS OF COFW AND 68 LANDMARKS OF IBUG ARE EVALUATED.</figDesc><table><row><cell>Method Simplified AM AM</cell><cell>Weighting Fine-Tuning √</cell><cell>Multi-Center Fine-Tuning √ √</cell><cell>COFW 6.08 6.00</cell><cell>IBUG 8.67 8.51</cell></row><row><cell>Weighting Simplified AM</cell><cell>√</cell><cell>√</cell><cell>6.05</cell><cell>8.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI MEAN</head><label>VI</label><figDesc>ERROR (ERROR) AND FAILURE RATE (FAILURE) OF BM, WM, AND AM. 5 LANDMARKS OF AFLW, 29 LANDMARKS OF COFW, AND 68 LANDMARKS OF IBUG ARE EVALUATED.</figDesc><table><row><cell>Method</cell><cell cols="2">AFLW Error Failure</cell><cell cols="2">COFW Error Failure</cell><cell cols="2">IBUG Error Failure</cell></row><row><cell>BM</cell><cell>5.67</cell><cell>4.43</cell><cell>6.25</cell><cell>5.13</cell><cell>8.89</cell><cell>27.43</cell></row><row><cell>WM</cell><cell>5.50</cell><cell>3.84</cell><cell>6.11</cell><cell>4.54</cell><cell>8.72</cell><cell>26.80</cell></row><row><cell>AM</cell><cell>5.38</cell><cell>3.47</cell><cell>6.00</cell><cell>3.94</cell><cell>8.51</cell><cell>25.93</cell></row><row><cell cols="4">5) Discussion of All Stages:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII MEAN</head><label>VII</label><figDesc>ERROR RESULTS OF WM AND AM ON COFW W/O OCCLU. AND W/ OCCLU. RESPECTIVELY. MEAN ERROR OF LANDMARKS FROM THE LEFT EYE CLUSTER, AND MEAN ERROR OF REMAINING LANDMARKS FROM OTHER CLUSTERS ARE BOTH SHOWN.</figDesc><table><row><cell>Method</cell><cell cols="2">Left Eye w/o occlu. w/ occlu.</cell><cell cols="2">Others w/o occlu. w/ occlu.</cell></row><row><cell>WM</cell><cell>5.98</cell><cell>6.60</cell><cell>6.17</cell><cell>6.30</cell></row><row><cell>AM</cell><cell>5.88</cell><cell>6.50</cell><cell>6.06</cell><cell>6.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII RESULTS</head><label>VIII</label><figDesc>OF MEAN ERROR OF DAN, RE-DAN, AND DAN-WM ON IBUG.</figDesc><table><row><cell>Method</cell><cell>DAN [45]</cell><cell>re-DAN</cell><cell>DAN-WM</cell></row><row><cell>IBUG</cell><cell>7.57</cell><cell>7.97</cell><cell>7.81</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is an extended version of<ref type="bibr" target="#b12">[13]</ref> with two improvements. The shape prediction layer is replaced from the fully-connected layer to the Global Average Pooling layer<ref type="bibr" target="#b13">[14]</ref>, which has a stronger feature learning ability. To exploit the correlations among landmarks more completely, challenging landmarks are focused firstly before each cluster of landmarks is respectively optimized.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Displaced dynamic expression regression for real-time facial tracking and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An efficient mesh-based face beautifier on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="134" to="142" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade shallow cnn structure for face verification and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page" from="232" to="240" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2887" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a multicenter convolutional network for unconstrained face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using a deformation field model for localizing faces and facial points under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3694" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face alignment using cascade gaussian process regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4204" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3-d face detection, landmark localization, and registration using a point distribution model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="611" to="623" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pose-free facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1944" to="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dense 3d face alignment from 2d videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Consensus of regression for occlusion-robust facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="105" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection under significant head poses and occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3658" to="3666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5743" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep representation from coarse to fine for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The muct landmarked face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nicolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Association of South Africa</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2930" to="2940" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face alignment by deep convolutional network with adaptive learning rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1283" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simultaneous facial landmark detection, pose and deformation estimation under facial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3471" to="3480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
