<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ward</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Dunning</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
						</author>
						<title level="a" type="main">IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in singlemachine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al.,  2016)) and Atari-57 (all available Atari games in Arcade Learning Environment <ref type="bibr" target="#b8">(Bellemare et al., 2013a)</ref>). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach. The source code is publicly available at github.com/deepmind/scalable agent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep reinforcement learning methods have recently mastered a wide variety of domains through trial and error learning <ref type="bibr" target="#b38">Silver et al., 2017;</ref><ref type="bibr" target="#b7">2016;</ref><ref type="bibr" target="#b43">Zoph et al., 2017;</ref><ref type="bibr" target="#b25">Lillicrap et al., 2015;</ref><ref type="bibr" target="#b5">Barth-Maron et al., 2018)</ref>. While the improvements on tasks like the game of Go <ref type="bibr" target="#b38">(Silver et al., 2017)</ref> and Atari games  have been dramatic, the progress has been primarily in single task performance, where an agent is trained on each task * Equal contribution 1 DeepMind Technologies, London, United Kingdom. Correspondence to: Lasse Espeholt &lt;lespe-holt@google.com&gt;. separately. We are interested in developing new methods capable of mastering a diverse set of tasks simultaneously as well as environments suitable for evaluating such methods.</p><p>One of the main challenges in training a single agent on many tasks at once is scalability. Since the current state-ofthe-art methods like A3C  or UNREAL <ref type="bibr" target="#b23">(Jaderberg et al., 2017b)</ref> can require as much as a billion frames and multiple days to master a single domain, training them on tens of domains at once is too slow to be practical.</p><p>We propose the Importance Weighted Actor-Learner Architecture (IMPALA) shown in <ref type="figure" target="#fig_0">Figure 1</ref>. IMPALA is capable of scaling to thousands of machines without sacrificing training stability or data efficiency. Unlike the popular A3C-based agents, in which workers communicate gradients with respect to the parameters of the policy to a central parameter server, IMPALA actors communicate trajectories of experience (sequences of states, actions, and rewards) to a centralised learner. Since the learner in IMPALA has access to full trajectories of experience we use a GPU to perform updates on mini-batches of trajectories while aggressively parallelising all time independent operations. This type of decoupled architecture can achieve very high throughput. However, because the policy used to generate a trajectory can lag behind the policy on the learner by several updates at the time of gradient calculation, learning becomes off-policy. Therefore, we introduce the V-trace off-policy actor-critic algorithm to correct for this harmful discrepancy.</p><p>With the scalable architecture and V-trace combined, IM-PALA achieves exceptionally high data throughput rates of 250,000 frames per second, making it over 30 times faster than single-machine A3C. Crucially, IMPALA is also more data efficient than A3C based agents and more robust to hyperparameter values and network architectures, allowing it to make better use of deeper neural networks. We demonstrate the effectiveness of IMPALA by training a single agent on multi-task problems using DMLab-30, a new challenge set which consists of 30 diverse cognitive tasks in the 3D DeepMind Lab <ref type="bibr">(Beattie et al., 2016)</ref> environment and by training a single agent on all games in the Atari-57 set of tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The earliest attempts to scale up deep reinforcement learning relied on distributed asynchronous SGD <ref type="bibr" target="#b14">(Dean et al., 2012)</ref> with multiple workers. Examples include distributed A3C  and Gorila <ref type="bibr" target="#b29">(Nair et al., 2015)</ref>, a distributed version of Deep Q-Networks . Recent alternatives to asynchronous SGD for RL include using evolutionary processes <ref type="bibr" target="#b35">(Salimans et al., 2017)</ref>, distributed BA3C <ref type="bibr">(Adamski et al., 2018)</ref> and Ape-X  which has a distributed replay but a synchronous learner.</p><p>There have also been multiple efforts that scale up reinforcement learning by utilising GPUs. One of the simplest of such methods is batched A2C <ref type="bibr">(Clemente et al., 2017)</ref>. At every step, batched A2C produces a batch of actions and applies them to a batch of environments. Therefore, the slowest environment in each batch determines the time it takes to perform the entire batch step (see <ref type="figure" target="#fig_6">Figure 2a</ref> and 2b). In other words, high variance in environment speed can severely limit performance. Batched A2C works particularly well on Atari environments, because rendering and game logic are computationally very cheap in comparison to the expensive tensor operations performed by reinforcement learning agents. However, more visually or physically complex environments can be slower to simulate and can have high variance in the time required for each step. Environments may also have variable length (sub)episodes causing a slowdown when initialising an episode.</p><p>The most similar architecture to IMPALA is GA3C <ref type="bibr" target="#b4">(Babaeizadeh et al., 2016)</ref>, which also uses asynchronous data collection to more effectively utilise GPUs. It decouples the acting/forward pass from the gradient calculation/backward pass by using dynamic batching. The actor/learner asynchrony in GA3C leads to instabilities during learning, which <ref type="bibr" target="#b4">(Babaeizadeh et al., 2016)</ref> only partially mitigates by adding a small constant to action probabilities (c) IMPALA <ref type="figure" target="#fig_6">Figure 2</ref>. Timeline for one unroll with 4 steps using different architectures. Strategies shown in (a) and (b) can lead to low GPU utilisation due to rendering time variance within a batch. In (a), the actors are synchronised after every step. In (b) after every n steps. IMPALA (c) decouples acting from learning. during the estimation of the policy gradient. In contrast, IMPALA uses the more principled V-trace algorithm.</p><p>Related previous work on off-policy RL include <ref type="bibr" target="#b31">(Precup et al., 2000;</ref><ref type="bibr" target="#b32">2001;</ref><ref type="bibr" target="#b41">Wawrzynski, 2009;</ref><ref type="bibr" target="#b15">Geist &amp; Scherrer, 2014;</ref><ref type="bibr" target="#b30">O'Donoghue et al., 2017)</ref> and <ref type="bibr" target="#b28">(Harutyunyan et al., 2016)</ref>. The closest work to ours is the Retrace algorithm <ref type="bibr" target="#b28">(Munos et al., 2016)</ref> which introduced an off-policy correction for multi-step RL, and has been used in several agent architectures <ref type="bibr" target="#b16">Gruslys et al., 2018)</ref>. Retrace requires learning state-action-value functions Q in order to make the off-policy correction. However, many actor-critic methods such as A3C learn a state-value function V instead of a state-action-value function Q. V-trace is based on the state-value function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMPALA</head><p>IMPALA <ref type="figure" target="#fig_0">(Figure 1</ref>) uses an actor-critic setup to learn a policy π and a baseline function V π . The process of generating experiences is decoupled from learning the parameters of π and V π . The architecture consists of a set of actors, repeatedly generating trajectories of experience, and one or more learners that use the experiences sent from actors to learn π off-policy.</p><p>At the beginning of each trajectory, an actor updates its own local policy µ to the latest learner policy π and runs it for n steps in its environment. After n steps, the actor sends the trajectory of states, actions and rewards x 1 , a 1 , r 1 , . . . , x n , a n , r n together with the corresponding policy distributions µ(a t |x t ) and initial LSTM state to the learner through a queue. The learner then continuously updates its policy π on batches of trajectories, each collected from many actors. This simple architecture enables the learner(s) to be accelerated using GPUs and actors to be easily distributed across many machines. However, the learner policy π is potentially several updates ahead of the actor's policy µ at the time of update, therefore there is a policy-lag between the actors and learner(s). V-trace cor-rects for this lag to achieve extremely high data throughput while maintaining data efficiency. Using an actor-learner architecture, provides fault tolerance like distributed A3C but often has lower communication overhead since the actors send observations rather than parameters/gradients.</p><p>With the introduction of very deep model architectures, the speed of a single GPU is often the limiting factor during training. IMPALA can be used with distributed set of learners to train large neural networks efficiently as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Parameters are distributed across the learners and actors retrieve the parameters from all the learners in parallel while only sending observations to a single learner. IMPALA use synchronised parameter update which is vital to maintain data efficiency when scaling to many machines <ref type="bibr" target="#b10">(Chen et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Efficiency Optimisations</head><p>GPUs and many-core CPUs benefit greatly from running few large, parallelisable operations instead of many small operations. Since the learner in IMPALA performs updates on entire batches of trajectories, it is able to parallelise more of its computations than an online agent like A3C. As an example, a typical deep RL agent features a convolutional network followed by a Long Short-Term Memory (LSTM) <ref type="bibr" target="#b20">(Hochreiter &amp; Schmidhuber, 1997</ref>) and a fully connected output layer after the LSTM. An IMPALA learner applies the convolutional network to all inputs in parallel by folding the time dimension into the batch dimension. Similarly, it also applies the output layer to all time steps in parallel once all LSTM states are computed. This optimisation increases the effective batch size to thousands. LSTM-based agents also obtain significant speedups on the learner by exploiting the network structure dependencies and operation fusion <ref type="bibr" target="#b3">(Appleyard et al., 2016)</ref>.</p><p>Finally, we also make use of several off the shelf optimisations available in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2017)</ref> such as preparing the next batch of data for the learner while still performing computation, compiling parts of the computational graph with XLA (a TensorFlow Just-In-Time compiler) and optimising the data format to get the maximum performance from the cuDNN framework <ref type="bibr" target="#b11">(Chetlur et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">V-trace</head><p>Off-policy learning is important in the decoupled distributed actor-learner architecture because of the lag between when actions are generated by the actors and when the learner estimates the gradient. To this end, we introduce a novel offpolicy actor-critic algorithm for the learner, called V-trace.</p><p>First, let us introduce some notations. We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP), see <ref type="bibr">(Puterman, 1994;</ref><ref type="bibr" target="#b39">Sutton &amp; Barto, 1998)</ref> where the goal is to find a policy π that maximises the expected sum of future discounted rewards:</p><formula xml:id="formula_0">V π (x) def = E π t≥0 γ t r t ,</formula><p>where γ ∈ [0, 1) is the discount factor, r t = r(x t , a t ) is the reward at time t, x t is the state at time t (initialised in x 0 = x) and a t ∼ π(·|x t ) is the action generated by following some policy π.</p><p>The goal of an off-policy RL algorithm is to use trajectories generated by some policy µ, called the behaviour policy, to learn the value function V π of another policy π (possibly different from µ), called the target policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">V-trace target</head><p>Consider a trajectory (x t , a t , r t ) t=s+n t=s generated by the actor following some policy µ. We define the n-steps V-trace target for V (x s ), our value approximation at state x s , as:</p><formula xml:id="formula_1">v s def = V (x s ) + s+n−1 t=s γ t−s t−1 i=s c i δ t V , (1) where δ t V def = ρ t r t + γV (x t+1 ) − V (x t ) is a temporal difference for V , and ρ t def = min ρ, π(at|xt)</formula><p>µ(at|xt) and c i def = min c, π(ai|xi) µ(ai|xi) are truncated importance sampling (IS) weights (we make use of the notation t−1 i=s c i = 1 for s = t). In addition we assume that the truncation levels are such thatρ ≥c.</p><p>Notice that in the on-policy case (when π = µ), and assuming thatc ≥ 1, then all c i = 1 and ρ t = 1, thus (1) rewrites</p><formula xml:id="formula_2">v s = V (x s ) + s+n−1 t=s γ t−s r t + γV (x t+1 ) − V (x t ) = s+n−1 t=s γ t−s r t + γ n V (x s+n ),<label>(2)</label></formula><p>which is the on-policy n-steps Bellman target. Thus in the on-policy case, V-trace reduces to the on-policy n-steps Bellman update. This property (which Retrace <ref type="bibr" target="#b28">(Munos et al., 2016)</ref> does not have) allows one to use the same algorithm for off-and on-policy data.</p><p>Notice that the (truncated) IS weights c i and ρ t play different roles. The weight ρ t appears in the definition of the temporal difference δ t V and defines the fixed point of this update rule. In a tabular case, where functions can be perfectly represented, the fixed point of this update (i.e., when V (x s ) = v s for all states), characterised by δ t V being equal to zero in expectation (under µ), is the value function V πρ of some policy πρ, defined by</p><formula xml:id="formula_3">πρ(a|x) def = min ρµ(a|x), π(a|x) b∈A min ρµ(b|x), π(b|x) ,<label>(3)</label></formula><p>(see the analysis in Appendix A ). So whenρ is infinite (i.e. no truncation of ρ t ), then this is the value function V π of the target policy. However if we choose a truncation levelρ &lt; ∞, our fixed point is the value function V πρ of a policy πρ which is somewhere between µ and π. At the limit whenρ is close to zero, we obtain the value function of the behaviour policy V µ . In Appendix A we prove the contraction of a related V-trace operator and the convergence of the corresponding online V-trace algorithm.</p><p>The weights c i are similar to the "trace cutting" coefficients in Retrace. Their product c s . . . c t−1 measures how much a temporal difference δ t V observed at time t impacts the update of the value function at a previous time s. The more dissimilar π and µ are (the more off-policy we are), the larger the variance of this product. We use the truncation levelc as a variance reduction technique. However notice that this truncation does not impact the solution to which we converge (which is characterised byρ only).</p><p>Thus we see that the truncation levelsc andρ represent different features of the algorithm:ρ impacts the nature of the value function we converge to, whereasc impacts the speed at which we converge to this function. Remark 1. V-trace targets can be computed recursively:</p><formula xml:id="formula_4">v s = V (x s ) + δ s V + γc s v s+1 − V (x s+1 ) .</formula><p>Remark 2. Like in Retrace(λ), we can also consider an additional discounting parameter λ ∈ [0, 1] in the definition of V-trace by setting c i = λ min c, π(ai|xi) µ(ai|xi) . In the onpolicy case, when n = ∞, V-trace then reduces to TD(λ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Actor-Critic algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POLICY GRADIENT</head><p>In the on-policy case, the gradient of the value function V µ (x 0 ) with respect to some parameter of the policy µ is</p><formula xml:id="formula_5">∇V µ (x 0 ) = E µ s≥0 γ s ∇ log µ(a s |x s )Q µ (x s , a s ) ,</formula><p>where Q µ (x s , a s ) def = E µ t≥s γ t−s r t |x s , a s is the state-action value of policy µ at (x s , a s ). This is usually implemented by a stochastic gradient ascent that updates the policy parameters in the direction of E as∼µ(·|xs) ∇ log µ(a s |x s )q s x s , where q s is an estimate of Q µ (x s , a s ), and averaged over the set of states x s that are visited under some behaviour policy µ. Now in the off-policy setting that we consider, we can use an IS weight between the policy being evaluated πρ and the behaviour policy µ, to update our policy parameter in the direction of</p><formula xml:id="formula_6">E as∼µ(·|xs) πρ(a s |x s ) µ(a s |x s ) ∇ log πρ(a s |x s )q s x s<label>(4)</label></formula><p>where q s def = r s + γv s+1 is an estimate of Q πρ (x s , a s ) built from the V-trace estimate v s+1 at the next state x s+1 .</p><p>The reason why we use q s instead of v s as the target for our Q-value Q πρ (x s , a s ) is that, assuming our value estimate is correct at all states, i.e. V = V πρ , then we have E[q s |x s , a s ] = Q πρ (x s , a s ) (whereas we do not have this property if we choose q t = v t ). See Appendix A for analysis and Appendix E.3 for a comparison of different ways to estimate q s .</p><p>In order to reduce the variance of the policy gradient estimate (4), we usually subtract from q s a state-dependent baseline, such as the current value approximation V (x s ).</p><p>Finally notice that (4) estimates the policy gradient for πρ which is the policy evaluated by the V-trace algorithm when using a truncation levelρ. However assuming the bias V πρ − V π is small (e.g. ifρ is large enough) then we can expect q s to provide us with a good estimate of Q π (x s , a s ). Taking into account these remarks, we derive the following canonical V-trace actor-critic algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V-TRACE ACTOR-CRITIC ALGORITHM</head><p>Consider a parametric representation V θ of the value function and the current policy π ω . Trajectories have been generated by actors following some behaviour policy µ. The V-trace targets v s are defined by (1). At training time s, the value parameters θ are updated by gradient descent on the l2 loss to the target v s , i.e., in the direction of</p><formula xml:id="formula_7">v s − V θ (x s ) ∇ θ V θ (x s ),</formula><p>and the policy parameters ω in the direction of the policy gradient:</p><formula xml:id="formula_8">ρ s ∇ ω log π ω (a s |x s ) r s + γv s+1 − V θ (x s ) .</formula><p>In order to prevent premature convergence we may add an entropy bonus, like in A3C, along the direction −∇ ω a π ω (a|x s ) log π ω (a|x s ).</p><p>The overall update is obtained by summing these three gradients rescaled by appropriate coefficients, which are hyperparameters of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We investigate the performance of IMPALA under multiple settings. For data efficiency, computational performance and effectiveness of the off-policy correction we look at the learning behaviour of IMPALA agents trained on individual tasks. For multi-task learning we train agents-each with one set of weights for all tasks-on a newly introduced collection of 30 DeepMind Lab tasks and on all 57 games of the Atari Learning Environment <ref type="bibr" target="#b8">(Bellemare et al., 2013a)</ref>.</p><p>For all the experiments we have used two different model architectures: a shallow model similar to    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>CPUs GPUs 1 FPS 2  <ref type="table" target="#tab_7">Table 1</ref>. Throughput on seekavoid arena 01 (task 1) and rooms keys doors puzzle (task 2) with the shallow model in <ref type="figure" target="#fig_1">Figure 3</ref>. The latter has variable length episodes and slow restarts. Batched A2C and IMPALA use batch size 32 if not otherwise mentioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Machine</head><p>with an LSTM before the policy and value (shown in <ref type="figure" target="#fig_1">Figure 3 (left)</ref>) and a deeper residual model <ref type="bibr" target="#b18">(He et al., 2016)</ref> (shown in <ref type="figure" target="#fig_1">Figure 3</ref> (right)). For tasks with a language channel we used an LSTM with text embeddings as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Computational Performance</head><p>High throughput, computational efficiency and scalability are among the main design goals of IMPALA. To demonstrate that IMPALA outperforms current algorithms in these metrics we compare A3C , batched A2C variations and IMPALA variants with various optimisations. For single-machine experiments using GPUs, we use dynamic batching in the forward pass to avoid several batch size 1 forward passes. Our dynamic batching module is implemented by specialised TensorFlow operations but is conceptually similar to the queues used in GA3C. <ref type="table" target="#tab_7">Table 1</ref> details the results for single-machine and multi-machine versions with the shallow model from <ref type="figure" target="#fig_1">Figure 3</ref>. In the singlemachine case, IMPALA achieves the highest performance on both tasks, ahead of all batched A2C variants and ahead of A3C. However, the distributed, multi-machine setup is where IMPALA can really demonstrate its scalability. With the optimisations from Section 3.1 to speed up the GPUbased learner, the IMPALA agent achieves a throughput rate of 250,000 frames/sec or 21 billion frames/day. Note, to reduce the number of actors needed per learner, one can use auxiliary losses, data from experience replay or other expensive learner-only computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Single-Task Training</head><p>To investigate IMPALA's learning dynamics, we employ the single-task scenario where we train agents individually on 5 different DeepMind Lab tasks. The task set consists of a planning task, two maze navigation tasks, a laser tag task with scripted bots and a simple fruit collection task.</p><p>We perform hyperparameter sweeps over the weighting of entropy regularisation, the learning rate and the RMSProp epsilon. For each experiment we use an identical set of 24 pre-sampled hyperparameter combinations from the ranges in Appendix D.1 . The other hyperparameters were fixed to values specified in Appendix D.3 . <ref type="figure">Figure 4</ref> shows a comparison between IMPALA, A3C and batched A2C with the shallow model in <ref type="figure" target="#fig_1">Figure 3</ref>. In all of the 5 tasks, either batched A2C or IMPALA reach the best final average return and in all tasks but seekavoid arena 01 they are ahead of A3C throughout the entire course of training. IMPALA outperforms the synchronous batched A2C on 2 out of 5 tasks while achieving much higher throughput (see <ref type="table" target="#tab_7">Table 1</ref>). We hypothesise that this behaviour could stem from the V-trace off-policy correction acting similarly to generalised advantage estimation <ref type="bibr" target="#b36">(Schulman et al., 2016)</ref> and asynchronous data collection yielding more diverse batches of experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">CONVERGENCE AND STABILITY</head><p>In addition to reaching better final performance, IMPALA is also more robust to the choice of hyperparameters than A3C. <ref type="figure">Figure 4</ref> compares the final performance of the aforementioned methods across different hyperparameter combinations, sorted by average final return from high to low. Note that IMPALA achieves higher scores over a larger number of combinations than A3C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">V-TRACE ANALYSIS</head><p>To analyse V-trace we investigate four different algorithms: 1. No-correction -No off-policy correction.   <ref type="table" target="#tab_7">Table 2</ref>. Average final return over 3 best hyperparameters for different off-policy correction methods on 5 DeepMind Lab tasks. When the lag in policy is negligible both V-trace and 1-step importance sampling perform similarly well and better than ε-correction/Nocorrection. However, when the lag increases due to use of experience replay, V-trace performs better than all other methods in 4 out 5 tasks.</p><p>2. ε-correction -Add a small value (ε = 1e-6) during gradient calculation to prevent log π(a) from becoming very small and leading to numerical instabilities, similar to <ref type="bibr" target="#b4">(Babaeizadeh et al., 2016)</ref>.</p><p>3. 1-step importance sampling -No off-policy correction when optimising V (x). For the policy gradient, multiply the advantage at each time step by the corresponding importance weight. This variant is similar to V-trace without "traces" and is included to investigate the importance of "traces" in V-trace. 4. V-trace as described in Section 4.</p><p>For V-trace and 1-step importance sampling we clip each importance weight ρ t and c t at 1 (i.e.c =ρ = 1). This reduces the variance of the gradient estimate but introduces a bias. Out ofρ ∈ [1, 10, 100] we found thatρ = 1 worked best.</p><p>We evaluate all algorithms on the set of 5 DeepMind Lab tasks from the previous section. We also add an experience replay buffer on the learner to increase the off-policy gap between π and µ. In the experience replay experiments we draw 50% of the items in each batch uniformly at random from the replay buffer. <ref type="table" target="#tab_7">Table 2</ref> shows the final performance for each algorithm with and without replay respectively. In the no replay setting, V-trace performs best on 3 out of 5 tasks, followed by 1-step importance sampling, ε-correction and No-correction. Although 1-step importance sampling performs similarly to V-trace in the no-replay setting, the gap widens on 4 out 5 tasks when using experience replay. This suggests that the cruder 1-step importance sampling approximation becomes insufficient as the target and behaviour policies deviate from each other more strongly. Also note that V-trace is the only variant that consistently benefits from adding experience replay. ε-correction improves significantly over No-correction on two tasks but lies far behind the importance-sampling based methods, particularly in the more off-policy setting with experience replay. <ref type="figure" target="#fig_0">Figure E.1</ref> shows results of a more detailed analysis. <ref type="figure" target="#fig_6">Figure E.2</ref> shows that the importance-sampling based methods also perform better across all hyperparameters and are typically more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Multi-Task Training</head><p>IMPALA's high data throughput and data efficiency allow us to train not only on one task but on multiple tasks in parallel with only a minimal change to the training setup. Instead of running the same task on all actors, we allocate a fixed number of actors to each task in the multi-task suite. Note, the model does not know which task it is being trained or evaluated on. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">DMLAB-30</head><p>To test IMPALA's performance in a multi-task setting we use DMLab-30, a set of 30 diverse tasks built on DeepMind Lab. Among the many task types in the suite are visually complex environments with natural-looking terrain, instruction-based tasks with grounded language <ref type="bibr" target="#b19">(Hermann et al., 2017)</ref>, navigation tasks, cognitive <ref type="bibr" target="#b24">(Leibo et al., 2018)</ref> and first-person tagging tasks featuring scripted bots as opponents. A detailed description of DMLab-30 and the tasks are available at github.com/deepmind/lab and deepmind.com/dm-lab-30.</p><p>We compare multiple variants of IMPALA with a distributed A3C implementation. Except for agents using populationbased training (PBT) <ref type="bibr" target="#b22">(Jaderberg et al., 2017a)</ref>, all agents are trained with hyperparameter sweeps across the same range given in Appendix D.1 . We report mean capped human normalised score where the score for each task is capped at 100% (see Appendix B ). Using mean capped human normalised score emphasises the need to solve multiple tasks instead of focusing on becoming super human on a single task. For PBT we use the mean capped human normalised score as fitness function and tune entropy cost, learning rate and RMSProp ε. See Appendix F for the specifics of the PBT setup.</p><p>In particular, we compare the following agent variants. A3C, deep, a distributed implementation with 210 workers (7 per task) featuring the deep residual network architecture <ref type="figure" target="#fig_1">(Figure 3 (Right)</ref>). IMPALA, shallow with 210 actors and IMPALA, deep with 150 actors both with a single learner. IMPALA, deep, PBT, the same as IMPALA, deep, but additionally using the PBT <ref type="bibr" target="#b22">(Jaderberg et al., 2017a)</ref> for hyperparameter optimisation. Finally IMPALA, deep, PBT, 8 learners, which utilises 8 learner GPUs to maximise learning speed. We also train IMPALA agents in an expert setting, IMPALA-Experts, deep, where a separate agent is trained per task. In this case we did not optimise hyperparameters for each task separately but instead across all tasks on which the 30 expert agents were trained. <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure">Figure 5</ref> show all variants of IMPALA performing much better than the deep distributed A3C. Moreover, the deep variant of IMPALA performs better than the shal-low network version not only in terms of final performance but throughout the entire training. Note in <ref type="table" target="#tab_3">Table 3</ref> that IMPALA, deep, PBT, 8 learners, although providing much higher throughput, reaches the same final performance as the 1 GPU IMPALA, deep, PBT in the same number of steps. Of particular importance is the gap between the IMPALA-Experts which were trained on each task individually and IMPALA, deep, PBT which was trained on all tasks at once. As <ref type="figure">Figure 5</ref> shows, the multi-task version is outperforms IMPALA-Experts throughout training and the breakdown into individual scores in Appendix B shows positive transfer on tasks such as language tasks and laser tag tasks.</p><p>Comparing A3C to IMPALA with respect to wall clock time ( <ref type="figure" target="#fig_3">Figure 6</ref>) further highlights the scalability gap between the two approaches. IMPALA with 1 learner takes only around 10 hours to reach the same performance that A3C approaches after 7.5 days. Using 8 learner GPUs instead of 1 further speeds up training of the deep model by a factor of 7 to 210K frames/sec, up from 30K frames/sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">ATARI</head><p>The Atari Learning Environment (ALE) <ref type="bibr" target="#b9">(Bellemare et al., 2013b)</ref> has been the testing ground of most recent deep reinforcement agents. Its 57 tasks pose challenging reinforcement learning problems including exploration, planning, reactive play and complex visual input. Most games feature very different visuals and game mechanics which makes this domain particularly challenging for multi-task learning.</p><p>We train IMPALA and A3C agents on each game individually and compare their performance using the deep network (without the LSTM) introduced in Section 5. We also provide results using a shallow network that is equivalent to the feed forward network used in  which features three convolutional layers. The network is provided with a short term history by stacking the 4 most recent observations at each step. For details on pre-processing and hyperparameter setup please refer to Appendix G .</p><p>In addition to individual per-game experts, trained for 200 million frames with a fixed set of hyperparameters, we train an IMPALA Atari-57 agent-one agent, one set of weightson all 57 Atari games at once for 200 million frames per game or a total of 11.4 billion frames. For the Atari-57 agent, we use population based training with a population size of 24 to adapt entropy regularisation, learning rate, RMSProp ε and the global gradient norm clipping threshold throughout training.</p><p>We compare all algorithms in terms of median human normalised score across all 57 Atari games. Evaluation follows a standard protocol, each game-score is the mean over 200 evaluation episodes, each episode was started with a random <ref type="figure">Figure 5</ref>. Performance of best agent in each sweep/population during training on the DMLab-30 task-set wrt. data consumed across all environments. IMPALA with multi-task training is not only faster, it also converges at higher accuracy with better data efficiency across all 30 tasks. The x-axis is data consumed by one agent out of a hyperparameter sweep/PBT population of 24 agents, total data consumed across the whole population/sweep can be obtained by multiplying with the population/sweep size.  As  <ref type="table" target="#tab_5">Table 4</ref>. Human normalised scores on Atari-57. Up to 30 no-ops at the beginning of each episode. For a level-by-level comparison to ACKTR <ref type="bibr" target="#b42">(Wu et al., 2017)</ref> and Reactor see Appendix C.1 .</p><p>the high diversity in visual appearance and game mechanics within the ALE suite, IMPALA multi-task still manages to stay competitive to A3C, shallow, experts, commonly used as a baseline in related work. ALE is typically considered a hard multi-task environment, often accompanied by negative transfer between tasks <ref type="bibr" target="#b34">(Rusu et al., 2016)</ref>. To our knowledge, IMPALA is the first agent to be trained in a multi-task setting on all 57 games of ALE that is competitive with a standard expert baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have introduced a new highly scalable distributed agent, IMPALA, and a new off-policy learning algorithm, V-trace. With its simple but scalable distributed architecture, IM-PALA can make efficient use of available compute at small and large scale. This directly translates to very quick turnaround for investigating new ideas and opens up unexplored opportunities.</p><p>V-trace is a general off-policy learning algorithm that is more stable and robust compared to other off-policy correction methods for actor critic agents. We have demonstrated that IMPALA achieves better performance compared to A3C variants in terms of data efficiency, stability and final performance. We have further evaluated IMPALA on the new DMLab-30 set and the Atari-57 set. To the best of our knowledge, IMPALA is the first Deep-RL agent that has been successfully tested in such large-scale multi-task settings and it has shown superior performance compared to A3C based agents (49.4% vs. 23.8% human normalised score on DMLab-30). Most importantly, our experiments on DMLab-30 show that, in the multi-task setting, positive transfer between individual tasks lead IMPALA to achieve better performance compared to the expert training setting. We believe that IMPALA provides a simple yet scalable and robust framework for building better Deep-RL agents and has the potential to enable research on new challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A. Analysis of V-trace A.1. V-trace operator</p><p>Define the V-trace operator R:</p><formula xml:id="formula_9">RV (x) def = V (x) + E µ t≥0 γ t c 0 . . . c t−1 ρ t r t + γV (x t+1 ) − V (x t ) x 0 = x, µ ,<label>(5)</label></formula><p>where the expectation E µ is with respect to the policy µ which has generated the trajectory (x t ) t≥0 , i.e.,</p><formula xml:id="formula_10">x 0 = x, x t+1 ∼ p(·|x t , a t ), a t ∼ µ(·|x t ).</formula><p>Here we consider the infinite-horizon operator but very similar results hold for the n-step truncated operator.</p><p>Theorem 1. Let ρ t = min ρ, π(at|xt) µ(at|xt) and c t = min c, π(at|xt) µ(at|xt) be truncated importance sampling weights, withρ ≥c. Assume that there exists β ∈ (0, 1] such that E µ ρ 0 ≥ β. Then the operator R defined by (5) has a unique fixed point V πρ , which is the value function of the policy πρ defined by</p><formula xml:id="formula_11">πρ(a|x) def = min ρµ(a|x), π(a|x) b∈A min ρµ(b|x), π(b|x) ,<label>(6)</label></formula><p>Furthermore, R is a η-contraction mapping in sup-norm, with</p><formula xml:id="formula_12">η def = γ −1 − (γ −1 − 1)E µ t≥0 γ t t−2 i=0 c i ρ t−1 ≤ 1 − (1 − γ)β &lt; 1.</formula><p>Remark 3. The truncation levelsc andρ play different roles in this operator:</p><p>•ρ impacts the fixed-point of the operator, thus the policy πρ which is evaluated. Forρ = ∞ (untruncated ρ t ) we get the value function of the target policy V π , whereas for finiteρ, we evaluate a policy which is in between µ and π (and when ρ is close to 0, then we evaluate V µ ). So the largerρ the smaller the bias in off-policy learning. The variance naturally grows withρ. However notice that we do not take the product of those ρ t coefficients (in contrast to the c s coefficients) so the variance does not explode with the time horizon.</p><p>•c impacts the contraction modulus η of R (thus the speed at which an online-algorithm like V-trace will converge to its fixed point V πρ ). In terms of variance reduction, here is it really important to truncate the importance sampling ratios in c t because we take the product of those. Fortunately, our result says that for any level of truncationc, the fixed point (the value function V πρ we converge to) is the same: it does not depend onc but onρ only.</p><p>Proof. First notice that we can rewrite R as</p><formula xml:id="formula_13">RV (x) = (1 − E µ ρ 0 )V (x) + E µ   t≥0 γ t t−1 s=0 c s ρ t r t + γ[ρ t − c t ρ t+1 ]V (x t+1 )   .</formula><p>Thus</p><formula xml:id="formula_14">RV 1 (x) − RV 2 (x) = (1 − E µ ρ 0 ) V 1 (x) − V 2 (x) + E µ   t≥0 γ t+1 t−1 s=0 c s [ρ t − c t ρ t+1 ] V 1 (x t+1 ) − V 2 (x t+1 )   . = E µ    t≥0 γ t t−2 s=0 c s [ρ t−1 − c t−1 ρ t αt ] V 1 (x t ) − V 2 (x t )    ,</formula><p>with the notation that c −1 = ρ −1 = 1 and t−2 s=0 c s = 1 for t = 0 and 1. Now the coefficients (α t ) t≥0 are non-negative in expectation. Indeed, sinceρ ≥c, we have</p><formula xml:id="formula_15">E µ α t = E ρ t−1 − c t−1 ρ t ≥ E µ c t−1 (1 − ρ t ) ≥ 0, since E µ ρ t ≤ E µ π(at|xt) µ(at|xt) = 1. Thus V 1 (x) − V 2 (x)</formula><p>is a linear combination of the values V 1 − V 2 at other states, weighted by non-negative coefficients whose sum is</p><formula xml:id="formula_16">t≥0 γ t E µ t−2 s=0 c s [ρ t−1 − c t−1 ρ t ] = t≥0 γ t E µ t−2 s=0 c s ρ t−1 − t≥0 γ t E µ t−1 s=0 c s ρ t = t≥0 γ t E µ t−2 s=0 c s ρ t−1 − γ −1   t≥0 γ t E µ t−2 s=0 c s ρ t−1 − 1   = γ −1 − (γ −1 − 1) t≥0 γ t E µ t−2 s=0 c s ρ t−1 ≥1+γEµρ0 ≤ 1 − (1 − γ)E µ ρ 0 ≤ 1 − (1 − γ)β &lt; 1. We deduce that RV 1 (x) − RV 2 (x) ≤ η V 1 − V 2 ∞ , with η = γ −1 − (γ −1 − 1) t≥0 γ t E µ t−2 s=0 c s ρ t−1 ≤ 1 − (1 − γ)β &lt; 1, so R is a contraction mapping.</formula><p>Thus R possesses a unique fixed point. Let us now prove that this fixed point is V πρ . We have:</p><formula xml:id="formula_17">E µ ρ t r t + γV πρ (x t+1 ) − V πρ (x t ) x t = a µ(a|x t ) min ρ, π(a|x t ) µ(a|x t ) r(x t , a) + γ y p(y|x t , a)V πρ (y) − V πρ (x t ) = a πρ(a|x t ) r(x t , a) + γ y p(y|x t , a)V πρ (y) − V πρ (x t ) =0 b</formula><p>min ρµ(b|x t ), π(b|x t ) = 0, since this is the Bellman equation for V πρ . We deduce that RV πρ = V πρ , thus V πρ is the unique fixed point of R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Online learning</head><p>Theorem 2. Assume a tabular representation, i.e. the state and action spaces are finite. Consider a set of trajectories, with the k th trajectory x 0 , a 0 , r 0 , x 1 , a 1 , r 1 , . . . generated by following µ: a t ∼ µ(·|x t ). For each state x s along this trajectory, update</p><formula xml:id="formula_18">V k+1 (x s ) = V k (x s ) + α k (x s ) t≥s γ t−s c s . . . c t−1 ρ t r t + γV k (x t+1 ) − V k (x t ) ,<label>(7)</label></formula><p>with c i = min c, π(ai|xi) µ(ai|xi) , ρ i = min ρ, π(ai|xi) µ(ai|xi) ,ρ ≥c. Assume that (1) all states are visited infinitely often, and (2) the stepsizes obey the usual Robbins-Munro conditions: for each state x, k α k (x) = ∞, k α 2 k (x) &lt; ∞. Then V k → V πρ almost surely.</p><p>The proof is a straightforward application of the convergence result for stochastic approximation algorithms to the fixed point of a contraction operator, see e.g. <ref type="bibr" target="#b45">Dayan &amp; Sejnowski (1994)</ref>; <ref type="bibr" target="#b44">Bertsekas &amp; Tsitsiklis (1996)</ref>; <ref type="bibr" target="#b47">Kushner &amp; Yin (2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. On the choice of q s in policy gradient</head><p>The policy gradient update rule (4) makes use of the coefficient q s = r s + γv s+1 as an estimate of Q πρ (x s , a s ) built from the V-trace estimate v s+1 at the next state x s+1 . The reason why we use q s instead of v s as target for our Q-value Q πρ (x s , a s ) is to make sure our estimate of the Q-value is as unbiased as possible, and the first requirement is that it is entirely unbiased in the case of perfect representation of the V-values. Indeed, assuming our value function is correctly estimated at all states, i.e. V = V πρ , then we have E[q s |x s , a s ] = Q πρ (x s , a s ) (whereas we do not have this property for v t ). Indeed,  Human Normalised Score  </p><formula xml:id="formula_19">E[q s |x s , a s ] = r s + γE V πρ (x s+1 ) + δ s+1 V πρ + γc s+1 δ s+2 V πρ + . . . = r s + γE V πρ (x s+1 ) = Q πρ (x s , a s ) whereas E[v s |x s , a s ] = V πρ (x s ) + ρ s r s + γE V πρ (x s+1 ) − V πρ (x s ) + γc s δ s+1 V πρ + . . . = V πρ (x s ) + ρ s r s + γE V πρ (x s+1 ) − V πρ (x s ) = V πρ (x s )(1 − ρ s ) + ρ s Q πρ (x s , a s ), which is different from Q πρ (x s , a s ) when V πρ (x s ) = Q πρ (x s , a s</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameters</head><p>In this section, the specific parameter settings that are used throughout our experiments are given in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Range Distribution</head><p>Entropy regularisation [5e-5, 1e-2] Log uniform Learning rate [5e-6, 5e-3] Log uniform RMSProp epsilon (ε) regularisation parameter [1e-1, 1e-3, 1e-5, 1e-7] Categorical <ref type="table" target="#tab_7">Table D</ref>.1. The ranges used in sampling hyperparameters across all experiments that used a sweep and for the initial hyperparameters for PBT. Sweep size and population size are 24. Note, the loss is summed across the batch and time dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action</head><p>Native DeepMind Lab Action Forward [ 0, 0, 0, 1, 0, 0, 0] Backward [ 0, 0, 0, -1, 0, 0, 0] Strafe Left [ 0, 0, -1, 0, 0, 0, 0] Strafe Right [ 0, 0, 1, 0, 0, 0, 0] Look Left [-20, 0, 0, 0, 0, 0, 0] Look Right [ 20, 0, 0, 0, 0, 0, 0] Forward + Look Left [-20, 0, 0, 1, 0, 0, 0] Forward + Look Right [ 20, 0, 0, 1, 0, 0, 0] Fire [ 0, 0, 0, 0, 1, 0, 0] </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Fixed Model Hyperparameters</head><p>In this section, we list all the hyperparameters that were kept fixed across all experiments in the paper which are mostly concerned with observations specifications and optimisation. We first show below the reward pre-processing function that is used across all experiments using DeepMind Lab, followed by all fixed numerical values.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Estimating the State Action Value for Policy Gradient</head><p>We investigated different ways of estimating the state action value function used to estimate advantages for the policy gradient calculation. The variant presented in the main section of the paper uses the V-trace corrected value function v s+1 to estimate q s = r s + γv s+1 . Another possibility is to use the actor-critic baseline V (x s+1 ) to estimate q s = r s + γV (x s+1 ).</p><p>Note that the latter variant does not use any information from the current policy rollout to estimate the policy gradient and relies on an accurate estimate of the value function. We found the latter variant to perform worse both when comparing the top <ref type="formula" target="#formula_3">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Population Based Training</head><p>For Population Based Training we used a "burn-in" period of 20 million frames where no evolution is done. This is to stabilise the process and to avoid very rapid initial adaptation which hinders diversity. After collecting 5,000 episode rewards in total, the mean capped human normalised score is calculated and a random instance in the population is selected. If the score of the selected instance is more than an absolute 5% higher, then the selected instance weights and parameters are copied.</p><p>No matter if a copy happened or not, each parameter (RMSProp epsilon, learning rate and entropy cost) is permuted with 33% probability by multiplying with either 1.2 or 1/1.2. This is different from <ref type="bibr" target="#b46">Jaderberg et al. (2017)</ref> in that our multiplication is unbiased where they use a multiplication of 1.2 or .8. We found that diversity is increased when the parameters are permuted even if no copy happened. We reconstruct the learning curves of the PBT runs in <ref type="figure">Figure 5</ref> by backtracking through the ancestry of copied checkpoints for selected instances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Atari Experiments</head><p>All agents trained on Atari are equipped only with a feed forward network and pre-process frames in the same way as described in . When training experts agents, we use the same hyperparameters for each game for both IMPALA and A3C. These hyperparameters are the result of tuning A3C with a shallow network on the following games: breakout, pong, space invaders, seaquest, beam rider, qbert. Following related work, experts use game-specific action sets. The multi-task agent was equipped with a feed forward residual network (see <ref type="figure" target="#fig_1">Figure 3</ref> ). The learning rate, entropy regularisation, RMSProp ε and gradient clipping threshold were adapted through population based training. To be able to use the same policy layer on all Atari games in the multi-task setting we train the multi-task agent on the full Atari action set consisting of 18 actions. Agents were trained using the following set of hyperparameters:  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Left: Single Learner. Each actor generates trajectories and sends them via a queue to the learner. Before starting the next trajectory, actor retrieves the latest policy parameters from learner. Right: Multiple Synchronous Learners. Policy parameters are distributed across multiple learners that work synchronously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Model Architectures. Left: Small architecture, 2 convolutional layers and 1.2 million parameters. Right: Large architecture, 15 convolutional layers and 1.6 million parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4. Top Row: Single task training on 5 DeepMind Lab tasks. Each curve is the mean of the best 3 runs based on final return. IMPALA achieves better performance than A3C. Bottom Row: Stability across hyperparameter combinations sorted by the final performance across different hyperparameter combinations. IMPALA is consistently more stable than A3C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Performance on DMLab-30 wrt. wall-clock time. All models used the deep architecture (Figure 3). The high throughput of IMPALA results in orders of magnitude faster learning.number of no-op actions (uniformly chosen from[1, 30]) to combat the determinism of the ALE environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure B. 1 .</head><label>1</label><figDesc>Human normalised scores across all DMLab-30 tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Optimistic Asymmetric Clipping -0.3 · min(tanh(reward), 0) + 5.0 · max(tanh(reward), 0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure E. 2 .</head><label>2</label><figDesc>As the policy-lag (the number of update steps the actor policy is behind learner policy) increases, learning with V-trace is more robust compared to ε-correction and pure on-policy learning. Stability across hyper parameter combinations for different off-policy correction variants using replay. V-trace is much more stable across a wide range of parameter combinations compared to ε-correction and pure on-policy learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>runs and an average over all runs of the hyperparameter sweep as can be see in figures E.3 and E.4. r s + γ ⋅ v s + 1 q s = r s + γ ⋅ V(x s + 1 ) Figure E.3. Variants for estimation of state action value function -average over top 3 runs. r s + γ ⋅ v s + 1 q s = r s + γ ⋅ V(x s + 1 ) Figure E.4. Variants for estimation of state action value function -average over all runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure F. 1 .</head><label>1</label><figDesc>Learning rate schedule that is discovered by the PBT Jaderberg et al. (2017) method compared against the linear annealing schedule of the best run from the parameter sweep (red line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Top Row: Single task training on 5 DeepMind Lab tasks. Each curve is the mean of the best 3 runs based on final return. IMPALA achieves better performance than A3C. Bottom Row: Stability across hyperparameter combinations sorted by the final performance across different hyperparameter combinations. IMPALA is consistently more stable than A3C.</figDesc><table><row><cell></cell><cell>Task 1 Task 2 Task 3 Task 4 Task 5</cell></row><row><cell>Without Replay</cell><cell></cell></row><row><cell>V-trace</cell><cell>46.8 32.9 31.3 229.2 43.8</cell></row><row><cell>1-Step</cell><cell>51.8 35.9 25.4 215.8 43.7</cell></row><row><cell>ε-correction</cell><cell>44.2 27.3 4.3 107.7 41.5</cell></row><row><cell>No-correction</cell><cell>40.3 29.1 5.0 94.9 16.1</cell></row><row><cell>With Replay</cell><cell></cell></row><row><cell>V-trace</cell><cell>47.1 35.8 34.5 250.8 46.9</cell></row><row><cell>1-Step</cell><cell>54.7 34.4 26.4 204.8 41.6</cell></row><row><cell>ε-correction</cell><cell>30.4 30.2 3.9 101.5 37.6</cell></row><row><cell>No-correction</cell><cell>35.0 21.1 2.8 85.0 11.2</cell></row><row><cell cols="2">Tasks: rooms watermaze, rooms keys doors puzzle,</cell></row><row><cell cols="2">lasertag three opponents small,</cell></row><row><cell cols="2">explore goal locations small, seekavoid arena 01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Mean capped human normalised scores on DMLab-30. All models were evaluated on the test tasks with 500 episodes per task. The table shows the best score for each architecture.</figDesc><table><row><cell>Model</cell><cell>Test score</cell></row><row><cell>A3C, deep</cell><cell>23.8%</cell></row><row><cell>IMPALA, shallow</cell><cell>37.1%</cell></row><row><cell>IMPALA-Experts, deep</cell><cell>44.5%</cell></row><row><cell>IMPALA, deep</cell><cell>46.5%</cell></row><row><cell>IMPALA, deep, PBT</cell><cell>49.4%</cell></row><row><cell>IMPALA, deep, PBT, 8 learners</cell><cell>49.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows, IMPALA experts provide both better final</cell></row><row><cell>performance and data efficiency than their A3C counterparts</cell></row><row><cell>in the deep and the shallow configuration. As in our Deep-</cell></row><row><cell>Mind Lab experiments, the deep residual network leads</cell></row><row><cell>to higher scores than the shallow network, irrespective of</cell></row><row><cell>the reinforcement learning algorithm used. Note that the</cell></row><row><cell>shallow IMPALA experiment completes training over 200</cell></row><row><cell>million frames in less than one hour.</cell></row></table><note>We want to particularly emphasise that IMPALA, deep, multi- task, a single agent trained on all 57 ALE games at once, reaches 59.7% median human normalised score. Despite</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table B</head><label>B</label><figDesc></figDesc><table><row><cell cols="7">IMPALA: Importance Weighted Actor-Learner Architectures</cell><cell></cell><cell></cell></row><row><cell>B.1. Final training scores on DMLab-30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">.1. DMLab-30 test scores.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>120</cell><cell>140</cell><cell>160</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table C.1. Atari scores after 200M steps environment steps of training. Up to 30 no-ops at the beginning of each episode.</figDesc><table><row><cell>C. Atari Scores</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">ACKTR The Reactor IMPALA (deep, multi-task) IMPALA (shallow) IMPALA (deep)</cell></row><row><cell>alien</cell><cell>3197.10</cell><cell>6482.10</cell><cell>2344.60</cell><cell>1536.05</cell><cell>15962.10</cell></row><row><cell>amidar</cell><cell>1059.40</cell><cell>833</cell><cell>136.82</cell><cell>497.62</cell><cell>1554.79</cell></row><row><cell>assault</cell><cell>10777.70</cell><cell>11013.50</cell><cell>2116.32</cell><cell>12086.86</cell><cell>19148.47</cell></row><row><cell>asterix</cell><cell>31583.00</cell><cell>36238.50</cell><cell>2609.00</cell><cell>29692.50</cell><cell>300732.00</cell></row><row><cell>asteroids</cell><cell>34171.60</cell><cell>2780.40</cell><cell>2011.05</cell><cell>3508.10</cell><cell>108590.05</cell></row><row><cell>atlantis</cell><cell>3433182.00</cell><cell>308258</cell><cell>460430.50</cell><cell>773355.50</cell><cell>849967.50</cell></row><row><cell>bank heist</cell><cell>1289.70</cell><cell>988.70</cell><cell>55.15</cell><cell>1200.35</cell><cell>1223.15</cell></row><row><cell>battle zone</cell><cell>8910.00</cell><cell>61220</cell><cell>7705.00</cell><cell>13015.00</cell><cell>20885.00</cell></row><row><cell>beam rider</cell><cell>13581.40</cell><cell>8566.50</cell><cell>698.36</cell><cell>8219.92</cell><cell>32463.47</cell></row><row><cell>berzerk</cell><cell>927.20</cell><cell>1641.40</cell><cell>647.80</cell><cell>888.30</cell><cell>1852.70</cell></row><row><cell>bowling</cell><cell>24.30</cell><cell>75.40</cell><cell>31.06</cell><cell>35.73</cell><cell>59.92</cell></row><row><cell>boxing</cell><cell>1.45</cell><cell>99.40</cell><cell>96.63</cell><cell>96.30</cell><cell>99.96</cell></row><row><cell>breakout</cell><cell>735.70</cell><cell>518.40</cell><cell>35.67</cell><cell>640.43</cell><cell>787.34</cell></row><row><cell>centipede</cell><cell>7125.28</cell><cell>3402.80</cell><cell>4916.84</cell><cell>5528.13</cell><cell>11049.75</cell></row><row><cell>chopper command</cell><cell>N/A</cell><cell>37568</cell><cell>5036.00</cell><cell>5012.00</cell><cell>28255.00</cell></row><row><cell>crazy climber</cell><cell>150444.00</cell><cell>194347</cell><cell>115384.00</cell><cell>136211.50</cell><cell>136950.00</cell></row><row><cell>defender</cell><cell>N/A</cell><cell>113128</cell><cell>16667.50</cell><cell>58718.25</cell><cell>185203.00</cell></row><row><cell>demon attack</cell><cell>274176.70</cell><cell>100189</cell><cell>10095.20</cell><cell>107264.73</cell><cell>132826.98</cell></row><row><cell>double dunk</cell><cell>-0.54</cell><cell>11.40</cell><cell>-1.92</cell><cell>-0.35</cell><cell>-0.33</cell></row><row><cell>enduro</cell><cell>0.00</cell><cell>2230.10</cell><cell>971.28</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>fishing derby</cell><cell>33.73</cell><cell>23.20</cell><cell>35.27</cell><cell>32.08</cell><cell>44.85</cell></row><row><cell>freeway</cell><cell>0.00</cell><cell>31.40</cell><cell>21.41</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>frostbite</cell><cell>N/A</cell><cell>8042.10</cell><cell>2744.15</cell><cell>269.65</cell><cell>317.75</cell></row><row><cell>gopher</cell><cell>47730.80</cell><cell>69135.10</cell><cell>913.50</cell><cell>1002.40</cell><cell>66782.30</cell></row><row><cell>gravitar</cell><cell>N/A</cell><cell>1073.80</cell><cell>282.50</cell><cell>211.50</cell><cell>359.50</cell></row><row><cell>hero</cell><cell>N/A</cell><cell>35542.20</cell><cell>18818.90</cell><cell>33853.15</cell><cell>33730.55</cell></row><row><cell>ice hockey</cell><cell>-4.20</cell><cell>3.40</cell><cell>-13.55</cell><cell>-5.25</cell><cell>3.48</cell></row><row><cell>jamesbond</cell><cell>490.00</cell><cell>7869.20</cell><cell>284.00</cell><cell>440.00</cell><cell>601.50</cell></row><row><cell>kangaroo</cell><cell>3150.00</cell><cell>10484.50</cell><cell>8240.50</cell><cell>47.00</cell><cell>1632.00</cell></row><row><cell>krull</cell><cell>9686.90</cell><cell>9930.80</cell><cell>10807.80</cell><cell>9247.60</cell><cell>8147.40</cell></row><row><cell>kung fu master</cell><cell>34954.00</cell><cell>59799.50</cell><cell>41905.00</cell><cell>42259.00</cell><cell>43375.50</cell></row><row><cell>montezuma revenge</cell><cell>N/A</cell><cell>2643.50</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>ms pacman</cell><cell>N/A</cell><cell>2724.30</cell><cell>3415.05</cell><cell>6501.71</cell><cell>7342.32</cell></row><row><cell>name this game</cell><cell>N/A</cell><cell>9907.20</cell><cell>5719.30</cell><cell>6049.55</cell><cell>21537.20</cell></row><row><cell>phoenix</cell><cell>133433.70</cell><cell>40092.20</cell><cell>7486.50</cell><cell>33068.15</cell><cell>210996.45</cell></row><row><cell>pitfall</cell><cell>-1.10</cell><cell>-3.50</cell><cell>-1.22</cell><cell>-11.14</cell><cell>-1.66</cell></row><row><cell>pong</cell><cell>20.90</cell><cell>20.70</cell><cell>8.58</cell><cell>20.40</cell><cell>20.98</cell></row><row><cell>private eye</cell><cell>N/A</cell><cell>15177.10</cell><cell>0.00</cell><cell>92.42</cell><cell>98.50</cell></row><row><cell>qbert</cell><cell>23151.50</cell><cell>22956.50</cell><cell>10717.38</cell><cell>18901.25</cell><cell>351200.12</cell></row><row><cell>riverraid</cell><cell>17762.80</cell><cell>16608.30</cell><cell>2850.15</cell><cell>17401.90</cell><cell>29608.05</cell></row><row><cell>road runner</cell><cell>53446.00</cell><cell>71168</cell><cell>24435.50</cell><cell>37505.00</cell><cell>57121.00</cell></row><row><cell>robotank</cell><cell>16.50</cell><cell>68.50</cell><cell>9.94</cell><cell>2.30</cell><cell>12.96</cell></row><row><cell>seaquest</cell><cell>1776.00</cell><cell>8425.80</cell><cell>844.60</cell><cell>1716.90</cell><cell>1753.20</cell></row><row><cell>skiing</cell><cell>N/A</cell><cell>-10753.40</cell><cell>-8988.00</cell><cell>-29975.00</cell><cell>-10180.38</cell></row><row><cell>solaris</cell><cell>2368.60</cell><cell>2760</cell><cell>1160.40</cell><cell>2368.40</cell><cell>2365.00</cell></row><row><cell>space invaders</cell><cell>19723.00</cell><cell>2448.60</cell><cell>199.65</cell><cell>1726.28</cell><cell>43595.78</cell></row><row><cell>star gunner</cell><cell>82920.00</cell><cell>70038</cell><cell>1855.50</cell><cell>69139.00</cell><cell>200625.00</cell></row><row><cell>surround</cell><cell>N/A</cell><cell>6.70</cell><cell>-8.51</cell><cell>-8.13</cell><cell>7.56</cell></row><row><cell>tennis</cell><cell>N/A</cell><cell>23.30</cell><cell>-8.12</cell><cell>-1.89</cell><cell>0.55</cell></row><row><cell>time pilot</cell><cell>22286.00</cell><cell>19401</cell><cell>3747.50</cell><cell>6617.50</cell><cell>48481.50</cell></row><row><cell>tutankham</cell><cell>314.30</cell><cell>272.60</cell><cell>105.22</cell><cell>267.82</cell><cell>292.11</cell></row><row><cell>up n down</cell><cell>436665.80</cell><cell>64354.20</cell><cell>82155.30</cell><cell>273058.10</cell><cell>332546.75</cell></row><row><cell>venture</cell><cell>N/A</cell><cell>1597.50</cell><cell>1.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>video pinball</cell><cell>100496.60</cell><cell>469366</cell><cell>20125.14</cell><cell>228642.52</cell><cell>572898.27</cell></row><row><cell>wizard of wor</cell><cell>702.00</cell><cell>13170.50</cell><cell>2106.00</cell><cell>4203.00</cell><cell>9157.50</cell></row><row><cell>yars revenge</cell><cell>125169.00</cell><cell>102760</cell><cell>14739.41</cell><cell>80530.13</cell><cell>84231.14</cell></row><row><cell>zaxxon</cell><cell>17448.00</cell><cell>25215.50</cell><cell>6497.00</cell><cell>1148.50</cell><cell>32935.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table D.2. Action set used in all tasks from the DeepMind Lab environment, including the DMLab-30 experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table D.3. Fixed model hyperparameters across all DeepMind Lab experiments. E. V-trace Analysis E.1. Controlled Updates Here we show how different algorithms (On-Policy, No-correction, ε-correction, V-trace) behave under varying levels of policy-lag between the actors and the learner.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table G .</head><label>G</label><figDesc></figDesc><table /><note>1. Hyperparameters for Atari experiments.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Denis Teplyashin, Ricardo Barreira, Manuel Sanchez for their work improving the performance on DMLab-30 environments and Matteo Hessel, Jony Hudson, Igor Babuschkin, Max Jaderberg, Ivo Danihelka, Jacob Menick and David Silver for their comments and insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A computational model for tensorflow: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<idno>978-1-4503-5071-6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Distributed deep reinforcement learning: Learn how to play atari games in 21 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Adamski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adamski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jedrych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kaczmarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1801.02852</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Optimizing performance of recurrent neural networks on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1604.01946</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ga3c</surname></persName>
		</author>
		<title level="m">GPU-based A3C for deep reinforcement learning. NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributional policy gradients. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Valdes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>York</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gaffney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepmind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lab</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1612.03801</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Revisiting distributed synchronous SGD. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<idno>abs/1604.00981</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cudnn</surname></persName>
		</author>
		<idno>abs/1410.0759</idno>
		<title level="m">Efficient primitives for deep learning. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient parallel methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Clemente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N C</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1705.04862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Off-policy learning with eligibility traces: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scherrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="333" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Reactor: A fast and sample-efficient actor-critic agent for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Q(λ) with Off-Policy Corrections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">305</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06551</idno>
		<title level="m">Grounded language learning in a simulated 3d world</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Distributed prioritized experience replay. ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Population based training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1711.09846</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D M</forename><surname>Autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Castañeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08116</idno>
		<title level="m">A psychology laboratory for deep reinforcement learning agents</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Asynchronous methods for deep reinforcement learning. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1046" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Massively parallel methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alcicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fearon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno>abs/1507.04296</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combining policy gradient and Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eligibility traces for off-policy policy evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Off-policy temporal-difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Laerning</title>
		<meeting>the 18th International Conference on Machine Laerning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition, 1994. ISBN 0471619779</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">and Hadsell, R. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03864</idno>
		<title level="m">Evolution strategies as a scalable alternative to reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature24270</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Cambridge Univ Press</publisher>
			<biblScope unit="volume">116</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real-time reinforcement learning by sequential actor-critics and experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wawrzynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1484" to="1497" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1708.05144</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neuro-Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">TD(λ) converges with probability 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1022657612745</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="295" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Population based training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1711.09846</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Stochastic Approximation and Recursive Algorithms and Applications. Stochastic Modelling and Applied Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kushner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">9780387008943</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Asynchronous methods for deep reinforcement learning. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
