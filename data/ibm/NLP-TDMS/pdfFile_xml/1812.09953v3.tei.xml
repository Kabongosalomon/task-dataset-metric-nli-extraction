<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Curriculum Domain Adaptation Approach to the Semantic Segmentation of Urban Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Philip</forename><surname>David</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Hassan</forename><forename type="middle">Foroosh</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
						</author>
						<title level="a" type="main">A Curriculum Domain Adaptation Approach to the Semantic Segmentation of Urban Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Domain Adaptation</term>
					<term>Semantic Segmentation</term>
					<term>Curriculum Learning</term>
					<term>Deep Learning</term>
					<term>Self-Driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>During the last half decade, convolutional neural networks (CNNs) have triumphed over semantic segmentation, which is one of the core tasks in many applications such as autonomous driving and augmented reality. However, to train CNNs requires a considerable amount of data, which is difficult to collect and laborious to annotate. Recent advances in computer graphics make it possible to train CNNs on photo-realistic synthetic imagery with computer-generated annotations. Despite this, the domain mismatch between the real images and the synthetic data hinders the models' performance. Hence, we propose a curriculum-style learning approach to minimizing the domain gap in urban scene semantic segmentation. The curriculum domain adaptation solves easy tasks first to infer necessary properties about the target domain; in particular, the first task is to learn global label distributions over images and local distributions over landmark superpixels. These are easy to estimate because images of urban scenes have strong idiosyncrasies (e.g., the size and spatial relations of buildings, streets, cars, etc.). We then train a segmentation network, while regularizing its predictions in the target domain to follow those inferred properties. In experiments, our method outperforms the baselines on two datasets and two backbone networks. We also report extensive ablation studies about our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S EMANTIC segmentation is one of the most challenging and fundamental problems in computer vision. It assigns a semantic label to each pixel of an input image <ref type="bibr" target="#b0">[1]</ref>. The resulting output is a dense and rich annotation of the image, with one semantic label per pixel. Semantic segmentation facilitates many downstream applications, including autonomous driving, which, over the past few years, has made great strides towards the use by the general population. Indeed, several datasets and test suites have been developed for research on autonomous driving <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and, among them, semantic segmentation is often considered one of the key tasks.</p><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> have become a hallmark backbone model to solve the semantic segmentation of large-scale image sets over the last half decade. All of the top-performing methods on the challenge board of the Cityscapes pixel-level semantic labeling task <ref type="bibr" target="#b1">[2]</ref> rely on CNNs. One of the reasons that CNNs are able to achieve a high level of accuracy for this task is that the training set is sufficiently large and welllabeled, covering the variability of the test set for the research purpose. In practice, however, it is often hard to acquire new training sets that fully cover the huge variability of real-life test scenarios. Even if one could compose a large-scale dataset with sufficient variability, it would be extremely tedious to label the images with pixel-wise semantic labels. For example, Cordts et al. report that the annotation and quality control took more than 1.5 hours per image in the popular Cityscapes dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>These challenges motivate researchers to approach the segmentation problem by using complementary synthetic data. With modern graphics engines, automatically synthesizing diverse urban-scene images along with pixel-wise labels would require . Code is available at https://github.com/YangZhang4065/AdaptationSeg. very little to zero human labor. <ref type="figure">Figure 4</ref> shows some synthetic images of the GTA dataset <ref type="bibr" target="#b5">[6]</ref>. They are quite photo-realistic, giving rise to the hope that a semantic segmentation neural network trained from them can perform reasonably well on the real images as well. However, our experiments show that this is not the case (cf. Section 4), signifying a severe mismatch between the real images and the synthesized ones. Multiple factors may contribute to the mismatch, such as the scene layout, capture device (camera vs. rendering engine), view angles, lighting conditions and shadows, textures, etc.</p><p>In this paper, our main objective is to investigate the use of domain adaptation techniques <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> to more effectively transfer the semantic segmentation neural networks trained using synthetic images to high-quality segmentation networks for real images. We build our efforts upon our prior work <ref type="bibr" target="#b10">[11]</ref>, in which we propose a novel domain adaptation approach to the semantic segmentation of urban scenes.</p><p>Domain adaption, which mainly aims to boost models' performance when the target domain of interest differs from the one where the models are trained, has long been a popular topic in machine learning and computer vision <ref type="bibr" target="#b12">[13]</ref>. It has recently drawn even greater attention along with transfer learning thanks to the prevalence of deep neural networks which are often "datahungry". An intuitive domain adaptation strategy is to learn domain-invariant feature representations for the images of both domains, where the source domain supplies a labeled training set and the target domain reveals zero to a few labeled images along with many unlabeled ones. In this case, the source domain features would resemble the target ones' characteristics. Thus, the model trained on the labeled source domain can be generalized to the target domain. Earlier "shallow" methods achieve such goals by exploiting various intrinsic structures of the data <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In contrast, the recent "deep" methods mainly devise new loss functions and/or network architectures to add domain-invariant ingredients to the gradients backpropagating through the neural networks <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>.</p><p>Upon observing the success of learning domain-invariant features in the prior domain adaptation tasks, it is a natural tendency to follow the same principle for the adaptation of semantic segmentation models. There have been some positive results along this line <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. However, the underlying assumption of this principle may prevent the methods designed around it from achieving high adaptation performance. By focusing on learning domain-invariant features X (i.e., such that P S (X) ≈ P T (X), where the subscripts S and T stand for the source and target domains, respectively), one assumes the conditional distribution P (Y |X), where Y are the pixel labels, is more or less shared by the two domains. This assumption is less likely to be true when the classification boundary becomes more and more sophisticated -the prediction function for semantic segmentation has to be sophisticated. The sets of pixel labels are high-dimensional, highly structured, and interdependent, implying that the learner has to resolve the predictions in an exponentially large label space. Besides, some discriminative cues in the data would be suppressed if one matches the feature representations of the two domains without taking careful account of the structured labels. Finally, data instances are the proxy to measure the domain difference <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. However, it is not immediately clear what comprises the instance in semantic segmentation <ref type="bibr" target="#b9">[10]</ref>, especially given that the top-performing segmentation methods are built upon deep neural networks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Hoffman et al. take each spatial unit in the fully convolutional network (FCN) <ref type="bibr" target="#b6">[7]</ref> as an instance <ref type="bibr" target="#b9">[10]</ref>. We contend that such instances are actually non-i.i.d. in either individual domain, as their receptive fields overlap with each other.</p><p>How can we avoid the assumption that the source and target domains share the same prediction function in a transformed domain-invariant feature space? Our proposed solution draws on two key observations. One is that the urban traffic scene images have strong idiosyncrasies (e.g., the size and spatial relations of buildings, streets, cars, etc.). Therefore, some tasks are "easy" and, more importantly, suffer less because of the domain discrepancy. For instance, it is easy to infer from a traffic scene image that the road often occupies a larger number of pixels than the traffic sign does. Second, the structured output in semantic segmentation enables convenient posterior regularization <ref type="bibr" target="#b30">[31]</ref>, as opposed to the generic (e.g., 2 ) regularization over model parameters.</p><p>Accordingly, we propose a curriculum-style <ref type="bibr" target="#b31">[32]</ref> domain adaptation approach. Recall that, in domain adaptation, only the source domain supplies many labeled data while there are no or only scarce labels from the target domain. Our curriculum domain adaptation begins with the easy tasks, in order to gain some highlevel properties about the unknown pixel-level labels for each target image. It then learns a semantic segmentation network, the hard task, whose predictions over the target images are constrained to follow those target-domain properties as much as possible.</p><p>To develop the easy tasks for the curriculum, we consider estimating label distributions over both global images and some landmark superpixels of the target domain. Take the former for instance. The label distribution indicates the percentage of pixels in an image that correspond to each category. We argue that these tasks are easier, despite the domain mismatch, than predicting pixel-wise labels. The label distributions are only rough estimations about the labels's statistics. Moreover, the size relations between road, building, sky, people, etc. constrain the shape of the distributions, effectively reducing the search space. Finally, models to estimate the label distributions over superpixels may benefit from the urban scenes' canonical layout that transcends domains, e.g., buildings stand beside streets.</p><p>Why and when are these seemingly simple label distributions useful for the domain adaptation of semantic segmentation? In our experiments, we find that the segmentation networks trained on the source domain perform poorly on many target images, giving rise to disproportionate label assignments (e.g., many more pixels are classified to sidewalks than to streets). To rectify this, the imagelevel label distribution informs the segmentation network how to update the predictions while the label distributions of the anchor superpixels tell the network where to update. Jointly, they guide the adaptation of the networks to the target domain to, at least, generate proportional label predictions. Note that additional "easy tasks" can be incorporated into our approach in the future.</p><p>Our main contribution is the proposed curriculum-style domain adaptation for the semantic segmentation of urban scenes. We select for the curriculum the easy and useful tasks of inferring label distributions for both target images and landmark superpixels in order to gain some necessary properties about the target domain. Built upon these, we learn a pixel-wise discriminative segmentation network from the labeled source data and, meanwhile, conduct a "sanity check" to ensure the network behavior is consistent with the previously learned knowledge about the target domain. Our approach effectively eludes the assumption about the existence of a common prediction function for both domains in a transformed feature space. It readily applies to different segmentation networks as it does not change the network architecture or impact any intermediate layers.</p><p>Beyond our prior work <ref type="bibr" target="#b10">[11]</ref>, we provide more algorithmic details and experimental studies about our approach, including new experiments using the GTA dataset <ref type="bibr" target="#b5">[6]</ref> and ablation studies about the number of superpixels, feature representations of the superpixels, various backbone neural networks, prediction confusion matrix, etc. In addition, we introduce a color constancy scheme into our framework, which significantly improves the adaptation performance and may be plugged into any domain adaptation method as a standalone image pre-processing step. We also quantitatively measure the "market value" of the synthetic data to reveal how much cost it could save out of the labeling of real images. Finally, we provide a comprehensive survey about the works published after ours <ref type="bibr" target="#b10">[11]</ref> on the domain adaptation for semantic segmentation. We group them into different categories and experimentally demonstrate that other methods are complementary to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We broadly discuss related work on domain adaptation and semantic segmentation in this section. Section 5 provides a more focused review about the domain adaptation methods for semantic segmentation, along with experimental studies about the complementary effect between them and ours of different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain adaptation</head><p>Conventional machine learning algorithms rely on the standard assumption that the training and test data are drawn i.i.d. from the same underlying distribution. However, it is often the case that there exists some discrepancy between the training and test stages. Domain adaptation aims to rectify this mismatch and tune the models toward better generalization at the test stage <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p><p>The existing work on domain adaptation mostly focuses on classification and regression problems <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, e.g., learning from online images to classify real world objects <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b38">[39]</ref>, and, more recently, aims to improve the adaptability of deep neural networks <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Among them, the most relevant works to ours are those exploring simulated data <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Sun and Saenko train generic object detectors from synthetic images <ref type="bibr" target="#b40">[41]</ref>, while Vazquez et al. use virtual images to improve pedestrian detections in real environments <ref type="bibr" target="#b43">[44]</ref>. The other way around, i.e., how to improve the quality of the simulated images using the real ones, is studied in <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic segmentation</head><p>Semantic segmentation is the task of assigning an object label to each pixel of an image. Traditional methods <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> rely on local image features manually designed by domain experts. After the pioneering works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref> that introduces the convolutional neural network (CNN) <ref type="bibr" target="#b49">[50]</ref> to semantic segmentation, most recent top-performing methods are also built on CNNs <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>.</p><p>Currently, there are multiple and increasing numbers of semantic segmentation datasets aiming for different computer vision applications. Some general ones include the PASCAL VOC2012 Challenge <ref type="bibr" target="#b55">[56]</ref>, which contains nearly 10,000 annotated images for the segmentation competition, and the MS COCO Challenge <ref type="bibr" target="#b56">[57]</ref>, which includes over 200,000 annotated images. In our paper, we focus on urban outdoor scenes. Several urban scene segmentation datasets are publicly available such as Cityscapes <ref type="bibr" target="#b1">[2]</ref>, a vehicle-centric dataset created primarily in German cities, KITTI <ref type="bibr" target="#b57">[58]</ref>, another vehicle-centric dataset captured in the German city Karlsruhe, Berkeley DeepDrive Video Dataset <ref type="bibr" target="#b2">[3]</ref>, a dashcam dataset collected in United States, Mapillary Vistas Dataset <ref type="bibr" target="#b58">[59]</ref>, so far known as the largest outdoor urban scene segmentation dataset collected from all over the world, WildDash, a much smaller yet diverse dataset for benchmark purpose, and CamVid <ref type="bibr" target="#b59">[60]</ref>, a small and low-resolution toy dashcam dataset. An enormous amount of labor-intensive work is required to annotate the images that are needed to obtain accurate segmentation models. According to <ref type="bibr" target="#b5">[6]</ref>, it took about 60 minutes to manually segment each image in <ref type="bibr" target="#b60">[61]</ref> and about 90 minutes for each in <ref type="bibr" target="#b1">[2]</ref>. A plausible approach to reducing the human annotation workload is to utilize weakly supervised information such as image labels and bounding boxes <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>.</p><p>We instead explore the almost labor-freely labeled virtual images for training high-quality segmentation networks. In <ref type="bibr" target="#b5">[6]</ref>, annotating a synthetic image took only 7 seconds on average through a computer game. For the urban scenes, we use the SYN-THIA <ref type="bibr" target="#b42">[43]</ref> and GTA <ref type="bibr" target="#b5">[6]</ref> datasets which contain images of virtual cities. Although not used in our experiments, another synthetic segmentation dataset worth mentioning is Virtual KITTI <ref type="bibr" target="#b64">[65]</ref>, a synthetic duplication of the original KITTI <ref type="bibr" target="#b57">[58]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain adaptation for semantic segmentation</head><p>Due to the clear visual mismatch between synthetic and real data <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b65">[66]</ref>, we expect to use domain adaptation to enhance the segmentation performance on real images by networks trained on synthetic imagery. To the best of our knowledge, our work <ref type="bibr" target="#b10">[11]</ref> and the FCNs in the wild <ref type="bibr" target="#b9">[10]</ref> are among the very first attempts to tackle this problem. It subsequently became an independent track in the Visual Domain Adaptation Challenge (VisDA) 2017 <ref type="bibr" target="#b11">[12]</ref>. After that, some feature-alignment based methods are developed <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>. We postpone further discussion to Section 5, which presents a comprehensive survey about the works published after ours and before December 2018. We group them into two major categories and describe their main methods. We also summarize their results by <ref type="table" target="#tab_10">Table 9</ref> and analyze their complementary relationship with ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section, we present the details of our approach to curriculum domain adaptation for the semantic segmentation of urban scene images. Unlike previous works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b36">[37]</ref> that align the domains via an intermediate feature space and thereby implicitly assume the existence of a single decision function for the two domains, it is our intuition that, for structured prediction (i.e., semantic segmentation here), the cross-domain generalization of machine learning models can be more efficiently improved if we avoid this assumption and instead train them subject to necessary properties they should retain in the target domain. After a brief introduction on the preliminaries, we will present how to facilitate semantic segmentation adaptation during training using estimated target domain properties in Section 3.2. Then we will focus on the types of target domain properties and how to estimate them in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>In particular, the properties of interest concern pixel-wise category labels Y t ∈ R W ×H×C of an arbitrary image I t ∈ R W ×H from the target domain, where W and H are the width and height of the image, respectively, and C is the number of categories. We use one-hot vector encoding for the groundtruth labels, i.e., Y t (i, j, c) takes the value of either 0 or 1, where the latter means that the c-th label is assigned by a human annotator to the pixel at (i, j). Correspondingly, the prediction Y t (i, j, c) ∈ [0, 1] by a segmentation network is realized by a softmax function per pixel.</p><p>We express each target property in the form of a distribution p t over the C categories, where C c p t (c) = 1 and 0 p t (c), ∀c. p t (c) represents the occupancy proportion of the category c over the t-th target image or a superpixel of that image. Therefore, one can immediately calculate the distribution p t given the human annotations Y t to the image. For instance, the image-level label distribution is expressed by</p><formula xml:id="formula_0">p t (c) = 1 W H W i=1 H j=1 Y t (i, j, c), ∀c.<label>(1)</label></formula><p>Similarly, we can compute the estimated target property/distribution from the network predictions Y t and denote it by p t ,</p><formula xml:id="formula_1">p t (c) = 1 W H W i=1 H j=1 Y t (i, j, c) max c ( Y t (i, j, c )) K , ∀c (2)</formula><p>where K &gt; 1 is a large constant whose effect is to "sharpen" the softmax activation per pixel Y (i, j, c) such that the summand is either 1 or very close to 0, in a similar shape as the summad Y t (i, j, c) of eq. (1). We set K = 6 in our experiment as larger K caused numerical instability. Finally, we 1 -normalize the vector ( p t (1), p t (2), · · · , p t (C)) T such that its elements are all greater than 0 and sum up to 1 -in other words, the vector remains a valid distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Domain adaptation observing the target properties</head><p>Ideally, we would like to have a segmentation network to imitate human annotators of the target domain. Therefore, necessarily, the properties of their annotation results should be the same too. We capture this notion by minimizing the cross entropy C(p t , p t ) = H(p t ) + KL(p t , p t ) at training, where the first term of the righthand side is the entropy and the second is the KL-divergence. Given a mini-batch consisting of both source images (S) and target images (T ), the overall objective function for training the cross-domain generalizing segmentation network is</p><formula xml:id="formula_2">min γ |S| s∈S L Y s , Y s + 1 − γ |T | t∈T k C p k t , p k t<label>(3)</label></formula><p>where L is the pixel-wise cross-entropy loss defined over the fully labeled source domain images, enforcing the network to have the pixel-level discriminative capabilities, and the second term is over the unlabeled target domain images, hinting the network what necessary properties its predictions should have in the target domain. We use γ ∈ [0, 1] to balance the two strengths in training and superscript k to index different types of label distributions (cf. p t in eq. (1) and Section 3.3). Note that, in the unsupervised domain adaptation context, we actually cannot directly compute the label distributions {p k t } because the groundtruth annotations of the target domain are unknown. Nonetheless, using the labeled source domain data, these distributions are easier to estimate than are the labels for every pixel of a target image. We present two types of such properties and the details for inferring them in the next section. In future work, it is worth exploring other properties.</p><p>Remarks. Mathematically, the objective function has a similar form as model compression <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>. Hence, we borrow some concepts to gain a more intuitive understanding of our domain adaptation procedure. The "student" network follows a curriculum to learn simple knowledge about the target domain before it addresses the hard one of semantically segmenting images. The models inferring the target properties act like "teachers", as they hint what label distributions the final solution (image annotation) may have in the target domain at the image and superpixel levels.</p><p>Another perspective is to understand the target properties as a posterior regularization <ref type="bibr" target="#b30">[31]</ref> for the network. The posterior regularization can conveniently encode a priori knowledge into the objective function. Some applications using this approach include weakly supervised segmentation <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b51">[52]</ref> and detection <ref type="bibr" target="#b72">[73]</ref> and rule-regularized training of neural networks <ref type="bibr" target="#b73">[74]</ref>. In addition to the domain adaptation setting and novel target properties, another key distinction of our work is that we decouple the label distributions from the network predictions and thus avoid the EM type of optimization, which is often involved and incurs extra computational overhead. Our approach learns the segmentation network with almost effortless changes to the popular deep learning tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inferring the target properties</head><p>Thus far, we have presented the "hard" task -learning the segmentation neural network -in the curriculum domain adaptation. In this section, we describe the "easy" tasks, i.e., how to infer the target domain properties without any annotations from the target domain. Our contributions also include selecting the particular form of label distributions to constitute the simple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Global label distributions over images</head><p>Due to the domain disparity, a baseline segmentation network trained on the source domain (i.e., using the first term of eq. (3)) could be easily crippled given the target images. In our experiments, we find that our baseline network constantly mistakes streets for sidewalks and/or cars (cf. <ref type="figure" target="#fig_2">Figure 3)</ref>. Consequently, the predicted labels for the pixels are highly disproportionate.</p><p>To rectify this, we employ the label distribution p t over the global image as our first property (cf. eq. (1)). Without access to the target labels, we have to train machine learning models from the labeled source images to estimate the label distribution p t of a target image. Nonetheless, we argue that this is less challenging than generating per-pixel predictions despite that both tasks are influenced by the domain mismatch.</p><p>In our experiments, we examine several different approaches to this task. We extract 1536D image features from the output of the average pooling layer in Inception-Resnet-v2 <ref type="bibr" target="#b74">[75]</ref> as the input to the following models. Logistic regression. Although multinomial logistic regression (LR) is mainly used for classification, its output is actually a valid distribution over the categories. For our purpose, we thus train it by replacing the one-hot vectors in the crossentropy loss with the groundtruth label distribution p s , which is counted by using eq. (1) from the human labels of the source domain. Given a target image, we directly take the LR's output as the estimated label distribution p t . Mean of nearest neighbors. We also test a nonparametric method by simply retrieving multiple nearest neighbor (NN) source images for each target image and then transferring the mean of their label distributions to the target image. We use the 2 distance in the Inception-Resnet-v2 feature space for the NN retrieval. Finally, we include two dumb predictions as the control experiments. One is, for any target image, to output the mean of all the label distributions in the source domain (source mean), and the other is to output a uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Local label distributions of landmark superpixels</head><p>The image-level label distribution globally penalizes potentially disproportional segmentation output in the target domain. It is yet inadequate in providing spatial regularization to the network. In this section, we consider the use of label distributions over some superpixels as the anchors to drive the network towards spatially desired target properties.</p><p>Note that it is not necessary, and is even harmful, to use all of the superpixels in a target image to regularize the segmentation network because it would be too strong a force and might overrule the pixel-wise discriminativeness (obtained from the fully labeled source domain), especially when the label distributions are not inferred accurately enough.</p><p>In order to have the dual effect of both estimating the label distributions of some superpixels and selecting them from all candidate superpixels, we employ a linear SVM in this work. We first segment each image into 100 superpixels using linear spectral clustering <ref type="bibr" target="#b75">[76]</ref>. For the superpixels of the source domain, we are able to assign a single dominant label to each of them and  then train a multi-class SVM using the "labeled" superpixels of the source domain. Given a test superpixel of a target image, the multi-class SVM returns a class label as well as a decision value, which is interpreted as the confidence score about classifying this superpixel. We keep the top 30% most confident superpixels in the target domain. The class labels are then encoded into one-hot vectors which serve as valid distributions about the category labels upon the selected landmark superpixels' area. Albeit simple, we find this method works very well. In order to train the aforementioned superpixel SVM, we need to find a way to represent the superpixels in a feature space. We encode both visual and contextual information to represent a superpixel. First, we use the FCN-8s <ref type="bibr" target="#b6">[7]</ref> pre-trained on the PASCAL CONTEXT <ref type="bibr" target="#b76">[77]</ref> dataset, which has 59 distinct classes, to obtain 59 detection scores for each pixel. We then average these scores within each superpixel. The final feature representation of a superpixel is a 295D concatenation of the 59D vectors of itself, its left and right superpixels, as well as the two respectively above and below it. As this feature representation relies on extra data source, we also examine handcrafted features and VGG features <ref type="bibr" target="#b77">[78]</ref> in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Curriculum domain adaptation: recapitulation</head><p>We recap the proposed curriculum domain adaptation using Figure 1 before presenting the experiments in the next section. Our main idea is to execute the domain adaptation step by step, starting from the easy tasks that, compared to semantic segmentation, are less sensitive to domain discrepancy. We choose the label distributions over global images and local landmark superpixels in this work; more tasks will be explored in the future. The solutions to them provide useful gradients originating from the target domain (cf. the arrows with brown color in <ref type="figure" target="#fig_0">Figure 1</ref>), while the source domain feeds the network with well-labeled images and segmentation masks (cf. the dark blue arrows in <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Color Constancy</head><p>In this subsection, we propose an color calibration preprocessing step which we find very effective in adapting semantic segmentation methods from the source synthetic domain to the target real image domain. We assume the colors of the two domains are drawn from different distributions. We calibrate the target domain images' colors to those of the source domain, hence reducing their discrepancy in terms of the color. We describe it here as an independent subsection because it can stand alone and can be added to any existing methods for the domain adaptation of semantic segmentation. Humans have the ability to perceive the same color of an object even when it is exposed in different illuminations <ref type="bibr" target="#b78">[79]</ref>, but image capturing sensors do not. As a result, different illuminations result in distinct RGB images captured by the cameras. Consequently, the perception incoherence hinders the performance of computer vision algorithms <ref type="bibr" target="#b79">[80]</ref> because the illumination is often among the key factors that cause the domain mismatch. To this end, we propose to use computational color constancy <ref type="bibr" target="#b80">[81]</ref> to eliminate the influence of illuminations.</p><p>The goal of color constancy is to correct the colors of images acquired under unconventional or biased lighting sources to the colors that are supposed to be under the reference lighting condition. In our domain adaptation scenario, we assume that the source domain resembles the reference lighting condition.</p><p>We learn a parametric model to describe both the target and source lighting sources and then try to restore the target images according to the source domain's light. However, not all the color constancy methods are applicable. For instance, some methods rely on physics priors or the statistics of natural images, both of which are unavailable in synthetic images.</p><p>Due to the above concerns, we instead use a gamut-based color constancy method <ref type="bibr" target="#b81">[82]</ref> to align the target and source images in terms of their colors. This method infers the property of the light source under the assumption that only a limited range of color could be observed under a certain light source. This matches our assumption that the target images' colors and the source images' colors belong to different distributions/ranges. In addition to the pixel values, the image edge and derivatives are also used to find the mapping. While we omit the details of this color constancy method and refer the readers to <ref type="bibr" target="#b81">[82]</ref> instead, we show in <ref type="figure" target="#fig_1">Figure 2</ref> how sensitive the segmentation model is to the illumination. We can see that, prior to applying color constancy, a large part of a CityScapes image is incorrectly classified as "terrain" only</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we run extensive experiments to verify the effectiveness of our approach and study several variations of it to gain a thorough understanding about how different components contribute to the overall results. We investigate the global imagewise label distribution and the local landmark superpixels by separate experiments. We also empirically examine the effects of distinct granularities of the superpixels as well as different feature representations of the superpixels. Moreover, we compare our approach with several competing baselines for the adaptation of various deep neural networks from two synthetic datasets for the urban scene segmentation task. We use confusion matrices to show how different classes of the urban scene images intervene with each other. Finally, we run few-shot adaptation experiments by gradually revealing some labels of the images of the target domain. The results show that, even when more than 1,000 target images are labeled (50% of the Cityscapes training set <ref type="bibr" target="#b1">[2]</ref>), the adaptation from synthetic images is still able to boost the results by a relatively large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Segmentation network and optimization</head><p>In most of our experiments, we use FCN-8s <ref type="bibr" target="#b6">[7]</ref> as our semantic segmentation network. We initialize its convolutional layers with VGG-19 <ref type="bibr" target="#b77">[78]</ref> and then train it using the AdaDelta optimizer <ref type="bibr" target="#b82">[83]</ref> with default parameters. Each mini-batch contains five source images and five randomly chosen target images. When we train the baseline network with no adaptation, however, we try to use the largest possible mini-batch which includes 15 source images. The network is implemented in Keras <ref type="bibr" target="#b83">[84]</ref> and Theano <ref type="bibr" target="#b84">[85]</ref>. We train different versions of the network on a single Tesla K40 GPU.</p><p>Additionally, we run an ablation experiment using the state-ofthe-art segmentation neural network ADEMXAPP <ref type="bibr" target="#b50">[51]</ref>. The training details and network architecture are described in Section 4.7.</p><p>We note that our curriculum domain adaptation can be readily applied to other segmentation networks (e.g., <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>). Once we infer the label distributions of the unlabeled target images and some of their landmark superpixels, we can use them to train different segmentation networks by eq. (3) without changing them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and evaluation</head><p>We use the publicly available Cityscapes <ref type="bibr" target="#b1">[2]</ref> as our target domain in the experiments. For the source domains of synthesized images, we test both GTA <ref type="bibr" target="#b5">[6]</ref> and SYNTHIA <ref type="bibr" target="#b42">[43]</ref>.</p><p>Cityscapes is a real-world vehicle-egocentric image dataset collected from 50 cities in Germany and the countries around. It provides four disjoint subsets: 2,993 training images, 503 validation image, 1,531 test images, and 20,021 auxiliary images. All the training, validation, and test images are accurately annotated with per-pixel category labels, and the auxiliary set is coarsely labeled. There are 34 distinct categories in the dataset. Among them, 19 categories are officially recommended for training and evaluation.</p><p>SYNTHIA <ref type="bibr" target="#b42">[43]</ref> is a large dataset of synthetic images and provides a particular subset, called SYNTHIA-RAND-CITYSCAPES, to pair with Cityscapes. This subset contains 9,400 images that are automatically annotated with 12 object categories, one void class, and some unnamed classes. Note that the virtual city used to generate the synthetic images does not correspond to any of the real cities covered by Cityscapes. GTA <ref type="bibr" target="#b5">[6]</ref> is a synthetic vehicle-egocentric image dataset collected from the open world in a realistically rendered computer game, Grand Theft Auto V (GTA). It contains 24,996 images. Unlike the SYNTHIA dataset, its semantic segmentation annotation is fully compatible with the Cityscapes dataset. Hence, we will use all the 19 official training classes in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Domain idiosyncrasies</head><p>Although all datasets depict urban scenes and both SYNTHIA and GTA are created to be as photo-realistic as possible, they are mismatched domains in several ways. The most noticeable difference is probably the coarse-grained textures in SYNTHIA; very similar texture patterns repeat in a regular manner across different images. The textures in GTA are better but still visibly artificial. In contrast, the Cityscapes images are captured by highquality dash-cameras. Another major distinction is the variability in view angles. Since Cityscapes images are recorded by the dash cameras mounted on a moving car, they are viewed from almost a constant angle that is about parallel to the ground. More diverse view angles are employed by SYNTHIA -it seems like some cameras are placed on the buildings that are significantly higher than a bus. Most GTA images are dashcam images, but some of them are captured from the view points of the pedestrians. In addition, some of the SYNTHIA images are severely shadowed by extreme lighting conditions, while we find no such conditions in the Cityscapes images. Finally, there is a subtle difference in color between the synthetic images and the real ones due to the graphics rendering engines' systematic performance. For instance, the GTA images are overly saturated (cf. <ref type="figure">Figure 4</ref>) and SYNTHIA images are overly bright in general. These combined factors, among others, make the domain adaptation from SYNTHIA and GTA to Cityscapes a very challenging problem. <ref type="figure" target="#fig_2">Figure 3</ref> and <ref type="figure">Figure 4</ref> show some example images from the three datasets. We pair each Cityscapes image with its nearest neighbor in SYNTHIA/GTA, retrieved by the Inception-Resnet-v2 <ref type="bibr" target="#b74">[75]</ref> features. However, many of the cross-dataset nearest neighbors are visually very different from the query images, verifying the dramatic disparity between the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evaluation</head><p>We use the evaluation code released along with the Cityscapes dataset to evaluate our results. It calculates the PASCAL VOC intersection-over-union, i.e., IoU = TP TP+FP+FN <ref type="bibr" target="#b55">[56]</ref>, where TP, FP, and FN are the numbers of true positive, false positive, and false negative pixels, respectively, determined over the whole test set. Since we have to resize the images before feeding them to the segmentation network, we resize the output segmentation mask back to the original image size before running the evaluation against the groundtruth annotations.  <ref type="figure">Fig. 4</ref>: Qualitative semantic segmentation results on the Cityscapes dataset <ref type="bibr" target="#b42">[43]</ref> (target domain). For each target image in the first column, we retrieve its nearest neighbor from the GTA <ref type="bibr" target="#b5">[6]</ref> dataset (source domain). The third column plots the label distributions due to the groundtruth pixel-wise semantic annotation, the predictions by the baseline network with no adaptation, and the inferred distribution by logistic regression. The last three columns are the segmentation results by the baseline network, our domain adaptation approach, and human annotators, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NN Source Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of inferring global label distribution</head><p>Before presenting the final semantic segmentation results, we first compare different approaches to inferring the global label distributions over the target images (cf. Section 3.3.1). We use SYNTHIA and Cityscapes' held-out validation images as the source domain and the target domain, respectively, in this experiment.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we compare the estimated label distributions with the groundtruth ones using the χ 2 distance, the smaller the better. We see that the baseline network (NoAdapt), which is directly learned from the source domain without any adaptation methods, outperforms the dumb uniform distribution (Uniform) and yet no other methods. This confirms that the baseline network gives rise to severely disproportional predictions on the target domain.</p><p>Another dumb prediction (Src mean), i.e., using the mean of all label distributions over the source domain as the prediction for any target image, however, performs reasonably well mainly because the protocol layouts of the urban scene images. This result implies that the images from the simulation environments share at least similar layouts as the real images, indicating the potential value of the simulated source domains for the semantic segmentation task of urban scenes.</p><p>Finally, the nearest neighbor (NN) based method and the multinomial logistic regression (LR) (cf. Section 3.3.1) perform the best. We use the output of LR on the target domain in our remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Domain adaptation experiments</head><p>In this section, we present our main results of this paper, i.e., comparison results for the domain adaptation from simulation to real images for the semantic segmentation task. Here we focus on the base segmentation neural network FCN-8s <ref type="bibr" target="#b6">[7]</ref>. Another network, ADEMXAPP <ref type="bibr" target="#b50">[51]</ref>, is studied in Section 4.7.</p><p>Since our ultimate goal is to solve the semantic segmentation problem for the real images of urban scenes, we take Cityscapes as the target domain and SYNTHIA/GTA as the source domain. We split 500 images out of the Cityscapes training set for the validation purpose (e.g., to monitor the convergence of the networks). In training, we randomly sample mini-matches from both the images and labels of SYNTHIA/GTA and the remaining images of Cityscapes yet with no labels. The original Cityscapes validation set is used as our test set.</p><p>All the 19 classes provided by GTA are used in the experiments. For the adaptation from SYNTHIA to Cityscapes, we manually find 16 common classes between the two datasets: sky, building, road, sidewalk, fence, vegetation, pole, car, traffic sign, person, bicycle, motorcycle, traffic light, bus, wall, and rider. The last four are unnamed and yet labeled in SYNTHIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Baselines</head><p>We mainly compare our approach to the following competing methods. Section 5 supplies additional discussions about and comparisons with more related works. No adaptation (NoAdapt). We directly train the FCN-8s model on the source domain (SYNTHIA or GTA) without applying any domain adaptation methods. This is the most basic baseline in our experiments. Superpixel classification (SP). Recall that we have trained a multi-class SVM using the dominant labels of the superpixels in the source domain. We then use it to classify the target superpixels.</p><p>Landmark superpixels (SP Lndmk). We keep the top 30% most confidently classified superpixels as the landmarks to regularize our segmentation network during training (cf. Section 3.3.2). It is worth examining the classification results of these superpixels. We execute the evaluation after assigning the void class label to the other pixels of the images. In addition to the IoU, we have also evaluated the classification results of the superpixels by accuracy for the domain adaptation experiments from SYNTHIA to Cityscapes. We find that the classification accuracy is 71% for all the superpixels of the target domain. For the top 30% landmark superpixels, the classification accuracy is more than 88%. FCNs in the wild (FCN Wld). Hoffman et al.'s work <ref type="bibr" target="#b9">[10]</ref> was the only existing one addressing the same problem as ours when we published the conference version <ref type="bibr" target="#b10">[11]</ref> of this work, to the best of our knowledge. They introduce a pixel-level adversarial loss to the intermediate layers of the network and impose constraints to the network output. Their experimental setup is about identical to ours except that they do not specify which part of Cityscapes is considered as the test set. Nonetheless, we include their results for comparison to put our work in a better perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Comparison results</head><p>The overall comparison results of adaptating from SYNTHIA to Cityscapes (SYNTHIA2Cityscapes) are shown in <ref type="table" target="#tab_2">Table 2</ref>. We also present the results of adapting from GTA to Cityscapes (GTA2Cityscapes) in <ref type="table" target="#tab_3">Table 3</ref>. Immediately, we note that all our domain adaptation results are significantly better than those without adaptation (NoAdapt) in both tables. We denote by (Ours (I)) the network regularized by the global label distributions over the target images. Although one may wonder that the image-wise label distributions are too abstract to supervise the pixel-wise discriminative network, the gain is actually significant. They are able to correct some obvious errors of the baseline network, such as the disproportional predictions about road and sidewalk (cf. the results of Ours (I) vs. NoAdapt in the last two columns of <ref type="table" target="#tab_2">Tables 2 and 3)</ref>.</p><p>It is interesting to see that both superpixel classification-based segmentation results (SP and SP Lndmk) are also better than the baseline network (NoAdapt). The label distributions obtained over the landmark superpixels boost the segmentation network (Ours (SP)) to the mean IoU of 28.1% and 27.8% respectively when adapting from SYNTHIA and GTA, which are better than those by either superpixel classification or the baseline network individually. We have also tried to use the label distributions over all the superpixels to train the network, and observe little improvement. This is probably because it is too forceful to regularize the network output at every single superpixel especially when the estimated label distributions are not accurate enough.</p><p>The superpixel-based methods, including Ours (SP), miss small objects, such as pole and traffic signs (t-sign), and instead are very accurate for categories like the sky, road, and building, which typically occupy larger image regions. On the contrary, the label distributions on the images give rise to a network (Ours (I)) that performs better on the small objects than Ours (SP). In other words, they mutually complement to some extent. Retraining the network by using the label distributions over both global images and local landmark superpixels (Ours (I+SP)), we achieve semantic segmentation results on the target domain that are superior over using either to regularize the network.  Finally, we report the results of our method and its ablated versions (i.e., Ours (I+SP), Ours (I), and Ours (SP)) after we apply color constancy to the images (accordingly, the methods are denoted by Ours (CC+I+SP), Ours (CC+I), and Ours (CC+SP)). We observe improvements of various degrees over those before the color constancy. Especially, the best results are obtained after we apply the color constancy for adapting from both SYNTHIA and GTA. <ref type="bibr" target="#b9">[10]</ref>: Although we use the same segmentation network (FCN-8s) as <ref type="bibr" target="#b9">[10]</ref>, our baseline results (NoAdapt) are better than those reported in <ref type="bibr" target="#b9">[10]</ref>. This may be due to subtle differences in terms of implementation or experimental setup. For both SYNTHIA2Cityscapes and GTA2Cityscapes, we gain larger improvements (7.7% and 9%) over the baseline <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.1">Comparison with FCNs in the wild</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.2">Comparison with learning domain-invariant features:</head><p>At our first attempt to solve the domain adaptation problem for the semantic segmentation of urban scenes, we tried to learn domain invariant features following the deep domain adaptation method <ref type="bibr" target="#b22">[23]</ref> for classification. In particular, we impose the maximum mean discrepancy <ref type="bibr" target="#b85">[86]</ref> over the layer before the output. We name such network layer the feature layer. Since there are virtually three output layers in FCN-8s, we experiment with all the three feature layers correspondingly. We have also tested the domain adaptation by reversing the gradients of a domain classifier <ref type="bibr" target="#b25">[26]</ref>. However, none of these efforts lead to any noticeable gain over the baseline network so the results are omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Confusion between classes</head><p>While <ref type="table" target="#tab_2">Tables 2 and 3</ref> show the overall and per-class results, they do not tell the confusion between different classes. In this section, we provide the confusion matrices of some methods in order to provide more informative analyses about the results. Considering the page limit, we present in <ref type="figure" target="#fig_4">Figure 5</ref> the confusion matrices of NoAdapt and Ours (CC+I+SP) for SYNTHIA2Cityscapes and GTA2Cityscapes.</p><p>We find that a lot of objects are misclassified to the "building" category, especially the classes "pole", "traffic sign", "traffic light", "fence" and "wall". It is probably because those classes often show up beside buildings and they all have huge intra-class variability. Moreover, the "pole", "traffic sign", and others are very small objects comparing to the "building". Some special care is required to disentangle these classes from the "building" in the future work.</p><p>Another noticeable confusion is between the "train" and the "bus". After analyzing the data, we find that this is likely due to the lack of discrimination between the two classes by the datasets themselves, rather than the algorithms. In <ref type="figure">Figure 4</ref>, we visualize some trains and buses in the GTA dataset and some trains in the Cityscapes dataset. The difference between the trains and the buses turns out very subtle. We humans could make mistakes too if we do not pay attention to the rails (e.g., the train on the bottom right could be easily misclassified as a bus). Probably this confusion between trains and buses could be alleviated if more training examples can be supplied.  GTA2Cityscapes baseline GTA2Cityscapes Ours </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Representations of the superpixels</head><p>One may wonder how the representations of the superpixels could change the overall domain adaptation results. We conduct detailed analyses in this section to reveal some insights about this question. Our main results <ref type="table" target="#tab_2">(Tables 2 and 3</ref>) are obtained by representing the landmark superpixels with the networks pre-trained on PASCAL CONTEXT <ref type="bibr" target="#b76">[77]</ref>. We are interested in examining whether or not such high-level semantic representations of the superpixels are necessary. Hence, we compare the high-level superpixel descriptors with the following low-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Handcrafted feature</head><p>BOW. We encode the superpixels with the bag-of-words (BOW)-SIFT features. We first extract dense SIFT features <ref type="bibr" target="#b86">[87]</ref> from the input image and then encode those of each superpixel into a 100D BOW vector. The dictionary for the encoding is obtained by K-means clustering. FV. We also replace the image-level CNN features with the Fisher vectors (FV) <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref> for estimating the label distribution of an target image. FV encodes the SIFT features per image into a fixed-dimensional descriptor through a Gaussian mixture "Train" GTA "Bus" GTA "Train" Cityscapes TABLE 4: Some "train" and "bus" images from the Cityscapes and GTA datasets. We can see that the Cityscapes "trains" are more visually similar to the GTA "buses" instead of the GTA "trains". model, which has 8 components in this work. An image is then represented by a 2048D vector. We train the dictionary for BOW and the Gaussian mixture model for FV using the SIFT features of the GTA dataset only. <ref type="table" target="#tab_5">Table 5</ref> shows the GTA2Cityscapes results using the handcrafted features. We denote the resulting methods respectively by Ours (CC+BOW), Ours (CC+FV), and Ours (CC+BOW+FV). It is interesting to see all of them outperform the baseline NoAdapt (CC), indicating that our curriculum domain adaptation method is able to leverage the handcrafted features as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">VGG feature</head><p>We also test the VGG features <ref type="bibr" target="#b77">[78]</ref> of the block5_conv4 and block4_conv4 layers. The network is pre-trained on ImageNet and does not bring in any extra knowledge as the segmentation networks are also pre-trained with ImageNet. In order to extract the features for each superpixel, we average-pool the activations per channel within the superpixel.</p><p>With the new VGG features, the superpixel classification accuracy on the Cityscapes validation set is 76%, a 5% boost from the accuracy due to the PASCAL-CONTEXT features. Moreover, the top 30%, which is the landmark superpixels used in our experiments, is labeled with up to 93% accuracy (vs. 88% with the PASCAL-CONTEXT features). Thanks to the boost in the classification accurcy of the landmark superpixels, we also observe an 0.6% gain in mIoU (from 29% to 29.6%) on the domain adaptation from SYNTHIA to Cityscapes. These results imply that the representations of the superpixels do influence the final results, but the representations via an extra knowledge base are not necessarily advantageous; the final results with the ImageNet-pretrained VGG features are superior over those with the PASCAL-CONTEXT features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Granularity of the superpixels</head><p>In this section, we study what is a proper granularity of the superpixels. Intuitively, small superpixels are fine-grained and precisely tracks object boundaries. However, they are less discriminative as a result. What is a proper granularity of the superpixels? How sensitive could the results be to the granularity? To quantitatively answer these questions, we vary the number of superpixels per image to examine their effects on the semantic segmentation results. The adaptation from GTA of the SP (CC) method, described in Section 4.4.1, is reported in <ref type="table" target="#tab_6">Table 6</ref> with various numbers of superpixels per image. In general, the performance increases as the number of superpixels grows until it reaches 300 per image.</p><p>Besides, we also presented the classification accuracy of the top x% superpixels in <ref type="figure" target="#fig_5">Figure 6</ref>, where x = 0, 20, · · · , 100. We can see that the accuracy is always more than 90% when we keep the top 20% or 40% superpixels per image -in our experiments, we keep top 30%. Besides, the accuracy of keep all the superpixels (top 100%) are not very high, indicating that it is not a good idea to use all superpixels to guide the training of the neural networks. Here the images are pre-processed with color constancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SP # per image IoU</head><p>GTA2Cityscapes <ref type="table" target="#tab_5">Class-wise IoU   bike  fence  wall  t-sign  pole  mbike  t-light  sky  bus  rider  veg  terrain  train  bldg  car  person  truck  sidewalk   road   50</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Domain adaptation experiments using ADEMXAPP</head><p>Our approach is agnostic to the base semantic segmentation neural networks. In this section, we further investigate a more recent network, ADEMXAPP <ref type="bibr" target="#b50">[51]</ref>, which is among the few top performing methods on the Cityscapes challenge board. Our experiment setup in this section resembles that of Section 4.4 except that we replace FCN-8s with the ADEMXAPP net. In particular, we reimplement the A1 model of ADEMXAPP using the Theano-Keras framework. However, we remove the batch normalization layers in our implementation due to their extensive GPU memory consumption. We follow the authors' suggestions otherwise and initialize the network with the weights pre-trained on Imagenet.</p><p>We set the size of the mini-batch to six, three images from the source domain and the other three from the target domain. <ref type="table" target="#tab_7">Table 7</ref> shows the comparison results for the ADEMXAPP net. We can see it indeed achieves much better results than FCN-8s in general. Nonetheless, the relative trend of our approach against the others remains the same for this ADEMXAPP net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">What is the "market value" of the synthetic data?</head><p>Despite the positive results thus far for our curriculum domain adaptation from simulation to reality for the semantic segmentation of urban scenes, we argue that the significance of our work is in its capability to complement training sets of real data, rather than replacing them. In the long run, we expect that learning from both simulation and reality will alleviate the strong dependency of deep learning models on massive labeled real training data. Therefore, it is interesting to evaluate the "market value" of the synthetic data in terms of the labeling effort: how many real training images can the GTA or SYNTHIA dataset obviate in order to achieve about the same level of segmentation accuracy?</p><p>In order to answer the above question, we design the following experiment. We train two versions of the VGG-19-FCN-8s network. One is trained on a portion of annotated Cityscapes images while another one is trained using the same subset of Cityscapes images plus the entire SYNTHIA dataset. The subset is sampled from 2380 Cityscapes training images in the experiment since the remaining ones are reserved for the validation purpose. We report both models' performances under different percentages subsampled from Cityscapes. <ref type="table" target="#tab_9">Table 8</ref> presents the results. First of all, it is somewhat surprising to see that even as few as five Cityscapes training images added to the SYNTHIA training set can significantly boost the results obtained from only synthetic images (from 22.0% to 33.8%). Second, the "N/A" results in the table mean that the corresponding neural networks either give rise to random predictions or have numerical issues. Note that such phenomena happen until there are more than 450 target images for the training without any synthetic images, implying that the "market value" of the SYNTHIA training set is at least worth 450 well-labeled real images. Actually, if we compare the results of the two rows, the network trained from the mixed training set outperforms the one from the real images only up to the 50% mix. In other words, augmenting the Cityscapes training set with the SYNTHIA training set improves performance when the Cityscapes training set is smaller than 1000 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">REVIEW OF THE RECENT WORKS ON DOMAIN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADAPTATION FOR SEMANTIC SEGMENTATION</head><p>After our work <ref type="bibr" target="#b10">[11]</ref> published in the IEEE International Conference on Computer Vision in 2017, there have been notably a rich line of works tackling the same problem, i.e., domain adaptation for the semantic segmentation of urban scenes by adapting from the synthetic imagery to real images. Some of them have reported very good results. Since our approach is "orthogonal" in some sense to these others, one may achieve even better results by fusing our method with these new ones. In this section, we give a comprehensive review of these new methods and also present the results of fusing ours with some of them. Most of the methods resort to adversarial training to reduce the domain discrepancy. We review such methods first, followed by the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Adversarial training based methods</head><p>If an adversarial classifier fails to differentiate the data instances of the source domain and the target domain, the discrepancy between the two should have been eliminated in certain sense.  <ref type="bibr" target="#b50">[51]</ref> from GTA to Cityscapes. The ADEMXAPP net is a more powerful semantic segmentation network than FCN-8s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method % IoU</head><p>GTA2Cityscapes  FCNs in the wild <ref type="bibr" target="#b9">[10]</ref>. Hoffman et al. generalize FCNs <ref type="bibr" target="#b6">[7]</ref> from the source domain of synthetic imagery to the target domain of real images for the semantic segmentation task. They employ a pixel-level adversarial loss to enforce the network to extract domain-invariant features. CyCADA <ref type="bibr" target="#b8">[9]</ref>. The main idea is to transform the synthetic images of the source domain to the style of the target domain (real images) using CycleGAN <ref type="bibr" target="#b89">[90]</ref> before feeding the source images to the segmentation network. CycleGAN and the segmentation network are trained simultaneously. ROAD <ref type="bibr" target="#b68">[69]</ref>. In the Reality Oriented Adaptation (ROAD) method <ref type="bibr" target="#b68">[69]</ref>, two losses are proposed to align the source and the target domain. The first one is called target guided distillation, which is a loss for regression from the segmentation network's hidden layer activation of the source domain to the image features of the target domain. Here the image features are obtained by a classifier pre-trained on ImageNet. The other loss takes care of the spatial-aware adaptation. The feature map of either a source image or a target image is partitioned into non-overlapping grids. After that, a maximum mean discrepancy loss <ref type="bibr" target="#b9">[10]</ref> is introduced over each grid. MCD <ref type="bibr" target="#b67">[68]</ref>. The Maximum Classifier Discrepancy (MCD) resembles the recently popularized generative adversarial methods <ref type="bibr" target="#b25">[26]</ref>. It learns two classifiers from the source domain and maximizes their disagreement on the target images in order to detect target examples that fall out of the support of the source domain. After that, it updates the generator to minimize the two classifiers' disagreement on the target domain. By alternating the two steps in the training, it ensures that the generator gives rise to feature representations over which the source and the target domains are well aligned. LSD <ref type="bibr" target="#b66">[67]</ref>. This work is an adversarial domain adaptation network built upon an auto-encoder network. The network takes as input both source and target images and reconstructs them due to an auto-encoder loss. Meanwhile, an intermediate layer is connected to the segmentation network whose loss is defined using the labeled source images. AdaptSegNet <ref type="bibr" target="#b90">[91]</ref>. Similar to FCN in the wild <ref type="bibr" target="#b9">[10]</ref>, this work also employs the adversarial feature learning over the base segmentation model. Instead of having only one discriminator over the feature layer, Tsai et al. propose to install another discriminator on one of the intermediate layers as well. Essentially, features of different scales are forced to align. CGAN <ref type="bibr" target="#b91">[92]</ref>. This work proposes to add a fully convolutional auxiliary pathway to inject random noise into the source domain. Hence, what the segmentation network receives is the source images with perturbations. The authors found such a structure, which is motivated by the conditional GAN, greatly boosts the adaptation performance. ADR <ref type="bibr" target="#b92">[93]</ref>. In ADR, the pixel classifier also serves as the domain classifier. It employs dropout to avoid generating features near the classification boundaries so as to avoid ambiguity to the classifier. NMD <ref type="bibr" target="#b93">[94]</ref>. Besides the global adversarial feature learning module, NMD proposes a local class-wise adversarial loss over image grids. Each image grid is associated with a label distribution. The class-wise adversarial learning then tries to differentiate the source domain's label distributions from those of the target domain due to the semantic segmentation network. DAM <ref type="bibr" target="#b94">[95]</ref>. Huang et al. train two separate networks for the source and target domains, respectively. Since there is no segmentation annotation in the target domain, the targetdomain network is trained by both regressing to the source network's weights and an adversarial loss over every layer of the two networks. FCAN <ref type="bibr" target="#b95">[96]</ref>. Zhang et al. apply the adversarial loss to the lower layers of the segmentation network in addition to the common practice of using it over the last one or a few layers. The intuition is that it plays a complementary role because the lower layers mainly capture the appearance information of the images. DCAN <ref type="bibr" target="#b69">[70]</ref>. DCAN is a two-stage end-to-end network. In the first stage, it is adversarially trained to transfer the source (synthetic) images to the target (real) style. In its second stage, adversarially learning aligns the intermediate features of the two domains. Unlike the other methods, its adversarial alignment only accounts for channel-wise features. I2I <ref type="bibr" target="#b96">[97]</ref>. Similar to DCAN, I2I is another domain adaptation method that learns domain agnostic features by training both an image translation network and a segmentation network. CLoss <ref type="bibr" target="#b97">[98]</ref>. Zhu et al. introduce a conservative loss in addition to the adversarial training. The conservative loss prevents overfitting the model to the source domain by penalizing overly confident source domain predictions during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Other methods</head><p>The adversarial training is so popular that there are only a few methods which are out of the adversarial vein for the domain adaptation of semantic segmentation. IBN <ref type="bibr" target="#b98">[99]</ref>. <ref type="bibr">Pan et al.</ref> show that a careful balance between the instance normalization and batch normalization could enhance a neural network's cross-domain generalization. EUSD <ref type="bibr" target="#b99">[100]</ref>. Arguing that object detectors have better generalization capacity in detecting foreground objects (e.g., car, pedestrian, etc.) than the background, Saleh et al. propose a simple yet powerful domain generalization segmentation framework by fusing Mask-RCNN's detection results of the foreground <ref type="bibr" target="#b101">[101]</ref> and DeepLab's segmentation results of the background <ref type="bibr" target="#b102">[102]</ref>. DAN <ref type="bibr" target="#b103">[103]</ref>. As normalization (e.g., batch normalization) plays a key role in many neural semantic segmentation networks, DAN improves their performance in the target domain by simply replacing the normalization parameters with the statistics of the target domain. This change of normalization boosts the network's results in the target domain. CBST <ref type="bibr" target="#b104">[104]</ref>. Similar to our approach, this paper proposes a curriculum learning method for domain adaptation of semantic segmentation. They introduce a class-balanced self-training strategy: the most confident predictions by a model on the unlabeled instances are likely to be correct. In each round of the training process, CBST selects some of the most confident predictions on the target images and include them in the training set of the next round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results reported in the papers</head><p>We summarize the results reported in the papers, along with ours, in <ref type="table" target="#tab_10">Table 9</ref>. Immediately, we can see that the backbone network has a huge impact on both the source-only performance without applying any adaptation techniques and the relative gain after adaptation. For instance, the performance gain of MCD jumps from 3.9% to 17.5% after switching the backbone network from VGG to DRN <ref type="bibr" target="#b105">[105]</ref>. Another interesting observation is that the results of no adaptation vary a lot even when the same backbone network (e.g., VGG16) is used, implying that subtle changes to the implementation (e.g., removing batch normalization in order to save computation cost) can result in big differences among the final results. Through private communications with some authors of these papers, we also learned that the image resolution is another key factor. In general, higher resolution of the input image gives rise to better results no matter with or without domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The existing methods and ours are complementary</head><p>In this section, we provide experimental evidence to show that our method is complementary to most existing methods. A late fusion of our method with another can yield better results than either individually. This is not surprising as our curriculum domain adaptation approach guides the network towards the target  <ref type="figure">Fig. 7</ref>: Pairwise comparison between different domain adaptation methods for the semantic segmentation task. The entry (i, j) of this table is the number of classes by which the i-th method outperforms the j-th. The results are obtained on GTA2Cityscapes. Our method is labeled Curriculum.</p><p>domain by inferring properties of the labels, whereas most existing methods learn domain-invariant features or image styles.</p><p>First of all, we analyze the class-wise prediction accuracy (evaluated by mIoU) of different methods on the target domain. <ref type="figure">Figure 7</ref> shows the pairwise comparison between our method and some other representative methods. The entry (i, j) shows the number of classes by which the i-th method outperforms the j-th on the target domain. This table is derived from the results of GTA2Cityscapes. Our method here is the GTA model trained without color constancy. The class-wise comparison matrix is best understood if we recall the results in <ref type="table" target="#tab_10">Table 9</ref>. Take I2I for instance. While it outperforms ours by 2.9% in mIoU, ours is superior over I2I in 10 out of the 19 classes. We can draw similar observations for the other methods. To this end, we find that our approach is genuinely complementary to the other methods. Finally, we find that CBST is complementary to other adversarial training based methods as well since it is in the same vein as our curriculum domain adaptation strategy.</p><p>It is worth emphasizing that the class-wise complementary relationship between these methods is only one of the possible perspectives for the analyses. More fine-grained analyses, for example image-wise, may reveal further insights about the methods.</p><p>Equipped with the class-wise comparison between any pair of methods, we design a simple late fusion scheme to ensemble two models. We first identify the classes for which one model gives rise to more accurate prediction than the other model by using the validation set. Given a test image, we keep its pixel-wise labels of those classes predicted by the former model. For the remaining pixels, we label them by the latter model. We apply this late fusion scheme to CYCADA and our approach. For GTA2Cityscapes, the mIoUs of CYCADA and ours (without color constancy) are 32.5% and 28%, respectively. In contrast, the late fusion of the two leads to an mIoU of 34.3% which is higher than either of them. This result and the analysis shown in <ref type="figure">Figure 7</ref> clearly evidence that most of the existing methods are complementary to ours for domain adaptation of the semantic segmentation task. .7 * DAM did not report the baseline performance of the SYNTHIA2Cityscapes experiment. Since their model is fine-tuned from our baseline, we report our baseline performance instead. ** NMD and AdaptSegNet used 13 SYNTHIA classes in their experiments instead of the commonly used 16. *** We do not use any external information (cf. Section 4.5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a curriculum domain adaptation approach for semantic segmentation of urban scenes. We learn to estimate the global label distributions over the target images and local label distributions over the superpixels of the target images. These tasks are easier to solve than the pixel-wise label assignment. We then use their results to effectively regularize the training of the semantic segmentation networks such that their pixel-wise predictions are consistent with the global and local label distributions. We experimentally verify the effectiveness of our approach by adapting from the source domain of synthetic images to the target domain of real images. Our method outperforms several competing baselines. Moreover, we report several key ablation studies that allow us to gain more insights about the proposed method. We also check the class-wise confusion matrices and find that some of the classes (e.g., train and bus) are almost indistinguishable in the current datasets, indicating that better simulation or more labeled real examples are required in order to achieve better segmentation results. In future work, we will explore more target properties that possess the same form as the global and local label distributions -they are easier to solve than the pixel-wise label prediction and meanwhile can be written as a function of the pixel-wise labels. We also would like to look into the possibility of directly applying our domain adaptation framework to virtual autonomous driving environments such as DeepGTAV <ref type="bibr" target="#b106">[106]</ref> and AirSim <ref type="bibr" target="#b3">[4]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The overall framework of our curriculum domain adaptation approach to the semantic segmentation of urban scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Predictions by the same FCN-8s model, without domain adaptation, before and after calibrating the image's colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative semantic segmentation results on the Cityscapes dataset<ref type="bibr" target="#b42">[43]</ref> (target domain). For each target image in the first column, we retrieve its nearest neighbor from the SYNTHIA<ref type="bibr" target="#b1">[2]</ref> dataset (source domain). The third column plots the label distributions due to the groundtruth pixel-wise semantic annotation, the predictions by the baseline network with no adaptation, and the inferred distribution by logistic regression. The last three columns are the segmentation results by the baseline network, our domain adaptation approach, and human annotators, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>e r B u s W a l l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Confusion matrices for the baseline of no adaptation (left) and Ours (CC+I+SP) (right) for the experiments of SYNTHIA-to-Cityscapes (SYNTHIA2Cityscapes, top) and GTA-to-Cityscapes (GTA2Cityscapes, bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>We evaluate how many superpixels are accurate in the top x% confidently predicted superpixels. The experiments are conducted on the validation set of Cityscapes with color constancy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>χ 2 distances between the groundtruth label distributions and those predicted by different methods for the adaptation from SYNTHIA to Cityscapes.</figDesc><table><row><cell>Method</cell><cell cols="3">Uniform NoAdapt Src mean</cell><cell>NN</cell><cell>LR</cell></row><row><cell>χ 2 Distance</cell><cell>1.13</cell><cell>0.65</cell><cell>0.44</cell><cell cols="2">0.33 0.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Comparison results for adapting the FCN-8s model from SYNTHIA to Cityscapes.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">SYNTHIA2Cityscapes Class-wise IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method %</cell><cell>IoU</cell><cell>bike</cell><cell>fence</cell><cell>wall</cell><cell>t-sign</cell><cell>pole</cell><cell>mbike</cell><cell>t-light</cell><cell>sky</cell><cell>bus</cell><cell>rider</cell><cell>veg</cell><cell>bldg</cell><cell>car</cell><cell>person</cell><cell>sidewalk</cell><cell>road</cell></row><row><cell>NoAdapt [10]</cell><cell>17.4</cell><cell>0.0</cell><cell cols="2">0.0 1.2</cell><cell>7.2</cell><cell cols="4">15.1 0.1 0.0 66.8</cell><cell>3.9</cell><cell>1.5</cell><cell cols="5">30.3 29.7 47.3 51.1 17.7</cell><cell>6.4</cell></row><row><cell>FCN Wld [10]</cell><cell>20.2</cell><cell>0.6</cell><cell cols="7">0.0 4.4 11.7 20.3 0.2 0.1 68.7</cell><cell>3.2</cell><cell>3.8</cell><cell cols="6">42.3 30.8 54.0 51.2 19.6 11.5</cell></row><row><cell>NoAdapt</cell><cell cols="4">22.0 18.0 0.5 0.8</cell><cell>5.3</cell><cell cols="4">21.5 0.5 8.0 75.6</cell><cell>4.5</cell><cell>9.0</cell><cell cols="5">72.4 59.6 23.6 35.1 11.2</cell><cell>5.6</cell></row><row><cell>NoAdapt (CC)</cell><cell cols="4">22.6 22.2 0.5 1.1</cell><cell>5.0</cell><cell cols="4">21.5 0.6 8.5 73.4</cell><cell>4.8</cell><cell>9.2</cell><cell cols="5">73.2 56.7 28.4 34.8 12.1</cell><cell>9.1</cell></row><row><cell>Ours (I)</cell><cell cols="4">25.5 16.7 0.8 2.3</cell><cell>6.4</cell><cell cols="5">21.7 1.0 9.9 59.6 12.1</cell><cell>7.9</cell><cell cols="6">70.2 67.5 32.0 29.3 18.1 51.9</cell></row><row><cell>Ours (CC+I)</cell><cell cols="4">27.3 31.2 1.3 3.9</cell><cell>6.0</cell><cell cols="5">19.4 2.1 9.2 61.2 11.2</cell><cell>7.4</cell><cell cols="6">68.3 65.1 41.4 29.3 18.9 60.6</cell></row><row><cell>SP Lndmk (CC)</cell><cell>23.1</cell><cell>0.0</cell><cell cols="2">0.0 0.0</cell><cell>0.0</cell><cell>0.0</cell><cell cols="4">0.0 0.0 82.6 27.8</cell><cell>0.0</cell><cell cols="3">73.1 67.9 40.7</cell><cell>5.8</cell><cell cols="2">10.3 62.2</cell></row><row><cell>SP (CC)</cell><cell>25.6</cell><cell>0.0</cell><cell cols="2">0.0 0.0</cell><cell>0.0</cell><cell>0.0</cell><cell cols="4">0.0 0.0 80.1 22.7</cell><cell>0.0</cell><cell cols="6">72.2 69.7 45.6 25.0 19.4 74.8</cell></row><row><cell>Ours (SP)</cell><cell cols="4">28.1 10.2 0.4 0.1</cell><cell>2.7</cell><cell>8.1</cell><cell cols="4">0.8 3.7 68.7 21.4</cell><cell>7.9</cell><cell cols="6">75.5 74.6 42.9 47.3 23.9 61.8</cell></row><row><cell>Ours (CC+SP)</cell><cell cols="4">28.9 17.7 0.5 0.5</cell><cell>3.4</cell><cell cols="5">10.9 1.8 5.4 73.4 17.6</cell><cell>9.9</cell><cell cols="6">76.8 74.5 43.7 44.4 22.4 59.6</cell></row><row><cell>Ours (I+SP)</cell><cell cols="4">29.0 13.1 0.5 0.1</cell><cell>3.0</cell><cell cols="5">10.7 0.7 3.7 70.6 20.7</cell><cell>8.2</cell><cell cols="6">76.1 74.9 43.2 47.1 26.1 65.2</cell></row><row><cell cols="5">Ours (CC+I+SP) 29.7 20.3 0.6 0.5</cell><cell>4.3</cell><cell cols="12">14.0 1.9 5.3 73.7 21.2 11.0 77.8 74.7 44.8 45.0 23.1 57.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Comparison results for adapting the FCN-8s model from GTA to Cityscapes. CC+I+SP) 31.4 12.0 13.2 12.1 14.1 15.3 19.3 16.8 75.5 19.0 10.0 79.3 14.5 0.0 74.9 62.1 35.7 20.6 30.0 72.9</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">GTA2Cityscapes Class-wise IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method %</cell><cell>IoU</cell><cell>bike</cell><cell>fence</cell><cell>wall</cell><cell>t-sign</cell><cell>pole</cell><cell>mbike</cell><cell>t-light</cell><cell>sky</cell><cell>bus</cell><cell>rider</cell><cell>veg</cell><cell>terrain</cell><cell>train</cell><cell>bldg</cell><cell>car</cell><cell>person</cell><cell>truck</cell><cell>sidewalk</cell><cell>road</cell></row><row><cell>NoAdapt [10]</cell><cell>21.1</cell><cell>0.0</cell><cell>3.1</cell><cell>7.4</cell><cell>1.0</cell><cell>16.0</cell><cell>0.0</cell><cell cols="2">10.4 58.9</cell><cell>3.7</cell><cell>1.0</cell><cell>76.5</cell><cell>13</cell><cell>0.0</cell><cell cols="2">47.7 67.1</cell><cell>36</cell><cell>9.5</cell><cell cols="2">18.9 31.9</cell></row><row><cell>FCN Wld [10]</cell><cell>27.1</cell><cell>0.0</cell><cell>5.4</cell><cell>14.9</cell><cell>2.7</cell><cell>10.9</cell><cell>3.5</cell><cell cols="2">14.2 64.6</cell><cell>7.3</cell><cell>4.2</cell><cell cols="2">79.2 21.3</cell><cell>0.0</cell><cell cols="3">62.1 70.4 44.1</cell><cell>8.0</cell><cell cols="2">32.4 70.4</cell></row><row><cell>NoAdapt</cell><cell cols="2">22.3 13.8</cell><cell>8.7</cell><cell>7.3</cell><cell cols="2">16.8 21.0</cell><cell>4.3</cell><cell cols="2">14.9 64.4</cell><cell>5.0</cell><cell cols="2">17.5 45.9</cell><cell>2.4</cell><cell>6.9</cell><cell cols="3">64.1 55.3 41.6</cell><cell>8.4</cell><cell>6.8</cell><cell>18.1</cell></row><row><cell>NoAdapt (CC)</cell><cell cols="3">26.2 16.2 10.9</cell><cell>8.8</cell><cell cols="2">18.5 23.3</cell><cell>7.0</cell><cell cols="2">13.2 62.7</cell><cell>5.4</cell><cell cols="2">19.0 65.1</cell><cell>5.8</cell><cell>2.3</cell><cell cols="3">64.8 63.9 42.2</cell><cell>9.2</cell><cell cols="2">13.8 45.0</cell></row><row><cell>Ours (I)</cell><cell>23.1</cell><cell>9.5</cell><cell>9.4</cell><cell cols="3">10.2 14.0 20.2</cell><cell>3.8</cell><cell cols="2">13.6 63.8</cell><cell>3.4</cell><cell cols="2">10.6 56.9</cell><cell>2.8</cell><cell cols="7">10.9 69.7 60.5 31.8 10.9 10.8 26.4</cell></row><row><cell>Ours (CC+I)</cell><cell>28.5</cell><cell>7.2</cell><cell>9.4</cell><cell cols="3">11.1 13.4 23.1</cell><cell>9.6</cell><cell cols="2">15.1 64.6</cell><cell>5.9</cell><cell cols="3">15.5 71.1 10.3</cell><cell>3.9</cell><cell cols="6">67.7 62.3 43.0 14.0 23.0 71.6</cell></row><row><cell>SP Lndmk (CC)</cell><cell>21.6</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>82.4</cell><cell>9.1</cell><cell>0.0</cell><cell cols="2">74.4 22.2</cell><cell>0.0</cell><cell cols="4">70.3 53.1 15.3 11.2</cell><cell>6.9</cell><cell>65.8</cell></row><row><cell>SP (CC)</cell><cell>26.8</cell><cell>0.3</cell><cell>2.3</cell><cell>6.8</cell><cell>0.0</cell><cell>0.2</cell><cell>3.4</cell><cell>0.0</cell><cell cols="2">80.5 25.5</cell><cell>4.1</cell><cell cols="2">73.5 31.4</cell><cell>0.0</cell><cell cols="6">71.0 61.6 28.2 30.4 17.3 73.3</cell></row><row><cell>Ours (SP)</cell><cell cols="3">27.8 15.6 11.7</cell><cell>5.7</cell><cell>12.0</cell><cell>9.2</cell><cell cols="4">12.9 15.5 64.9 15.5</cell><cell>9.1</cell><cell cols="2">74.6 11.1</cell><cell>0.0</cell><cell cols="6">70.5 56.1 34.8 15.9 21.8 72.1</cell></row><row><cell>Ours (CC+SP)</cell><cell cols="10">30.2 10.4 13.6 10.3 14.0 13.9 18.8 16.5 73.6 14.1</cell><cell>9.5</cell><cell cols="2">79.2 12.9</cell><cell>0.0</cell><cell cols="6">74.3 63.5 33.1 18.9 27.5 70.5</cell></row><row><cell>Ours (I+SP)</cell><cell cols="3">28.9 14.6 11.9</cell><cell>6.0</cell><cell>11.1</cell><cell>8.4</cell><cell cols="4">16.8 16.3 66.5 18.9</cell><cell>9.3</cell><cell cols="2">75.7 13.3</cell><cell>0.0</cell><cell cols="6">71.7 55.2 38.0 18.8 22.0 74.9</cell></row><row><cell>Ours (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Results for the adaptation of FCN-8s from GTA to Cityscapes when we use handcrafted features instead of the CNN features.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">GTA2Cityscapes Class-wise IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method %</cell><cell>IoU</cell><cell>bike</cell><cell>fence</cell><cell>wall</cell><cell>t-sign</cell><cell>pole</cell><cell>mbike</cell><cell>t-light</cell><cell>sky</cell><cell>bus</cell><cell>rider</cell><cell>veg</cell><cell>terrain</cell><cell>train</cell><cell>bldg</cell><cell>car</cell><cell>person</cell><cell>truck</cell><cell>sidewalk</cell><cell>road</cell></row><row><cell>NoAdapt (CC)</cell><cell>26.2</cell><cell cols="2">16.2 10.9</cell><cell>8.8</cell><cell>18.5</cell><cell>23.3</cell><cell cols="2">7.0 13.2</cell><cell>62.7</cell><cell>5.4</cell><cell cols="2">19.0 65.1</cell><cell>5.8</cell><cell>2.3</cell><cell>64.8</cell><cell>63.9</cell><cell>42.2</cell><cell>9.2</cell><cell>13.8</cell><cell>45.0</cell></row><row><cell>Ours (CC+BOW)</cell><cell>27.9</cell><cell>13.8</cell><cell>14.0</cell><cell>9.6</cell><cell>17.9</cell><cell>23.9</cell><cell>6.4</cell><cell>16.7</cell><cell>64.6</cell><cell>3.0</cell><cell>18.0</cell><cell>69.1</cell><cell>7.0</cell><cell>2.4</cell><cell>69.2</cell><cell>60.1</cell><cell cols="2">44.0 10.7</cell><cell cols="2">19.1 60.8</cell></row><row><cell>Ours (CC+FV)</cell><cell>28.1</cell><cell>13.3</cell><cell>10.5</cell><cell>12.8</cell><cell cols="2">18.6 24.4</cell><cell>5.1</cell><cell>10.8</cell><cell>63.5</cell><cell>1.7</cell><cell>14.6</cell><cell>73.5</cell><cell cols="2">10.0 0.3</cell><cell>71.6</cell><cell cols="2">66.6 40.8</cell><cell>6.1</cell><cell>11.5</cell><cell>79.0</cell></row><row><cell>Ours (CC+BOW+FV)</cell><cell>28.3</cell><cell>15.3</cell><cell>13.3</cell><cell>11.6</cell><cell>18.5</cell><cell cols="2">25.1 6.7</cell><cell>16.8</cell><cell cols="2">66.5 2.9</cell><cell>18.4</cell><cell>72.2</cell><cell>8.7</cell><cell cols="2">2.6 70.0</cell><cell>59.4</cell><cell>43.9</cell><cell cols="2">10.8 19.1</cell><cell>56.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>Results for the adaptation of FCN-8s from GTA to Cityscapes when we use different numbers of superpixels per image.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 :</head><label>7</label><figDesc>Results for the adaptation of ADEMXAPP</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>IoUs after mixing different percentages of Cityscapes images into the SYNTHIA training dataset (SYN+CS), and of models trained with different percentages of Cityscapes images without any SYNTHIA images (CS only).</figDesc><table><row><cell># CS images</cell><cell>None</cell><cell>5 images</cell><cell>1%</cell><cell>2.5%</cell><cell>5%</cell><cell>10%</cell><cell>20%</cell><cell>50%</cell><cell>100%</cell></row><row><cell>SYN+CS</cell><cell>22.0%</cell><cell>33.8%</cell><cell cols="7">38.4% 41.0% 43.0% 46.5% 48.5% 53.2% 57.3%</cell></row><row><cell>CS only</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="3">38.4% 52.2% 57.8%</cell></row><row><cell cols="5">Many methods depend on this principle and differ on how to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">incorporate it to the training of the segmentation network.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 :</head><label>9</label><figDesc>Comparison with the recent works published after the conference version of our approach<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table><row><cell>Method</cell><cell>Backbone Network</cell><cell>Base Adapt</cell><cell cols="2">SYN2CS IoU Gain</cell><cell cols="2">GTA2CS IoU Gain</cell><cell>Mechanism</cell><cell>Code</cell></row><row><cell>CyCADA [9]</cell><cell>VGG-16 DRN-26</cell><cell></cell><cell>----</cell><cell>--</cell><cell>17.9 35.4 21.7 39.5</cell><cell>17.5 17.8</cell><cell>Adversary</cell><cell>Link</cell></row><row><cell>ROAD [69]</cell><cell>VGG-16</cell><cell></cell><cell>25.4 36.2</cell><cell>10.8</cell><cell>21.9 35.9</cell><cell>13.0</cell><cell>Adversary</cell><cell>-</cell></row><row><cell>MCD [68]</cell><cell>VGG-16 DRN-105</cell><cell></cell><cell>--23.4 37.3</cell><cell>-13.9</cell><cell>24.9 28.8 22.2 39.7</cell><cell>3.9 17.5</cell><cell>Adversary</cell><cell>Link</cell></row><row><cell>LSD [67]</cell><cell>VGG-16</cell><cell></cell><cell>26.8 36.1</cell><cell>9.3</cell><cell>29.6 37.1</cell><cell>7.5</cell><cell>Adversary</cell><cell>Link</cell></row><row><cell>AdaptSegNet [91]</cell><cell>VGG-16 ResNet-101</cell><cell></cell><cell>-37.6 ** 38.6 ** 46.7 **</cell><cell>-9.1</cell><cell>-35.0 36.6 42.4</cell><cell>-5.8</cell><cell>Adversary</cell><cell>Link</cell></row><row><cell>FCAN [96]</cell><cell>ResNet-101</cell><cell></cell><cell>--</cell><cell>-</cell><cell>29.2 46.6</cell><cell>17.4</cell><cell>Adversary</cell><cell>-</cell></row><row><cell>ADR [93]</cell><cell>ResNet-50</cell><cell></cell><cell>--</cell><cell>-</cell><cell>25.3 33.3</cell><cell>8.0</cell><cell>Adversary</cell><cell>-</cell></row><row><cell>I2I [97]</cell><cell>ResNet-34 DenseNet-121</cell><cell></cell><cell>----</cell><cell>--</cell><cell>21.1 31.8 29.0 35.7</cell><cell>10.7 6.7</cell><cell>Adversary</cell><cell>-</cell></row><row><cell></cell><cell>VGG-16</cell><cell></cell><cell>25.9 35.4</cell><cell>9.5</cell><cell>27.8 36.2</cell><cell>8.4</cell><cell></cell></row><row><cell>DCAN [70]</cell><cell>ResNet-101</cell><cell></cell><cell>28.0 36.5</cell><cell>8.5</cell><cell>29.8 38.5</cell><cell>8.7</cell><cell>Adversary</cell><cell>-</cell></row><row><cell></cell><cell>PSPNet</cell><cell></cell><cell>29.5 38.4</cell><cell>8.9</cell><cell>33.3 41.7</cell><cell>8.4</cell><cell></cell></row><row><cell></cell><cell>VGG-16</cell><cell></cell><cell>22.0 * 30.7</cell><cell>8.7 *</cell><cell>18.8 32.6</cell><cell>13.8</cell><cell></cell></row><row><cell>DAM [95]</cell><cell>DRN-26</cell><cell></cell><cell>--</cell><cell>-</cell><cell>-40.2</cell><cell>-</cell><cell>Adversary</cell><cell>Link</cell></row><row><cell></cell><cell>ERFNet</cell><cell></cell><cell>--</cell><cell>-</cell><cell>15.8 31.3</cell><cell>15.5</cell><cell></cell></row><row><cell>CGAN [92]</cell><cell>VGG-16</cell><cell></cell><cell>17.4 41.2</cell><cell>23.8</cell><cell>21.1 44.5</cell><cell>23.4</cell><cell>Adversary</cell><cell>-</cell></row><row><cell>NMD [94]</cell><cell>Dialation Frontend</cell><cell></cell><cell>30.7 ** 35.7 **</cell><cell>5.0</cell><cell>--</cell><cell>-</cell><cell>Adversary</cell><cell>-</cell></row><row><cell>CLoss [98]</cell><cell>VGG-16</cell><cell></cell><cell>24.9 34.2</cell><cell>9.3</cell><cell>30.0 38.1</cell><cell>8.1</cell><cell>Adversary</cell><cell>-</cell></row><row><cell>FCN Wld [10]</cell><cell>Dialation Frontend</cell><cell></cell><cell>17.4 20.2</cell><cell>2.8</cell><cell>21.1 27.1</cell><cell>6.0</cell><cell>Adversary</cell><cell>-</cell></row><row><cell>CBST [104]</cell><cell>VGG-16 ResNet-38</cell><cell></cell><cell>22.6 35.4 29.2 42.5</cell><cell>12.8 13.3</cell><cell>24.3 36.1 35.4 47.0</cell><cell>11.8 11.6</cell><cell>Self-Training</cell><cell>Link</cell></row><row><cell>IBN [99]</cell><cell>ResNet-50</cell><cell></cell><cell>--</cell><cell>-</cell><cell>22.2 29.6</cell><cell>7.4</cell><cell>Normalization</cell><cell>Link</cell></row><row><cell>EUSD [100]</cell><cell>DeepLab Mask R-CNN</cell><cell></cell><cell>--</cell><cell>-</cell><cell>31.3 42.5</cell><cell>11.2</cell><cell>Ensemble Detector &amp; Segmenter</cell><cell>-</cell></row><row><cell>DAN [103]</cell><cell>ResNet-50</cell><cell></cell><cell>--</cell><cell>-</cell><cell>34.8 38.2</cell><cell>3.4</cell><cell>Normalization</cell><cell>Link</cell></row><row><cell></cell><cell>VGG-19</cell><cell></cell><cell>22.0 29.7</cell><cell>7.7</cell><cell>22.3 31.4</cell><cell>9.1</cell><cell></cell></row><row><cell>Ours</cell><cell>VGG-19 *** DRN-26</cell><cell></cell><cell>22.0 29.6 21.9 28.2</cell><cell>7.6 6.3</cell><cell>----</cell><cell>--</cell><cell>Curriculum</cell><cell>Link</cell></row><row><cell></cell><cell>ADEMXAPP</cell><cell></cell><cell>--</cell><cell>-</cell><cell>30.0 35</cell><cell>5.7</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">because the image is a little greenish.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the NSF award IIS #1566511, a gift from Adobe Systems Inc., and a GPU from NVIDIA. It was also in part supported by the NSF grant IIS-1212948, and a gift from Uber Technologies Inc.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A review on deep learning techniques applied to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06857</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01079</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Airsim: High-fidelity visual and physical simulation for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lovett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and Service Robotics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Annual Conference on Robot Learning</title>
		<meeting>the 1st Annual Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visda: The visual domain adaptation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Connecting the dots with landmarks: Discriminatively learning domain invariant features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="222" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Landmarks-based kernelized subspace alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07818</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Covariate shift by kernel mean matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmittfull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dataset Shift in Machine Learning</title>
		<editor>J. QuiÃ±onero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1796" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2001-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A deeper look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01257</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Overcoming dataset bias: An unsupervised domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Large Scale Visual Recognition and Retrieval (LSVRR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From virtual to reality: Fast adaptation of virtual object detectors to real domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVA British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical adaptive structural svm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5400</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Virtual and real world adaptation for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gerónimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of Pattern Recognition and Machine Analyses (PAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="797" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Synthetic to real adaptation with deep generative correlation alignment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05524</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07828</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantic texton forests for image categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes using dense depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="708" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Wider or Deeper: Revisiting the ResNet Model for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Training constrained deconvolutional networks for road scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01545</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of Pattern Recognition and Machine Analyses (PAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning transferrable knowledge for semantic segmentation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3204" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">From image-level to pixellevel labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Play and learn: using video Games to train computer vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shafaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01745</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVA British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06318</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Superpixel segmentation using linear spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1356" to="1363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Color constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="674" to="700" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Improving dermoscopy image classification using color constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1146" to="1152" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Computational color constancy: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2475" to="2489" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Generalized gamut mapping using image derivative structures for color constancy</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="127" to="139" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belopolsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2011" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Domain transfer through deep activation matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="611" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Penalizing top performers: Conservative loss for semantic segmentation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Effective use of synthetic data for urban scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="86" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of Pattern Recognition and Machine Analyses (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">A domain agnostic normalization layer for unsupervised adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE Winter Conference on Applications of Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Vijaya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Deepgtav</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruano</surname></persName>
		</author>
		<ptr target="https://github.com/aitorzip/DeepGTAV" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
