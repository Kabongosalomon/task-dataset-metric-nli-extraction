<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-SGD: Learning to Learn Quickly for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<email>li.zhenguo@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
							<email>zhou.fengwei@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
							<email>chenfei100@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-SGD: Learning to Learn Quickly for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to learn and adapt rapidly from small data is essential to intelligence. However, current success of deep learning relies greatly on big labeled data. It learns each task in isolation and from scratch, by fitting a deep neural network over data through extensive, incremental model updates using stochastic gradient descent (SGD). The approach is inherently data-hungry and time-consuming, with fundamental challenges for problems with limited data or in dynamic environments where fast adaptation is critical. In contrast, humans can learn quickly from a few examples by leveraging prior experience. Such capacity in data efficiency and fast adaptation, if realized in machine learning, can greatly expand its utility. This motivates the study of few-shot learning, which aims to learn quickly from only a few examples <ref type="bibr" target="#b14">[15]</ref>.</p><p>Several existing ideas may be adapted for few-shot learning. In transfer learning, one often fine-tunes a pre-trained model using target data <ref type="bibr" target="#b21">[22]</ref>, where it is challenging not to unlearn the previously acquired knowledge. In multi-task learning, the target task is trained jointly with auxiliary ones to distill inductive bias about the target problem <ref type="bibr" target="#b3">[4]</ref>. It is tricky to decide what to share in the joint model. In semi-supervised learning, one augments labeled target data with massive unlabeled data to leverage a holistic distribution of the data <ref type="bibr" target="#b27">[28]</ref>. Strong assumptions are required for this method to work. While these efforts can alleviate the issue of data scarcity to some extend, the way prior knowledge is used is specific and not generalizable. A principled approach for few-shot learning to representing, extracting and leveraging prior knowledge is in need.</p><p>Meta-learning offers a new perspective to machine learning, by lifting the learning level from data to tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>. Consider supervised learning. The common practice learns from a set of labeled examples, while meta-learning learns from a set of (labeled) tasks, each represented as a labeled training set and a labeled testing set. The hypothesis is that by being exposed to a broad scope of a task space, a learning agent may figure out a learning strategy tailored to the tasks in that space. <ref type="figure">Figure 1</ref>: Illustrating the two-level learning process of Meta-SGD. Gradual learning is performed across tasks at the meta-space (θ, α) that learns the meta-learner. Rapid learning is carried out by the meta-learner in the learner space θ that learns task-specific learners.</p><p>Specifically, in meta-learning, a learner for a specific task is learned by a learning algorithm called meta-learner, which is learned on a bunch of similar tasks to maximize the combined generalization power of the learners of all tasks. The learning occurs at two levels and in different time-scales. Gradual learning is performed across tasks, which learns a meta-learner to carry out rapid learning within each task, whose feedback is used to adjust the learning strategy of the meta-learner. Interestingly, the learning process can continue forever, thus enabling life-long learning, and at any moment, the meta-learner can be applied to learn a learner for any new task. Such a two-tiered learning to learn strategy for meta-learning has been applied successfully to few-shot learning on classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>, regression <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, and reinforcement learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>The key in meta-learning is in the design of meta-learners to be learned. In general terms, a metalearner is a trainable learning algorithm that can train a learner, influence its behavior, or itself function as a learner. Meta-learners developed so far include recurrent models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>, metrics <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref>, or optimizers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. A recurrent model such as Long Short-Term Memory (LSTM) <ref type="bibr" target="#b8">[9]</ref> processes data sequentially and figures out its own learning strategy from scratch in the course <ref type="bibr" target="#b18">[19]</ref>. Such meta-learners are versatile but less comprehensible, with applications in classification <ref type="bibr" target="#b18">[19]</ref>, regression <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>, and reinforcement learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>. A metric influences a learner by modifying distances between examples. Such meta-learners are more suitable for non-parametric learners such as the k-nearest neighbors algorithm or its variants <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref>. Meta-learners above do not learn an explicit learner, which is typically done by an optimizer such as SGD. This suggests that optimizers, if trainable, can serve as meta-learners. The meta-learner perspective of optimizers, which is used to be hand-designed, opens the door for learning optimizers via meta-learning.</p><p>Recently, LSTM is used to update models such as Convolutional Neural Network (CNN) iteratively like SGD <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>, where both initialization and update strategy are learned via meta-learning, thus called Meta-LSTM in what follows. This should be in sharp contrast to SGD where the initialization is randomly chosen, the learning rate is set manually, and the update direction simply follows the gradient. While Meta-LSTM shows promising results on few-shot learning <ref type="bibr" target="#b17">[18]</ref> or as a generic optimizer <ref type="bibr" target="#b1">[2]</ref>, it is rather difficult to train. In practice, each parameter of the learner is updated independently in each step, which greatly limits its potential. In this paper, we develop a new optimizer that is very easy to train. Our proposed meta-learner acts like SGD, thus called Meta-SGD ( <ref type="figure">Figure 1</ref>), but the initialization, update direction, and learning rates are learned via meta-learning, like Meta-LSTM. Besides much easier to train than Meta-LSTM, Meta-SGD also learns much faster than Meta-LSTM. It can learn effectively from a few examples even in one step. Experimental results on regression, classification, and reinforcement learning unanimously show that Meta-SGD is highly competitive on few-show learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>One popular approach to few-shot learning is with generative models, where one notable work is by <ref type="bibr" target="#b13">[14]</ref>. It uses probabilistic programs to represent concepts of handwritten characters, and exploits the specific knowledge of how pen strokes are composed to produce characters. This work shows how knowledge of related concepts can ease learning of new concepts from even one example, using the principles of compositionality and learning to learn <ref type="bibr" target="#b14">[15]</ref>.</p><p>A more general approach to few-shot learning is by meta-learning, which trains a meta-learner from many related tasks to direct the learning of a learner for a new task, without relying on ad hoc knowledge about the problem. The key is in developing high-capacity yet trainable metalearners. <ref type="bibr" target="#b24">[25]</ref> suggest metrics as meta-learners for non-parametric learners such as k-nearest neighbor classifiers. Importantly, it matches training and testing conditions in meta-learning, which works well for few-shot learning and is widely adopted afterwards. Note that a metric does not really train a learner, but influences its behavior by modifying distances between examples. As such, metric meta-learners mainly work for non-parametric learners.</p><p>Early studies show that a recurrent neural network (RNN) can model adaptive optimization algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>. This suggests its potential as meta-learners. Interestingly, <ref type="bibr" target="#b9">[10]</ref> find that LSTM performs best as meta-learner among various architectures of RNNs. <ref type="bibr" target="#b1">[2]</ref> formulate LSTM as a generic, SGD-like optimizer which shows promising results compared to widely used hand-designed optimization algorithms. In <ref type="bibr" target="#b1">[2]</ref>, LSTM is used to imitate the model update process of the learner (e.g., CNN) and output model increment at each timestep. <ref type="bibr" target="#b17">[18]</ref> extend <ref type="bibr" target="#b1">[2]</ref> for few-shot learning, where the LSTM cell state represents the parameters of the learner and the variation of the cell state corresponds to model update (like gradient descent) of the learner. Both initialization and update strategy are learned jointly <ref type="bibr" target="#b17">[18]</ref>. However, using LSTM as meta-learner to learn a learner such as CNN incurs prohibitively high complexity. In practice, each parameter of the learner is updated independently in each step, which may significantly limit its potential. <ref type="bibr" target="#b18">[19]</ref> adapt a memory-augmented LSTM <ref type="bibr" target="#b7">[8]</ref> for few-shot learning, where the learning strategy is figured out as the LSTM rolls out. <ref type="bibr" target="#b6">[7]</ref> use SGD as meta-learner, but only the initialization is learned. Despite its simplicity, it works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Meta-SGD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Meta-Learner</head><p>In this section, we propose a new meta-learner that applies to both supervised learning (i.e., classification and regression) and reinforcement learning. For simplicity, we use supervised learning as running case and discuss reinforcement learning later. How can a meta-learner M φ initialize and adapt a learner f θ for a new task from a few examples T = {(x i , y i )}? One standard way updates the learner iteratively from random initialization using gradient descent:</p><formula xml:id="formula_0">θ t = θ t−1 −α∇L T (θ t−1 ), (1) where L T (θ) is the empirical loss L T (θ) = 1 |T | (x,y)∈T (f θ (x), y)</formula><p>with some loss function , ∇L T (θ) is the gradient of L T (θ), and α denotes the learning rate that is often set manually.</p><p>With only a few examples, it is non-trivial to decide how to initialize and when to stop the learning process to avoid overfitting. Besides, while gradient is an effective direction for data fitting, it may lead to overfitting under the few-shot regime. This also makes it tricky to choose the learning rate. While many ideas may be applied for regularization, it remains challenging to balance between the induced prior and the few-shot fitting. What in need is a principled approach that determines all learning factors in a way that maximizes generalization power rather than data fitting. Another important aspect regards the speed of learning: can we learn within a couple of iterations? Besides an interesting topic on its own <ref type="bibr" target="#b13">[14]</ref>, this will enable many emerging applications such as self-driving cars and autonomous robots that require to learn and react in a fast changing environment.</p><p>The idea of learning to learn appears to be promising for few-shot learning. Instead of hand-designing a learning algorithm for the task of interest, it learns from many related tasks how to learn, which may include how to initialize and update a learner, among others, by training a meta-learner to do the learning. The key here is in developing a high-capacity yet trainable meta-learner. While other meta-learners are possible, here we consider meta-learners in the form of optimizers, given their broad generality and huge success in machine learning. Specifically, we aim to learn an optimizer for few-shot learning.</p><formula xml:id="formula_1">Meta-SGD train(T i ) test(T i ) {L test(Ti) (✓ 0 i )} (✓, ↵) ✓ ↵ update (✓, ↵) ✓ 0 i batch 1 Meta-SGD train(T i ) test(T i ) {L test(Ti) (✓ 0 i )} (✓, ↵) ✓ ↵ ✓ 0 i Meta-SGD train(T i ) test(T i ) {L test(Ti) (✓ 0 i )} (✓, ↵) ✓ ↵ ✓ 0 i update (✓, ↵)</formula><p>There are three key ingredients in defining an optimizer: initialization, update direction, and learning rate. The initialization is often set randomly, the update direction often follows gradient or some variant (e.g., conjugate gradient), and the learning rate is usually set to be small, or decayed over iterations. While such rules of thumb work well with a huge amount of labeled data, they are unlikely reliable for few-shot learning. In this paper, we present a meta-learning approach that automatically determines all the ingredients of an optimizer in an end-to-end manner.</p><p>Mathematically, we propose the following meta-learner composed of an initialization term and an adaptation term:</p><formula xml:id="formula_2">θ = θ − α • ∇L T (θ),<label>(2)</label></formula><p>where θ and α are (meta-)parameters of the meta-learner to be learned, and • denotes element-wise product. Specifically, θ represents the state of a learner that can be used to initialize the learner for any new task, and α is a vector of the same size as θ that decides both the update direction and learning rate. The adaptation term α • ∇L T (θ) is a vector whose direction represents the update direction and whose length represents the learning rate. Since the direction of α • ∇L T (θ) is usually different from that of the gradient ∇L T (θ), it implies that the meta-learner does not follow the gradient direction to update the learner, as does by SGD. Interestingly, given α, the adaptation is indeed fully determined by the gradient, like SGD.</p><p>In summary, given a few examples T = {(x i , y i )} for a few-shot learning problem, our metalearner first initializes the learner with θ and then adapts it to θ in just one step, in a new direction α • ∇L T (θ) different from the gradient ∇L T (θ) and using a learning rate implicitly implemented in α • ∇L T (θ). As our meta-learner also relies on the gradient as in SGD but it is learned via meta-learning rather than being hand-designed like SGD, we call it Meta-SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Meta-training</head><p>We aim to train the meta-learner to perform well on many related tasks. For this purpose, assume there is a distribution p(T ) over the related task space, from which we can randomly sample tasks. A task T consists of a training set train(T ) and a testing set test(T ). Our objective is to maximize the expected generalization power of the meta-learner on the task space. Specifically, given a task T sampled from p(T ), the meta-learner learns the learner based on the training set train(T ), but the generalization loss is measured on the testing set test(T ). Our goal is to train the meta-learner to minimize the expected generalization loss.</p><p>Mathematically, the learning of our meta-learner is formulated as the optimization problem as follows:</p><formula xml:id="formula_3">min θ,α E T ∼p(T ) [L test(T ) (θ )] = E T ∼p(T ) [L test(T ) (θ − α • ∇L train(T ) (θ))].<label>(3)</label></formula><p>The above objective is differentiable w.r.t. both θ and α, which allows to use SGD to solve it efficiently, as shown in Algorithm 1 and illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Sample batch of tasks T i ∼ p(T ); <ref type="bibr">4:</ref> for all T i do 5:</p><formula xml:id="formula_4">L train(Ti) (θ) ← 1 |train(Ti)| (x,y)∈train(Ti) (f θ (x), y); 6: θ i ← θ − α •∇L train(Ti) (θ); 7: L test(Ti) (θ i ) ← 1 |test(Ti)| (x,y)∈test(Ti) (f θ i (x), y); 8: end 9: (θ, α) ← (θ, α) − β∇ (θ,α) Ti L test(Ti) (θ i ); 10: end</formula><p>Reinforcement Learning. In reinforcement learning, we regard a task as a Markov decision process (MDP). Hence, a task T contains a tuple (S, A, q, q 0 , T, r, γ), where S is a set of states, A is a set of actions, q : S × A × S → [0, 1] is the transition probability distribution, q 0 : S → [0, 1] is the initial state distribution, T ∈ N is the horizon, r : S × A → R is the reward function, and γ ∈ [0, 1] is the discount factor. The learner f θ : S × A → [0, 1] is a stochastic policy, and the loss L T (θ) is the negative expected discounted reward</p><formula xml:id="formula_5">L T (θ) = −E st,at∼f θ ,q,q0 T t=0 γ t r(s t , a t ) .<label>(4)</label></formula><p>As in supervised learning, we train the meta-learner to minimize the expected generalization loss. Specifically, given a task T sampled from p(T ), we first sample N 1 trajectories according to the policy f θ . Next, we use policy gradient methods to compute the empirical policy gradient ∇L T (θ) and then apply equation 2 to get the updated policy f θ . After that, we sample N 2 trajectories according to f θ and compute the generalization loss.</p><p>The optimization problem for reinforcement learning can be rewritten as follows:</p><formula xml:id="formula_6">min θ,α E T ∼p(T ) [L T (θ )] = E T ∼p(T ) [L T (θ − α • ∇L T (θ))],<label>(5)</label></formula><p>and the algorithm is summarized in Algorithm 2. for all T i do 5:</p><p>Sample N 1 trajectories according to f θ ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Compute policy gradient ∇L Ti (θ); 7:</p><formula xml:id="formula_7">θ i ← θ − α • ∇L Ti (θ); 8:</formula><p>Sample N 2 trajectories according to f θ i ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Compute policy gradient ∇ (θ,α) L Ti (θ i ); (θ, α) ← (θ, α) − β∇ (θ,α) Ti L Ti (θ i ); 12: end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Related Meta-Learners</head><p>Let us compare Meta-SGD with other meta-learners in the form of optimizer. MAML <ref type="bibr" target="#b6">[7]</ref> uses the original SGD as meta-learner, but the initialization is learned via meta-learning. In contrast, Meta-SGD also learns the update direction and the learning rate, and may have a higher capacity. Meta-LSTM <ref type="bibr" target="#b17">[18]</ref> relies on LSTM to learn all initialization, update direction, and learning rate, like Meta-SGD, but it incurs a much higher complexity than Meta-SGD. In practice, it learns each parameter of the learner independently at each step, which may limit its potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate the proposed meta-learner Meta-SGD on a variety of few-shot learning problems on regression, classification, and reinforcement learning. We also compare its performance with the stateof-the-art results reported in previous work. Our results show that Meta-SGD can learn very quickly from a few examples with only one-step adaptation. All experiments are run on Tensorflow <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Regression</head><p>In this experiment, we evaluate Meta-SGD on the problem of K-shot regression, and compare it with the state-of-the-art meta-learner MAML <ref type="bibr" target="#b6">[7]</ref>. The target function is a sine curve y(x) = A sin(ωx+b), where the amplitude A, frequency ω, and phase b follow the uniform distribution on intervals . The prediction loss is measured by the mean squared error (MSE). For the regressor, we follow <ref type="bibr" target="#b6">[7]</ref> to use a small neural network with an input layer of size 1, followed by 2 hidden layers of size 40 with ReLU nonlinearities, and then an output layer of size 1. All weight matrices use truncated normal initialization with mean 0 and standard deviation 0.01, and all bias vectors are initialized by 0. For Meta-SGD, all entries in α have the same initial value randomly chosen from [0.005, 0.1]. For MAML, a fixed learning rate α = 0.01 is used following <ref type="bibr" target="#b6">[7]</ref>. Both meta-learners use one-step adaptation and are trained for 60000 iterations with meta batch-size of 4 tasks.</p><p>For performance evaluation (meta-testing), we randomly sample 100 sine curves. For each curve, we sample K examples for training with inputs randomly chosen from [−5.0, 5.0], and another 100 examples for testing with inputs evenly distributed on [−5.0, 5.0]. We repeat this procedure 100 times and take the average of MSE. The results averaged over the sampled 100 sine curves with 95% confidence intervals are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>By <ref type="table" target="#tab_0">Table 1</ref>, Meta-SGD performs consistently better than MAML on all cases with a wide margin, showing that Meta-SGD does have a higher capacity than MAML by learning all the initialization, update direction, and learning rate simultaneously, rather than just the initialization as in MAML. By learning all ingredients of an optimizer across many related tasks, Meta-SGD well captures the problem structure and is able to learn a learner with very few examples. In contrast, MAML regards the learning rate α as a hyper-parameter and just follows the gradient of empirical loss to learn the learner, which may greatly limit its capacity. Indeed, if we change the learning rate α from 0.01 to 0.1, and re-train MAML via 5-shot meta-training, the prediction losses for 5-shot, 10-shot, and 20-shot meta-testing increase to 1.77 ± 0.30, 1.37 ± 0.23, and 1.15 ± 0.20, respectively.  <ref type="table" target="#tab_0">(Table 1</ref>). This shows that our learned optimization strategy is better than gradient descent even when applied to solve the tasks with large training data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification</head><p>We evaluate Meta-SGD on few-shot classification using two benchmark datasets Omniglot and MiniImagenet.</p><p>Omniglot. The Omniglot dataset <ref type="bibr" target="#b12">[13]</ref> consists of 1623 characters from 50 alphabets. Each character contains 20 instances drawn by different individuals. We randomly select 1200 characters for meta-training, and use the remaining characters for meta-testing. We consider 5-way and 20-way classification for both 1 shot and 5 shots.</p><p>MiniImagenet. The MiniImagenet dataset consists of 60000 color images from 100 classes, each with 600 images. The data is divided into three disjoint subsets: 64 classes for meta-training, 16 classes for meta-validation, and 20 classes for meta-testing <ref type="bibr" target="#b17">[18]</ref>. We consider 5-way and 20-way classification for both 1 shot and 5 shots.</p><p>We train the model following <ref type="bibr" target="#b24">[25]</ref>. For an N -way K-shot classification task, we first sample N classes from the meta-training dataset, and then in each class sample K images for training and 15 other images for testing. We update the meta-learner once for each batch of tasks. After metatraining, we test our model with unseen classes from the meta-testing dataset. Following <ref type="bibr" target="#b6">[7]</ref>, we use a convolution architecture with 4 modules, where each module consists of 3 × 3 convolutions, followed by batch normalization <ref type="bibr" target="#b10">[11]</ref>, a ReLU nonlinearity, and 2 × 2 max-pooling. For Omniglot, the images are downsampled to 28 × 28, and we use 64 filters and add an additional fully-connected layer with dimensionality 32 after the convolution modules. For MiniImagenet, the images are downsampled to 84 × 84, and we use 32 filters in the convolution modules.</p><p>We train and evaluate Meta-SGD that adapts the learner in one step. In each iteration of meta-training, Meta-SGD is updated once with one batch of tasks. We follow <ref type="bibr" target="#b6">[7]</ref> for batch size settings. For Omniglot, the batch size is set to 32 and 16 for 5-way and 20-way classification, respectively. For MiniImagenet, the batch size is set to 4 and 2 for 1-shot and 5-shot classification, respectively. We add a regularization term to the objective function.</p><p>The results of Meta-SGD are summarized in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref>, together with results of other state-of-the-art models, including Siamese Nets <ref type="bibr" target="#b11">[12]</ref>, Matching Nets <ref type="bibr" target="#b24">[25]</ref>, Meta-LSTM <ref type="bibr" target="#b17">[18]</ref>, and MAML <ref type="bibr" target="#b6">[7]</ref>. The results of previous models for 5-way and 20-way classification on Omniglot,  and 5-way classification on MiniImagenet are reported in previous work <ref type="bibr" target="#b6">[7]</ref>, while those for 20way classification on MiniImagenet are obtained in our experiment. For the 20-way results on MiniImagenet, we run Matching Nets and Meta-LSTM using the implementation by <ref type="bibr" target="#b17">[18]</ref>, and MAML using our own implementation 1 . For MAML, the learning rate α is set to 0.01 as in the 5-way case, and the learner is updated with one gradient step for both meta-training and meta-testing tasks like Meta-SGD. All models are trained for 60000 iterations. The results represent mean accuracies with 95% confidence intervals over tasks.</p><p>For Omniglot, our model Meta-SGD is slightly better than the state-of-the-art models on all classification tasks. In our experiments we noted that for 5-shot classification tasks, the model performs better when it is trained with 1-shot tasks during meta-training than trained with 5-shot tasks. This phenomenon was observed in both 5-way and 20-way classification. The 5-shot (meta-testing) results of Meta-SGD in <ref type="table" target="#tab_3">Table 2</ref> are obtained via 1-shot meta-training.</p><p>For MiniImagenet, Meta-SGD outperforms all other models in all cases. Note that Meta-SGD learns the learner in just one step, making it faster to train the model and to adapt to new tasks, while still improving accuracies. In comparison, previous models often update the learner using SGD with multiple gradient steps or using LSTM with multiple iterations. For 20-way classification, the results of Matching Nets shown in <ref type="table" target="#tab_4">Table 3</ref> are obtained when the model is trained with 10-way classification tasks. When trained with 20-way classification tasks, its accuracies drop to 12.27 ± 0.18 and 21.30 ± 0.21 for 1-shot and 5-shot, respectively, suggesting that Matching Nets may need more iterations for sufficient training, especially for 1-shot. We also note that for 20-way classification, MAML with the learner updated in one gradient step performs worse than Matching Nets and Meta-LSTM. In comparison, Meta-SGD has the highest accuracies for both 1-shot and 5-shot. We also run experiments on MAML for 5-way classification where the learner is updated with 1 gradient step for both meta-training and meta-testing, the mean accuracies of which are 44.40% and 61.11% for 1-shot and 5-shot classification, respectively. These results show the capacity of Meta-SGD in terms of learning speed and performance for few-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reinforcement Learning</head><p>In this experiment, we evaluate Meta-SGD on 2D navigation tasks, and compare it with MAML <ref type="bibr" target="#b6">[7]</ref>. The purpose of this reinforcement learning experiment is to enable a point agent in 2D to quickly acquire a policy for the task where the agent should move from a start position to a goal position. We experiment with two sets of tasks separately. In the first set of tasks, proposed by MAML, we fix the start position, which is the origin (0, 0), and randomly choose a goal position from the unit square  <ref type="figure">Figure 4</ref>: Left: Meta-SGD vs MAML on a 2D navigation task with fixed start position and randomly sampled goal position. Right: Meta-SGD vs MAML on a 2D navigation task with randomly sampled start and goal positions.</p><p>Given a task, the state is the position of the agent in the 2D plane and the action is the velocity of the agent in the next step (unit time). The new state after the agent's taking the action is the sum of the previous state and the action. The action is sampled from a Gaussian distribution produced by a policy network, which takes the current state as input and outputs the mean and log variance of the Gaussian distribution. For the policy network, we follow <ref type="bibr" target="#b6">[7]</ref>. The mean vector is created from the state via a small neural network consisting of an input layer of size 2, followed by 2 hidden layers of size 100 with ReLU nonlinearities, and then an output layer of size 2. The log variance is a diagonal matrix with two trainable parameters. For the agent to move to the goal position, we define the reward as the negative distance between the state and the goal.</p><p>For meta-training, we sample 20 tasks as a mini-batch in each iteration. We first sample 20 trajectories per task according to the policy network and each trajectory terminates when the agent is within 0.01 of the goal or at the step of 100. Next, we use vanilla policy gradient <ref type="bibr" target="#b26">[27]</ref> to compute the empirical policy gradient ∇L(θ) and apply θ = θ − α • ∇L(θ) to update the policy network. After that, we sample 20 trajectories according to the updated policy network. Finally, we use Trust Region Policy Optimization <ref type="bibr" target="#b20">[21]</ref> to update θ and α for all 20 tasks. For additional optimization tricks, we follow <ref type="bibr" target="#b6">[7]</ref>. We take 100 iterations in total.</p><p>For meta-testing, we randomly sample 600 tasks. For each task, we sample 20 trajectories according to the policy network initialized by the meta-learner, and then update the policy network by the vanilla policy gradient and the meta-learner. To evaluate the performance of the updated policy network on this task, 20 new trajectories are sampled and we calculate the return, the sum of the rewards, for each trajectory and take average over these returns as the average return for this task. The results averaged over the sampled 600 tasks with 95% confidence intervals are summarized in <ref type="table" target="#tab_5">Table 4</ref>, which show that Meta-SGD has relatively higher returns than MAML on both sets of tasks.  <ref type="figure">Figure 4</ref> shows some qualitative results of Meta-SGD and MAML. For the set of tasks with fixed start position, the initialized policies with Meta-SGD and MAML perform quite similar -the agents walk around near the start position. After one step update of the policies, both of the agents move to the goal position, and the agent guided by Meta-SGD has a stronger perception of the target. For the set of tasks with different start positions, the initialized policies with Meta-SGD and MAML both lead the agents to the origin. The updated policies confidently take the agents to the goal position, and still, the policy updated by Meta-SGD performs better. All these results show that our optimization strategy is better than gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have developed a new, easily trainable, SGD-like meta-learner Meta-SGD that can learn faster and more accurately than existing meta-learners for few-shot learning. We learn all ingredients of an optimizer, namely initialization, update direction, and learning rate, via meta-learning in an endto-end manner, resulting in a meta-learner with a higher capacity compared to other optimizer-like meta-learners. Remarkably, in just one step adaptation, Meta-SGD leads to new state-of-the-art results on few-shot regression, classification, and reinforcement learning.</p><p>One important future work is large-scale meta-learning. As training a meta-learner involves training a large number of learners, this entails a far more computational demand than traditional learning approaches, especially if a big learner is necessary when the data for each task increases far beyond "few shots". Another important problem regards the versatility or generalization capacity of metalearner, including dealing with unseen situations such as new problem setups or new task domains, or even multi-tasking meta-learners. We believe these problems are important to greatly expand the practical value of meta-learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Meta-training process of Meta-SGD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[0.1, 5.0], [0.8, 1.2], and [0, π], respectively. The input range is restricted to the interval [−5.0, 5.0]. The K-shot regression task is to estimate the underlying sine curve from only K examples. For meta-training, each task consists of K ∈ {5, 10, 20} training examples and 10 testing examples with inputs randomly chosen from [−5.0, 5.0]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>shows how the meta-learners perform on a random 5-shot regression task. FromFigure 3 (left), compared to MAML, Meta-SGD can adapt more quickly to the shape of the sine curve after just one step update with only 5 examples, even when these examples are all in one half of the input range. This shows that Meta-SGD well captures the meta-level information across all tasks. Moreover, it continues to improve with additional training examples during meta-tesing, as shown in Figure 3 (right). While the performance of MAML also gets better with more training examples, the regression results of Meta-SGD are still better than those of MAML</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Left: Meta-SGD vs MAML on 5-shot regression. Both initialization (dotted) and result after one-step adaptation (solid) are shown. Right: Meta-SGD (10-shot meta-training) performs better with more training examples in meta-testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>[−0.5, 0.5] × [−0.5, 0.5] for each task. In the second set of tasks, both of the start and goal positions are randomly chosen from the unit square [−0.5, 0.5] × [−0.5, 0.5].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Meta-SGD for Supervised Learning Input: task distribution p(T ), learning rate β Output: θ, α 1: Initialize θ, α; 2: while not done do</figDesc><table /><note>3:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Meta-SGD vs MAML on few-shot regression</figDesc><table><row><cell cols="2">Meta-training</cell><cell></cell><cell>Models</cell><cell cols="7">5-shot testing 10-shot testing 20-shot testing</cell></row><row><cell cols="2">5-shot training</cell><cell></cell><cell cols="3">MAML Meta-SGD 0.90 ± 0.16 1.13 ± 0.18</cell><cell cols="2">0.85 ± 0.14 0.63 ± 0.12</cell><cell></cell><cell cols="2">0.71 ± 0.12 0.50 ± 0.10</cell></row><row><cell cols="2">10-shot training</cell><cell></cell><cell cols="3">MAML Meta-SGD 0.88 ± 0.14 1.17 ± 0.16</cell><cell cols="2">0.77 ± 0.11 0.53 ± 0.09</cell><cell></cell><cell cols="2">0.56 ± 0.08 0.35 ± 0.06</cell></row><row><cell cols="2">20-shot training</cell><cell></cell><cell cols="3">MAML Meta-SGD 1.01 ± 0.17 1.29 ± 0.20</cell><cell cols="2">0.76 ± 0.12 0.54 ± 0.08</cell><cell></cell><cell cols="2">0.48 ± 0.08 0.31 ± 0.05</cell></row><row><cell>4 6</cell><cell></cell><cell></cell><cell></cell><cell>Ground Truth MAML Meta-SGD</cell><cell>4 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ground Truth 10-shot 20-shot 40-shot</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>2</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell></cell><cell>4</cell><cell>2</cell><cell>0</cell><cell>2</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Classification accuracies on Omniglot</cell><cell></cell></row><row><cell></cell><cell cols="2">5-way Accuracy</cell><cell cols="2">20-way Accuracy</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Siamese Nets</cell><cell>97.3%</cell><cell>98.4%</cell><cell>88.2%</cell><cell>97.0%</cell></row><row><cell>Matching Nets</cell><cell>98.1%</cell><cell>98.9%</cell><cell>93.8%</cell><cell>98.5%</cell></row><row><cell>MAML</cell><cell>98.7 ± 0.4%</cell><cell>99.9 ± 0.1%</cell><cell>95.8 ± 0.3%</cell><cell>98.9 ± 0.2%</cell></row><row><cell>Meta-SGD</cell><cell cols="4">99.53 ± 0.26% 99.93 ± 0.09% 95.93 ± 0.38% 98.97 ± 0.19%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Classification accuracies on MiniImagenet</cell><cell></cell></row><row><cell></cell><cell cols="2">5-way Accuracy</cell><cell cols="2">20-way Accuracy</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Matching Nets</cell><cell>43.56 ± 0.84%</cell><cell>55.31 ± 0.73%</cell><cell>17.31 ± 0.22%</cell><cell>22.69 ± 0.20%</cell></row><row><cell>Meta-LSTM</cell><cell>43.44 ± 0.77%</cell><cell>60.60 ± 0.71%</cell><cell>16.70 ± 0.23%</cell><cell>26.06 ± 0.25%</cell></row><row><cell>MAML</cell><cell>48.70 ± 1.84%</cell><cell>63.11 ± 0.92%</cell><cell>16.49 ± 0.58%</cell><cell>19.29 ± 0.29%</cell></row><row><cell>Meta-SGD</cell><cell cols="4">50.47 ± 1.87% 64.03 ± 0.94% 17.56 ± 0.64% 28.92 ± 0.35%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Meta-SGD vs MAML on 2D navigation fixed start position varying start position MAML −9.12 ± 0.66 −10.71 ± 0.76 Meta-SGD −8.64 ± 0.68 −10.15 ± 0.62</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code provided by<ref type="bibr" target="#b6">[7]</ref> does not scale for this 5-shot 20-way problem in one GPU with 12G memory used in our experiment.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cloutier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fixed-weight networks can learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="1990-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">RL 2 : Fast reinforcement learning via slow reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02779</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter R Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<editor>CogSci</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tomer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="101" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning to optimize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<title level="m">Meta-learning with temporal convolutions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning. On learning how to learn: The meta-meta-... hook.) Diploma thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Institut f. Informatik, Tech. Univ. Munich</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to learn: Meta-critic networks for sample efficient learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09529</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeb</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruva</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05763</idno>
		<title level="m">Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning with partially absorbing random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fixed-weight on-line learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">E</forename><surname>Peter R Conwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
