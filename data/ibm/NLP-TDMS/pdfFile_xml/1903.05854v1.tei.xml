<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MirrorGAN: Learning Text-to-image Generation by Redescription</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
							<email>qiaott@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>jing.zhang@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The institute of System Science and Control Engineering</orgName>
								<orgName type="institution" key="instit2">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MirrorGAN: Learning Text-to-image Generation by Redescription</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating an image from a given text description has two goals: visual realism and semantic consistency. Although significant progress has been made in generating high-quality and visually realistic images using generative adversarial networks, guaranteeing semantic consistency between the text description and visual content remains very challenging. In this paper, we address this problem by proposing a novel global-local attentive and semantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN exploits the idea of learning textto-image generation by redescription and consists of three modules: a semantic text embedding module (STEM), a global-local collaborative attentive module for cascaded image generation (GLAM), and a semantic text regeneration and alignment module (STREAM). STEM generates word-and sentence-level embeddings. GLAM has a cascaded architecture for generating target images from coarse to fine scales, leveraging both local word attention and global sentence attention to progressively enhance the diversity and semantic consistency of the generated images. STREAM seeks to regenerate the text description from the generated image, which semantically aligns with the given text description. Thorough experiments on two public benchmark datasets demonstrate the superiority of Mirror-GAN over other representative state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text-to-image (T2I) generation refers to generating a visually realistic image that matches a given text descrip- <ref type="bibr" target="#b0">1</ref>  <ref type="figure">Figure 1</ref>: (a) Illustration of the mirror structure that embodies the idea of learning text-to-image generation by redescription. (b)-(c) Semantically inconsistent and consistent images/redescriptions generated by <ref type="bibr" target="#b33">[34]</ref> and the proposed MirrorGAN, respectively.</p><p>tion. Due to its significant potential in a number of applications but its challenging nature, T2I generation has become an active research area in both natural language processing and computer vision communities. Although significant progress has been made in generating visually realistic images using generative adversarial networks (GANs) such as in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13]</ref>, guaranteeing semantic alignment of the generated image with the input text remains challenging. In contrast to fundamental image generation problems, T2I generation is conditioned on text descriptions rather than starting with noise alone. Leveraging the power of GANs <ref type="bibr" target="#b9">[10]</ref>, different T2I methods have been proposed to generate visually realistic and text-relevant images. For instance, <ref type="bibr">Reed et al. proposed</ref> to tackle text to image synthesis problem by finding a visually discriminative representation for the text descriptions and using this representation to generate realistic images <ref type="bibr" target="#b23">[24]</ref>. <ref type="bibr">Zhang et al. proposed</ref> Stack-GAN to generate images in two separate stages <ref type="bibr" target="#b37">[38]</ref>. Hong et al. proposed extracting a semantic layout from the input text and then converting it into the image generator to guide the generative process <ref type="bibr" target="#b12">[13]</ref>. <ref type="bibr">Zhang et al.</ref> proposed training a T2I generator with hierarchically nested adversarial objectives <ref type="bibr" target="#b40">[41]</ref>. These methods all utilize a discriminator to distinguish between the generated image and corresponding text pair and the ground truth image and corresponding text pair. However, due to the domain gap between text and images, it is difficult and inefficient to model the underlying semantic consistency within each pair when relying on such a discriminator alone. Recently, the attention mechanism <ref type="bibr" target="#b33">[34]</ref> has been exploited to address this problem, which guides the generator to focus on different words when generating different image regions. However, using word-level attention alone does not ensure global semantic consistency due to the diversity between text and image modalities. <ref type="figure">Figure 1 (b)</ref> shows an example generated by <ref type="bibr" target="#b33">[34]</ref>.</p><p>T2I generation can be regarded as the inverse problem of image captioning (or image-to-text generation, I2T) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b15">16]</ref>, which generates a text description given an image. Considering that tackling each task requires modeling and aligning the underlying semantics in both domains, it is natural and reasonable to model both tasks in a unified framework to leverage the underlying dual regulations. As shown in <ref type="figure">Figure 1</ref> (a) and (c), if an image generated by T2I is semantically consistent with the given text description, its redescription by I2T should have exactly the same semantics with the given text description. In other words, the generated image should act like a mirror that precisely reflects the underlying text semantics. Motivated by this observation, we propose a novel text-to-image-to-text framework called MirrorGAN to improve T2I generation, which exploits the idea of learning T2I generation by redescription.</p><p>MirrorGAN has three modules: STEM, GLAM and STREAM. STEM generates word-and sentence-level embeddings, which are then used by the GLAM. GLAM is a cascaded architecture that generates target images from coarse to fine scales, leveraging both local word attention and global sentence attention to progressively enhance the diversity and semantic consistency of the generated images. STREAM tries to regenerate the text description from the generated image, which semantically aligns with the given text description.</p><p>To train the model end-to-end, we use two adversarial losses: visual realism adversarial loss and text-image paired semantic consistency adversarial loss. In addition, to leverage the dual regulation of T2I and I2T, we further employ a text-semantics reconstruction loss based on crossentropy (CE). Thorough experiments on two public benchmark datasets demonstrate the superiority of MirrorGAN over other representative state-of-the-art methods with respect to both visual realism and semantic consistency.</p><p>The contributions of this work can be summarized as follows:</p><p>• We propose a novel unified framework called Mirror-GAN for modeling T2I and I2T together, specifically targeting T2I generation by embodying the idea of learning T2I generation by redescription.</p><p>• We propose a global-local collaborative attention model that is seamlessly embedded in the cascaded generators to preserve cross-domain semantic consistency and to smoothen the generative process.</p><p>• Except commonly used GAN losses, we additionally propose a CE-based text-semantics reconstruction loss to supervise the generator to generate visually realistic and semantically consistent images. Consequently, we achieve new state-of-the-art performance on two benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Similar ideas to our own have recently been used in CycleGAN and DualGAN, which handle the bi-directional translations within two domains together <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b0">1]</ref>, significantly advance image-to-image translation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">23]</ref>. Our MirrorGAN is partly inspired by CycleGAN but has two main differences: 1) we specifically tackle the T2I problem rather than image-to-image translation. The cross-media domain gap between text and images is probably much larger than the one between images with different attributes, e.g., styles. Moreover, the diverse semantics present in each domain make it much more challenging to maintain cross-domain semantic consistency. 2) Mir-rorGAN embodies a mirror structure rather than the cycle structure used in CycleGAN. MirrorGAN conducts supervised learning by using paired text-image data rather than training from unpaired image-image data. Moreover, to embody the idea of learning T2I generation by redescription, we use a CE-based reconstruction loss to regularize the semantic consistency of the redescribed text, which is different from the L1 cycle consistency loss in CycleGAN, which addresses visual similarities.</p><p>Attention models have been extensively exploited in computer vision and natural language processing, for instance in object detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40]</ref>, image/video captioning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref>, visual question answering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22]</ref>, and neural machine translation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b7">8]</ref>. Attention can be modeled spatially in images or temporally in language, or even both in video-or image-text-related tasks. Different attention models have been proposed for image captioning to enhance the embedded text feature representations during both encoding and decoding. Recently, Xu et al. proposed an attention model to guide the generator to focus on different words when generating different image subregions <ref type="bibr" target="#b33">[34]</ref>. However, using only word-level attention does not ensure global semantic consistency due to the diverse nature of both the text and image modalities, e.g., each image has 10 captions in CUB and 5 captions in COCO,however, they express the same underlying semantic  information. In particular, for multi-stage generators, it is crucial to make "semantically smooth" generations. Therefore, global sentence-level attention should also be considered in each stage such that it progressively and smoothly drives the generators towards semantically well-aligned targets. To this end, we propose a global-local collaborative attentive module to leverage both local word attention and global sentence attention and to enhance the diversity and semantic consistency of the generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MirrorGAN for text-to-image generation</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, MirrorGAN embodies a mirror structure by integrating both T2I and I2T. It exploits the idea of learning T2I generation by redescription. After an image is generated, MirrorGAN regenerates its description, which aligns its underlying semantics with the given text description. Technically, MirrorGAN consists of three modules: STEM, GLAM and STREAM. Details of the model will be introduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">STEM: Semantic Text Embedding Module</head><p>First, we introduce the semantic text embedding module to embed the given text description into local word-level features and global sentence-level features. As shown in the leftmost part of <ref type="figure" target="#fig_1">Figure 2</ref>, a recurrent neural network (RNN) <ref type="bibr" target="#b3">[4]</ref> is used to extract semantic embeddings from the given text description T , which include a word embedding w and a sentence embedding s.</p><formula xml:id="formula_0">w, s = RN N (T ) ,<label>(1)</label></formula><p>where T = {T l |l = 0, . . . , L − 1 }, L represents the sentence length, w = w l |l = 0, . . . , L − 1 ∈ R D×L is the concatenation of hidden state w l of each word, s ∈ R D is the last hidden state, and D is the dimension of w l and s. Due to the diversity of the text domain, text with few permutations may share similar semantics. Therefore, we follow the common practice of using the conditioning augmentation method <ref type="bibr" target="#b37">[38]</ref> to augment the text descriptions. This produces more image-text pairs and thus encourages robustness to small perturbations along the conditioning text manifold. Specifically, we use F ca to represent the conditioning augmentation function and obtain the augmented sentence vector:</p><formula xml:id="formula_1">s ca = F ca (s) ,<label>(2)</label></formula><p>where s ca ∈ R D , D is the dimension after augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GLAM: Global-Local collaborative Attentive Module in Cascaded Image Generators</head><p>We next construct a multi-stage cascaded generator by stacking three image generation networks sequentially. We adopt the basic structure described in <ref type="bibr" target="#b33">[34]</ref> due to its good performance in generating realistic images. Mathematically, we use {F 0 , F 1 , ..., F m−1 } to denote the m visual feature transformers and {G 0 , G 1 , ..., G m−1 } to denote the m image generators. The visual feature f i and generated image I i in each stage can be expressed as:</p><formula xml:id="formula_2">f 0 = F 0 (z, s ca ) , f i = F i (f i−1 , F atti (f i−1 , w, s ca )) , i ∈ {1, 2, . . . , m − 1} , I i = G i (f i ) , i ∈ {0, 1, 2, . . . , m − 1} ,<label>(3)</label></formula><p>where f i ∈ R Mi×Ni and I i ∈ R qi×qi , z ∼ N (0, 1) denotes random noises. F atti is the proposed global-local collaborative attention model which includes two components Att w i−1 and Att s i−1 , i.e., F atti (f i−1 , w, s ca ) = concat Att w i−1 , Att s i−1 . First, we use the word-level attention model proposed in <ref type="bibr" target="#b33">[34]</ref> to generate an attentive word-context feature. It takes the word embedding w and the visual feature f as the input in each stage. The word embedding w is first converted into an underlying common semantic space of visual features by a perception layer U i−1 as U i−1 w. Then, it is multiplied with the visual feature f i−1 to obtain the attention score. Finally, the attentive word-context feature is obtained by calculating the inner product between the attention score and U i−1 w:</p><formula xml:id="formula_3">Att w i−1 = L−1 l=0 U i−1 w l sof tmax f T i−1 U i−1 w l T , (4) where U i−1 ∈ R Mi−1×D and Att w i−1 ∈ R Mi−1×Ni−1 .</formula><p>The attentive word-context feature Att w i−1 has the exact same dimension as f i−1 , which is further used for generating the i th visual features f i by concatenation with f i−1 .</p><p>Then, we propose a sentence-level attention model to enforce a global constraint on the generators during generation. By analogy to the word-level attention model, the augmented sentence vector s ca is first converted into an underlying common semantic space of visual features by a perception layer V i−1 as V i−1 s ca . Then, it is element-wise multiplied with the visual feature f i−1 to obtain the attention score. Finally, the attentive sentence-context feature is obtained by calculating the element-wise multiplication of the attention score and V i−1 s ca :</p><formula xml:id="formula_4">Att s i−1 = (V i−1 s ca ) • (sof tmax (f i−1 • (V i−1 s ca ))) , (5) where • denotes the element-wise multiplication, V i ∈ R Mi×D and Att s i−1 ∈ R Mi−1×Ni−1 .</formula><p>The attentive sentence-context feature Att s i−1 is further concatenated with f i−1 and Att w i−1 for generating the i th visual features f i as depicted in the second equality in Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">STREAM: Semantic Text REgeneration and Alignment Module</head><p>As described above, MirrorGAN includes a semantic text regeneration and alignment module (STREAM) to regenerate the text description from the generated image, which semantically aligns with the given text description. Specifically, we employ a widely used encoderdecoder-based image caption framework <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref> as the basic STREAM architecture. Note that a more advanced image captioning model can also be used, which is likely to produce better results. However, in a first attempt to validate the proposed idea, we simply exploit the baseline in the current work.</p><p>The image encoder is a convolutional neural network (CNN) <ref type="bibr" target="#b10">[11]</ref> pretrained on ImageNet <ref type="bibr" target="#b4">[5]</ref>, and the decoder is a RNN <ref type="bibr" target="#b11">[12]</ref>. The image I m−1 generated by the final stage generator is fed into the CNN encoder and RNN decoder as follows:</p><p>x −1 = CN N (I m−1 ),</p><formula xml:id="formula_5">x t = W e T t , t ∈ {0, ...L − 1}, p t+1 = RN N (x t ), t ∈ {0, ...L − 1},<label>(6)</label></formula><p>where x −1 ∈ R Mm−1 is a visual feature used as the input at the beginning to inform the RNN about the image content. W e ∈ R Mm−1×D represents a word embedding matrix, which maps word features to the visual feature space. p t+1 is a predicted probability distribution over the words. We pre-trained STREAM as it helped MirrorGAN achieve a more stable training process and converge faster, while jointly optimizing STREAM with MirrorGAN is instable and very expensive in terms of time and space. The encoderdecoder structure in <ref type="bibr" target="#b28">[29]</ref> and then their parameters keep fixed when training the other modules of MirrorGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective functions</head><p>Following common practice, we first employ two adversarial losses: a visual realism adversarial loss and a textimage paired semantic consistency adversarial loss, which are defined as follows.</p><p>During each stage of training MirrorGAN, the generator G and discriminator D are trained alternately. Specially, the generator G i in the i th stage is trained by minimizing the loss as follows:</p><formula xml:id="formula_6">L Gi = − 1 2 E Ii∼p I i [log (D i (I i ))] − 1 2 E Ii∼p I i [log (D i (I i , s))] ,<label>(7)</label></formula><p>where I i is a generated image sampled from the distribution p Ii in the i th stage. The first term is the visual realism adversarial loss, which is used to distinguish whether the image is visually real or fake, while the second term is the text-image paired semantic consistency adversarial loss, which is used to determine whether the underlying image and sentence semantics are consistent. We further propose a CE-based text-semantic reconstruction loss to align the underlying semantics between the redescription of STREAM and the given text description. Mathematically, this loss can be expressed as:</p><formula xml:id="formula_7">L stream = − L−1 t=0 log p t (T t ).<label>(8)</label></formula><p>It is noteworthy that L stream is also used during STREAM pretraining. When training G i , gradients from L stream are backpropagated to G i through STREAM, whose network weights are kept fixed. The final objective function of the generator is defined as:</p><formula xml:id="formula_8">L G = m−1 i=0 L Gi + λL stream ,<label>(9)</label></formula><p>where λ is a loss weight to handle the importance of adversarial loss and the text-semantic reconstruction loss. The discriminator D i is trained alternately to avoid being fooled by the generators by distinguishing the inputs as either real or fake. Similar to the generator, the objective of the discriminators consists of a visual realism adversarial loss and a text-image paired semantic consistency adversarial loss. Mathematically, it can be defined as:</p><formula xml:id="formula_9">L Di = − 1 2 E I GT i ∼p I GT i log D i I GT i − 1 2 E Ii∼p I i [log (1 − D i (I i ))] − 1 2 E I GT i ∼p I GT i log D i I GT i , s − 1 2 E Ii∼p I i [log (1 − D i (I i , s))] ,<label>(10)</label></formula><p>where I GT i is from the real image distribution p I GT i in i th stage. The final objective function of the discriminator is defined as:</p><formula xml:id="formula_10">L D = m−1 i=0 L Di .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present extensive experiments that evaluate the proposed model. We first compare MirrorGAN with the state-of-the-art T2I methods GAN-INT-CLS <ref type="bibr" target="#b23">[24]</ref>, GAWWN <ref type="bibr" target="#b24">[25]</ref>, StackGAN <ref type="bibr" target="#b37">[38]</ref>, StackGAN++ <ref type="bibr" target="#b38">[39]</ref>, PPGN <ref type="bibr" target="#b19">[20]</ref> and AttnGAN <ref type="bibr" target="#b33">[34]</ref>. Then, we present ablation studies on the key components of MirrorGAN including GLAM and STREAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setup 4.1.1 Datasets</head><p>We evaluated our model on two commonly used datasets, CUB bird dataset <ref type="bibr" target="#b29">[30]</ref> and MS COCO dataset <ref type="bibr" target="#b16">[17]</ref>. The CUB bird dataset contains 8,855 training images and 2,933 test images belonging to 200 categories, each bird image has 10 text descriptions. The COCO dataset contains 82,783 training images and 40,504 validation images, each image has 5 text descriptions. Both datasets were preprocessed using the same pipeline as in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation metric</head><p>Following common practice <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34]</ref>, the Inception Score <ref type="bibr" target="#b25">[26]</ref> was used to measure both the objectiveness and diversity of the generated images. Two fine-tuned inception models provided by <ref type="bibr" target="#b37">[38]</ref> were used to calculate the score.</p><p>Then, the R-precision introduced in <ref type="bibr" target="#b33">[34]</ref> was used to evaluate the visual-semantic similarity between the generated images and their corresponding text descriptions. For each generated image, its ground truth text description and 99 randomly selected mismatched descriptions from the test set were used to form a text description pool. We then calculated the cosine similarities between the image feature and the text feature of each description in the pool, before counting the average accuracy at three different settings: top-1, top-2, and top-3. The ground truth entry falling into the top-k candidates was treated as correct, otherwise, it was wrong. A higher score represents a higher visual-semantic similarity between the generated images and input text.</p><p>The Inception Score and the R-precision were calculated accordingly as in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation details</head><p>MirrorGAN has three generators in total and GLAM is employed over the last two generators, as shown in <ref type="figure" target="#fig_2">Eq. (3)</ref>. 64×64, 128×128, 256×256 images are generated progressively. Followed <ref type="bibr" target="#b33">[34]</ref>, a pre-trained bi-directional LSTM <ref type="bibr" target="#b26">[27]</ref> was used to calculate the semantic embedding from text descriptions. The dimension of the word embedding D was 256. The sentence length L was 18. The dimension M i of the visual embedding was set to 32. The dimension of the visual feature was N i = q i × q i , where q i was 64, 128, and 256 for the three stages. The dimension of augmented sentence embedding D was set to 100. The loss weight λ of the text-semantic reconstruction loss was set to 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main results</head><p>In this section, we present both qualitative and quantitative comparisons with other methods to verify the effectiveness of MirrorGAN. First, we compare MirrorGAN with state-of-the-art text-to-image methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref> using the Inception Score and R-precision score on both CUB and COCO datasets. Then, we present subjective visual comparisons between MirrorGAN and the state-of-theart method AttnGAN <ref type="bibr" target="#b33">[34]</ref>. We also present the results of a human study designed to test the authenticity and visual semantic similarity between input text and images generated by MirrorGAN and AttnGAN <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative results</head><p>The Inception Scores of MirrorGAN and other methods are shown in <ref type="table" target="#tab_1">Table 1</ref>. MirrorGAN achieved the highest Inception Score on both CUB and COCO datasets. Specifically, compared with the state-of-art method AttnGAN <ref type="bibr" target="#b33">[34]</ref>, Mir-rorGAN improved the Inception Score from 4.36 to 4.56 on CUB and from 25.89 to 26.47 on the more difficult COCO dataset. These results show that MirrorGAN can generate more diverse images of better quality.</p><p>The R-precision scores of AttnGAN <ref type="bibr" target="#b33">[34]</ref> and Mirror-GAN on CUB and COCO datasets are listed in <ref type="table" target="#tab_2">Table 2</ref>. MirrorGAN consistently outperformed AttnGAN <ref type="bibr" target="#b33">[34]</ref> at all settings by a large margin, demonstrating the superiority of the proposed text-to-image-to-text framework and the global-local collaborative attentive module, since Mirror-GAN generated high-quality images with semantics consistent with the input text descriptions.    <ref type="figure" target="#fig_2">Figure 3</ref>. MirrorGAN Baseline refers to the model using only word-level attention for each generator in the MirrorGAN framework.</p><p>It can be seen that the image details generated by At-tnGAN are lost, colors are inconsistent with the text descriptions (3 rd and 4 th column), and the shape looks strange (2 nd , 3 rd , 5 th and 8 th column) for some hard examples. Furthermore, the skier is missing in the 5 th column. Mirror-GAN Baseline achieved better results with more details and consistent colors and shapes compared to AttnGAN. For example, the wings are vivid in the 1 st and 2 nd columns, demonstrating the superiority of MirrorGAN and that it takes advantage of the dual regularization by redescription, i.e., a semantically consistent image should be generated if it can be redescribed correctly. By comparing Mirror-GAN with MirrorGAN Baseline, we can see that GLAM contributes to producing fine-grained images with more details and better semantic consistency. For example, the color of the underbelly of the bird in the 4 th column was corrected to white, and the skier with a red jacket was recovered. The boats and city backdrop in the 7 th column and the horses on the green field in the 8 th column look real at first glance. Generally, content in the CUB dataset is less diverse than in COCO dataset. Therefore, it is easier to generate visually realistic and semantically consistent results on CUB. These results confirm the impact of GLAM, which uses global and local attention collaboratively.</p><p>Human perceptual test: To compare the visual realism and semantic consistency of the images generated by AttnGAN and MirrorGAN, we next performed a human perceptual test on the CUB test dataset. We recruited 100 volunteers with different professional backgrounds to conduct two tests: the Image Authenticity Test and the Semantic Consistency Test. The Image Authenticity Test aimed to compare the authenticity of the images generated using different methods. Participants were presented with 100 groups of images consecutively. Each group had 2 images arranged in random order from AttnGAN and MirrorGAN <ref type="figure">Figure 4</ref>: Results of Human perceptual test. A higher value of the Authenticity Test means more convincing images. A higher value of the Semantic Consistency Test means a closer semantics between input text and generated images. given the same text description. Participants were given unlimited time to select the more convincing images. The Semantic Consistency Test aimed to compare the semantic consistency of the images generated using different methods. Each group had 3 images corresponding to the ground truth image and two images arranged at random from At-tnGAN and MirrorGAN. The participants were asked to select the images that were more semantically consistent with the ground truth. Note that we used ground truth images instead of the text descriptions since it is easier to compare the semantics between images. After the participants finished the experiment, we counted the votes for each method in the two scenarios. The results are shown in <ref type="figure">Figure 4</ref>. It can be seen that the images from MirrorGAN were preferred over ones from AttnGAN. MirrorGAN outperformed AttnGAN with respect to authenticity, MirrorGAN was even more effective in terms of semantic consistency. These results demonstrate the superiority of MirrorGAN for generating visually realistic and semantically consistent images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>Ablation studies on MirrorGAN components: We next conducted ablation studies on the proposed model and its variants. To validate the effectiveness of STREAM and GLAM, we conducted several comparative experiments by excluding/including these components in MirrorGAN. The results are listed in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>First, the hyper-parameter λ is important. A larger λ led to higher Inception Scores and R-precision on both datasets. On the CUB dataset, when λ increased from 5 to 20, the Inception Score increased from 4.01 to 4.54 and R-precision increased from 32.07% to 57.67%. On the COCO dataset, the Inception Score increased from 21.85 to 26.21 and Rprecision increased from 52.55% to 74.52%. We set λ to 20 as default.</p><p>MirrorGAN without STREAM (λ = 0) and global attention (GA) achieved better results than StackGAN++ <ref type="bibr" target="#b38">[39]</ref> and PPGN <ref type="bibr" target="#b19">[20]</ref>. Integrating STREAM into MirrorGAN led to further significant performance gains. The Inception Score increased from 3.91 to 4.47 and from 19.01 to 25.99 on CUB and COCO, respectively, and R-precision showed the same trend. Note that MirrorGAN without GA already outperformed the state-of-the-art AttnGAN ( <ref type="table" target="#tab_1">Table 1)</ref> which also used the word-level attention. These results indicate that STREAM is more effective in helping the generators achieve better performance. This attributes to the introduction of a more strict semantic alignment between generated images and input text, which is provided by STREAM. Specifically, STREAM forces the generated images to be redescribed as the input text sequentially, which potentially prevents possible mismatched visual-text concept. Moreover, MirrorGAN integration with GLAM further improved the Inception Score and R-precision to achieve new state-ofthe-art performance. These results show that the global and local attention in GLAM collaboratively help the generator to generate visually realistic and semantically consistent results by telling it where to focus on.</p><p>Visual inspection on the cascaded generators: To better understand the cascaded generation process of Mirror-GAN, we visualized both the intermediate images and the attention maps in each stage ( <ref type="figure">Figure 5</ref>). In the first stage, low-resolution images were generated with primitive shape and color but lacking details. With guidance from GLAM in the following stages, MirrorGAN generated images by focusing on the most relevant and important areas. Consequently, the quality of the generated images progressively improved, e.g., the colors and details of the wings and crown. The top-5 global and local attention maps in each stage are shown below the images. It can be seen that: 1) the global attention concentrated more on the global context in the earlier stage and then the context around specific regions in later stages, 2) the local attention helped the generator synthesize images with fine-grained details by guiding it to focus on the most relevant words, and 3) the global attention is complementary to the local attention, they collaboratively contributed to the progressively improved generation.</p><p>In addition, we also present the images generated by MirrorGAN by modifying the text descriptions by a single a little bird with white belly, gray cheek patch and yellow crown and wing bars  <ref type="figure">Figure 5</ref>: Attention visualization on the CUB and the COCO test sets. The first row shows the output 64 × 64 images generated by G 0 , 128 × 128 images generated by G 1 and 256 × 256 images generated by G 2 . And the following rows show the Global-Local attention generated in stage 1 and 2. Please refer to the supplementary material for more examples.</p><p>this bird has a yellow crown and a white belly this bird has a black crown and a white belly this bird has a black crown and a red belly this bird has blue wings and a red belly <ref type="figure">Figure 6</ref>: Images generated by MirrorGAN by modifying the text descriptions by a single word and the corresponding top-2 attention maps in the last stage.</p><p>word ( <ref type="figure">Figure 6</ref>). MirrorGAN captured subtle semantic differences in the text descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Limitation and discussion</head><p>Although our proposed MirrorGAN shows superiority in generating visually realistic and semantically consistent images, some limitations must be taken into consideration in future studies. First, STREAM and other MirrorGAN modules are not jointly optimized with complete end-toend training due to limited computational resources. Second, we only utilize a basic method for text embedding in STEM and image captioning in STREAM, which could be further improved, for example, by using the recently proposed BERT model <ref type="bibr" target="#b6">[7]</ref> and state-of-the-art image captioning models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Third, although MirrorGAN is initially designed for the T2I generation by aligning cross-media semantics, we believe that its complementarity to the stateof-the-art CycleGAN can be further exploited to enhance model capacity for jointly modeling cross-media content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we address the challenging T2I generation problem by proposing a novel global-local attentive and semantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN successfully exploits the idea of learning text-to-image generation by redescription. STEM generates word-and sentence-level embeddings. GLAM has a cascaded architecture for generating target images from coarse to fine scales, leveraging both local word attention and global sentence attention to progressively enhance the diversity and semantic consistency of the generated images. STREAM further supervises the generators by regenerating the text description from the generated image, which semantically aligns with the given text description. We show that MirrorGAN achieves new stateof-the-art performance on two benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Schematic of the proposed MirrorGAN for text-to-image generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples of images generated by (a) AttnGAN [34], (b) MirrorGAN Baseline, and (c) MirrorGAN conditioned on text descriptions from CUB and COCO test sets and (d) the corresponding ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.The work was performed when Tingting Qiao was a visiting student at UBTECH Sydney AI Centre in the School of Computer Science, FEIT, in the University of Sydney 2.*corresponding author</figDesc><table><row><cell></cell><cell>T2I</cell><cell>I2T</cell></row><row><cell>(a)</cell><cell>this bird is blue with white and has a pointy beak</cell><cell></cell><cell>a small bird with a white breast and blue wings</cell></row><row><cell></cell><cell>text</cell><cell>image</cell><cell>text</cell></row><row><cell cols="2">this bird is blue with white</cell><cell>this bird is blue with white</cell></row><row><cell cols="2">and has a pointy beak</cell><cell>and has a pointy beak</cell></row><row><cell cols="2">this bird has a grey side</cell><cell>a small bird with a white</cell></row><row><cell cols="2">and a brown back</cell><cell>breast and blue wings</cell></row><row><cell>×</cell><cell></cell><cell>T2I</cell></row><row><cell></cell><cell></cell><cell>I2T</cell></row><row><cell></cell><cell>(b)</cell><cell>(c)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Inception Scores of state-of-the-art methods and MirrorGAN on CUB and COCO datasets.</figDesc><table><row><cell>Model</cell><cell>CUB</cell><cell>COCO</cell></row><row><cell>GAN-INT-CLS [24]</cell><cell>2.88 ± 0.04</cell><cell>7.88 ± 0.07</cell></row><row><cell>GAWWN [25]</cell><cell>3.62 ± 0.07</cell><cell>-</cell></row><row><cell>StackGAN [38]</cell><cell>3.70 ± 0.04</cell><cell>8.45 ± 0.03</cell></row><row><cell>StackGAN++ [39]</cell><cell>3.82 ± 0.06</cell><cell>-</cell></row><row><cell>PPGN [20]</cell><cell>-</cell><cell>9.58 ± 0.21</cell></row><row><cell>AttnGAN [34]</cell><cell>4.36 ± 0.03</cell><cell>25.89 ± 0.47</cell></row><row><cell>MirrorGAN</cell><cell>4.56 ± 0.05</cell><cell>26.47 ± 0.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>R-precision [%] of the state-of-the-art AttnGAN<ref type="bibr" target="#b33">[34]</ref> and MirrorGAN on CUB and COCO datasets.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>CUB</cell><cell></cell><cell></cell><cell>COCO</cell><cell></cell></row><row><cell>top-k</cell><cell>k=1</cell><cell>k=2</cell><cell>k=3</cell><cell>k=1</cell><cell>k=2</cell><cell>k=3</cell></row><row><cell>AttnGAN [34]</cell><cell>53.31</cell><cell>54.11</cell><cell>54.36</cell><cell>72.13</cell><cell>73.21</cell><cell>76.53</cell></row><row><cell>MirrorGAN</cell><cell>57.67</cell><cell>58.52</cell><cell>60.42</cell><cell>74.52</cell><cell>76.87</cell><cell>80.21</cell></row><row><cell cols="3">4.2.2 Qualitative results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Subjective visual comparisons: Subjective visual compar-</cell></row><row><cell cols="7">isons between AttnGAN [34], MirrorGAN Baseline, and</cell></row><row><cell cols="3">MirrorGAN are presented in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Inception Score and R-precision results of Mirror-GAN with different weight settings.</figDesc><table><row><cell>Evaluation Metric</cell><cell cols="2">Inception Score</cell><cell cols="2">R-precision (top-1)</cell></row><row><cell></cell><cell>CUB</cell><cell>COCO</cell><cell>CUB</cell><cell>COCO</cell></row><row><cell>MirrorGAN w/o GA, λ=0</cell><cell>3.91± .09</cell><cell>19.01± .42</cell><cell>39.09</cell><cell>50.69</cell></row><row><cell>MirrorGAN w/o GA, λ=20</cell><cell>4.47± .07</cell><cell>25.99± .41</cell><cell>55.67</cell><cell>73.28</cell></row><row><cell>MirrorGAN, λ=5</cell><cell>4.01± .06</cell><cell>21.85± .43</cell><cell>32.07</cell><cell>52.55</cell></row><row><cell>MirrorGAN, λ=10</cell><cell>4.30± .07</cell><cell>24.11± .31</cell><cell>43.21</cell><cell>63.40</cell></row><row><cell>MirrorGAN, λ=20</cell><cell>4.54 ± .17</cell><cell>26.47± .41</cell><cell>57.67</cell><cell>74.52</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented cyclegan: Learning many-tomany mappings from unpaired data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4328</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Groupcap: Groupbased image captioning with structured relevance and diversity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4328</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of conference on Empirical Methods on Natural Language Processing</title>
		<meeting>conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">4324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Saccade target selection and object recognition: Evidence for a common attentional mechanism. Vision research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Schneider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4328</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video captioning with attention-based lstm and semantic consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page">4322</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4321</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="page">4324</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>2017. 4322</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decidenet: Counting varying density crowds through attention guided detection and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>2015. 4322</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of conference on Empirical Methods on Natural Language Processing</title>
		<meeting>conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno>2017. 4325</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">4326</biblScope>
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Henderson. Top-down control of visual attention in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring human-like attention supervision in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ancient painting to natural image: A new solution for painting processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4325</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="page">4325</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised crossdomain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno>2017. 4322</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4323</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<idno>2017. 4322</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<idno>2017. 4321</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<biblScope unit="volume">4323</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stackgan++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10916</idno>
		<title level="m">Realistic image synthesis with stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4325</biblScope>
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Photographic text-to-image synthesis with a hierarchically-nested adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>2017. 4322</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
