<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GAP: Generalizable Approximate Graph Partitioning Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azade</forename><surname>Nazi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research; † † Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hang</surname></persName>
							<email>††willhang@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research; † † Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research; † † Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research; † † Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
							<email>azalia@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research; † † Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GAP: Generalizable Approximate Graph Partitioning Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph partitioning is the problem of dividing the nodes of a graph into balanced partitions while minimizing the edge cut across the partitions. Due to its combinatorial nature, many approximate solutions have been developed, including variants of multi-level methods and spectral clustering. We propose GAP, a Generalizable Approximate Partitioning framework that takes a deep learning approach to graph partitioning. We define a differentiable loss function that represents the partitioning objective and use backpropagation to optimize the network parameters. Unlike baselines that redo the optimization per graph, GAP is capable of generalization, allowing us to train models that produce performant partitions at inference time, even on unseen graphs. Furthermore, because we learn the representation of the graph while jointly optimizing for the partitioning loss function, GAP can be easily tuned for a variety of graph structures. We evaluate the performance of GAP on graphs of varying sizes and structures, including graphs of widely used machine learning models (e.g., ResNet, VGG, and Inception-V3), scale-free graphs, and random graphs. We show that GAP achieves competitive partitions while being up to 100 times faster than the baseline and generalizes to unseen graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph partitioning is an important optimization problem with numerous applications in domains spanning computer vision, VLSI design, biology, * Members of the Google Brain Residency Program social networks, transportation networks and more. The objective is to find balanced partitions of a graph while minimizing the number of edge cut. This problem is NP-complete which is formulated as a discrete optimization problem and solutions are generally derived using heuristics and approximation algorithms.</p><p>Some notable approaches include multi-level methods and spectral partitioning methods <ref type="bibr" target="#b14">[Karypis and Kumar, 1998</ref><ref type="bibr" target="#b13">, Karypis et al., 1999</ref><ref type="bibr" target="#b15">, Karypis and Kumar, 2000</ref><ref type="bibr" target="#b21">, Miettinen et al., 2006</ref><ref type="bibr" target="#b0">, Andersen et al., 2006</ref><ref type="bibr" target="#b4">, Chung, 2007</ref>.</p><p>In this work, we introduce a learning based approach, GAP, for the continuous relaxation of the problem. We define a differentiable loss function which captures the objective of partitioning a graph into disjoint balanced partitions while minimizing the number of edge cut across those partitions. We train a deep model to optimize for this loss function. The optimization is done in an unsupervised manner without the need for labeled datasets.</p><p>Our approach, GAP, does not assume anything about the graph structure (e.g., sparse vs. dense, or scale-free). Instead, GAP learns and adapts to the graph structure using graph embedding techniques while optimizing the partitioning loss function. This representation learning allows our approach to be selfadaptive without the need for us to design different strategies for different types of graphs.</p><p>Our learning based approach is also capable of generalization, meaning that we can train a model on a set of graphs and then use it at inference time on unseen graphs of varying sizes. In particular, we show that when GAP is trained on smaller graphs (e.g., 1k nodes), it transfers what it learned to much larger ones (e.g, 20k nodes). This generalization allows trained GAP models to quickly infer partitions on large unseen graphs, whereas baseline methods have to redo the entire optimization for each new graph.</p><p>In summary, this paper makes the following contributions:</p><p>• We propose GAP, a Generalizable Approximate Partitioning framework, which is an unsupervised learning approach to the classic problem of balanced graph partitioning. We define a differentiable loss function for partitioning that uses a continuous relaxation of the normalized cut. We then train a deep model and apply backpropagation to optimize the loss.</p><p>• GAP models can produce efficient partitions on unseen graphs at inference time. Generalization is an advantage over existing approaches which must redo the entire optimization for each new graph.</p><p>• GAP leverages graph embedding techniques <ref type="bibr" target="#b17">[Kipf and</ref><ref type="bibr">Welling, 2017, Hamilton et al., 2017</ref>] and learns to partition graphs based on their underlying structure, allowing it to generate efficient partitions across a wide variety of graphs.</p><p>• To encourage reproducible research, we provide source code in the supplementary materials and are in the process of open-sourcing the framework.</p><p>• We show that GAP achieves competitive partitions while being up to 100 times faster than top performing baselines on a variety of synthetic and real-world graphs with up to 27000 nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph Partitioning: Graph partitioning is an important combinatorial optimization problem that has been exhaustively studied. The most widely used graph partitioning algorithms generate partitions by performing operations on the input graph until convergence <ref type="bibr" target="#b0">[Andersen et al., 2006</ref><ref type="bibr" target="#b4">, Chung, 2007</ref>. On the other hand, multilevel partitioning approaches first reduce the size of the graph by collapsing nodes and edges, then partition on the smaller graph, and finally expand the graph to recover the partitioning for the original graph <ref type="bibr" target="#b15">[Karypis and Kumar, 2000</ref><ref type="bibr" target="#b13">, Karypis et al., 1999</ref><ref type="bibr" target="#b14">, Karypis and Kumar, 1998</ref><ref type="bibr" target="#b21">, Miettinen et al., 2006</ref>. These algorithms are shown to provide high-quality partitions <ref type="bibr" target="#b21">[Miettinen et al., 2006]</ref>.</p><p>Another approach is to use simulated annealing. <ref type="bibr">[Van Den Bout and Miller, 1990]</ref> proposed mean field annealing, which combines simulated annealing with Hopfield neural networks. <ref type="bibr" target="#b16">[Kawamoto et al., 2018]</ref> studied a different formulation of graph partitioning in which a graph is generated by a statistical model, and the task is to infer the preassigned group labels of the generative model. They developed a mean-field theory of a minimal graph neural network architecture for this version of the problem.</p><p>This line of inquiry formulates graph partitioning as a discrete optimization problem, while our GAP framework is one of the first deep learning approaches for the continuous relaxation of the problem. Moreover, GAP generalizes to unseen graphs, generating partitions on the fly, rather than having to redo the optimization per graph. Clustering: Given a set of points, the goal of clustering is to identify groups of similar points. Clustering problems with different objectives such as selfbalanced k-means and balanced min-cut have been exhaustively studied <ref type="bibr" target="#b20">[Liu et al., 2017</ref><ref type="bibr" target="#b3">, Chen et al., 2017</ref><ref type="bibr" target="#b2">, Chang et al., 2014</ref>. One of the most effective techniques for clustering is spectral clustering, which first generates node embeddings in the eigenspace of the graph Laplacian, and then applies k-means clustering to these vectors <ref type="bibr" target="#b27">[Shi and Malik, 2000</ref><ref type="bibr" target="#b24">, Ng et al., 2002</ref><ref type="bibr" target="#b31">, Von Luxburg, 2007</ref>.</p><p>However, generalizing clustering to unseen nodes and graphs is nontrivial. To address generalization, SpectralNet <ref type="bibr" target="#b26">[Shaham et al., 2018</ref>] is a deep learning approach to spectral clustering which generates spectral embeddings for unseen data points. Other deep learning approaches for clustering attempt to encode the input in a way that is amenable to clustering by k-means or Gaussian Mixture Models <ref type="bibr" target="#b34">[Yang et al., 2017</ref><ref type="bibr" target="#b32">, Xie et al., 2016</ref><ref type="bibr" target="#b36">, Zheng et al., 2016</ref><ref type="bibr" target="#b5">, Dilokthanakul et al., 2016</ref>.</p><p>Although related, graph clustering and graph partitioning are different problems in that graph clustering attempts to maximize locality of clusters, whereas graph partitioning seeks to preserve locality while maintaining balance among partitions. Our approach also treats the partitioning problem as an end-to-end learning problem with a differentiable loss, whereas the aforementioned approaches generate embeddings that are then clustered using non-differentiable techniques like k-means. Device Placement: The practical significance of graph partitioning is demonstrated by the task of device placement for TensorFlow computation graphs, where the objective is to minimize execution time by assigning operations to devices. <ref type="bibr" target="#b23">[Mirhoseini et al., 2017]</ref> proposed a reinforcement learning method to optimize device placement for TensorFlow graphs. They used a seq2seq policy to assign operations to devices. The execution time of the generated placements is then used as a reward signal to optimize the policy. A hierarchical model for device placement has been proposed in <ref type="bibr" target="#b22">[Mirhoseini et al., 2018]</ref>, where the graph partition and placement are learned jointly. While this work also uses a neural network to learn the partitions, their objective is to optimize the runtime of the resulting partitions, forcing them to use policy gradient to optimize their non-differentiable loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition and Background</head><formula xml:id="formula_0">Let G = (V, E) be a graph where V = {v i } and E = {e(v i , v j )|v i ∈ V, v j ∈ V }</formula><p>are the set of nodes and edges in the graph. Let n be the number of nodes. A graph G can be partitioned into g disjoint sets S 1 , S 2 , . . . S g , where the union of the nodes in those sets are V ( g k=1 S k = V ), and each node belongs to only one set ( g k=1 S k = ∅), by simply removing edges connecting those sets. Minimum Cut: The total number of edges that are removed from G in order to form disjoint sets is called cut. Given sets S k , andS k , the cut(S k ,S k ) is formally defined as:</p><formula xml:id="formula_1">cut(S k ,S k ) = v i ∈S k ,v j ∈S k e(vi, vj)<label>(1)</label></formula><p>This formula can be generalized to multiple disjoint sets S 1 , S 2 , . . . S g , whereS k is the union of all sets except S k .</p><p>cut(S1, S2, . . . Sg) = 1 2</p><formula xml:id="formula_2">g i=k cut(S k ,S k )<label>(2)</label></formula><p>Normalized Cut:</p><p>The optimal partitioning of a graph that minimizes the cut (Equation 2) is a well-studied problem and there exist efficient polynomial algorithms for solving it <ref type="bibr" target="#b25">[Papadimitriou and Steiglitz, 1982]</ref>. However, the minimum cut criteria favors cutting nodes whose degree are small and leads to unbalanced sets/partitions. To avoid such bias, normalized cut (Ncut), which is based on the graph conductance, has been studied by <ref type="bibr" target="#b27">[Shi and</ref><ref type="bibr">Malik, 2000, Zhang and</ref><ref type="bibr" target="#b35">Rohe, 2018]</ref>, where the cost of a cut is computed as a fraction of the total edge connections to all nodes.</p><formula xml:id="formula_3">Ncut(S1, S2, . . . Sg) = g k=1 cut(S k ,S k ) vol(S k , V ) (3) Where vol(S k , V ) = vi∈S k ,vj ∈V e(v i , v j ), i.e.</formula><p>, total degree of nodes belong to S k in graph G. One way to minimize the normalized cut is based on the eigenvectors of the graph Laplacian which has been studied in <ref type="bibr" target="#b27">[Shi and</ref><ref type="bibr">Malik, 2000, Zhang and</ref><ref type="bibr" target="#b35">Rohe, 2018]</ref>. Previous research has shown that across a wide range of social and information networks, the clusters with the smallest graph conductance are often small <ref type="bibr">[Leskovec, 2009, Zhang and</ref><ref type="bibr" target="#b35">Rohe, 2018]</ref>. Regularized spectral clustering has been proposed by <ref type="bibr" target="#b35">[Zhang and Rohe, 2018]</ref> to address this problem.</p><p>In this paper, however, we propose GAP as an unsupervised learning approach with a differentiable loss function that can be trained to find balanced partitions with minimum normalized cuts. We show that GAP enables generalization to unseen graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generalizable Approximate Partitioning</head><p>We now introduce the Generalizable Approximate Partitioning framework (GAP). As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, GAP has two main components: graph representation learning for generating partition probabilities per node (the model), and a differentiable formulation of the normalized cut objective (the loss function). GAP enables us to train a neural network to optimize a previously undifferentiable objective by generating balanced partitions with minimum edge-cut. We first present the loss function before discussing the model.</p><formula xml:id="formula_4">Adjacency matrix A n ⨉ n Node features X n ⨉ f Node embeddings H n ⨉ d Partition probabilities Y n ⨉ g GAP Model GAP Loss Function Graph Embedding Module Graph Partitioning Module Γ = Y T D = ⨉ = g x 1 n x 1 g x n Graph embedder ... ... ... Softmax S 1 S 2 S g Node degrees D n ⨉ 1 [Ncut] = ∑ (Y ⊘ Γ)(1 -Y T ) ☉ A = ∑ ⊘ ⨉ ☉ n x g g x n n x n n x g reduce-sum reduce-sum n x g ∑ (∑ Y ik -n/g) 2 = ∑ ([1 … 1] x -n/g) 2 k=1 g i=1 n 1 x n reduce-sum</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GAP Loss Function</head><p>We assume that our model returns Y ∈ R n×g where Y ik represents the probability that node v i ∈ V belongs to partition S k . We propose a loss function based on Y to calculate the normalized cut in Equation 3 and evaluate the balancedness of the partitions. Later in subsection 4.2, we discuss the model that generates Y . Normalized Cut: As we discussed in Section 3,</p><formula xml:id="formula_5">cut(S k ,S k ) is the number of edges e(v i , v j ), where v i ∈ S k and v j / ∈ S k .</formula><p>Let Y ik be the probability that node v i belongs to partition S k . The probability that node v j does not belong to partition S k would be 1−Y jk . Therefore, E[cut(S k ,S k )] can be formulated by Equation 4, where N (v i ) is the set of nodes adjacent to v i (visual illustration in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><formula xml:id="formula_6">E[cut(S k ,S k )] = v i ∈S k v j ∈N (v i ) g z=1 Yiz(1 − Yjz)<label>(4)</label></formula><p>Since the set of adjacent nodes for a given node can be retrieved from the adjacency matrix of graph A, we can rewrite Equation 4 as follows:</p><formula xml:id="formula_7">E[cut(S k ,S k )] = reduce-sum Y :,k (1 − Y :,k ) A (5)</formula><p>The element-wise product with the adjacency matrix ( A) ensures that only the adjacent nodes are considered. Moreover, the result of</p><formula xml:id="formula_8">Y :,k (1 − Y :,k )</formula><p>A is an n × n matrix and E[cut(S k ,S k )] is the sum over all of its elements.</p><p>From Equation 3, vol(S k , V ) is the sum over the degree of all nodes that belong to S k . Let D be a column vector of size n where D i is the degree of the node v i ∈ V . Given Y , we can calculate the E[vol(S k , V )] as follows:</p><formula xml:id="formula_9">Γ = Y D E[vol(S k , V )] = Γ k (6)</formula><p>Where Γ is a vector in R g , and g is the number of partitions.</p><p>With E[cut(S k ,S k )] and E[vol(S k , V )] from Equations 5 and 6, we can calculate the expected normalized cut in Equation 3 as follows:</p><formula xml:id="formula_10">E[Ncut(S1, S2, . . . Sg)] = reduce-sum (Y Γ)(1 − Y ) A (7)</formula><p>is element-wise division and the result of</p><formula xml:id="formula_11">(Y Γ)(1 − Y ) A is an n × n matrix where E[cut(S 1 , S 2 , . . . S g )]</formula><p>is the sum over all of its elements. Balanced Cut: So far, we have shown how one can calculate the expected normalized cut of a graph given the matrix Y (probabilities of nodes belonging to partitions). Here, we show that given Y we can also evaluate how balanced those partitions are.</p><p>Given the number of nodes in the graph |V | = n and the number of partitions g, to have balanced partitions the number of nodes per partition should be n g . The sum of the columns in Y gives us the expected number of nodes in each partition due to the fact that Y ik represents the probability that node v i ∈ V belongs to partition S k . Thus, for the balanced partitions we minimize the following error:</p><formula xml:id="formula_12">g k=1 ( n i=1 Y ik − n g ) 2 = reduce-sum (1 Y − n g ) 2<label>(8)</label></formula><p>Combining expected normalized cut <ref type="formula">(Equation 7</ref>) with the balanced partition error (Equation 8), we have the following loss function:</p><formula xml:id="formula_13">L = reduce-sum (Y Γ)(1 − Y ) A + reduce-sum (1 Y − n g ) 2<label>(9)</label></formula><p>Next, we discuss the GAP neural model that finds the graph partition Y to minimize the loss in Equation 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The GAP Model</head><p>The GAP model ingests a graph definition, generates node embeddings that leverage local graph structure, and projects each embedding into logits that define a probability distribution to minimize the expected normalized cut <ref type="formula" target="#formula_13">(Equation 9</ref>). Graph Embedding Module: The purpose of the graph embedding module is to learn node embeddings using the graph structure and node features. Recently, there have been several advances on applying graph neural networks for node embedding and classification tasks using approaches such as Graph Convolution Network <ref type="bibr" target="#b17">[Kipf and Welling, 2017]</ref> (GCN), GraphSAGE <ref type="bibr" target="#b11">[Hamilton et al., 2017]</ref>, Neural Graph Machines <ref type="bibr" target="#b1">[Bui et al., 2017]</ref>, Graph Attention Networks <ref type="bibr" target="#b30">[Veličković et al., 2018]</ref> and other variants. In this work, we leverage GCN and GraphSAGE to learn graph representations across a variety of graphs, which helps with generalization. GCN : <ref type="bibr" target="#b17">[Kipf and Welling, 2017]</ref> showed that untrained GCN with random weights can serve as a powerful feature extractor for graph nodes. In our implementation, we used a 3-layer GCN with weight matrices (W (l) ) using Xavier initialization described in <ref type="bibr" target="#b7">[Glorot and Bengio, 2010]</ref>.</p><formula xml:id="formula_14">Z = tanh(Â tanh(Â tanh(ÂXW (0) )W (1) )W (2) )</formula><p>whereÂ =D − 1 2ÃD − 1 2 ,Ã = A + I n is the adjacency matrix of the undirected graph G with added selfconnections. I n is the identity matrix, andD i,i = jÃ ij . The input feature matrix X depends on the graph. In TensorFlow computation graphs, each operation type (such as MatMul, Conv2d, Sum, etc.) would be a feature. GraphSAGE : <ref type="bibr" target="#b11">[Hamilton et al., 2017]</ref> developed a node embedding technique that generates high dimensional graph node representations based on node input features. Central to this technique is sample and aggregate, where given a node v i , we sample a set of v i 's neighbors from N (v i ), and aggregate their representations (with max pooling) to generate an embedding for the sampled neighbors of v i . This neighbor representation, along with the representation of v i itself, is combined to generate a new representation for v i . Iterating this process multiple times results in message passing among nodes for an increasing number of hops.</p><p>Our implementation of GraphSAGE is based on Algorithm 1 in <ref type="bibr" target="#b11">[Hamilton et al., 2017]</ref>. For each message passing step k, we perform the following operations per node v i ∈ V :</p><formula xml:id="formula_15">h k N (vi) = maxpool({W agg k h k−1 vj + b agg k , ∀v j ∈ N (v i )}) h k vi = relu(W proj k [h k−1 vi , h k N (vi) ] + b agg k )</formula><p>h k vi = h k vi /||h k vi || 2 where agg and proj denote the aggregation and projection matrices respectively. Graph Partitioning Module: The second module in our GAP framework is responsible for partitioning the graph, taking in node embeddings and generating the probability that each node belongs to partitions S 1 , S 2 , ..., S g (Y in <ref type="figure" target="#fig_0">Figure 1</ref>). This module is a fully connected layer followed by softmax, trained to minimize Equation 9.</p><p>We also note that for particularly large graphs, it is possible to optimize on randomly sampled minibatches of nodes from the larger graph. Furthermore, it is possible to stop gradient flow from the partitioning module to the embedding module, resulting in unsupervised node embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The main goals of our experiments are to (a) evaluate the performance of the GAP framework against hMETIS <ref type="bibr" target="#b15">[Karypis and Kumar, 2000]</ref>, a widely used partitioner that uses multilevel partitioning and (b) evaluate the generalizability of GAP over unseen graphs and provide insights on how the structural similarities between train and test graphs affect the generalization performance. Source code is provided for reproducibility and is in the process of being opensourced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We conducted experiments on real and synthetic graphs. Specifically, we use five widely used Ten-sorFlow graphs. We also generate Random as well as Scale-free graphs as synthetic datasets to show the effectivenesss of GAP on graphs with different structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Datasets</head><p>• ResNet <ref type="bibr" target="#b12">[He et al., 2016</ref>] is a deep convolutional network with residual connections to avoid vanishing gradients. The TensorFlow implementation of ResNet v1 50 with 50 layers contains 20, 586 operations.</p><p>• Inception-v3 <ref type="bibr" target="#b29">[Szegedy et al., 2017]</ref> consists of multiple blocks, each composed of several convolutional and pooling layers. The TensorFlow graph of this model contains 27, 114 operations.</p><p>• AlexNet <ref type="bibr" target="#b18">[Krizhevsky et al., 2012]</ref> consists of 5 convolutional layers, some of which are followed by max-pooling layers, and 3 fully-connected layers with a final softmax. The TensorFlow graph of this model has 798 operations.</p><p>• MNIST-conv has 3 convolutional layers for the MNIST classification task. The TensorFlow graph of this model contains 414 operations.</p><p>• VGG <ref type="bibr" target="#b28">[Simonyan and Zisserman, 2014]</ref> contains 16 convolutional layers. The TensorFlow graph of VGG contains 1, 325 operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Datasets</head><p>• Random: Randomly generated networks of size 10 3 and 10 4 nodes using the Erdös-Rényi model <ref type="bibr" target="#b6">[Erdos and Rényi, 1960]</ref>, where the probability of having an edge between any two nodes is 0.1.</p><p>• Scale-free: Randomly generated scale-free networks of size 10 3 and 10 4 nodes using Net-workX <ref type="bibr" target="#b10">[Hagberg et al., 2008]</ref>    is a network whose degree distribution follows a power law <ref type="bibr" target="#b0">[Bollobás et al., 2003]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline:</head><p>Since graph partitioning is NPcomplete, solutions are generally derived using heuristics and approximation algorithms.</p><p>While there has been a substantial amount of work on graph partitioning for specific graph structure/applications <ref type="bibr" target="#b8">[Gonzalez et al., 2012</ref><ref type="bibr" target="#b9">, Hada et al., 2018</ref>, hMETIS <ref type="bibr" target="#b13">[Karypis and</ref><ref type="bibr">Kumar, 2000, Karypis et al., 1999]</ref> is a general framework that works across a wide variety of graphs and is shown to provide high quality partitions in different domains (e.g., VLSI, road network <ref type="bibr">[Miettinen et al., 2006, Xu and</ref><ref type="bibr" target="#b33">Tan, 2012]</ref>. Similar to hMETIS, GAP is a general framework that makes no assumptions about graph structure. In our experiments, we compare GAP against hMETIS. We set the hMETIS parameters to return balanced partitions with minimum edge cut.</p><p>Performance Measures: As we discussed in Section 3, balanced partitions with minimum edge cut is the goal of graph partitioning. We evaluate the performance of the resulting partitions by examining 1) Edge cut: the ratio of the cut to the total number of edges, and 2) Balancedness: is one minus the MSE of number of nodes in every partition and balances partition ( n g ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation graphs</head><p>AlexNet   Number of partitions is three and they are denoted by colors. We only show the nodes whose operation type is convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance</head><p>In this set of experiments, we find that GAP outperforms hMETIS. Since hMETIS does not generalize to unseen graphs and optimizes one graph at a time, we also constrain GAP to optimize one graph at a time for a fair comparison. We discuss the generalization ability of GAP in Section 5.3. <ref type="table">Table 1</ref> shows the performance of GAP against hMETIS on a 3-partition problem over real Tensor-Flow graphs. Both techniques generate very balanced partitions, with GAP outperforming hMETIS on edge cut for the VGG graph. <ref type="figure" target="#fig_2">Figure 3</ref> shows the performance of GAP against hMETIS on random graphs when the number of partitions is varied from 2 to 10. The plots represent the average value across 5 random graphs. Both GAP and hMETIS produce 99% balanced partitions. However, GAP is also able to find lower edge cut partitions than hMETIS. By examining the degree histograms of our datasets <ref type="figure" target="#fig_1">(Figures 2a to 2d)</ref>, we found that while hMETIS heuristics work reasonably well on sparse TensorFlow graphs, GAP outperforms hMETIS on dense graphs.  <ref type="figure">Figure 5</ref>: Here, GAP is trained on VGG and is tested on other computation graphs. We observe that the Jaccard similarity between the operation types in VGG and other graphs affects the generalization of GAP. Higher Jaccard similarities between the train and validation/test datasets enable GAP to find better partitions with smaller edge cut.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Generalization</head><p>In this section, we show that GAP generalizes effectively on real and synthetic datasets. To the best of our knowledge, we are the first to propose a learning approach for graph partitioning that can generalize to unseen graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Generalization on real graphs</head><p>In this set of experiments, we train GAP with a single TensorFlow graph, VGG, and validate on MNISTconv. At inference time, we test the trained model on unseen TensorFlow graphs: AlexNet, ResNet, and Inception-v3. <ref type="table" target="#tab_2">Table 2</ref> shows the result of our experiments, and illustrates the importance of node features and graph embeddings in generalization. In GAP-id, we use the index of a node as its feature, while in GAPop, the operation type (such as Add, Conv2d, and L2loss in TensorFlow) is used as the node feature. We encode all features as one-hots. Following Section 4.2, we leverage Graph Convolution Networks <ref type="bibr" target="#b17">[Kipf and Welling, 2017]</ref> (GCN) and Graph-SAGE <ref type="bibr" target="#b11">[Hamilton et al., 2017]</ref> to capture similarities across graphs. In GCN offline and GraphSAGE offline, we do not train the graph embedding module ( <ref type="figure" target="#fig_0">Figure 1</ref>) without gradient flow from the partitioning module, while in GraphSAGE trained both modules are trained jointly. <ref type="table" target="#tab_2">Table 2</ref> shows that GAP-op with GraphSAGE trained (last row) achieves the best performance and generalizes better than the other models. Note that this model is trained on a single graph, VGG with only 1325 nodes, and it is tested on AlexNet, ResNet, and Inception-v3 with 798, 20586, and 27114 nodes, respectively. <ref type="figure" target="#fig_3">Figure 4</ref> shows the GAP partitioning of Inception-v3 using a model trained on the same graph (4a) and a model trained on VGG (4b). Note that partitions are denoted by colors and we only show nodes whose operation type is convolution. In the scenario <ref type="formula" target="#formula_6">(4a)</ref> where we train and test GAP on Inception-v3, we achieve 99% balanced partitions with 4% edge cut <ref type="table">(Table 1)</ref>. Where GAP is trained on VGG and tested over the unseen graph (Inception-v3 ), it achieves 99% balanced partitions with 6% edge cut (last row of <ref type="table" target="#tab_2">Table 2</ref>). The partition assignments in Figures 4a and 4b are quite similar (75%), which demonstrates GAP generalization.</p><p>We also observed that the similarity of the node features (operation types) in VGG and other computation graphs used in inference and validation is correlated with the edge cut score of GAP partitioning ( <ref type="figure">Figure 5</ref>). For example, let A and B be the set of the operation types in VGG and ResNet, respectively, with a Jaccard similarity of |A∩B| |A∪B| = 0.592). <ref type="figure">Figure 5</ref> shows that as Jaccard similarity of a graph with VGG increases, the edge cut decreases. In other words, the presence of similar node types across train and test graphs aids the generalization of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture and Hyper-parameters:</head><p>Here, we describe the details of the model with the best performance (corresponding to the last row of <ref type="table" target="#tab_2">Table 2</ref>). The number of features (TensorFlow operation types) is 1518. GraphSAGE has 5 layers of 512 units with shared pooling, and the graph partitioning module is a 3 layer dense network of 64 units with a final softmax layer. We use ReLU as activation function and all weights are initialized using Xavier initialization <ref type="bibr" target="#b7">[Glorot and Bengio, 2010]</ref>. We use the Adam optimizer with a learning rate of 7.5e-5.  <ref type="figure">Figure 6</ref>: Generalization of GAP on scale-free graphs. GAP-Scalefree-1 is trained on only one scale-free graph, while GAP-Scalefree-10 is trained on 10 scale-free graphs. The result is the average over the 5 scale-free graphs of 1k and 10k nodes. GAP-Scalefree-10 is slightly faster than hMETIS and it produces partitions which are as balanced as hMETIS partitions but with smaller edge cut. . Figure 7: Generalization of GAP on random graphs. GAP-Random-1 is trained on only one random graph, while GAP-Random-10 is trained on 10 random graphs. The result is the average over the 5 random graphs of 1k and 10k nodes. Performance of GAP-Random-1 and GAP-Random-10 is almost the same as hMetis but the inference time is 10 to 100 times faster than the runtime of hMETIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Generalization on synthetic graphs</head><p>We further evaluate the generalization of GAP on Random and Scale-free graphs. Note that we train and test GAP on the same type of graph, but number of nodes may vary. For example, we train GAP on random graphs of 1k nodes and test on random graphs of 1k and 10k nodes. Similarly, we train GAP on scalefree graphs of 1k nodes and test on scale-free graphs of 1k and 10k nodes. <ref type="figure">, 6b, and 6c</ref> show the edge cut, balancedness, and execution time of GAP against hMETIS over the scale-free graphs (every point is the average of 5 experiments). In GAP-Scalefree-1 we train GAP with only one scale-free graph, while GAP-Scalefree-10 is trained on 10 scale-free graphs. We then test the trained models GAP-Scalefree-1 and GAP-Scalefree-10 over 5 unseen scale-free graphs of 1k and 10k nodes and we report the average results. <ref type="figure">Figure 6a</ref> shows that both GAP-Scalefree-1 and GAP-Scalefree-10 partition the unseen graphs of 1k and 10k nodes with lower edge cut than hMETIS. Despite the balancedness of GAP-Scalefree-1 being lower than that of hMETIS, by increasing the number of graphs in the training set (GAP-Scalefree-10) balancedness is improved as shown in <ref type="figure">Figure 6b</ref>, while its edge cut is still smaller (6a). Furthermore, GAP-Scalefree-10 runs slightly faster than hMETIS (6c) and its partitions are just as balanced as those of hMETIS (6b) but with lower edge cut (6a) <ref type="figure">.  Figures 7a, 7b, and 7c</ref> show the edge cut, balancedness, and execution time of GAP against hMETIS on random graphs. Every point is the average of 5 experiments. In GAP-Random-1, we train GAP on only one random graph, while in GAP-Random-10, we train on 10 random graphs. We then test the trained models GAP-random-1 and GAP-Random-10 on 5 unseen random graphs with 1k and 10k nodes and we report the average results. The performance of GAP when generalizing on unseen random graphs of 1k and 10k nodes is almost the same as the performance of hMETIS, while <ref type="figure">Figure 7c</ref> shows that during inference, GAP is 10 to 100 times faster than the runtime of hMETIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figures 6a</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architectures and Hyper-parameters:</head><p>Unlike computation graphs where node features are operation types, nodes in synthetic graphs have no features. Furthermore, we must train a model that generalizes to graphs of different sizes. For example, we train a model on a random graph with 1k nodes and test it on a random graph with 10k nodes. To do so, we apply PCA to the adjacency matrix of a featureless graph and retrieve embeddings of size 1000 as our node features. We use ReLU as our activation function and all weights are initialized using Xavier initialization. We also use the Adam optimizer. Here are the rest of the hyperparameters for each model. GAP-Scalefree-1: model is trained with one scale-free graph. GraphSAGE has 5 layers of 512 units, and graph partitioning module is 3 layer dense network of 128 units with softmax. Learning rate is 2.5e-6. GAP-Scalefree-10: Trained with 10 scale-free graphs. GraphSAGE has 4 layers of 128 units, and graph partitioning module is 1 layer dense network of 64 units with softmax. Learning rate is 7.5e-6. GAP-Random-1: Trained with only random graph. GraphSAGE has 5 layers of 128 units with shared pooling, and graph partitioning module is 2 layer dense network of 64 units with softmax. Learning rate is 7.5e-4. GAP-Random-10: Trained with 10 random graphs. GraphSAGE has 2 layers of 256 units with shared pooling, and graph partitioning module is 3 layer dense network of 128 units with softmax. Learning rate is 7.5e-6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a deep learning framework, GAP, for the graph partitioning problem, where the objective is to assign the nodes of a graph into balanced partitions while minimizing the edge cut across the partitions. Our GAP framework enables generalization: we can train models that produce performant partitions at inference time, even on unseen graphs. This generalization is an advantage over existing baselines which redo the optimization for each new graph. Our results over widely used machine learning models (ResNet, VGG, and Inception-v3), scale-free graphs, and random graphs confirm that GAP achieves competitive partitions while being up to 100 times faster than the baseline and generalizing to unseen graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Generalizable Approximate graph Partitioning (GAP) Framework (see Section 4 for more details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Degree histogram of MNIST-conv, VGG, AlexNet and synthetic Random graphs. Random graphs are denser than the others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Edge cut of the partitions on random graphs by varying the number of partitions using GAP and hMETIS. Both GAP and hMETIS produce 99% balanced partitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>GAP partitioning of the Inception-v3 (a) using the trained model on Inception-v3 and (b) the trained model on VGG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Generalization results: GAP is trained on VGG and validated on MNIST-conv. During inference, the model is applied to unseen TensorFlow graphs: ResNet. Inception-v3, and AlexNet. In GAP-id, we use node index features, while in GAP-op, we use TensorFlow operation types as features. According toTable 1, the ground truth for VGG, MNIST-conv, and AlexNet is 99% balanced partitions with 5% edge cut and for ResNet and Inception-v3, it is 99% balanced partitions with 4% edge cut. GAP-op with GraphSAGE trained (last row) generalizes better than the other models.</figDesc><table><row><cell>(a) Training a GAP model on Inception-v3, and test-</cell><cell>(b) Generalization: training a GAP model on VGG,</cell></row><row><cell>ing on the same computation graph (Inception-v3 )</cell><cell>and testing it on unseen graphs (Inception-v3 )</cell></row><row><cell>achieves 99% balanced partitions with 4% edge cut</cell><cell>achieves 99% balanced partitions with 6% edge cut</cell></row><row><cell>(Table 1)</cell><cell>(last row of Table 2)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA &apos;03</title>
		<meeting>the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA &apos;03</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
	<note>FOCS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural graph machines: Learning neural networks using graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bui</surname></persName>
		</author>
		<idno>abs/1703.04818</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Balanced k-means and min-cut clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.6235</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A self-balanced mincut algorithm for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2080" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Four proofs for the cheeger inequality and graph partition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCM</title>
		<meeting>ICCM</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">378</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02648</idno>
		<title level="m">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the evolution of random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rényi ;</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Inst. Hung. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="60" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<editor>Teh, Y. W. and Titterington, M.</editor>
		<meeting><address><addrLine>Sardinia, Italy. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
		<respStmt>
			<orgName>Chia Laguna Resort</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Powergraph: distributed graph-parallel computation on natural graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable minimum-cost balanced partitioning of large-scale social networks: Online and offline solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1636" to="1649" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring network structure, dynamics, and function using networkx</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hagberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Los Alamos, NM (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Los Alamos National Lab.(LANL)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilevel hypergraph partitioning: applications in vlsi domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T VLSI SYST</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilevelk-way partitioning scheme for irregular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kumar ; Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="129" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multilevel k-way hypergraph partitioning. VLSI design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kumar ; Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mean-field theory of graph neural networks in graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kawamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4366" to="4376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Welling ; Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="123" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Balanced clustering with least square regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2231" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Using METIS and hMETIS algorithms in circuit partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Miettinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Helsinki University of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hierarchical model for device placement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mirhoseini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Device placement optimization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mirhoseini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steiglitz ;</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steiglitz</surname></persName>
		</author>
		<title level="m">Combinatorial Optimization: Algorithms and Complexity</title>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Prentice-Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Upper Saddle River</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename><surname>Nj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01587</idno>
		<title level="m">Spectralnet: Spectral clustering using deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><forename type="middle">;</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisserman ;</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>D. E. and Miller, T. K.</editor>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="192" to="203" />
		</imprint>
	</monogr>
	<note>Graph partitioning using annealed neural networks</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Veličković</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>[von Luxburg ; Von Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">hmetis-based offline road network partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><forename type="middle">;</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="221" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards k-meansfriendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding regularized spectral clustering via graph conductance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohe ;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10654" to="10663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Variational deep embedding: A generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05148</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
