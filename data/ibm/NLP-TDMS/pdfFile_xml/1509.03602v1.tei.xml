<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepSat -A Learning framework for Satellite Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-09-14">September 14, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Basu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangram</forename><surname>Ganguly</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bay Area Environmental Research Institute/ NASA Ames Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Mukhopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dibiano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Karki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Nemani</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NASA Advanced Supercomputing Division/ NASA Ames Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepSat -A Learning framework for Satellite Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-09-14">September 14, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels. The contributions of this paper are twofold -(1) first, we present two new satellite datasets called SAT-4 and SAT-6, and (2) then, we propose a classification framework that extracts features from an input image, normalizes them and feeds the normalized feature vectors to a Deep Belief Network for classification. On the SAT-4 dataset, our best network produces a classification accuracy of 97.95% and outperforms three state-of-the-art object recognition algorithms, namely -Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by ∼11%. On SAT-6, it produces a classification accuracy of 93.9% and outperforms the other algorithms by ∼15%. Comparative studies with a Random Forest classifier show the advantage of an unsupervised learning approach over traditional supervised learning techniques. A statistical analysis based on Distribution Separability Criterion and Intrinsic Dimensionality Estimation substantiates the effectiveness of our approach in learning better representations for satellite imagery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Learning has gained popularity over the last decade due to its ability to learn data representations in an unsupervised manner and generalize to unseen data samples using hierarchical representations. The most recent and best-known Deep learning model is the Deep Belief Network <ref type="bibr" target="#b14">[15]</ref>. Over the last decade, numerous breakthroughs have been made in the field of Deep Learning; a notable one being <ref type="bibr" target="#b21">[22]</ref>, where a locally connected sparse autoencoder was used to detect objects in the ImageNet dataset <ref type="bibr" target="#b10">[11]</ref> producing state-of-the-art results. In <ref type="bibr" target="#b26">[27]</ref>, Deep Belief Networks have been used for modeling acoustic signals and have been shown to outperform traditional approaches using Gaussian Mixture Models for Automatic Speech Recognition (ASR). They have also been found useful in hybrid learning models for noisy handwritten digit classification <ref type="bibr" target="#b1">[2]</ref>. Another closely related approach, which has gained much traction over the last decade, is the Convolutional Neural Network <ref type="bibr" target="#b22">[23]</ref>. This has been shown to outperform Deep Belief Network in classical object recognition tasks like MNIST <ref type="bibr" target="#b38">[39]</ref>, and CIFAR <ref type="bibr" target="#b19">[20]</ref>.</p><p>A related and equally hard problem is Satellite 1 image classification. It involves terabytes of data and significant variations due to conditions in data acquisition, pre-processing and filtering. Traditional supervised learning methods like Random Forests <ref type="bibr" target="#b5">[6]</ref> do not generalize well for such a large-scale learning problem. A novel classification algorithm for detecting roads in Aerial imagery using Deep Neural Networks was proposed in <ref type="bibr" target="#b25">[26]</ref>. The problem of detecting various land cover classes in general is a difficult problem considering the significantly higher intra-class variability in land cover types such as trees, grasslands, barren lands, water bodies, etc. as compared to that of roads. Also, in <ref type="bibr" target="#b25">[26]</ref>, the authors used a window of size 64×64 to derive contextual information. For our general classification problem, a 64×64 window is too big a context covering a total area of 64m×64m. A tree canopy, or a grassy patch can typically be much smaller than this area and hence we are constrained to use a contextual window having a maximum dimension of 28m×28m.</p><p>Traditional supervised learning approaches require carefully selected handcrafted features and substantial amounts of labeled data. On the other hand, purely unsupervised approaches are not able to learn the higher order dependencies inherent in the land cover classification problem. So, we propose a combination of handcrafted features that were first used in <ref type="bibr" target="#b13">[14]</ref> and an unsupervised learning framework using Deep Belief Network <ref type="bibr" target="#b14">[15]</ref> that can learn data representations from large amounts of unlabeled data.</p><p>There has been limited research in the field of satellite image classification due to a dearth of labeled satellite image datasets. The most well known labeled satellite dataset is the NLCD 2006 <ref type="bibr" target="#b37">[38]</ref>, which covers the entire globe and provide a spatial resolution of 30m. However, at this resolution, it becomes extremely dif-ficult to distinguish between various landcover types. A high-resolution dataset acquired at a spatial resolution of 1.2m was used in <ref type="bibr" target="#b25">[26]</ref>. However, the total area covered by the datasets namely URBAN1 and URBAN2 was ∼600 square kilometers, which included both training and testing datasets. The labeling was also available only for roads. Satellite/airborne image classification at a spatial resolution of 1-m was addressed in <ref type="bibr" target="#b0">[1]</ref>. However, they performed tree-cover delineation by training a binary classifier based on Feedforward Backpropagation Neural Networks.</p><p>The main contributions of our work are twofold -(1) We first present two labeled datasets of airborne images -SAT-4 and SAT-6 covering a total area of ∼800 square kilometers, which can be used to further the research and investigate the use of various learning models for airborne image classification. Both SAT-4 and SAT-6 are sampled from a much larger dataset <ref type="bibr" target="#b39">[40]</ref>, which covers the whole of continental United States and can be used to create labeled landcover maps, which can then be used for various applications such as measuring ground carbon content or estimating total area of rooftops for solar power generation.</p><p>(2) Next, we present a framework for the classification of satellite/airborne imagery that a) extracts features from the image, b) normalizes the features, and c) feeds the normalized feature vectors to a Deep Belief Network for classification. On the SAT-4 dataset, our framework outperforms three state-of-the-art object recognition algorithms -Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by ∼11% and produces an accuracy of 97.95%. On SAT-6, it produces an accuracy of 93.9% and outperforms the other algorithms by ∼15%. We also present a statistical analysis based on Distribution Separability Criterion and Intrinsic Dimensionality Estimation to justify the effectiveness of our feature extraction approach to obtain better representations for satellite data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset 2</head><p>Images were extracted from the National Agriculture Imagery Program (NAIP <ref type="bibr" target="#b39">[40]</ref>) dataset. The NAIP dataset consists of a total of 330,000 scenes spanning the whole of the Continental United States (CONUS). We used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ∼6000 pixels in width and ∼7000 pixels in height, measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ∼65 terabytes. The imagery is acquired at a ground sample distance (GSD) of 1 meter. The horizontal accuracy lies within 6 meters of ground control points identifiable from the acquired imagery <ref type="bibr" target="#b40">[41]</ref>. The images consist of 4 bands -red, green, blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class. Once labeled, 28×28 nonoverlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28×28 as the window size to maintain a significantly bigger context as pointed by <ref type="bibr" target="#b25">[26]</ref>, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch. Sample images from the dataset are shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SAT-4</head><p>SAT-4 consists of a total of 500,000 image patches covering four broad land cover classes. These include -barren land, trees, grassland and a class that consists of all land cover classes other than the above three. 400,000 patches (comprising of four-fifths of the total dataset) were chosen for training and the remaining 100,000 (one-fifths) were chosen as the testing dataset. We ensured that the training and test datasets belong to disjoint set of image tiles. Each image patch is size normalized to 28×28 pixels. Once generated, both the training and testing datasets were randomized using a pseudo-random number generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SAT-6</head><p>SAT-6 consists of a total of 405,000 image patches each of size 28×28 and covering 6 landcover classes -barren land, trees, grassland, roads, buildings and water bodies. 324,000 images (comprising of four-fifths of the total dataset) were chosen as the training dataset and 81,000 (one fifths) were chosen as the testing dataset. Similar to SAT-4, the training and test sets were selected from disjoint NAIP tiles. Once generated, the images in the dataset were randomized in the same way as that for SAT-4. The specifications for the various landcover classes of SAT-4 and SAT-6 were adopted from those used in the National Land Cover Data (NLCD) algorithm <ref type="bibr" target="#b43">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Investigation of various</head><p>Deep Learning Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Belief Network</head><p>Deep Belief Network (DBN) consists of multiple layers of stochastic, latent variables trained using an unsupervised learning algorithm followed by a supervised learning phase using feedforward backpropagation Neural Networks. In the unsupervised pre-training stage, each layer is trained using a Restricted Boltzmann Machine (RBM). Unsupervised pre-training is an important step in solving a classification problem with terabytes of data and high variability. A DBN is a graphical model <ref type="bibr" target="#b18">[19]</ref> where neurons of the hidden layer are conditionally independent of one another for a particular configuration of the visible layer and vice versa. A DBN can be trained layer-wise by iteratively maximizing the conditional probability of the input vectors or visible vectors given the hidden vectors and a particular set of layer weights. As shown in <ref type="bibr" target="#b14">[15]</ref>, this layer-wise training can help in improving the variational lower bound on the probability of the input training data, which in turn leads to an improvement of the overall generative model. We first provide a formal introduction to the Restricted Boltzmann Machine. The RBM can be denoted by the energy function:</p><formula xml:id="formula_0">E(v, h) = − i a i v i − j b j h j − i j h j w i,j v i<label>(1)</label></formula><p>where, the RBM consists of a matrix of layer weights W = (w i,j ) between the hidden units h j and the visible units v i . The a i and b j are the bias weights for the visible units and the hidden units respectively. The RBM takes the structure of a bipartite graph and hence it only has inter-layer connections between the hidden or visible layer neurons but no intra-layer connections within the hidden or visible layers. So, the activations of the visible unit neurons are mutually independent for a given set of hidden unit activations and vice versa <ref type="bibr" target="#b6">[7]</ref>. Hence, by setting either h or v constant, we can compute the conditional distribution of the other as follows:</p><formula xml:id="formula_1">P (h j = 1|v) = σ(b j + m i=1 w i,j v i ) (2) P (v i = 1|h) = σ(a i + n j=1 w i,j h j )<label>(3)</label></formula><p>where, σ denotes the log sigmoid function:</p><formula xml:id="formula_2">f (x) = 1 1 + e −x<label>(4)</label></formula><p>The training algorithm maximizes the expected log probability assigned to the training dataset V . So if the training dataset V consists of the visible vectors v, then the objective function is as follows:</p><formula xml:id="formula_3">argmax W E v∈V log P (v)<label>(5)</label></formula><p>A RBM is trained using a Contrastive Divergence algorithm <ref type="bibr" target="#b6">[7]</ref>. Once trained, the DBN can be used to initialize the weights of the Neural Network for the supervised learning phase <ref type="bibr" target="#b2">[3]</ref>.</p><p>Next, we investigate the classification accuracy of various architectures of DBN on both SAT-4 and SAT-6 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">DBN Results on SAT-4 &amp; SAT-6</head><p>To investigate the performance of the DBN, we experiment with both big and deep neural architectures. This is done by varying the number of neurons per layer as well as the total number of layers in the network. Our objective is to investigate whether the more complex features learned in the deeper layers of the DBN are able to provide the network with the discriminative power required to handle higherorder texture features typical of satellite imagery data. The results from the DBN for various network architectures for SAT-4 and SAT-6 are enumerated in <ref type="table" target="#tab_0">Table 1</ref>. Each network was trained for a maximum of 500 epochs and the network state with the lowest validation error was used for testing. Regularization is done using L 2 norm-regularization. It can be seen from the table that for both SAT-4 and SAT-6, the classifier accuracy initially improves and then falls as more neurons or layers are added to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Neural Network</head><p>Convolutional Neural Network (CNN) first introduced in <ref type="bibr" target="#b12">[13</ref>] is a hierarchical model inspired by the human visual cortical system <ref type="bibr" target="#b15">[16]</ref>. It was significantly improved and applied to document recognition in <ref type="bibr" target="#b22">[23]</ref>. A committee of 35 convolutional neural nets with elastic distortions and width normalization <ref type="bibr" target="#b8">[9]</ref> has produced state-of-the-art results on the MNIST handwritten digits dataset. CNN consists of SAT-4 (%) SAT-6 (%) 100 <ref type="bibr" target="#b1">[2]</ref> 79.74 68.51 100 <ref type="bibr" target="#b2">[3]</ref> 81.78 76.47 100 <ref type="bibr" target="#b3">[4]</ref> 79.802 74.44 100 <ref type="bibr" target="#b4">[5]</ref> 62.776 63.14 500 <ref type="bibr" target="#b1">[2]</ref> 68.916 60.35 500 <ref type="bibr" target="#b2">[3]</ref> 71.674 61.12 500 <ref type="bibr" target="#b3">[4]</ref> 65.002 57.31 500 <ref type="bibr" target="#b4">[5]</ref> 64.174 55.78 a hierarchical representation using convolutional layers and fully connected layers, with non-linear transformations and feature pooling. They also include local or global pooling layers. Pooling can be implemented in the form of subsampling, averaging, max-pooling or stochastic pooling. Each of these pooling architectures has its own advantages and limitations and numerous studies are in place that investigate the effect of different pooling functions on representation power of the model ( <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b29">[30]</ref>). A very important feature of Convolutional Neural Network is weight sharing in the convolutional layers, so that the same filter bank is applied to all pixels in a particular layer; thereby generating sparse networks that can generalize well to unseen data samples while maintaining the representational power inherent in deep hierarchical architectures.</p><p>We investigate the use of different CNN architectures for SAT-4 and SAT-6 as detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">CNN Results on SAT-4 &amp; SAT-6</head><p>For CNN, we vary the number of feature maps in each layer as well as the total number of convolutional and subsampling layers. The results from various network configurations with increasing number of maps and layers is enumerated in <ref type="table" target="#tab_1">Table  2</ref>. For the experiments, we used both 3×3 and 5×5 kernels for the convolutional layers and 3×3 averaging and max-pooling kernels for the sub-sampling layers. We also use overlapping pooling windows with a stride size of 2 pixels. The last subsampling layer is connected to a fully-connected layer with 64 neurons. The output of the fully-connected layer is fed into a 4-way softmax function that generates a probability distribution over the 4 class labels of SAT-4 and a 6-way softmax for the 6 class labels of SAT-6. In <ref type="table" target="#tab_1">Table 2</ref>, the "Ac-Bs(n)" notation denotes that the network has a convolutional layer with A feature maps followed by a sub-sampling layer with a kernel of size B×B. 'n' denotes the type of pooling function in the sub-sampling layer, 'a' denotes average pooling while 'm' denotes max-pooling.</p><p>From the table, it can be seen that the smallest networks consistently produce the best results. Also, both for SAT-4 and SAT-6, using networks with convolution kernels of size 3×3 leads to a significant drop in classifier accuracy. The biggest networks with 50 maps per layer also exhibit significant drop in classifier accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stacked Denoising Autoencoder</head><p>A Stacked Denoising Autoencoder (SDAE) <ref type="bibr" target="#b36">[37]</ref> consists of a combination of multiple sparse autoencoders, which can be trained in a greedy-layerwise fashion similar to that of Restricted Boltzmann Machines in a DBN. Each autoencoder is associated with a set of weights and biases. In the SDAE, each layer can be trained independent of the other layers. Once trained, the parameters of an autoencoder are frozen in place. The training algorithm is comprised of two phases -a forward pass phase and a backward pass phase. The forward pass, also called as the encoding phase encodes raw image pixels into an increasingly higher-order representation. The backward pass simply performs the reverse operation by decoding these higher-order features into simpler representations. The encoding step is given as:</p><formula xml:id="formula_4">a (l) = f (z (l) )<label>(6)</label></formula><formula xml:id="formula_5">z (l+1) = W (l,1) a (l) + b (l,1)<label>(7)</label></formula><p>And the decoding step is as follows:</p><formula xml:id="formula_6">a (n+l) = f (z (n+l) )<label>(8)</label></formula><formula xml:id="formula_7">z (n+l+1) = W (n−l,2) a (n+l) + b (n−l,2)<label>(9)</label></formula><p>The hidden unit activations of the neurons in the deepest layer are used for classification after a supervised fine-tuning using backpropagation. Different network configurations were chosen for the SDAE in a manner similar to that described above for DBN and CNN. The results are enumerated in <ref type="table" target="#tab_2">Table  3</ref>. Similar to DBN, each network is trained for a maximum of 500 epochs and the lowest test error is considered for evaluation. As highlighted in the <ref type="table">Table,</ref> networks with 5 layers and 100 neurons in each layer produce the best results on both SAT-4 and SAT-6. It can be seen from the table that on both datasets, the classifier accuracy initially improves and then drops with increasing number of neurons and layers, similar to that of DBN. Also, the biggest networks with 500 and 2352 neurons in each layer exhibit a significant drop in classifier accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Arch. Classifier</head><p>Classifier Neurons/layer Accuracy Accuracy <ref type="bibr">[Layers]</ref> SAT-4 (%) SAT-6 (%) 100 <ref type="bibr" target="#b0">[1]</ref> 75.88 74.89 100 <ref type="bibr" target="#b1">[2]</ref> 76.854 76.12 100 <ref type="bibr" target="#b2">[3]</ref> 77.804 76.45 100 <ref type="bibr" target="#b3">[4]</ref> 78.674 76.52 100 <ref type="bibr" target="#b4">[5]</ref> 79.978 78.43 100 <ref type="bibr" target="#b5">[6]</ref> 75.766 76.72 500 <ref type="bibr" target="#b2">[3]</ref> 63.832 54.37 2352 <ref type="bibr" target="#b1">[2]</ref> 51.766 37.121  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Extraction</head><p>The feature extraction phase computes 150 features from the input imagery. The key features that we use for classification are mean, standard deviation, variance, 2nd moment, direct cosine transforms, correlation, co-variance, autocorrelation, energy, entropy, homogeneity, contrast, maximum probability and sum of variance of the hue, saturation, intensity, and NIR channels as well as those of the color co-occurrence matrices. These features were shown to be useful descriptors for classification of satellite imagery in previous studies ( <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b9">[10]</ref>). Since two of the classes in SAT-4 and SAT-6 are trees and grasslands, we incorporate features that are useful determinants for segregation of vegetated areas from non-vegetated ones. The red band already provides a useful feature for discrimination of vegetated and non-vegetated areas based on chlorophyll reflectance, however, we also use derived features (vegetation indices derived from spectral band combinations) that are more representative of vegetation greenness -this includes the Enhanced Vegetation Index (EVI <ref type="bibr" target="#b16">[17]</ref>), Normalized Difference Vegetation Index (NDVI <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b34">[35]</ref>) and Atmospherically Resistant Vegetation Index (ARVI <ref type="bibr" target="#b17">[18]</ref>). These indices are expressed as follows:</p><formula xml:id="formula_8">EV I = G × N IR − Red N IR + c red × Red − c blue × Blue + L<label>(10)</label></formula><p>Here, the coefficients G, c red , c blue and L are chosen to be 2.5, 6, 7.5 and 1 following those adopted in the MODIS EVI algorithm <ref type="bibr" target="#b40">[41]</ref>.</p><formula xml:id="formula_9">N DV I = N IR − Red N IR + Red<label>(11)</label></formula><formula xml:id="formula_10">ARV I = N IR − (2 × Red − Blue) N IR + (2 × Red + Blue)<label>(12)</label></formula><p>The performance of our learner depends to a large extent on the selected features. Some features contribute more than others towards optimal classification. The 150 features extracted are narrowed down to 22 using a feature-ranking algorithm based on Distribution Separability Criterion <ref type="bibr" target="#b4">[5]</ref>. Details of the feature ranking method along with the ranking for all the 22 features used in our framework is listed in Section 6.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Normalization</head><p>The feature vectors extracted from the training and test datasets are separately normalized to lie in the range [0, 1]. This is done using the following equation:</p><formula xml:id="formula_11">F normalized = F − F min F max − F min<label>(13)</label></formula><p>where, F min and F max are computed for a particular feature type over all images in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification</head><p>The set of normalized feature descriptors extracted from the input image is fed into the DBN, which is then trained using Contrastive divergence in the same way as explained in Section 3.1. Once trained the DBN is used to initialize the weights of a feedforward backpropagation neural network.</p><p>The neural network gives an estimate of the posterior probabilities of the class labels, given the input vectors, which is the feature vector in our case. As illustrated in <ref type="bibr" target="#b3">[4]</ref>, the outputs of a neural network which is obtained by optimizing the sumsquared error-gradient function approximates the average of the class conditional distributions of the target variables</p><formula xml:id="formula_12">y k (x) = t k |x = t k p(t k |x)dt k<label>(14)</label></formula><p>Here, t k are the set of target values that represent the class membership of the input vector x k . For a binary classification problem, in order to map the outputs of the neural network to the posterior probabilities of the labeling, we use a single output y and a target coding that sets t n = 1 if x n is from class C 1 and t n = 0 if x n is from class C 2 . The target distribution would then be given as</p><formula xml:id="formula_13">p(t k |x) = δ(t − 1)P (C 1 |x) + δ(t)P (C 2 |x)<label>(15)</label></formula><p>Here, δ denotes the Dirac delta function which has the properties δ(x) = 0 if</p><formula xml:id="formula_14">x = 0 and ∞ −∞ δ(x) dx = 1<label>(16)</label></formula><p>From 14 and 15, we get</p><formula xml:id="formula_15">y(x) = P (C 1 |x)<label>(17)</label></formula><p>So, the network output y(x) represents the posterior probability of the input vector x having the class membership C 1 and the probability of the class membership C 2 is given by P (C 2 |x) = 1 − y(x). This argument can easily be extended to multiple class labels for a generalized multi-class classification problem.</p><p>The feature extraction phase proves to be a useful dimensionality reduction technique that helps improve the discriminative power of the DBN based classifier significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Comparative Studies</head><p>The feature vectors extracted from the dataset are fed into DBNs with different configurations. Since, the feature vectors create a low dimensional representation of the data, so, DeepSat converges to high accuracy even with a much smaller network with fewer layers and very few neurons per layer. This speeds up network training by several orders of magnitude. Various network architectures along with the classification accuracy for DeepSat on the SAT-4 and SAT-6 datasets are listed in <ref type="table" target="#tab_3">Table 4</ref>. For regularization, we again use L 2 norm-regularization. From the <ref type="table">Table,</ref> it is evident that the best performing DeepSat network outperforms the best traditional Deep Learning approach (CNN) by ∼11% on the SAT-4 dataset and by ∼15% on the SAT-6 dataset.</p><p>We also compare DeepSat with a Random Forest classifier to investigate the advantages gained by unsupervised pre-training in DBN as opposed to the traditional supervised learning in Random Forests. On SAT-4, the Random forest classifier produces an accuracy of 69% while on SAT-6, it produces an accuracy of 54%. The highest accuracy was obtained for a forest with 100 trees. Further increase in the number of trees did not yield any significant improvement in classifier accuracy. It can be easily seen that the various Deep architectures produce better classification accuracy than the Random Forest classifier which relies solely on supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Arch. Classifier Classifier Neurons/layer Accuracy Accuracy [Layers]</head><p>SAT-4 (%) SAT-6 (%) 10 <ref type="bibr" target="#b1">[2]</ref> 96.585 91.91 10 <ref type="bibr" target="#b2">[3]</ref> 96.8 87.716 20 <ref type="bibr" target="#b1">[2]</ref> 97.115 86.21 20 <ref type="bibr" target="#b2">[3]</ref> 95.473 93.42 50 <ref type="bibr" target="#b1">[2]</ref> 97.946 93.916 50 <ref type="bibr" target="#b2">[3]</ref> 97.654 92.65 100 <ref type="bibr" target="#b1">[2]</ref> 97.292 89.08 100 <ref type="bibr" target="#b2">[3]</ref> 95.609 91.057  While traditional Deep Learning approaches have produced state-of-the-art results for various pattern recognition problems like handwritten digit recognition <ref type="bibr" target="#b38">[39]</ref>, object recognition <ref type="bibr" target="#b19">[20]</ref>, face recognition <ref type="bibr" target="#b32">[33]</ref>, etc., but satellite datasets have high intra and inter-class variability and the amount of labeled data is much smaller as compared to the total size of the dataset. Also, higher-order texture features are a very important discriminative parameter for various landcover classes. On the contrary, shape/edge based features which are predominantly learned by various Deep architectures are not very useful in learning data representations for satellite imagery. This explains the fact why traditional Deep architectures are not able to converge to the global optima even for reasonably large as well as Deep architectures. Also, spatially contextual information is another important parameter for modeling satellite imagery. In traditional Deep Learning approaches like DBN and SDAE, the relative spatial information of the pixels is lost. As a result the orderless pool of pixel values which acts as input to the Deep Networks lack sufficient discriminative power to be well-represented even by very big and/or deep networks. CNN however, involves feature-pooling from a local spatial neighborhood, which justifies its improved performance over the other two algorithms on both SAT-4 and SAT-6. Even though our approach extracts an orderless pool of feature vectors, the spatial context is already well-represented in the individual feature values themselves. We substantiate our arguments about the effectiveness of our feature extraction approach from a statistical point of view as detailed in the analysis below.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dist. b/w Standard Means Deviations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">A Statistical Perspective based on Distribution Separability Criterion</head><p>Improving classification accuracy can be viewed as maximizing the separability between the class-conditional distributions. Following the analysis presented in <ref type="bibr" target="#b4">[5]</ref>, we can view the problem of maximizing distribution separability as maximizing the distance between distribution means and minimizing their standard deviations. <ref type="figure" target="#fig_5">Figure 3</ref> shows the histograms that represent the class-conditional distributions of the NIR channel and a sample feature extracted in the DeepSat framework. As illustrated in <ref type="table" target="#tab_5">Table 5</ref>, the features extracted in DeepSat have a higher distance between means and a lower standard deviation as compared to the original image distributions, thereby ensuring better class separability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Feature Ranking</head><p>Following the analysis proposed in Section 6.1 above, we can derive a metric for the Distribution Separability Criterion as follows:</p><formula xml:id="formula_16">D s = δ mean δ σ<label>(18)</label></formula><p>where δ mean indicates the mean of distance between means and δ σ indicates the mean of standard deviations of the class conditional distributions. Maximizing D s over the feature space, a feature ranking can be obtained. <ref type="table" target="#tab_7">Table 6</ref> shows the ranking of the various features used in our framework along with the values of the corresponding distance between means δ mean , standard deviation δ σ and Distribution Separability Criterion D s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Distribution Separability and Classifier Accuracy</head><p>In order to analyze the improvements achieved in the learning framework due to the feature extraction step, we measured the Distribution Separability of the mean  activation of the neurons in each layer of the DBN and that of DeepSat. The results are noted in <ref type="figure" target="#fig_6">Figure 4</ref>. It can be seen that the mean activation learned by each layer of DeepSat exhibit a significantly higher distribution separability (by several orders of magnitude) than the neurons of a DBN. This justifies the significant improvement in performance of DeepSat (using the features) as compared to the DBN based framework (using the raw pixel values as input). Also, a comparison of <ref type="figure" target="#fig_6">Figure 4</ref> with <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_3">Table 4</ref> shows that the distribution separabilities using the various architectures of the DBN and DeepSat are positively correlated to the final classifier accuracy. This justifies the effectiveness of our distribution separability metric D s as a measure of the final classifier accuracy. 7 What is the difference between MNIST, CIFAR-10 and SAT-6 in terms of dimensionality?</p><p>We argue that handwritten digit datasets like MNIST and object recognition datasets like CIFAR-10 lie on a much lower dimensional manifold than the airborne SAT-6 dataset. Hence, even if Deep Neural Networks can effectively classify the raw feature space of object recognition datasets but the dimensionality of the airborne image datasets is such that Deep Neural Networks cannot classify them. In order to estimate the dimensionality of the datasets, we use the concept of intrinsic dimension <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Intrinsic Dimension Estimation using the DanCo algorithm</head><p>To estimate the intrinsic dimension of a dataset, we use the DANCo algorithm <ref type="bibr" target="#b7">[8]</ref>. It uses the complementary information provided by the normalized nearest neighbor distances and angles calculated on pairs of neighboring points. Taking 10 rounds of 1000 random samples and averaging, we obtain the intrinsic dimension for the MNIST, CIFAR-10 and SAT-6 datasets and the Haralick features extracted from the SAT-6 dataset. The results are listed in <ref type="table" target="#tab_9">Table 7</ref>.</p><p>So, it can be seen that the intrinsic dimensionality of the SAT-6 dataset is orders of magnitude higher than that of MNIST. So, a deep neural network finds it difficult to classify the SAT-6 dataset because of its intrinsically high dimensionality. However, as seen in the equation above, the features extracted from SAT-6 have a much lower intrinsic dimensionality and lie on a much lower dimensional manifold than the raw vectors and hence can be classified even by networks with relatively smaller architectures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intrinsic Dimension</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Visualizing Data in an n-dimensional space</head><p>We can visualize the data as distributed in an n-dimensional unit hypersphere Volume of the sphere,</p><formula xml:id="formula_17">V sphere = π n 2 Γ( n 2 + 1) R n = π n 2 Γ( n 2 + 1)<label>(19)</label></formula><p>for n-dimensional Euclidean space and Γ is Euler's gamma function. Now, the total volume of the n-dimensional space can be accounted by the volume of an ndimensional hypercube of length 2 embedding the hypersphere, i.e, Volume of the n-cube,</p><formula xml:id="formula_18">V cube = R n = 2 n<label>(20)</label></formula><p>So, the relative fraction of the data points which lie on the sphere as compared to the data points on the n-dimensional embedding space is given as</p><formula xml:id="formula_19">V relative = V sphere V cube = π n 2</formula><p>2 n Γ( n 2 + 1)</p><p>V relative → 0 as n → ∞</p><p>This means that as the dimensionality of sample data approaches ∞, the spread or scatter of the data points approaches 0 with respect to the total search space. As a result, various classification and clustering algorithms lose their discriminative power in higher dimensional feature spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Present classification algorithms used for Moderate-resolution Imaging Spectroradiometer (MODIS)(500-m) <ref type="bibr" target="#b11">[12]</ref> or Landsat(30-m) based land cover maps like NLCD <ref type="bibr" target="#b37">[38]</ref> produce accuracies of 75% and 78% resp. The relatively lower resolution of the datasets makes it difficult to analyze the performance of these algorithms for 1-m imagery. A method based on object detection using Bayes framework and subsequent clustering of the objects using Latent Dirichlet Allocation was proposed in <ref type="bibr" target="#b35">[36]</ref>. However, their approach detects object groups at a higher level of abstraction like parking lots. Detecting the objects like cars or trees in itself is not addressed in their work. A deep convolutional hierarchical framework was proposed recently by <ref type="bibr" target="#b27">[28]</ref>. However, they report results on the AVIRIS Indiana's Indian Pines test site. The spatial resolution of the dataset is limited to 20m and it is difficult to evaluate the performance of their algorithm for object recognition tasks at a higher resolution. An evaluation of various feature learning strategies was done in <ref type="bibr" target="#b33">[34]</ref>. They evaluated both feature extraction techniques as well as classifiers like DBN and Random Forest for various aerial datasets. However, since the training data was significantly limited, the DBN was not able to produce any improvements over Random Forest even when raw pixel values were fed into the classifier. In contrast, our study shows that DBNs can be better classifiers when there is significant amount of training data to initialize the neural network at a global error basin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions and Future Directions</head><p>Our semi-supervised learning framework produces an accuracy of 97.95% and 93.9% on the SAT-4 and SAT-6 datasets and significantly outperforms the stateof-the-art by ∼11% and ∼15% respectively. The Feature extraction phase is inspired by the remote sensing literature and significantly improves the discriminative power of the framework. For satellite datasets, with inherently high variability, traditional deep learning approaches are unable to converge to a global optima even with significantly big and deep architectures. A statistical analysis based on Distribution Separability Criterion justifies the effectiveness of our feature extraction approach. We plan to investigate the use of various pooling techniques like SPM <ref type="bibr" target="#b20">[21]</ref> as well as certain sparse representations like sparse coding <ref type="bibr" target="#b23">[24]</ref> and Hierarchical representations like Convolutional DBN <ref type="bibr" target="#b24">[25]</ref> to handle satellite datasets. We believe that SAT-4 and SAT-6 will enable researchers to learn better representations for satellite datasets and create benchmarks for the classification of satellite imagery.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sample images from the SAT-6 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>m)(5×5) 50c-3s(a)-50c-3s(m)-50c 73.85 75.689 -3s(m)(5×5) 6c-3s(a)-12c-3s(m) (3×3) 73.811 54.385 6c-3s(m)-12c-3s(m) (5×5) 85.612 77.636</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Schematic of the DeepSat classification framework 3.3.1 SDAE Results on SAT-4 &amp; SAT-6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2</head><label>2</label><figDesc>schematically describes our proposed classification framework. Instead of the traditional DBN model described in Section 3.1, which takes as input the multichannel image pixels reshaped as a linear vector, our classification framework first extracts features from the image which in turn are fed as input to the DBN after normalizing the feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Distributions of the raw NIR values for traditional Deep Learning Algorithms and a sample DeepSat feature for various classes on SAT-4 (Best viewed in color) 6 Why Traditional Deep Architectures are not enough for SAT-4 &amp; SAT-6?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>(a) Distribution Separability Criterion of DBN (b) Distribution Separability Criterion of DeepSat Distribution Separability Criterion of the neurons in the layers of a DBN and DeepSat with various architectures on SAT-6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Classification Accuracy of DBN with various architectures on SAT-4 and SAT-6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Classification Accuracy of CNN with various architectures on SAT-4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>4 and</cell></row></table><note>Classification Accuracy of SDAE with various architectures on SAT-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Classification Accuracy of DeepSat with various network architectures on</cell></row><row><cell>SAT-4 and SAT-6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Distance between Means and Standard Deviations for raw image values and DeepSat feature vectors for SAT-4 and SAT-6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ranking of features based on Distribution Separability Criterion for SAT-6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Intrinsic Dimension estimation using DANCo on the MNIST, CIFAR-10, and SAT-6 datasets and the Haralick features extracted from the SAT-6 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we use the terms satellite and airborne interchangeably in this paper because the extracted features and learning algorithms are generic enough to handle both satellite and airborne datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The SAT-4 and SAT-6 datasets are available at the web link<ref type="bibr" target="#b42">[42]</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgments</head><p>The project is supported by NASA Carbon Monitoring System through Grant #NN-H14ZDA001-N-CMS and Army Research Office (ARO) under Grant #W911NF1-010495. We are grateful to the United States Department of Agriculture for providing us the National Agriculture Imagery Program (NAIP) airborne imagery dataset for the Continental United States.</p><p>This research was partially supported by the Cooperative Agreement Number NASA-NNX12AD05A, CFDA Number 43.001, for the project identified as "Ames Research Center Cooperative for Research in Earth Science and Technology (ARC-CREST)". Any opinions findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect that of NASA, ARO or the United States Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A semiautomated probabilistic framework for tree-cover delineation from 1-m naip imagery using a high-performance computing architecture. Geoscience and Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Milesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Votava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dubayah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duncanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5690" to="5708" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning sparse feature representations using probabilistic quadtrees and deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Symposium on Artificial Neural Networks</title>
		<meeting>the European Symposium on Artificial Neural Networks</meeting>
		<imprint>
			<publisher>ESANN</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI. Found</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural Networks for Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Oxford University Press, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th International Conference on Machine Learning</title>
		<meeting><address><addrLine>Haifa, Isreal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On contrastive divergence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Danco: An intrinsic dimensionality estimator exploiting angle and norm concentration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ceruti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bassis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Casiraghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2569" to="2581" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12</title>
		<meeting>the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis of co-occurrence texture statistics as a function of grey level quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="62" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modis collection 5 global land cover: Algorithm refinements and characterization of new datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sulla-Menashe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramankutty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sibley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="168" to="182" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textural features for image classification. Systems, Man and Cybernetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction, and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overview of the radiometric and biophysical performance of the MODIS vegetation indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Didan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="195" to="213" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Atmospherically resistant vegetation index (arvi) for eos-modis. Geoscience and Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="1992-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques -Adaptive Computation and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to detect roads in high-resolution aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 11th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised deep feature extraction of hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monitoring vegetation systems in the great plains with ERTS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Schell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Deering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NASA Goddard Space Flight Center 3d ERTS-1 Symposium</title>
		<imprint>
			<biblScope unit="page" from="309" to="317" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On random weights and unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bhand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML &apos;11</title>
		<editor>L. Getoor and T. Scheffer, editors</editor>
		<meeting>the 28th International Conference on Machine Learning (ICML-11), ICML &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1089" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks -ICANN 2010</title>
		<editor>K. Diamantaras, W. Duch, and L. Iliadis</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6354</biblScope>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Texture analysis of sar sea ice imagery using gray level co-occurrence matrices. Geoscience and Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tsatsoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="page" from="780" to="795" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An evaluation of feature learning methods for high resolution image classification. ISPRS Annals of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Montoya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences, I</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="389" to="394" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Red and photographic infrared linear combinations for monitoring vegetation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="150" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning in very high resolution remote sensing image information mining communication concept</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vaduva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gavat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Conference (EUSIPCO), 2012 Proceedings of the 20th European</title>
		<imprint>
			<date type="published" when="2012-08" />
			<biblScope unit="page" from="2506" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Accuracy assessment of nlcd 2006 land cover and impervious surface. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wickham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Stehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dewitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Wade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="294" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Www1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnist</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Www2</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Naip</surname></persName>
		</author>
		<ptr target="http://www.fsa.usda.gov/Internet/FSA_File/naip_2009_info_final.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Www3</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modis</surname></persName>
		</author>
		<ptr target="http://vip.arizona.edu/documents/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modis/Modis_Vi_Usersguide_01_2012</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Www4</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datasets</surname></persName>
		</author>
		<ptr target="http://csc.lsu.edu/˜saikat/deepsat/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Www5</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nlcd</surname></persName>
		</author>
		<ptr target="http://www.gsd.harvard.edu/gis/manual/earthshelter/National%20Land-Cover%20Dataset%20%28NLCD%29%20Metadata%20%20US%20EPA.htm" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
