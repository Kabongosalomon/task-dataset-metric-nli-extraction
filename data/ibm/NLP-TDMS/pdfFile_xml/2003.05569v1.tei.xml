<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extended Batch Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjie</forename><surname>Luo</surname></persName>
							<email>luochunjie@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhan</surname></persName>
							<email>zhanjianfeng@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanling</forename><surname>Gao</surname></persName>
							<email>gaowanling@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Extended Batch Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Neural networks, Normalization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Batch normalization (BN) has become a standard technique for training the modern deep networks. However, its effectiveness diminishes when the batch size becomes smaller, since the batch statistics estimation becomes inaccurate. That hinders batch normalizations usage for 1) training larger model which requires small batches constrained by memory consumption, 2) training on mobile or embedded devices of which the memory resource is limited. In this paper, we propose a simple but effective method, called extended batch normalization (EBN). For NCHW format feature maps, extended batch normalization computes the mean along the (N, H, W) dimensions, as the same as batch normalization, to maintain the advantage of batch normalization. To alleviate the problem caused by small batch size, extended batch normalization computes the standard deviation along the (N, C, H, W) dimensions, thus enlarges the number of samples from which the standard deviation is computed. We compare extended batch normalization with batch normalization and group normalization on the datasets of MNIST, CIFAR-10/100, STL-10, and ImageNet, respectively. The experiments show that extended batch normalization alleviates the problem of batch normalization with small batch size while achieving close performances to batch normalization with large batch size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have received great success in many areas. Batch normalization (BN) has become a standard technique for training the modern deep networks. It normalizes the features by the mean and the standard deviation computed within a batch of samples. When training with batch normalization, examples are seen in conjunction with each other in the minibatch. The model looks at multiple training examples in integration, rather than in isolation. The coordination between examples helps the learning process. Moreover, the random selection of examples in the minibatch brings the sampling noises, that provides similar regularization benefits as Dropout <ref type="bibr" target="#b25">[26]</ref>.</p><p>However, its effectiveness diminishes when the batch size becomes smaller, since the noises are too much and make inaccurate batch statistics estimation.</p><p>That hinders batch normalizations usage for 1) training larger (deeper and wider) models which requires small batches constrained by memory consumption. 2) training on mobile or embedded devices of which the memory resource is limited. Training model on edge devices has received more and more attention. For example, federated learning, a hot topic in machine learning, aims at training a model across multiple decentralized edge devices or servers to address privacy and security issues. The heterogeneous environments require a robust training algorithm with large or small batch size.</p><p>Several normalization methods have been proposed to address the problem caused by small batch size, e.g. normalization propagation <ref type="bibr" target="#b0">[1]</ref>, batch renormalization <ref type="bibr" target="#b7">[8]</ref>, kalman normalization <ref type="bibr" target="#b27">[28]</ref>. The complexity limits their usage in the community. Group normalization <ref type="bibr" target="#b28">[29]</ref> is simple and effective method to handle the training with small batch size. However, there is a extra hyper parameter G (the number of group) to be tuned in group normalization. When G is large, the number of channels per group decreases. Then the estimation of the mean and the standard deviation (std) may be inaccurate. As G equals to the number of channel of the layer, group normalization becomes instance normalization <ref type="bibr" target="#b26">[27]</ref>. When G is small, the number of channels in the same group increases, but this makes the channels more correlative with each other. When G equals to 1, group normalization becomes layer normalization <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this paper, we propose a simple but effective method, called extended batch normalization (EBN). The key difference between extended batch normalization and other normalization methods is that extended batch normalization compute the mean and the standard deviation from different set of pixels. For NCHW format feature, let N refer to batch dimension, C refer to channel dimension, H and W refer to the spatial height and width dimensions. To maintain the advantage of batch normalization, extended batch normalization computes the mean along the (N,H,W) dimensions just as the same as batch normalization. To alleviate the problem caused by small batch size, extended batch normalization computes the standard deviation along the (N, C, H, W) dimensions, thus enlarges the number of pixel samples from which the standard deviation is computed.</p><p>At inference time for extended batch normalization, the mean and the standard deviation are pre-computed from the training set by moving average as the same as batch normalization. There is no need to compute the mean and the standard deviation at inference time comparing to group normalization. Moreover, since the mean and the standard deviation are pre-computed and fixed at inference time, the normalization can be fused into convolution operation. That is very helpful to speed up the inference especially on the mobile or embedded devices.</p><p>We compare extended batch normalization with batch normalization and group normalization on the datasets of MNIST, CIFAR-10/100, STL-10, and ImageNet, respectively. The experiments show that extended batch normalization alleviates the problem of batch normalization with small batch size while achieving close performances to batch normalization with large batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Batch normalization <ref type="bibr" target="#b8">[9]</ref> is effective at accelerating and improving the training of deep neural networks by reducing internal covariate shift. It performs the normalization for each training minibatch along (N,H,W) dimensions in the case of NCHW format feature. Since batch normalization uses the statistics on minibatch examples, its effect is dependent on the minibatch size.</p><p>To mitigate this problem, normalization propagation <ref type="bibr" target="#b0">[1]</ref> uses a data-independent parametric estimate of the mean and standard deviation instead of explicitly calculating from data. Normalization propagation depends on the strong assumptions that the activation values follow Gaussian distribution and the weight matrix of hidden layers are roughly incoherent.</p><p>Batch renormalization <ref type="bibr" target="#b7">[8]</ref> introduces two extra parameters to correct the fact that the minibatch statistics differ from the population ones. It needs train the model for a certain number of iterations with batch normalization alone, without the correction, then ramp up the amount of allowed correction.</p><p>There is a family of methods which avoid normalizing along the batch dimension. Layer normalization <ref type="bibr" target="#b1">[2]</ref> computes the mean and standard deviation along (C,H,W) dimensions. Instance normalization <ref type="bibr" target="#b26">[27]</ref> computes the mean and standard deviation along (H,W) dimensions. When the batch size is 1, batch normalization is equivalent to instance normalization. Group normalization <ref type="bibr" target="#b28">[29]</ref> is a intermediate state between layer normalization and instance normalization. It uses a group of channels to compute the mean and standard deviation, while layer normalization uses all channels, and instance normalization uses one channel.</p><p>Weight normalization <ref type="bibr" target="#b21">[22]</ref> normalizes the filter weights instead of the activations by re-parameterizing the incoming weight vector. Cosine normalization <ref type="bibr" target="#b13">[14]</ref> normalizes both the filter weights and the activations by using cosine similarity or Pearson correlation coefficient instead of dot product in neural networks.</p><p>Kalman normalization <ref type="bibr" target="#b27">[28]</ref> estimates the mean and standard deviation of a certain layer by considering the distributions of all its preceding layers, mimicking the merits of Kalman Filtering Process. It takes much overhead since it introduces a transition matrix and a covariance matrix to carry out Kalman transform, and needs sampling according to the distribution of previous layer before Kalman transform. On the other hand, it can be combined with other normalization, e.g. batch Kalman normalization, group Kalman normalization. Extended batch normalization could also be equipped with Kalman normalization.</p><p>Some researches try to use other statistics instead of mean and standard deviation in normalization. Instead of the standard L 2 batch normalization, <ref type="bibr" target="#b6">[7]</ref> uses normalization in L 1 and L ∞ spaces, and shows that can improve numerical stability in low-precision implementations as well as provide computational and memory benefits. Generalized batch normalization <ref type="bibr" target="#b29">[30]</ref> investigates a variety of alternative deviation measures for scaling and alternative mean measures for centering.</p><p>Batch-instance normalization <ref type="bibr" target="#b17">[18]</ref> uses a learnable gate parameter to combine batch and instance normalization together, and switchable normalization <ref type="bibr" target="#b14">[15]</ref> uses learnable parameters to combine batch, instance and layer normalization. Virtual batch normalization <ref type="bibr" target="#b20">[21]</ref> and spectral normalization <ref type="bibr" target="#b16">[17]</ref> focus on the normalization in generative adversarial networks. Self-Normalizing <ref type="bibr" target="#b9">[10]</ref> focuses on standard feed-forward neural networks (fully-connected networks). Recurrent batch normalization <ref type="bibr" target="#b3">[4]</ref> modifies batch normalization to use in recurrent networks. EvalNorm <ref type="bibr" target="#b24">[25]</ref> estimates corrected normalization statistics to use for batch normalization during evaluation. <ref type="bibr" target="#b18">[19]</ref> provides a unifying view of the different normalization approaches. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b2">[3]</ref> try to explain how batch normalization works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extended Batch Normalization</head><p>We first describe some notations which will be used next. In the case of NCHW format feature, let U denote the universal set of the features in the same layer, and i = (i N , i C , i H , i W ) refer to a 4D tensor indexing the features, N is the batch dimension, C is the channel dimension, H and W are the spatial height and width dimensions. A family of normalization can be formalized as:</p><formula xml:id="formula_0">x i = 1 σ S i (x i − µ Si )<label>(1)</label></formula><formula xml:id="formula_1">y i = γ x i + β<label>(2)</label></formula><p>Where x is the feature computed by a layer, and i is an index. µ is the mean and σ is the standard deviation (std). S i is the set of pixels from which the mean is computed, and S i is the set of pixels from which the standard deviation is computed. γ and β are learned parameters of affine transform.</p><p>In extended batch normalization, the set S i and S i are defined as:</p><formula xml:id="formula_2">S i = {k|k C = i C } (3) S i = {k|k ∈ U }<label>(4)</label></formula><p>Where i C (and k C ) refer to the sub-index of i (and k) along the C dimension, U is the universal set of the features.</p><p>The key difference between extended batch normalization and other normalization methods is that extended batch normalization computes the mean and the standard deviation from different set of pixels. As the same as batch normalization, extended batch normalization computes the mean along the (N, H, W) dimensions. However, it computes the standard deviation along the (N,C,H,W) dimensions, thus enlarges the pixel set from which the standard deviation is computed.</p><p>The different normalization methods are shown in  </p><formula xml:id="formula_3">S i = S i = {k|k C = i C } (5) S i = S i = {k|k N = i N } (6) S i = S i = {k|k C = i C , k N = i N } (7) S i = S i = {k| k C C/G = i C C/G , k N = i N }<label>(8)</label></formula><p>In the case of NC format input, e.g. input in fully-connected networks, extended batch normalization computes the mean along the (N) dimension, and computes the standard deviation along the (N, C) dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Small Batch Size</head><p>When training with batch normalization, examples are seen in conjunction with each other in the minibatch. The model looks at multiple training examples in combination, rather than in isolation. The coordination between examples helps the learning process. Moreover, the random selection of examples in the minibatch brings the sampling noises, and that provides similar regularization benefits as Dropout <ref type="bibr" target="#b25">[26]</ref>. However, when the batch size is small, the noises are too much and make inaccurate batch statistics estimation.</p><p>Let m N refer to the size of N dimension (batch size), m C refer to the size of C dimension, m H refer to the size of H dimension, m W refer to the size of W dimension, m refer to the size of S i . Then, in batch normalization,</p><formula xml:id="formula_4">m = m N * m H * m W<label>(9)</label></formula><p>When m N is small, the m is small. That is to say, the pixel set from which the mean and the standard deviation are computed is small. That makes the estimation of mean and standard deviation inaccurate. In layer normalization,</p><formula xml:id="formula_5">m = m C * m H * m W<label>(10)</label></formula><p>The m in layer normalization is independent on m N , thus has no problem caused by small batch size. However, normalizing along the channel dimension increases the correlation between the channels since they share the same mean and standard deviation. Correlation between features is harmful for training neural networks <ref type="bibr" target="#b12">[13]</ref>. Group normalization alleviates this problem by normalizing along a group of channels. The size m in group normalization is:</p><formula xml:id="formula_6">m = m C G * m H * m W<label>(11)</label></formula><p>In the same group, the correlation between the channels still exists. Moreover, there is a extra hyper parameter G to be tuned. When G is large, the m is small. Then the estimation of mean and standard deviation may be inaccurate. As G equals to m C , group normalization becomes instance normalization. When G is small, the number of channels in the same group increase, but this aggravate the problem of correlation. When G equals to 1, group normalization becomes layer normalization. Lastly, for low resolution input, the m H * m W is small, that also makes the estimation of the mean and the standard deviation inaccurate. The extreme situation is m H * m W = 1, the input becomes 1-dimension vector.</p><p>In extended batch normalization, the mean is computed along (N, H, W) dimensions as the same as batch normalization. Thus it maintains the merits of batch normalization. On the other hand, extended batch normalization enlarges the pixel set S i from which the standard deviation is computed along (N, C, H, W) dimensions, thus alleviates the problem caused by small batch size or low resolution. The size of S i is:</p><formula xml:id="formula_7">m = m N * m C * m H * m W<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>At inference time for extended batch normalization, the mean and the standard deviation are pre-computed from the training set by the moving average as the same as batch normalization.</p><formula xml:id="formula_8">y i = γ( 1 σ t r (x i − µ t r )) + β = γ σ t r x i + (β − γµ t r σ t r )<label>(13)</label></formula><formula xml:id="formula_9">µ t r = (1 − ρ)µ t−1 r + ρµ t b (14) σ t r = (1 − ρ)σ t−1 r + ρσ t b<label>(15)</label></formula><p>Where µ r refers to the moving average of the mean, σ r refers to the moving average of the standard deviation, µ b refers to the mean of a batch, σ b refers to the standard deviation of a batch, t refers to the number of times of computation, and ρ is a momentum constant less than one.</p><p>There is no need to compute the mean and the standard deviation at inference time comparing to group normalization. Moreover, since the mean and the standard deviation are pre-computed and fixed at inference time, the normalization can be fused into convolution operation. That is very helpful to speed up the inference especially on the mobile or embedded devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we compare extended batch normalization (EBN) with batch normalization (BN) and group normalization (GN) on the datasets of MNIST, CIFAR-10/100, STL-10, and ImageNet, respectively. We use Pytorch 1.1.0 in our experiments 3 . BN and GN have already been implemented by Pytorch. For BN, we use the default settings in Pytorch. For EBN, we use the same settings as BN. For GN, we set the group G to 32 which is used in the original paper <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MNIST</head><p>The MNIST <ref type="bibr" target="#b11">[12]</ref> data set consists of 28x28 pixel handwritten digit black and white images. The task is to classify the images into 10 digit classes. There are 60, 000 training images and 10, 000 test images in the MNIST data set. Each channel of the input is normalized into 0 mean and 1 std globally.</p><p>Our purpose is to compare different normalization methods, rather than renew the record. We used a very simple network, with a 28x28 binary image as input, and 4 fully-connected hidden layers with 128 hidden units each. Normalization followed by ReLU activation is used each layer. We evaluate the batch sizes of 128 and 4 with different normalization. For the batch size of 128, the learning rate is set to 0.1. And for the batch size of 4, the learning rate is set to 0.1 * 4/128, following the linear scaling rule <ref type="bibr" target="#b4">[5]</ref> to adapt to batch size changes. We train the network for 50 epochs using SGD with 0.5 momentum. We use 1 GPU to train all models.</p><p>The results of MNIST are shown in <ref type="figure">Fig 2 and</ref>  <ref type="table" target="#tab_0">Table 1</ref>. For the batch size of 128, shown in <ref type="figure">Fig 2(a)</ref>, we can see BN and EBN achieve similar performances which are better than GN. For the batch size of 4, shown in <ref type="figure">Fig 2(b)</ref>, the test accuracy of BN has large fluctuation. EBN achieves the most stable and highest accuracy. We show the accuracy numbers in <ref type="table" target="#tab_0">Table 1</ref>. To reduce random variations, we report the average accuracy of the final 5 epochs (This is adopted in our following experiments). With the batch size of 128, EBN achieves 98.37% test accuracy, 0.52% higher than GN, which achieves 97.85%. With the batch size of 4, the test accuracies of all normalization methods have decreased. Compared to the batch size of 128, BN, EBN and GN reduces 3.76%, 0.3% and 0.07% respectively. EBN is still better than GN by 0.29%.  Summary In the case of fully-connected network, n H * n W = 1. The size of pixel set, from which the statistics are computed for batch normalization, depends only on the batch size. Thus the performance of BN is very poor when using small batch size. EBN maintains the advantage of BN in case of the large batch size. Meanwhile, it alleviates the problem caused by small batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CIFAR</head><p>CIFAR-10 <ref type="bibr" target="#b10">[11]</ref> is a data set of natural 32x32 RGB images in 10 classes with 50, 000 images for training and 10, 000 for testing. CIFAR-100 is similar with CIFAR-10 but with 100 classes. To augment data, the training images are padded with 0 to 36x36 and then randomly cropped to 32x32 pixels. Then randomly horizontal flipping is made. Each channel of the input is normalized into 0 mean and 1 std globally. Weight decay of 0.0005, and SGD with 0.9 momentum are used.</p><p>On CIFAR-10 and CIFAR-100, We evaluate both ResNet18 <ref type="bibr" target="#b5">[6]</ref> and VGG16 <ref type="bibr" target="#b23">[24]</ref> with BN, EBN and GN, respectively. For the batch size of 128, the initial learning rate is set to 0.1. And for the batch size of 4, the initial learning rate is set to 0.1 * 4/128, following the linear scaling rule. We train the network for 200 epochs, and decrease the learning rate by 10x at 100 and 150 epochs.. The training of VGG16 with GN is failed when using the initial learning rate of 0.1, thus we decrease the initial learning rate to 0.01 (we also try the initial learning rate of 0.05, but the training is still failed) when training the VGG16 with GN. We use 1 GPU to train all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of ResNet18</head><p>The results of ResNet18 on CIFAR-10 are shown in <ref type="figure">Fig 3</ref> and <ref type="table" target="#tab_1">Table 2</ref>. <ref type="figure">From Fig 3(a)</ref>, we can see BN and EBN achieve close performances with the large batch size of 128. EBN has 94.96% test accuracy, slightly better than BN, and better than GN by 1.62%. As shown is <ref type="figure">Fig 3(b)</ref>, with the small size of 4, BN also has large fluctuation of test accuracy, while GN achieves better performance than with the batch size of 128. Stably, EBN achieves the best test accuracy 94.92% in the case of the small batch size. The results of ResNet18 on CIFAR-100 are shown in <ref type="figure">Fig 4 and</ref>  <ref type="table" target="#tab_2">Table 3</ref>. With the batch size of 128, BN achieves the best test accuracy of 77.50%. EBN has 76.53%, worse than BN by 0.97%, but better than GN by 2.68% which only has 73.85%. When the batch size decreases to 4, BN' performance decreases,   Results of VGG16 The results of VGG16 on CIFAR-10 are shown in <ref type="figure">Fig 5</ref> and <ref type="table" target="#tab_3">Table 4</ref>.With the large batch size of 128, BN achieves best accuracy 93.71%. EBN have 93.41% test accuracy, worse than BN by 0.,3%, but better than GN by 1.4%. With the small size of 4, BN' accuracy decreases to 93.53%, while EBN' increases to 93.55%. They are better than the GN' accuracy 92.12%.  The results of VGG16 on CIFAR-100 are shown in <ref type="figure">Fig 6 and</ref>    Summary With the large batch size, BN works well both in ResNet18 and VGG16. EBN performs slightly worse than BN, but better than GN with the large batch size. Finally, EBN performs better than both BN and GN when using the small batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">STL-10</head><p>The STL-10 dataset is a data set of natural 96x96 RGB images in 10 classes. Each class has fewer labeled training examples, but a very large set of unlabeled examples. We use only the labeled examples (500 training images, 800 test images per class) in our experiments. To augment data, the training images are padded with 0 to 100x100 and then randomly cropped to 96x96 pixels. We evaluate ResNet18 with different normalization methods. The experiment settings are the same as CIFAR-10.</p><p>The results of STL-10 are shown in <ref type="figure" target="#fig_8">Fig 7</ref> and <ref type="table" target="#tab_6">Table 6</ref>. With the batch size of 128 and 4, BN achieves best performance, and EBN performs better than GN. With the batch size of 4, all normalizations achieve better results than with the batch size of 128. STL-10 has fewer labeled training examples in total, thus the batch size could be smaller when training. That is to say, the batch size of 4 has not triggered the problem of small batch size of batch normalization. When using the batch size of 2, the performance of BN decreases to 76.91%, and EBN achieves highest accuracy 77.96%.   Summary The STL-10 dataset has higher resolution and fewer total training examples comparing to CIFAR dataset. Relatively, the batch size of 4 is not small enough to deteriorate the effect of BN. When using smaller batch size of 2, EBN achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ImageNet</head><p>ImageNet classification dataset <ref type="bibr" target="#b19">[20]</ref> has 1.28M training images and 50,000 validation images with 1000 classes. To augment data, the training images are cropped with random size of 0.08 to 1.0 of the original size and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio, and then resized to 224x224. Then random horizontal flipping is made. The validation image is resized to 256x256, and then cropped by 224x224 at the center. Each channel of the input is normalized into 0 mean and 1 std globally. Weight decay of 0.0001, and SGD with 0.9 momentum are used.</p><p>On ImageNet, we evaluate the ResNet18 with BN, EBN and GN. We use 4 GPUs. We evaluate batch sizes of 64 and 4 images per GPU. The mean and the standard deviation of BN and EBN are computed within each GPU. For the batch size of 64, the initial learning rate is set to 0.1. And for the batch size of 4, the initial learning rate is set to 0.1 * 4/64, following the linear scaling rule. We train the network for 100 epochs, and decrease the learning rate by 10x at 30, 60 and 90 epochs.</p><p>The results of ImageNet are shown in <ref type="figure" target="#fig_9">Fig 8 and</ref>  <ref type="table" target="#tab_7">Table 7</ref>. <ref type="figure" target="#fig_9">From Fig 8(a)</ref>, we can see EBN achieves close performance to BN, better than GN, in the case of large batch size. <ref type="figure" target="#fig_9">From Fig 8(b)</ref>, we can see EBN achieves close performance to GN, better than BN, in the case of small batch size. As shown in <ref type="table" target="#tab_7">Table 7</ref>, with the batch size of 64 per GPU, BN achieves the best validation accuracy of 70.37%. EBN has 70.12%, worse than BN by 0.25%, but better than GN by 1.35%. With the batch size of 4 per GPU, the accuracy of BN decreases to 65.78%. GN achieves the best accuracy of 69.08%. EBN achieves 68.54%, worse than GN by 0.54%, better than BN by 2.76%.  Summary In the case of large batch size, EBN achieves close performance to BN, better than GN. In the case of small batch size, EBN achieves close performance to GN, better than BN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a simple but effective method, called extended batch normalization. The key difference from other normalization methods is that extended batch normalization computes the mean and the standard deviation from different set of pixels. To maintain the advantage of batch normalization, extended batch normalization computes the mean along the (N, H, W) dimensions just as the standard batch normalization. To alleviate the problems caused by small batch size, extended batch normalization enlarges the pixel set from which the standard deviation is computed along (N, C, H, W) dimensions. The experiments show that extended batch normalization alleviates the problem of batch normalization with small batch size while achieving close performances to batch normalization with large batch size. Many other techniques could be equipped with extended batch normalization, e.g. kalman normalization <ref type="bibr" target="#b27">[28]</ref>, switchable normalization <ref type="bibr" target="#b14">[15]</ref>. Moreover, as the same as batch normalization, the moving average of minibatch statistics is maintained during training and used during evaluation in extended batch normalization. As pointed by <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b24">[25]</ref>, that inconsistence hurts the performance of batch normalization in the case of small batch size. We will investigate the inconsistence between the training and the evaluation in extended batch normalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 1 .</head><label>1</label><figDesc>Batch normalization computes both the mean and the standard deviation along the (N, H, W) dimensions (Equation 5). Layer normalization computes both the mean and the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>EBN (mean) (f) EBN (std) Different normalization methods. N refers to the batch dimension, C refers to the channel dimension, and (H, W) refers to the spatial dimension. The pixels in blue are the pixel set from which the mean and the standard deviation (std) are computed. (a) Batch norm. (b) Instance norm .(c) Layer norm. (d) Group norm. (e) Extended batch norm (mean). (f) Extended batch norm (std). In extended batch normalization, the mean and the standard deviation are computed from different pixel sets standard deviation along the (C, H, W) dimensions (Equation 6). Instance normalization computes both the mean and the standard deviation along the (H, W) dimensions (Equation 7). Group normalization computes both the mean and the standard deviation in (H, W) dimensions and along a group of C/G channels where G is the number of channel groups (Equation 8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>size is 4 Fig. 2 .</head><label>42</label><figDesc>The test accuracy of the fully-connected network on MNIST vs. the number of training epoch, with different normalization methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>size is 4 Fig. 3 .</head><label>43</label><figDesc>The test accuracy of ResNet18 on CIFAR-10 vs. the number of training epoch, with different normalization methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>size is 4 Fig. 4 .</head><label>44</label><figDesc>The test accuracy of ResNet18 on CIFAR-100 vs. the number of training epoch, with different normalization methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>size is 4 Fig. 5 .</head><label>45</label><figDesc>The test accuracy of VGG16 on CIFAR-10 vs. the number of training epoch, with different normalization methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>size is 4 Fig. 6 .</head><label>46</label><figDesc>The test accuracy of VGG16 on CIFAR-100 vs. the number of training epoch, with different normalization methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>The test accuracy of ResNet18 on STL vs. the number of training epoch, with different normalization methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>The test accuracy of ResNet18 on ImageNet vs. the number of training epoch, with different normalization methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The test accuracy (%) of the fully-connected network on MNIST</figDesc><table><row><cell>BN EBN GN</cell></row><row><cell>batch size=128 98.33 98.37 97.85</cell></row><row><cell>batch size=4 94.57 98.07 97.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The test accuracy (%) of ResNet18 on CIFAR-10 BN EBN GN batch size=128 94.92 94.96 93.34 batch size=4 91.94 94.92 94.18 while EBN' and GN' performance increase. EBN achieves the best test accuracy 77.17%, better than BN by 1.32%, and GN by 2.71%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The</figDesc><table><row><cell>test accuracy (%) of ResNet18 on CIFAR-100</cell></row><row><cell>BN EBN GN</cell></row><row><cell>batch size=128 77.50 76.53 73.85</cell></row><row><cell>batch size=4 75.85 77.17 74.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The test accuracy (%) of VGG16 on CIFAR-10</figDesc><table><row><cell>BN EBN GN</cell></row><row><cell>batch size=128 93.71 93.41 92.0</cell></row><row><cell>batch size=4 93.53 93.55 92.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>With the batch size of 128, BN achieves the best test accuracy of 74.03 %. EBN has 72.80%, and GN only has 67.29%. With the batch size of 4, EBN achieves the best test accuracy 72.66%, better than BN by 0.93%, and than GN by 5.94%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The test accuracy (%) of VGG16 on CIFAR-100.</figDesc><table><row><cell>BN EBN GN</cell></row><row><cell>batch size=128 74.03 72.80 67.29</cell></row><row><cell>batch size=4 71.73 72.66 66.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The test accuracy (%) of ResNet18 on STL</figDesc><table><row><cell>BN EBN GN</cell></row><row><cell>batch size=128 78.65 75.57 72.66</cell></row><row><cell>batch size=4 81.04 79.3 76.49</cell></row><row><cell>batch size=2 76.91 77.96 76.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>The test accuracy (%) of ResNet18 on ImageNet BN EBN GN batch size=128 70.37 70.12 68.77 batch size=4 65.78 68.54 69.08</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The codes for MNIST and ImageNet experiments are based on Pytorch examples (https://github.com/pytorch/examples), and CIFAR is based on pytorch-cifar (https://github.com/kuangliu/pytorch-cifar).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Normalization propagation: A parametric technique for removing internal covariate shift in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Kota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01431</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7694" to="7705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09025</idno>
		<title level="m">Recurrent batch normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Norm matters: efficient and accurate normalization schemes in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2160" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batchnormalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1945" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
	<note>Efficient backprop</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cosine normalization: Using cosine similarity instead of dot product in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10779</idno>
		<title level="m">Differentiable learning-to-normalize via switchable normalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<title level="m">Towards understanding regularization in batch normalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch-instance normalization for adaptively style-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2558" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04520</idno>
		<title level="m">Normalizing the normalizers: Comparing and extending network normalization schemes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06031</idno>
		<title level="m">Evalnorm: Estimating batch normalization statistics for evaluation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kalman normalization: Normalizing internal representations across network layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="21" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalized batch normalization: Towards accelerating deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1682" to="1689" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
