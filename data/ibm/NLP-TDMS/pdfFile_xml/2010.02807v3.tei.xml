<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
							<email>aettinger@uchicago.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
							<email>kgimpel@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memoryaugmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Long document coreference resolution poses runtime and memory challenges. Current best models for coreference resolution have large memory requirements and quadratic runtime in the document length <ref type="bibr" target="#b4">(Joshi et al., 2019;</ref><ref type="bibr">Wu et al., 2020)</ref>, making them impractical for long documents.</p><p>Recent work revisiting the entity-mention paradigm <ref type="bibr" target="#b11">(Luo et al., 2004;</ref><ref type="bibr">Webster and Curran, 2014)</ref>, which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-the-art models <ref type="bibr">(Xia et al., 2020)</ref>. In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters <ref type="bibr">(Rahman and Ng, 2011;</ref><ref type="bibr">Stoyanov and Eisner, 2012;</ref><ref type="bibr" target="#b2">Clark and Manning, 2015;</ref><ref type="bibr">Wiseman et al., 2016;</ref><ref type="bibr" target="#b9">Lee et al., 2018)</ref> , the entity-mention paradigm stores representations only of the entity clusters, which are updated incrementally as coreference predictions are made. While such an approach requires less memory than those that additionally store mention representations, the number of entities can be impractically large when processing long documents, making the storing of all entity representations problematic.</p><p>Is it necessary to maintain an unbounded number of mentions or entities? Psycholinguistic evidence suggests it is not, as human language processing is incremental <ref type="bibr">(Tanenhaus et al., 1995;</ref><ref type="bibr" target="#b5">Keller, 2010)</ref> and has limited working memory <ref type="bibr" target="#b0">(Baddeley, 1986)</ref>. In practice, we find that most entities have a small spread (number of tokens from first to last mention of an entity), and thus do not need to be kept persistently in memory. This observation suggests that tracking a limited, small number of entities at any time can resolve the computational issues, albeit at a potential accuracy tradeoff.</p><p>Previous work on bounded memory models for coreference resolution has shown potential, but has been tested only on short documents <ref type="bibr" target="#b10">(Liu et al., 2019;</ref><ref type="bibr">Toshniwal et al., 2020)</ref>. Moreover, this previous work makes token-level predictions while standard coreference datasets have span-level annotations. We propose a bounded memory model that performs quasi-online coreference resolution, 1 and test it on LitBank <ref type="bibr" target="#b1">(Bamman et al., 2020)</ref> and <ref type="bibr">OntoNotes (Pradhan et al., 2012)</ref>. The model is trained to manage its limited memory by predicting whether to "forget" an entity already being tracked in exchange for a new (currently untracked) entity. Our empirical results show that: (a) the model is competitive with an unbounded memory version, and (b) the model's learned memory management outperforms a strong rule-based baseline. 2 2 Entity Spread and Active Entities Given input document D, let (x n ) N n=1 represent the N mention spans corresponding to M underlying entities (e m ) M m=1 . Let START(x i ) and END(x i ) denote the start and end token indices of the mention span x i in document D. Let ENT(x i ) denote the entity of which x i is a mention. Given this notation we next define the following concepts.</p><p>Entity Spread Entity spread denotes the interval of token indices from the first mention to the last mention of an entity. The entity spread ES(e) of entity e is given by:</p><formula xml:id="formula_0">ES(e) = [ min ENT(x)=e START(x), max ENT(x)=e END(x)]</formula><p>Active Entity Count Active entity count AE(t) at token index t denotes the number of unique entities whose spread covers the token t, i.e., AE(t) = |{e | t ∈ ES(e)}|.</p><p>Maximum Active Entity Count Maximum active entity count MAE(D) for a document D denotes the maximum number of active entities at any token index in D, i.e., MAE(D) = max t∈[|D|] AE(t). This measure can be simply extended to a corpus C as: MAE(C) = max D∈C MAE(D). <ref type="table" target="#tab_0">Table 1</ref> shows the MAE and the maximum total entity count in a single document, for LitBank and OntoNotes. For both datasets the maximum active entity count is much smaller than the maximum total entity count. Thus, rather than keeping all the entities in memory at all times, models can in principle simply focus on the far fewer active entities at any given time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Based on the preceding finding, we will next describe models that require tracking only a small, bounded number of entities at any time.</p><p>To make coreference predictions for a document, we first encode the document and propose candidate mentions. The proposed mentions are then processed sequentially and are either: (a) added to an existing entity cluster, (b) added to a new cluster, (c) ignored due to limited memory capacity (for bounded memory models), or (d) ignored as an invalid mention. Document Encoding is done using the SpanBERT LARGE model finetuned for OntoNotes and released as part of the coreference model of <ref type="bibr" target="#b3">Joshi et al. (2020)</ref>. We don't further finetune the SpanBERT model. To encode long documents, we segment the document using the independent and overlap strategies described in <ref type="bibr" target="#b4">Joshi et al. (2019)</ref>. <ref type="bibr">3</ref> In overlap segmentation, for a token present in overlapping BERT windows, the token's representation is taken from the BERT window with the most neighboring tokens of the concerned token. For both datasets we find that overlap slightly outperforms independent.</p><p>Mention Proposal Given the encoded document, we next predict the top-scoring mentions which are to be clustered. The goal of this step is to have high recall, and we follow previous work to threshold the number of spans chosen <ref type="bibr" target="#b8">(Lee et al., 2017)</ref>. Given a document D, we choose 0.3 × |D| top spans for LitBank, and 0.4 × |D| for OntoNotes.</p><p>We pretrain the mention proposal model before training the mention proposal and mention clustering pipeline end-to-end, as done by <ref type="bibr">Wu et al. (2020)</ref>. The reason is that without pretraining, most of the mentions proposed by the mention proposal model would be invalid mentions, i.e., spans that are not mentions, which would not provide any training signal to the mention clustering stage.</p><p>Mention Clustering Let (x i ) K i=1 represent the top-K candidate mention spans from the mention proposal step and let s m (x i ) represent the mention score for span x i , which indicates how likely it is that a span constitutes a mention. Assume that the mentions are already ordered based on their position in the document and are processed sequentially in that order. 4 Let E = (e m ) M m=1 represent the M entities currently being tracked by the model (initially M = 0). For ease of discussion, we will overload the terms x i and e j to also correspond to their respective representations.</p><p>In the first step, the model decides whether the span x i refers to any of the entities in E as follows:</p><formula xml:id="formula_1">s c (x i , e j ) = f c ([x i ; e j ; x i e j ; g(x i , e j )])+s m (x i ) s top c = max j=1...M s c (x i , e j ) e top = arg max j=1...M s c (x i , e j )</formula><p>where represents the element-wise product, and f c (·) corresponds to a learned feedforward neural network. The term g(x i , e j ) correponds to a concatenation of feature embeddings that includes embeddings for (a) number of mentions in e j , (b) number of mentions between x i and last mention of e j , (c) last mention decision, and (d) document genre (only for OntoNotes). Now if s top c &gt; 0 then x i is considered to refer to e top , and e top is updated accordingly. <ref type="bibr">5</ref> Otherwise, x i does not refer to any entity in E and a second step is executed, which will depend on the choice of memory architecture. We test three memory architectures, described below.</p><p>1. Unbounded Memory (U-MEM): If s m (x i ) &gt; 0 then we create a new entity e M +1 = x i and append it to E. Otherwise the mention is ignored as invalid, i.e., it doesn't correspond to an entity. Ignoring invalid mentions is important for datasets such as LitBank where singletons are explicitly annotated and used for evaluation. For OntoNotes, where singletons are not annotated and ignored for evaluation, we also consider a variant U-MEM* which appends all non-coreferent mentions, as done in Xia et al. (2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Bounded Memory:</head><p>Suppose the model has a capacity of tracking C entities at a time. If C &gt; M , i.e., the memory capacity has not been fully utilized, then the model behaves like U-MEM. Otherwise, the bounded memory models must decide between: (a) evicting an entity already being tracked, (b) ignoring x i due to limited capacity, and (c) ignoring the mention as invalid. We test two bounded memory variants that are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Learned Bounded Memory (LB-MEM):</head><p>The proposed LB-MEM architecture tries to predict a score f r (.) corresponding to the anticipated number of remaining mentions for any entity or mention, and compares it against the mention score s m (x i ) as follows:</p><formula xml:id="formula_2">d = arg min[f r (e 1 ), . . . , f r (e M ), f r (x i ), s m (x i )] 5</formula><p>We use weighted averaging where the weight for e top corresponds to the number of previous mentions seen for e top .   <ref type="table" target="#tab_1">Tables 2 and 3</ref> show results of all the proposed models for LitBank and OntoNotes respectively. Detailed results with the performance on different coreference metrics is presented in <ref type="table" target="#tab_0">Table 11</ref> for Lit-Bank, and <ref type="table" target="#tab_0">Table 12</ref> for OntoNotes (Appendix A.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>As expected, the bounded memory models improve with increase in memory. For both the datasets, the LB-MEM model with 20 memory cells is competitive with the U-MEM model though the gap between them for LitBank is non-trivial. Among the bounded memory models, the LB-MEM model is significantly better than RB-MEM for lower numbers of memory cells. We analyze the reasons for this in the next section. For OntoNotes, the U-MEM* model easily outperforms the U-MEM model which is trained to ignore all non-gold mentions. These non-gold mentions also include singletons in case of OntoNotes. Thus, the U-MEM model essentially has to predict if a non-coreferent mention will be coreferent with future mentions or not. U-MEM* avoids this difficult problem by adding all non-coreferent mentions to memory. Since in OntoNotes singletons are removed during evaluation, the U-MEM* model is not penalized for predicting singletons corresponding to invalid mentions, and otherwise. Note that the U-MEM* model doesn't make sense for LitBank where singletons are used for evaluation. The initial empirical results also confirmed that decisively.</p><p>Between the two datasets, we see that the increase in memory results in larger improvement for LitBank. We also establish a new state-of-the-art for LitBank with the U-MEM model. For OntoNotes, the U-MEM* model matches the performance of a similar model by <ref type="bibr">Xia et al. (2020)</ref>. Remarkably, the two U-MEM* models almost match the performance of the computationally and memory intensive span-ranking model of <ref type="bibr" target="#b3">Joshi et al. (2020)</ref> whose finetuned SpanBERT document encoder is used by these two models. We expect gains by further finetuning the SpanBERT model and learning a parameterized global entity representation, but we leave them for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>In this section we analyze the behavior of the three memory models on LitBank and OntoNotes.  Memory Utilization <ref type="table" target="#tab_3">Table 4</ref> compares the memory and inference time statistics for the different memory models for the LitBank cross-validation split zero. 8 For training, the bounded memory models are significantly less memory intensive than the U-MEM model. The table also shows that the bounded memory models are faster than the U-MEM memory model during inference (inference time calculated by averaging over three runs). This is because the number of entities tracked by the U-MEM memory model grows well beyond the maximum of 20 memory slots reserved for the bounded models as shown in <ref type="table" target="#tab_4">Table 5</ref>. Surprisingly, for inference we see that the bounded models have a slightly larger memory footprint than the U-MEM model. This is because the document encoder, SpanBERT, dominates the memory usage during inference (as also observed by Xia et al., 2020). Thus the peak memory usage during inference is determined by the mention proposal stage rather than the mention clustering stage. And during the mention proposal stage, the additional parameters of bounded memory models, which are loaded as part of the whole model, cause the slight uptick in peak inference memory. Note that using a cheaper encoder or running on a sufficiently long document, such as a book, can change these results. <ref type="table" target="#tab_4">Table 5</ref> compares the maximum number of entities kept in memory by the different memory models for the LitBank cross-validation dev sets and the OntoNotes dev set. As expected, the U-MEM model keeps more entities in memory than the bounded memory models on average for both datasets. For LitBank the difference is especially stark with the U-MEM model tracking about 4/8 times more entities in memory 8 Peak memory usage estimated via torch.cuda.max_memory_allocated()  on average/worst case, respectively. The difference between the U-MEM and U-MEM* model is striking, with U-MEM* tracking more than 10 times the entities of U-MEM in both the average and worst case. Also, while some OntoNotes documents do not use even the full 5 memory cell capacity, all Lit-Bank documents fully utilize even the 20 memory cell capacity. This is because LitBank documents are more than four times as long as OntoNotes documents, and LitBank has singletons marked. These results also justify our initial motivation that with long documents, the memory requirement will increase even if we only keep the entity representations. <ref type="table" target="#tab_5">Table 6</ref> compares the number of mentions ignored by LB-MEM and RB-MEM. The LB-MEM model ignores far fewer mentions than RB-MEM. This is because while the RB-MEM model can only evict the LRU entity, which might not be optimal, the LB-MEM model can choose any entity for eviction. These statistics combined with the fact that the LB-MEM model typically outperforms RB-MEM mean that the LB-MEM model is able to anticipate which entities are important and which are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Entities in Memory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LB-MEM vs. RB-MEM</head><p>Error Analysis <ref type="table" target="#tab_6">Table 7</ref> presents the results of automated error analysis done using the Berkeley Coreference Analyzer <ref type="bibr" target="#b7">(Kummerfeld and Klein, 2013)</ref> for the OntoNotes dev set. As the memory capacity of models increases, the errors shift from missing mention, missing entity, and divided entity categories, to conflated entities, extra mention, and extra entity categories. The LB-MEM model outperforms RB-MEM in terms of tracking more entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We propose a memory model which tracks a small, bounded number of entities. The proposed model guarantees a linear runtime in document length, and in practice significantly reduces peak memory usage during training. Empirical results on LitBank and OntoNotes show that the model is competitive with an unbounded memory version and outperforms a strong rule-based baseline. In particular, we report state of the art results on LitBank. In future work we plan to apply our model to longer, book length documents, and plan to add more structure to the memory.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Maximum Active Entities <ref type="figure" target="#fig_2">Figure 1</ref> visualizes the histograms of length of Entity Spread (ES), defined in Section 2, as a fraction of document length for documents in LitBank and OntoNotes. For LitBank we only visualize the entity spread of non-singleton clusters because otherwise the histogram is too skewed towards one. <ref type="figure" target="#fig_3">Figure 2</ref> visualizes the histograms of Maximum Active Entity Count (MAE), defined in Section 2, for documents in LitBank and OntoNotes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Model Details</head><p>Hyperparameter Choices We stick with the hyperparameters for feedforward neural network (FFNN) size and depth from <ref type="bibr" target="#b3">Joshi et al. (2020)</ref>. We didn't do much exploration with dropout but with the limited experiments our finding was that there was little separating dropout probability of 0.3 and 0.4. Among choices for how to segment document into BERT windows, we found overlapping windows to work better than independent BERT windows. Two very important hyperparameters that affect peak memory usage during training are: (a) maximum number of BERT segments, and (b) sampling probability of invalid mentions. Truncating the document by selecting a chosen maximum number of contiguous BERT segments essentially caps the length of documents during training. And the second hyperparameter of sampling invalid mentions controls the number of invalid mentions, which happens to be the overwhelming category of proposed mentions, the model sees during training. We also explore two hyperparameters for the cross-entropy loss of the first step of mention clustering: (a) label smoothing for regularization, and (b) weight of the non-coreferent term in the crossentropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specific</head><p>Hyperparameter Choices for OntoNotes We didn't see any gain by increasing the maximum number of BERT segments from 3 to 5 in our initial experiments. The U-MEM and bounded models preferred lower sampling probabilities for invalid mentions but no clear winner in label smoothing weight. The U-MEM* model preferred low label smoothing weight and higher sampling probabilities for invalid mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specific Hyperparameter Choices for LitBank</head><p>Initial experiments with cross validation splits {0, 1, 2} showed that models preferred maximum number of BERT segments to be 5 in comparison to 3. This might be because most of the LitBank documents are really long, and training on a maximum of 3 BERT segments might lead to a bigger mismatch between training and inference. Another hyperparameter that proved important for LitBank was the non-coreferent entity weight of 2.0. Due to explosion of combinations driven by the fact that there are 10 cross validation splits, we didn't explore label smoothing for LitBank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Ground Truth Generation</head><p>In this section we explain how the ground truth action sequence is generated corresponding to the predicted mention sequence. The ground truth for U-MEM model is fairly straight forward. For the bounded memory models, we keep growing the number of entities till we hit the memory ceiling. For all the entities in memory, we maintain the number of mentions remaining in the ground truth cluster. For example, a cluster with a total of five mentions, two of which have already been processed by the model, has three remaining mentions. Suppose now a mention corresponding to a currently untracked entity comes in and the memory is already at full capacity. Then for the LB-MEM model, we compare the number of mentions of this new entity (along with the current mention) against the number of mentions remaining for all the entities currently being tracked. If there are entities in memory with number of remaining mentions less than or equal to the number of mentions of this currently untracked entity, then the untracked entity replaces the entity with the least number of remaining mentions. Ties among the entities with least number of remaining mentions are broken by the least recently seen entity. If there's no such entity in the memory, then the mention is ignored. For the RB-MEM model, the comparison is done in a similar way but is limited to just the LRU entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Miscellany</head><p>Computing Infrastructure &amp; Runtime All the models for a single cross validation split of Lit-Bank can be trained within 4 hours. Training on OntoNotes finishes within 12-20 hours. The U-MEM* model where all invalid mentions are seen during training is the only configuration that requires 24GB memory GPUs, all other configurations can be trained on 12GB memory GPUs.</p><p>Number of model parameters. <ref type="table" target="#tab_0">Table 10</ref> shows the number of trainable parameters for all the model and dataset combinations. LB-MEM and RB-MEM have additional parameters in comparison to U-MEM for predicting a score corresponding to the number of remaining mentions for an entity. Comparing across datasets, the OntoNotes models have a few additional parameters than their LitBank counterparts for modeling the document genre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>Metric Code. We use the coreference scorer Perl script available at https://github.com/conll/ reference-coreference-scorers.</p><p>We also use the Python implementation by Kenton Lee available at https://github.com/kentonl/    <ref type="table" target="#tab_0">Table 13</ref> and 14 presents the Spearman correlation of document F1 score with document length and number of entities in the document. The correlations are mostly negative because the task becomes more challenging with an increase in doc- ument length and entities, though for LitBank the length of the document doesn't seem to be a great indicator of the hardness of the task. The increase in memory capacity for bounded models results in less negative correlations, suggesting improved performance for challenging documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Histograms of Maximum Active Entities for documents in LitBank and OntoNotes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Histograms of Entity Spread as fraction of document length for LitBank and OntoNotes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Max. Total Entity Count vs. Max. Active Entity Count.</figDesc><table><row><cell></cell><cell cols="2">LitBank OntoNotes</cell></row><row><cell>Max. Total Entity Count Max. Active Entity Count</cell><cell>199 18</cell><cell>94 24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results for LitBank (CoNLL F1).<ref type="bibr" target="#b13">Rae et al., 2016;</ref>  Santoro et al., 2016). While LB-MEM considers all potential entities for eviction, with RB-MEM this choice is restricted to just the LRU entity, i.e., the entity whose mention was least recently seen. The rest of the steps are similar to the LB-MEM model.Training All the models are trained using teacher forcing. The ground truth decisions for bounded memory models are chosen to maximize the number of mentions tracked by the model (details in Appendix A.3). Finally, the training loss is calculated via the addition of the cross-entropy losses for the two steps of mention clustering.</figDesc><table><row><cell>Model</cell><cell cols="2">Dev F1 Test F1</cell></row><row><cell>U-MEM LB-MEM 5 cells 10 cells 20 cells RB-MEM 5 cells 10 cells 20 cells</cell><cell>77.1 71.9 75.0 75.7 58.5 69.9 75.3</cell><cell>76.5 70.3 74.7 75.1 57.8 69.0 74.4</cell></row><row><cell>Bamman et al. (2020)</cell><cell>-</cell><cell>68.1</cell></row><row><cell>where f</cell><cell></cell><cell></cell></row></table><note>r (·) is a learned feedforward neural network. If 1 ≤ d ≤ M then then the model evicts the previous entity e d and reinitialize it to x i . Otherwise if d = M + 1 then the model ignores x i due to limited capacity. Finally if d = M + 2 then the model predicts the mention to be invalid. (b) Rule-based Bounded Memory (RB-MEM) The Least Recently Used (LRU) principle is a popu- lar choice among memory models (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results for OntoNotes (CoNLL F1) . × 10 −4 which is linearly decayed. For span representation, we use the embedding function described in<ref type="bibr" target="#b8">Lee et al. (2017)</ref>. For OntoNotes we follow the setup of Xia et al.(2020). We differ, however, in training all the model parameters, except SpanBERT, from scratch. The models are trained for a maximum of 15 epochs for OntoNotes, and 25 epochs for LitBank. For both the datasets, the training stops if dev performance doesn't improve for 5 epochs. For more details see Appendix A.2.</figDesc><table><row><cell>Model</cell><cell cols="2">Dev F1 Test F1</cell></row><row><cell>U-MEM U-MEM* LB-MEM 5 cells 10 cells 20 cells RB-MEM 5 cells 10 cells 20 cells</cell><cell>78.4 79.6 74.0 77.1 78.1 69.8 75.9 78.2</cell><cell>78.1 79.6 73.3 76.8 78.2 69.6 75.5 77.8</cell></row><row><cell>U-MEM* (Xia et al., 2020)</cell><cell>79.7</cell><cell>79.4</cell></row><row><cell>Joshi et al. (2020) Wu et al. (2020)</cell><cell>80.1 83.4</cell><cell>79.6 83.1</cell></row><row><cell>4.2 Hyperparameters</cell><cell></cell><cell></cell></row><row><cell cols="3">Document encoding is done using the</cell></row><row><cell cols="3">SpanBERT LARGE model of Joshi et al. (2020) which was finetuned for OntoNotes. 7 The Span-</cell></row><row><cell cols="3">BERT model is not further finetuned. The other</cell></row><row><cell cols="3">model parameters are trained using the Adam</cell></row><row><cell cols="3">optimizer (Kingma and Ba, 2014) with an initial</cell></row><row><cell>learning rate of 2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Peak memory and inference time statistics for the LitBank cross-validation split 0. Note that the training memory statistics depend on document truncation and sampling probability of invalid mentions. The models in this table are trained without document truncation and sample 20% of invalid mentions during training.</figDesc><table><row><cell>Model</cell><cell cols="3">Peak training Peak inference mem. (in GB) mem. (in GB) time (in s) Inference</cell></row><row><cell>U-MEM LB-MEM 5 cells 10 cells 20 cells RB-MEM 5 cells 10 cells 20 cells</cell><cell>11.6 8.0 8.4 9.1 8.0 8.3 8.9</cell><cell>3.1 3.2 3.2 3.2 3.2 3.2 3.2</cell><cell>29.25 27.31 27.44 27.86 26.19 26.50 26.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of number of entities in memory.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Average number of mentions ignored by the two bounded memory models.</figDesc><table><row><cell>Memory size</cell><cell cols="4">LitBank LB-MEM RB-MEM LB-MEM RB-MEM OntoNotes</cell></row><row><cell>5 10 20</cell><cell>4.5 0.0 0.0</cell><cell>70.0 14.2 0.4</cell><cell>0.3 0.0 0.0</cell><cell>3.7 0.4 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Error Analysis for OntoNotes dev set.</figDesc><table><row><cell cols="6">CE=Conflated Entities, DE=Divided Entity, EM=Extra Mention, EE=Extra Entity, MM=Missing Mention, ME=Missing Entity.</cell></row><row><cell>Model</cell><cell>CE</cell><cell>DE EM</cell><cell>EE</cell><cell>MM</cell><cell>ME</cell></row><row><cell cols="6">U-MEM U-MEM* LB-MEM 5 cells 10 cells 757 425 340 853 496 515 754 466 504 706 381 340 20 cells 752 396 402 RB-MEM 5 cells 672 369 365 1146 923 1359 904 545 603 816 527 583 972 844 1116 868 655 894 859 613 799 10 cells 722 393 403 986 696 931 20 cells 713 420 380 833 559 853</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameter options for OntoNotes with preferred choices highlighted in bold.</figDesc><table><row><cell>Parameter</cell><cell>Range</cell></row><row><cell cols="2">Dropout FFNN hidden layer FFNN # of hidden layers Document Encoding Label Smoothing Sampling Prob. Invalid Mentions {0.25, 0.5, 0.75, 1.0} {0.4} {3000} 1 {Independent, Overlap} {0.0, 0.01, 0.1} Max. # of BERT Segments {3, 5} Non-coreferent entity weight {1.0}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameter options for LitBank with preferred choices highlighted in bold.</figDesc><table><row><cell>Parameter</cell><cell>Range</cell></row><row><cell cols="2">Dropout FFNN hidden layer FFNN # of hidden layers Document Encoding Label Smoothing Sampling Prob. Invalid Mentions {0.25, 0.5, 0.75, 1.0} {0.3} {3000} 1 {Independent, Overlap} {0.0} Max. # of BERT Segments {3, 5} Non-coreferent entity weight {1.0, 2.0}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Number of model parameters (in millions).</figDesc><table><row><cell cols="2">LitBank OntoNotes</cell></row><row><cell>U-MEM LB-MEM 46.83 37.36 RB-MEM 46.83</cell><cell>37.42 46.95 46.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Detailed results of the proposed models on the aggregated LitBank cross-validation test set. Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Avg. F1</figDesc><table><row><cell>Model</cell><cell>MUC</cell><cell>B 3</cell><cell>CEAF φ 4</cell></row><row><cell>U-MEM</cell><cell cols="3">90.8 85.7 88.2 80.0 72.1 75.9 65.1 66.0 65.5</cell><cell>76.5</cell></row><row><cell>LB-MEM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 cells</cell><cell cols="3">90.9 80.0 85.1 77.4 64.0 70.1 57.8 53.8 55.7</cell><cell>70.3</cell></row><row><cell cols="4">10 cells 90.0 84.6 87.2 78.1 70.8 74.2 64.2 61.1 62.6</cell><cell>74.7</cell></row><row><cell cols="4">20 cells 90.3 85.0 87.6 79.2 70.9 74.8 64.1 62.0 63.0</cell><cell>75.1</cell></row><row><cell>RB-MEM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 cells</cell><cell cols="3">91.4 74.6 82.2 75.7 51.1 61.0 52.2 21.3 30.3</cell><cell>57.8</cell></row><row><cell cols="4">10 cells 91.1 81.3 85.9 78.5 62.1 69.3 56.3 47.8 51.7</cell><cell>69.0</cell></row><row><cell cols="4">20 cells 90.5 85.1 87.7 79.7 69.8 74.4 61.1 61.0 61.1</cell><cell>74.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Detailed results of the proposed models on the OntoNotes 5.0 test set. Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Avg. F1</figDesc><table><row><cell>Model</cell><cell>MUC</cell><cell>B 3</cell><cell>CEAF φ 4</cell></row><row><cell>U-MEM</cell><cell cols="3">84.6 84.1 84.3 77.2 76.2 76.7 72.5 74.3 73.4</cell><cell>78.1</cell></row><row><cell cols="4">U-MEM* 85.5 85.1 85.3 78.7 77.3 78.0 74.2 76.5 75.3</cell><cell>79.6</cell></row><row><cell>LB-MEM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 cells</cell><cell cols="3">76.4 86.2 81.0 66.4 78.4 71.9 62.0 72.7 66.9</cell><cell>73.3</cell></row><row><cell cols="4">10 cells 81.7 85.9 83.8 72.8 77.9 75.3 67.0 76.4 71.4</cell><cell>76.8</cell></row><row><cell cols="4">20 cells 83.2 86.2 84.7 74.8 78.9 76.8 70.0 76.7 73.2</cell><cell>78.2</cell></row><row><cell cols="4">30 cells 83.8 85.6 84.7 76.1 78.2 77.1 70.4 77.1 73.6</cell><cell>78.5</cell></row><row><cell>RB-MEM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 cells</cell><cell cols="3">72.0 85.7 78.3 60.1 78.9 68.2 57.0 68.9 62.4</cell><cell>69.6</cell></row><row><cell cols="4">10 cells 80.1 85.7 82.8 70.5 78.3 74.2 66.0 73.4 69.5</cell><cell>75.5</cell></row><row><cell cols="4">20 cells 82.8 85.9 84.3 74.8 78.3 76.5 68.0 77.4 72.4</cell><cell>77.8</cell></row><row><cell cols="4">30 cells 84.0 85.2 84.6 76.2 78.2 77.2 72.1 75.6 73.8</cell><cell>78.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Spearman correlation of F1 score with document length and # of entities in OntoNotes dev set.</figDesc><table><row><cell>Model</cell><cell cols="2">Document Length # of Entities</cell></row><row><cell>U-MEM U-MEM* LB-MEM 5 cells 10 cells 20 cells RB-MEM 5 cells 10 cells 20 cells</cell><cell>-0.31 -0.28 -0.36 -0.34 -0.34 -0.37 -0.29 -0.31</cell><cell>-0.28 -0.25 -0.37 -0.33 -0.31 -0.41 -0.30 -0.29</cell></row><row><cell cols="3">Effect of Document Length and Number of En-</cell></row><row><cell>tities.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>Spearman correlation of F1 score with document length and # of entities in aggregated LitBank cross-validation dev set.</figDesc><table><row><cell>Model</cell><cell cols="2">Document Length # of Entities</cell></row><row><cell>U-MEM LB-MEM 5 cells 10 cells 20 cells RB-MEM 5 cells 10 cells 20 cells</cell><cell>-0.07 0.01 -0.06 -0.03 -0.00 0.01 -0.02</cell><cell>-0.36 -0.23 -0.35 -0.32 -0.41 -0.36 -0.33</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We modify the overlap segmentation to respect sentence boundary or token boundary when possible.4  Specifically, they are ordered based on START(·) index with ties broken using END(·).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/dbamman/ lrec2020-coref/tree/master/data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">From the original models, we stripped out just the Span-BERT part which is available at https://huggingface. co/shtoshni/spanbert_coreference_large</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">e2e-coref/blob/master/metrics.py. The two scripts can have some rounding differences.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank David Bamman for help with the Lit-Bank setup, and Patrick Xia for answering questions about their coreference model. We also thank the anonymous EMNLP reviewers for their valuable feedback. This material is based upon work supported by the National Science Foundation under Award Nos. 1941178 and 1941160.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Working Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Baddeley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Annotated Dataset of Coreference in English Literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Lewke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anya</forename><surname>Mansoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entity-Centric Coreference Resolution with Model Stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<title level="m">SpanBERT: Improving Pre-training by Representing and Predicting Spans. TACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT for Coreference Resolution: Baselines and Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cognitively Plausible Models of Human Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Error-Driven Analysis of Challenges in Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end Neural Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Higher-Order Coreference Resolution with Coarseto-Fine Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Referential Reader: A Recurrent Entity Network for Anaphora Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Mention-Synchronous Coreference Resolution Algorithm Based On the Bell Tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
