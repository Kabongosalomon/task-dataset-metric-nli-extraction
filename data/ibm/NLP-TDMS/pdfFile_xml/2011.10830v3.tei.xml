<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boundary-sensitive Pre-training for Temporal Localization in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
							<email>mengmeng.xu@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Pérez-Rúa</surname></persName>
							<email>j.perez-rua@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martínez</surname></persName>
							<email>brais.a@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<email>xiatian.zhu@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<email>li.zhang1@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff1">
								<orgName type="department">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>tao.xiang@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boundary-sensitive Pre-training for Temporal Localization in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many video analysis tasks require temporal localization for the detection of content changes. However, most existing models developed for these tasks are pre-trained on general video action classification tasks. This is due to large scale annotation of temporal boundaries in untrimmed videos being expensive. Therefore, no suitable datasets exist that enable pre-training in a manner sensitive to temporal boundaries. In this paper for the first time, we investigate model pre-training for temporal localization by introducing a novel boundary-sensitive pretext (BSP) task. Instead of relying on costly manual annotations of temporal boundaries, we propose to synthesize temporal boundaries in existing video action classification datasets. By defining different ways of synthesizing boundaries, BSP can then be simply conducted in a self-supervised manner via the classification of the boundary types. This enables the learning of video representations that are much more transferable to downstream temporal localization tasks. Extensive experiments show that the proposed BSP is superior and complementary to the existing action classification-based pre-training counterpart, and achieves new state-of-the-art performance on several temporal localization tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, the focus on video analysis has shifted beyond trimmed video action classification to video temporal localization. This is because in many real-world applications, instead of short (e.g., few seconds long) video clips, long, untrimmed videos are often presented (e.g., from social media websites as YouTube, Instagram) with both non-interesting background and foreground contained e.g., a particular action of interest. This requires a video model * Work done during an internship at Samsung AI Centre.  <ref type="figure">Figure 1</ref>. Pre-training datasets for different tasks. The wellestablished pre-training-then-fine-tuning paradigm for image and video classification model optimization is effective thanks to the availability of large related datasets (e.g., ImageNet and Kinetics). For video temporal localization tasks however, existing datasets are either too small for model pre-training or less effective due to the lack of temporal boundary annotations. We solve this problem by introducing a novel boundary-sensitive pretext (BSP) task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training</head><p>to conduct temporal localization tasks. Examples of these tasks include temporal action localization <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b4">5]</ref>, video grounding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">45]</ref> and step localization <ref type="bibr" target="#b79">[80]</ref>. As in most other visual recognition tasks, recent models designed for video temporal localization are based on deep learning. As such, model pre-training is critical. In particular, a two-staged model training strategy is commonly adopted <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4]</ref>: first, a video encoder is pre-trained on a large action classification dataset (e.g., Kinetics <ref type="bibr" target="#b5">[6]</ref> 1 , Sports-1M <ref type="bibr" target="#b28">[29]</ref>), then, a temporal localization head is trained on the target small-scale temporal localization dataset leaving the video encoder fixed. Thus, there is a clear mismatch between the pre-training of the video encoder and the target task. Ideally, model pre-training should be carried out on temporal boundary-sensitive tasks. However, this is not possible due to the lack of large scale video datasets with temporal boundary annotation. This is because temporal boundaries' labeling is much more expensive and tedious than video-level class labeling due to the need for manually examining every single frame <ref type="bibr" target="#b1">2</ref> .</p><p>In this paper, we investigate the under-studied yet critical problem of model pre-training for temporal localization in videos. Due to the difficulties in collecting a largescale video dataset with temporal boundary annotations, we propose to synthesize large-scale untrimmed videos with temporal boundary annotations by transforming the existing trimmed video action classification datasets. Once the pre-training data problem is tackled, we focus on defining and evaluating a number of pretext tasks capable of exploiting the particularities of the synthesized data through selfsupervision.</p><p>In particular, the first key challenge boils down to how to obtain large scale training video data with temporal boundary information in a scalable and cheap manner. To that end, we introduce a simple yet effective method for generating three types of temporal boundary at large scale using existing action classification video data (e.g.Kinetics). More specifically, we generate artificial temporal boundaries corresponding to video content changes by either stitching trimmed videos containing different classes, stitching two video isntances of the same class, or by manipulating the speed of different parts of a video instance. The associated pretext task used to train the video model uses standard supervised classification learning, where the task is to distinguish between the types of temporal boundaries as defined above. We experimentally show that such task offers superior performance to other possible pretext tasks, such as regressing the temporal boundary location, and that combining the different boundary types into a multi-class classification problem is superior to all binary classification tasks in isolation.</p><p>The following contributions are made in this work: (I) We investigate the problem of model pre-training for temporal localization tasks in videos, which is largely understudied yet particularly significant to video analysis. (II) We propose a scalable video synthesis method that can generate a large number of videos with temporal boundary information. This approach not only solves the key challenge of lacking large pre-training data, but also facilitates the design labelled as containing the target action. Therefore the rest of the untrimmed video cannot be treated as background. <ref type="bibr" target="#b1">2</ref> Boundary annotations are 3.8× more expensive than class annotations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b75">76]</ref> of model pre-training. (III) Extensive experiments show that temporal action localization, video grounding, and step localization tasks can significantly benefit from the proposed model pre-training, yielding compelling or new stateof-the-art performance on a number of benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Temporal localization tasks. Temporal localization in videos encompasses tasks such as temporal action localization (TAL), video grounding and step localization. Although those tasks have their own particularities, they share the same target: recognizing the specific point in time where the semantic content of the video changes. TAL focuses on predicting the temporal boundaries and class of an action instance in untrimmed videos <ref type="bibr" target="#b22">[23]</ref>. Instead, video grounding generalizes temporal action localization by not relying on a predefined set of action categories <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b54">55]</ref>, the task being to localize the segment in the video that best matches a given language query.</p><p>Step localization <ref type="bibr" target="#b79">[80]</ref> is associated with the detection of different actions involved in the execution of a complex task, e.g. Change tire, in instructional videos <ref type="bibr" target="#b42">[43]</ref>. Instructional videos are highly edited audiovisual tutorials with aesthetical transitions and cuts.</p><p>Due to constraints on computational cost, solutions to these tasks are usually not based on end-to-end training. Instead, a video encoding network is pre-trained on an action classification task to enable large-scale training. A temporal localization model is then trained using a fixed feature extraction backbone.</p><p>Video encoding networks. It is common among state of the art methods to use a pre-trained network as the video encoder. Such network is trained on a classification task using standard cross-entropy, typically on large-scale datasets such as Kinetics <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b71">72]</ref>. For example, it is common for TAL, e.g., <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b2">3]</ref>, to use features extracted with a two-stream <ref type="bibr" target="#b51">[52]</ref> TSN model <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b77">78]</ref>. That is, the model comprises two TSN networks, one with ResNet50 <ref type="bibr" target="#b20">[21]</ref> backbone trained on RGB, and another with a BN-Inception backbone <ref type="bibr" target="#b25">[26]</ref> trained on Optical Flow. Other methods use 3D CNN-based models such as two-stream I3D models <ref type="bibr" target="#b5">[6]</ref>, e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b70">71]</ref>, or Pseudo-3D <ref type="bibr" target="#b47">[48]</ref>, e.g., <ref type="bibr" target="#b38">[39]</ref>. Alternatively, some methods exploit the temporal segment annotations on the downstream temporal localization datasets to define a classification task, and use it to pre-train the video encoder <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">45]</ref>. This results in less of a domain gap, at the cost of large-scale training.</p><p>A few methods further add an end-to-end fine-tuning stage directly on the downstream tasks, e.g. R-C3D <ref type="bibr" target="#b63">[64]</ref>, PBR-net <ref type="bibr" target="#b36">[37]</ref>. However, end-to-end training is achieved by compromising other important aspects, e.g. using batch size of 1, resulting in lower performance in practice.</p><p>Although an action classifier trained through cross-entropy can represent the overall content of a video segment, a feature extractor trained in this manner is not tuned to be sensitive to specific temporally-localized structures, such as the start or end of an action. Instead, we propose a boundary-sensitive self-supervised pre-training that results in features with the desired temporally-localized sensitivity.</p><p>Temporal localization heads. Temporal action localization methods follow either a two-stage or a one-stage approach. Two-stage methods first generate candidate action segments (e.g., proposals) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14]</ref>, and then use a classifier on each proposal to obtain a class score <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b33">34]</ref>. One-stage methods predict instead the temporal action boundaries or generate the proposals, and classify them in a shared network <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3]</ref>. Video grounding is similar to temporal action localization, but requires a language model. The current literature can also be clustered into the two groups. (1) Proposalbased approaches adopt a propose-and-rank pipeline <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b73">74]</ref>, relying first on a proposal model much like for temporal action localization, and then rank the resulting snippets based on their similarity with the textual query <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b72">73]</ref>. (2) Proposal-free methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b44">45]</ref> directly regress the temporal boundaries of the queried moment from the multi-modal fused feature information.</p><p>Step localization in instructional videos. The task corresponds to the alignment of a set of steps required to complete a task, in the form of text entries, and a video exemplifying such task <ref type="bibr" target="#b42">[43]</ref>. Recently, <ref type="bibr" target="#b80">[81]</ref> showed improved step localization performance over multiple models when using an actionness-based proposal generation method.</p><p>In order to show the benefit of our proposed boundarysensitive pre-training, we adopt a set of recent publiclyavailable methods representative of the current state of the art for these tasks: G-TAD <ref type="bibr" target="#b65">[66]</ref>, LGI <ref type="bibr" target="#b44">[45]</ref> and 2D-TAN <ref type="bibr" target="#b74">[75]</ref>. In order to keep a fair comparison with prior work, we do not fine-tune the video encoding network on the downstream dataset. When use our BSP features in conjunction with these models, we use the default training configurations defined by the respective authors, and report the performance using the provided evaluation scripts.</p><p>Self-supervised learning in videos. While current temporal localization literature focuses on pre-training through supervised learning, the rapid advancement of selfsupervised learning makes it a promising alternative to sidestep end-to-end training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b61">62]</ref>. Among these, a large body of research has focused on finding effective temporal-related pretext tasks. Some works considered frame ordering, either learning through triplets of frames <ref type="bibr" target="#b43">[44]</ref>, through sorting a sequence <ref type="bibr" target="#b30">[31]</ref>, or by distinguishing whether sequences are played forward or backwards <ref type="bibr" target="#b61">[62]</ref>. Alternatively, pretext tasks related to the speed of the video have recently become popular <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b56">57</ref>]. An effective variant on this theme was proposed in <ref type="bibr" target="#b26">[27]</ref>, where clips undergo one among a set of possible augmentations, and at the same time are also sampled at one of a set of possible frame-rates. The pretext task is then to correctly classify both the playback speed and the temporal augmentation applied. Alternative approaches include predicting motion-related statistics <ref type="bibr" target="#b55">[56]</ref> and more direct extensions of successful imagebased contrastive learning methods to the video realm <ref type="bibr" target="#b46">[47]</ref>.</p><p>These methods exploit video-specific characteristics to force the network to focus on the semantic content within the video, inducing representations capturing long-term temporal semantic relations, but forces invariance to the relative positioning of the sampled segment within the action. They thus cannot be adopted for pre-training a temporal localization model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem context</head><p>We consider the model pre-training problem for temporal localization in videos. This is the first stage of the common first pre-train backbone, then train localization head paradigm, and the downstream tasks in this context include temporal action localization <ref type="bibr" target="#b65">[66]</ref>, video grounding <ref type="bibr" target="#b1">[2]</ref>, and step localization <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b80">81]</ref>. The vanilla pretraining method simply conducts supervised learning on a large video dataset (e.g., Kinetics) D tr = {V i } N i=1 with action class labels as ground-truth supervision. This brings about action content awareness to the model. As a result, such a pretrained model is usually superior to those with random initialization and pretrained on image data (e.g., Im-ageNet). However, this method is limited in capturing temporal boundary information as required by temporal localization tasks, because boundary annotations are not available in existing large video datasets.</p><p>To overcome the above limitation of action classification-based pre-training, we aim to directly address the unavailability of boundary labels in videos, which is intuitive though non-trivial. As a large number of video data is needed for pre-training, the method must be scalable without expensive manual annotation. We therefore adopt a data synthesis approach. Existing trimmed video datasets are selected as the video source since they exist in large scale. In our implementation, Kinetics <ref type="bibr" target="#b5">[6]</ref> is chosen as it is the most common selection for vanilla pre-training of a video encoder model in the literature. Moreover, this selection allows to avoid extra training data which further ensures accurate evaluation and fair comparisons.</p><p>Given trimmed video data with action class labels, we introduce four temporal boundary concepts including different-class boundary, same-class boundary, different- speed boundary, same-speed boundary. They all require zero extra annotation in video synthesis and hence enable us to generate an arbitrary number of video samples with boundary labels. Next, we will describe the proposed boundary-sensitive video synthesis method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Boundary-sensitive video synthesis</head><p>Temporal boundary refers to a transition of shots or scenes, or a change of action content. In this work, we consider two perspectives of the video source: class semantics and motion speed, both of which are available in Kinetics (i.e., class label and frame rate). Four different boundary classes are then formulated as detailed below.</p><p>(1) Diff-class boundary. This boundary is defined as the edge between two action instances from different classes. It is the most intuitive boundary, as typically presented in untrimmed videos with different actions taking place continuously. To synthesize a video with this boundary, we use two videos V 1 and V 2 sampled randomly from different action categories. Specifically, we first perform uniform frame sampling in each video,</p><formula xml:id="formula_0">F 1 = {f 1,i } τ + i=1 ⊂ V 1 and F 2 = {f 2,i } τ + i=1 ⊂ V 2 , where f 1,i (f 2,i ) denotes the i-th frame from V 1 (V 2 )</formula><p>, and τ + frames are sampled from each video. Then a new video S dc = {f dc i } 2τ i=1 ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>) is synthesized with each frame formed as:</p><formula xml:id="formula_1">f dc i =    f 1,i i ∈ [1, τ − ], ω 1 (i)f 1,i + ω 2 (i)f 2,i−τ + i ∈ (τ − , τ + ], f 2,i−τ + i ∈ (τ + , 2τ ].<label>(1)</label></formula><p>where controls the period of action transition. The transition is made by a weighted blending approach with the weights defined as ω 1 (i) = 1 2 (τ + −i), ω 2 (i) = 1 2 (i−τ + ). This transition injects some smoothing effect from one action to the other. As a result, the output video S dc eliminates abrupt content change, making the following model pre-training meaningful without trivial solution.</p><p>(2) Same-class boundary. Complementary to diff-class boundary, this aims to simulate the scenarios where the same action happens repeatedly and continuously. This is frequently observed in untrimmed videos with multiple different shots of the same action class in a row. Similarly, we select two videos V 1 and V 2 from the same action class and sample τ frames <ref type="figure" target="#fig_1">Fig. 2(b)</ref>) is synthesized by concatenation as:</p><formula xml:id="formula_2">F 1 = {f 1,i } τ i=1 and F 2 = {f 2,i } τ i=1 from each selected video. A new video S sc = {f sc i } 2τ i=1 (</formula><formula xml:id="formula_3">f sc i = f 1,i i ∈ [1, τ ], f 2,i−τ i ∈ (τ, 2τ ].<label>(2)</label></formula><p>Transition is not applied in this case, since the semantic content is similar in the two input videos.</p><p>(3) Diff-speed boundary. This boundary class is motivated by an observation that the speed of content change varies from background (e.g., without action) to foreground (e.g., with action) and from one action instance to the other. Hence speed change entraps potentially useful temporal boundary information. Formally, we start with sampling a random video</p><formula xml:id="formula_4">V = {f 1 , f 2 , . . . } from the source data. Then, a new video S ds = {f ds i } 2τ i=1</formula><p>with two different speed rates ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>) is generated by</p><formula xml:id="formula_5">f ds i = f i i ≤ t (original rate), f t+γ(i−t) i &gt; t (new rate).<label>(3)</label></formula><p>where t is the change point of speed and γ = 1 denotes the sampling rate introduced in video synthesis. When γ ∈ (0, 1), temporal upsampling is triggered. When γ &gt; 1, temporal downsampling takes place. By varying γ, a large number of speed change cases can be simulated. In case the frame index t + γ(i − t) is not an integer, we simply use the nearest frame. More complex frame interpolation can be considered which however was found to have no clear advantage in performance.</p><p>(4) Same-speed boundary. This is introduced to serve as a non-boundary class for conceptual completion. For this class, the same videos from the source set are used with the coherent original speed rate throughout all the frames in each video. For notation consistency, we denote the videos of this type as S ss . Collectively, we denote all four types of boundarysensitive videos as S = {S dc , S sc , S ds , S ss }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Boundary-sensitive pre-training</head><p>Given the boundary-sensitive video data S as generated in Sec. 3.2, we now describe how to use them for video model pre-training so that the pre-trained model can benefit the temporal localization downstream tasks. For simplicity and easy adoption of our method, we consider two common supervised learning algorithms based on the synthetic boundary information.</p><p>Pre-training by classification (default choice). An intuitive pre-training method is supervised classification by  treating each type of synthetic video as a unique class. That is, a four-way classification task. Formally, we first label a boundary class y ∈ {0, 1, 2, 3} to each video x ∈ S according to their boundary type. With a target video model θ, we predict the boundary classification vector p = {p 0 , p 1 , p 2 , p 3 } for a given training video x. To pre-train the model, we use the cross-entropy loss function. The main merit of this method is to easily accommodate different types of boundary supervision in a principled manner. This allows the effective use of our synthetic video data. Pre-training by regression. An alternative way for pre-training with our training data is the change point regression. For more stable learning, we convert the ground-truth change point µ into a 1D Gaussian heatmap y = [y 1 , · · · , y t , · · · , y L ] as the regression target,</p><formula xml:id="formula_6">y t = exp [− 1 2τ (t − µ) 2 ] for t ∈ [1, L],</formula><p>where we sample a snippet comprising τ frames from the video with length L. Let the model prediction output be r = [r 1 , · · · , r t , · · · , r L ]. We minimize the smooth L 1 norm of (y − r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Integration with action classification-based pre-training</head><p>We integrate our method with the classification-based pre-training features for enhancing boundary awareness as required for temporal localization downstream tasks. Three architecture designs are considered: two-steam, two-head, and feature distillation.</p><p>Two-stream. This design consists of two steams in parallel, one for action classification-based pre-training and one for our boundary-sensitive pre-training ( <ref type="figure" target="#fig_2">Fig. 3(a)</ref>). For simplicity, we use the same backbone for both. To integrate their information, feature concatenation is adopted at the penultimate layer.</p><p>Two-head. In contrast to the two-stream design, this is a more compact and efficient architecture with all layers shared for both tasks except the classification layer ( <ref type="figure" target="#fig_2">Fig. 3(b)</ref>). One implicit assumption is that the two types of feature representation can be well fused throughout the feature backbone through end-to-end joint training.</p><p>Feature distillation. An alternative approach to the twostream or two-head network design is to employ a single network, and to train it to produce the same features as the independent networks through the imposition of feature matching losses <ref type="figure" target="#fig_2">(Fig. 3(c)</ref>). In particular, let f v be the network trained through standard supervision on Kinetics, and f b a network trained in the proposed self-supervised manner. We then train a single network f s and two pointwise projection layers, h 1 and h 2 to minimize a feature matching loss in the form of <ref type="bibr" target="#b1">2</ref> 2 , shown as the dashed line in <ref type="figure" target="#fig_2">Fig 3(c)</ref>. In practice, we also use a standard cross-entropy loss on top of the two feature matching losses.</p><formula xml:id="formula_7">f v (x) − h 1 (f s (x)) 2 2 + f b (x) − h 2 (f s (x))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setup</head><p>Temporal localization tasks. In our evaluation, three representative temporal localization tasks for untrimmed videos are considered: temporal action localization <ref type="bibr" target="#b22">[23]</ref>, video grounding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>, and step localization <ref type="bibr" target="#b80">[81]</ref>.</p><p>As discussed in Sec. 2, these tasks share the same target of recognizing the specific point in time where the semantic content of the video changes. Solutions to the three tasks use the same two-step training paradigm: first pre-training the video encoder with the BSP method, followed by training the task-specific model with our BSP model frozen as video feature extractor. This allows to explicitly examine the quality and efficacy of model pre-training.</p><p>Datasets. We use three different video datasets to evaluate the performance of temporal localization tasks.</p><p>(1) ActivityNet-1.3 <ref type="bibr" target="#b22">[23]</ref> is a popular benchmark for temporal action localization. It contains 19,994 annotated untrimmed videos with 200 different action classes. The split ratio of train:val:test is 2:1:1. Each video has an average of 1.65 action instances. Following the common practice, we train and test the models on the training and validation set.</p><p>(2) Charades-STA <ref type="bibr" target="#b14">[15]</ref> is a commonly used video grounding dataset extended from the action recognition dataset Charades <ref type="bibr" target="#b50">[51]</ref>. It contains 9, 848 videos of daily indoors activities, with 12, 408/3, 720 moment-sentence pairs in train/test set respectively.</p><p>(3) CrossTask <ref type="bibr" target="#b79">[80]</ref> is an instructional video dataset depicting complex tasks e.g. make pancakes. We evaluate the step localization performance in terms of actionness prediction <ref type="bibr" target="#b80">[81]</ref>. Following the evaluation protocol described in <ref type="bibr" target="#b80">[81]</ref>, we focus on the 18 primary tasks with temporal annotations i.e. 2750 videos with a 3:1 train:test split ratio. LGI's evaluation metrics <ref type="bibr" target="#b44">[45]</ref> to report the mean tIoU between predictions and ground-truth. For step localization, we evaluate it in terms of actionness prediction <ref type="bibr" target="#b80">[81]</ref>. We follow its evaluation protocol, dividing the test videos onto non-overlapping 0.2s segments and report the framewise binary average precision (AP). Any segment associated with a step annotation is considered as foreground (positive), and background (negative) otherwise.</p><p>Implementation details. Throughout the experiments, we only use RGB input to compute the video representation since optical flow is computationally expensive and adds complexity to the feature extraction model. However, current standard features rely on TSN <ref type="bibr" target="#b58">[59]</ref>, which is insensitive to time. Thus, removing the optical flow stream can have a very negative impact. In order to alleviate this issue, we adopt the Temporal Shift Module (TSM) architecture <ref type="bibr" target="#b31">[32]</ref>. Given a variable-length video, we firstly sample every 8 consecutive frames as a snippet. Then we feed the snippet into our pre-trained models, and save the features before the fully connected layer. Thus, we obtain a set of snippet-level feature for the untrimmed video. For language representation as required in video grounding task, the pre-processing for text queries includes lowercase conversion and tokenization. A pre-trained GloVe model <ref type="bibr" target="#b45">[46]</ref> us then used to obtain the initial 300-dimensional word embeddings. A three-layer LSTM <ref type="bibr" target="#b24">[25]</ref> follows to produce the feature representation of the input sentence.</p><p>For the state-of-the-art temporal localization models, we select G-TAD <ref type="bibr" target="#b65">[66]</ref> for temporal action localization on Activity-1.3, 2D-TAN <ref type="bibr" target="#b74">[75]</ref> and LGI <ref type="bibr" target="#b44">[45]</ref> for video grounding on Charades-STA. A simple linear classifier <ref type="bibr" target="#b78">[79]</ref> is used for step localization in CrossTask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to the state-of-the-art</head><p>In this section, we compare the performance of the proposed BSP features under different tasks and different temporal localization networks. We combine the BSP features with those obtained using classification-based pre-training in a two-stream manner as per Sec. 3.4.</p><p>On the TAL task, our BSP feature can significantly boost the performance of G-TAD, see Tab. 1. Adding BSP features increases performance by 0.93% at 0.5 IoU, and by 0.5% at average mAP. Compared to the features originally used by G-TAD, our method does not require time-consuming computations to extract optical flow, neither fine-tune our video encoder on ActivityNet. To further validate our method, we conducted experiments on THUMOS-14 and HACS-1.1 dataset against an RGB-only baseline. We observed a gain of +9.66% mAP@IoU=0.5 and +0.78% on AmAP respectively. Please refer to our supplementary for details.</p><p>We also validate our BSP feature on the video grounding task in Tab. 2. Our BSP benefits both an anchor-based method such as 2D-TAN <ref type="bibr" target="#b74">[75]</ref> and an anchor-free method such as LGI <ref type="bibr" target="#b44">[45]</ref>. The latter is also comparable to state-ofthe-art performance on Charades-STA.</p><p>LGI follows <ref type="bibr" target="#b11">[12]</ref> to use a pre-trained video encoder on the downstream dataset. For a fair comparison to BSP, we only include methods that do not fine-tune on Charade-STA. <ref type="table">Table 3</ref> showcases the results for step localization in terms of actionness prediction. Our baseline, Linear clf., solely pre-trained on Kinetics400, achieves competitive re- <ref type="table">Table 3</ref>.</p><p>Step localization results on CrossTask dataset. Model performance is shown as framewise average precision (AP) for actionness prediction, as in <ref type="bibr" target="#b80">[81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>HowTo100M Pretrained   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature Visualization</head><p>We compare the BSP and vanilla snippet-feature in <ref type="figure" target="#fig_3">Fig. 4</ref>. Concretely, we visualize the absolute difference of feature representations from two consecutive snippets, while a natural boundary only appears in the first snippet. The feature representations are reshaped into 32 × 32 matrices, and the absolute differences are visualized in the top-right of <ref type="figure" target="#fig_3">Fig. 4</ref>. The upper matrix from BSP video encoder contains more high-difference (red) values, indicating that our method produces distinguishable snippet representations that are sensitive to boundary. Moreover, we compute the distribution of the absolute difference of the two <ref type="table">Table 4</ref>. Different boundary-sensitive pretext tasks. We compare our four pretext tasks with two loss functions. The random baseline refers to a randomly initialized video encoder, while the vanilla baseline refers to the video encoder pre-trained on Kinetics-400 in a fully supervised manner. We compare the performance of G-TAD on ActivityNet 1.3 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edition</head><p>Task 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>To verify the effectiveness of our proposed boundarysensitive pre-training strategy, we perform in-depth ablation analysis of different boundary-sensitive tasks, fusion methods, and backbone depth. For the ease of experimentation, all the ablation studies employ a ResNet18 backbone.</p><p>Boundary-sensitive tasks We first show in Tab. 8 the performance results for a number of pretext tasks. Given the four types of augmented data, S = {S dc , S sc , S ds , S ss }, we consider the regression loss described in Sec. 3.3 for each individual task, binary classification task for each of the pretext tasks, and the final 4-way classification task. We further include a randomly initialized video encoder as a baseline. Although four different boundary classes together (S) output the best overall performance, the change of speed ({S ds , S ss }) gives more boost than the change of class semantics, ({S dc , S ss } and {S sc , S ss }). Moreover, the classification task is consistently better than the regression counterpart. We believe this is due to it being a more challenging learning task and its inability to exploit the original source data. Moreover, for temporal localization, the model needs to understand whether a segment completes, i.e., boundaries are present, rather than where exactly the boundary is. It is also clear that there is still a significant performance drop against classification-based pre-training. Only leveraging boundary information is not enough to localize actions, as global semantic information is also required. This is given by classification-based pre-training. Feature Integration We further investigate different ways to integrate both features, including two-stream, two-head, and feature distillation. We conduct experiments on both G-TAD and 2D-TAN. According to Tab. 5, the two-stream method gives the best overall performance for temporal localization tasks. Among the single-network solutions, feature distillation is better than multi-task learning (twohead).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to model backbone and capacity</head><p>We investigate the impact of different model backbones and capacity in Tab. 6. We use G-TAD for the TAL task on ActivityNet 1.3, with TSM-18, TSM-50, and R(2+1)D-34 as backbone choices. It is clear that a deeper model produces better BSP features, and that the net contribution when concatenated to classification-based BSP features is consistently and similarly positive in all cases. This verifies the general efficacy of our method on different backbones with varying capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we have investigated the under-studied problem of model pre-training for temporal localization tasks in videos by introducing a novel Boundary-Sensitive Pretext (BSP) task. Beyond vanilla pre-training on trimmed video data, we exploited a large number of videos with different types of synthetic temporal boundary information, and explored a number of pretext task designs using these boundary-sensitive videos. We evaluated extensively the BSP model on three representative temporal localization tasks with different input modalities and motion complexity.</p><p>The results demonstrate that our BSP can strongly enhance the vanilla model with boundary-sensitive feature representations, yielding competitive or new state-of-the-art performance on these temporal localization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison on other TAL datasets</head><p>To further validate our method, we conduct experiments on two more datasets with different sizes for temporal action localization. On both datasets, we firstly use a Kinetics pre-trained TSM-50 model to extract feature and then run G-TAD on top of it. Then, for fair comparison, we use our BSP feature extractor for the same process. Both experiments are tested with the same environment and same hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Evaluation on larger dataset</head><p>Human Action Clips and Segments (HACS-1.1) <ref type="bibr" target="#b75">[76]</ref> is a recent large-scale temporal action localization dataset. It contains 140K complete segments on 50K videos in 200 action categories. Compared with ActivityNet holding 20K videos, HACS dataset is larger and more challenging.</p><p>We show the performance of BSP on HACS-1.1 dataset against an RGB-only baseline in Tab. 7. We observe a gain of +0.78% on Average mAP, shown by the grey column of the table. The performance of our method on THUMOS-14 dataset against an RGB-only baseline is shown in Tab. 7. As shown by the grey column of the table, a significant gain of +9.66% on mAP at IoU=0.5 further demonstrates the effectiveness of our proposed approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison on other TAL methods</head><p>To test the robustness of BSP to different TAL methods, we also conduct experiments on Boundary-Matching Network (BMN) <ref type="bibr" target="#b32">[33]</ref> based on a publicly available reimplementation <ref type="bibr" target="#b2">3</ref> . BMN is an effective, efficient and endto-end trainable proposal generation method, which generates proposals with precise temporal boundaries and reliable confidence scores simultaneously. BMN proposed the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed temporal action proposals. Please note that BMN targets the problem of temporal action proposal generation, which is a sub-task of temporal action localization. In the proposal generation task, models only predict actions segments, but do not assign action labels to them. Thus, the evaluation metric of BMN is different than for TAL. The proposal generation task is evaluated by the top-k average recall of predictions (AR@k), where k ∈ {1, 5, 10, 100}. The area under the recall curve (AUC) is used to evaluate the overall performance.</p><p>It can be observed from Tab. 9 that BSP brings consistent improvements over all the evaluation metrics on BMN, showing that our method can generalize to different methods. Particularly, our method can get 67.61 AUC which is close to the value, 67.7, reported by the code repository. However, no optical flow is used in BSP method. <ref type="table">Table 9</ref>. Evaluation of temporal action proposal generation on ActivityNet. "*" indicates RGB-only Kinetics pre-trained TSM feature without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>AR@1 AR@5 AR@10 AR@100 AUC </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison to other pre-training methods</head><p>We compare BSP to two other popular video selfsupervised methods in Tab. 10: Arrow of Time <ref type="bibr" target="#b61">[62]</ref> and SpeedNet <ref type="bibr" target="#b3">[4]</ref>. Furthermore, we also include the results when ensembling two classification-based pre-trained models and for a randomly-initialized network. All comparisons use the two-steam integration with the same classificationbased pre-trained TSM-18 video encoder. A random model does not give extra information, while Speednet <ref type="bibr" target="#b3">[4]</ref> and Arrow of Time <ref type="bibr" target="#b61">[62]</ref> can enrich the original features. Since they are not sensitive to the action boundaries, these methods are experimentally equivalent to training an ensemble of two vanilla video encoder (last row). Their performance is thus clearly inferior to that of our BSP method. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of our boundary-sensitive video synthesis. For each group of three rows we show from top to bottom: (a) a real clip from ActivityNet with real action class boundary; (b) a clip from Kinetics-400 with no boundaries; and (c) a synthesized video by one of the proposed methods using samples from Kinetics-400 with synthetic boundaries. (Top) Diff-class boundary ( ); two clips from different categories are smoothly merged around frame #5. (Middle) Same-class boundary ( ); two clips from the same Kinetics category are stitched together from frame #4. (Bottom) Diff-speed boundary ( ); a clip from Kinetics is artificially sped up from frame #4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Integrating BSP with vanilla action-classification pre-training. (a) Two independently-trained feature streams produce vanilla action and BSP features that are concatenated as output. (b) Single feature stream with two-head classification. (c) Task specialisation enforced by feature distillation from two teacher streams pre-trained on vanilla and BSP tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of BSP snippet-feature and vanilla snippet-feature. Top: we visualize the consecutive snippet feature difference from BSP and vanilla video encoders. Comparing the absolute differences, our BSP encoder produces distinguishable snippet representations sensitive to the boundary. Bottom: The distributions of the absolute difference of boundary and nonboundary snippets from BSP and vanilla encoders show that the vanilla encoder fails to properly capture boundary information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>sults w.r.t. Zhukov et al. [81] (47.6% vs. 46.9%). Enhancing the Linear clf. with our BSP feature boosts the performance by 1.2%). These results demonstrate the versatility of our pre-training to capture semantic temporal changes for a different visual domain, namely instructional videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>TAL on ActivityNet-1.3 validation set. "*" indicates RGB-only Kinetics pre-trained TSM feature without fine-tuning.</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell>0.75</cell><cell cols="2">0.95 Average</cell></row><row><cell>Singh et al. [53]</cell><cell>34.47</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang et al. [60]</cell><cell>43.65</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Chao et al. [7]</cell><cell cols="3">38.23 18.30 1.30</cell><cell>20.22</cell></row><row><cell>SCC [22]</cell><cell cols="3">40.00 17.90 4.70</cell><cell>21.70</cell></row><row><cell>CDC [49]</cell><cell cols="3">45.30 26.00 0.20</cell><cell>23.80</cell></row><row><cell>R-C3D [64]</cell><cell>26.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BSN [34]</cell><cell cols="3">46.45 29.96 8.02</cell><cell>30.03</cell></row><row><cell>P-GCN [71]</cell><cell cols="3">48.26 33.16 3.27</cell><cell>31.11</cell></row><row><cell>BMN [33]</cell><cell cols="3">50.07 34.78 8.29</cell><cell>33.85</cell></row><row><cell>BC-GNN [3]</cell><cell cols="3">50.56 34.75 9.37</cell><cell>34.26</cell></row><row><cell>G-TAD [66]</cell><cell cols="3">50.36 34.60 9.02</cell><cell>34.09</cell></row><row><cell>G-TAD*</cell><cell cols="3">50.01 35.07 8.02</cell><cell>34.26</cell></row><row><cell>G-TAD*+BSP (Ours)</cell><cell cols="3">50.94 35.61 7.98</cell><cell>34.75</cell></row></table><note>Evaluation metrics. We adopt the standard performance metrics specific for each downstream task. For temporal ac- tion localization, mean Average Precision (mAP) at vary- ing temporal Intersection over Union (tIoU) thresholds is used. Following the official evaluation setting, we report mAP scores at three tIoU thresholds of {0.5, 0.75, 0.95} and the average mAP over ten thresholds of [0.05 : 0.95] with step at 0.05 for ActivityNet-1.3. For video grounding, we report the top-1 recall at three different tIoU thresholds, {0.3, 0.5, 0.7} for Charades-STA dataset. We also follow</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Video grounding on Charades-STA. "*" indicates RGBonly Kinetics pre-trained TSM feature without fine-tuning. We use the original evaluation code of LGI and 2D-TAN. 2D-TAN does not compute R@0.3 and mIoU, hence why it is not reported.</figDesc><table><row><cell>Method</cell><cell cols="4">R@0.3 R@0.5 R@0.7 mIoU</cell></row><row><cell>CTRL [15]</cell><cell>-</cell><cell>21.42</cell><cell>7.15</cell><cell>-</cell></row><row><cell>SMRL [61]</cell><cell>-</cell><cell>24.36</cell><cell>9.01</cell><cell>-</cell></row><row><cell>SAP [10]</cell><cell>-</cell><cell>27.42</cell><cell>13.36</cell><cell>-</cell></row><row><cell>MLVI [65]</cell><cell>54.70</cell><cell>35.60</cell><cell>15.80</cell><cell>-</cell></row><row><cell>MAN [73]</cell><cell>-</cell><cell>46.53</cell><cell>22.72</cell><cell>-</cell></row><row><cell>DRN [72]</cell><cell>-</cell><cell>53.09</cell><cell>31.75</cell><cell>-</cell></row><row><cell>2D-TAN [75]</cell><cell>-</cell><cell>42.80</cell><cell>23.25</cell><cell>-</cell></row><row><cell>2D-TAN*</cell><cell>-</cell><cell>48.36</cell><cell>27.88</cell><cell>-</cell></row><row><cell>2D-TAN*+BSP</cell><cell>-</cell><cell>51.64</cell><cell>29.57</cell><cell>-</cell></row><row><cell>LGI* [45]</cell><cell>60.67</cell><cell>45.65</cell><cell>23.87</cell><cell>43.40</cell></row><row><cell>LGI*+BSP</cell><cell>68.76</cell><cell>53.63</cell><cell>29.27</cell><cell>50.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Different feature fusion methods. We compare three fusion methods: two-stream, two-head, and feature distillation. The performance is measured by Recall (%) of 2D-TAN on Charades and mAP (%) of G-TAD on ActivityNet 1.3 dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">2D-TAN</cell><cell></cell><cell></cell><cell>G-TAD</cell></row><row><cell>tIoU</cell><cell>0.5</cell><cell>0.7</cell><cell>0.5</cell><cell cols="2">0.75 0.95</cell><cell>Avg</cell></row><row><cell>vanilla</cell><cell cols="6">39.78 22.15 49.64 34.16 7.68 33.59</cell></row><row><cell>2-stream</cell><cell cols="6">44.01 24.95 50.09 34.66 7.95 33.96</cell></row><row><cell>2-head</cell><cell cols="6">39.95 22.85 49.50 34.00 8.38 33.54</cell></row><row><cell>feat dist</cell><cell cols="6">44.65 24.73 49.67 34.40 7.74 33.74</cell></row><row><cell cols="7">Table 6. Model backbone and capacity. We compare perfor-</cell></row><row><cell cols="7">mance when using TSM-18, TSM-50, and R(2+1)d-34 for G-TAD</cell></row><row><cell cols="3">on ActivityNet 1.3 dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell>BSP</cell><cell>0.5</cell><cell></cell><cell>0.75</cell><cell cols="2">0.95 Average</cell></row><row><cell>TSM-18</cell><cell></cell><cell cols="4">49.64 34.16 7.68</cell><cell>33.59</cell></row><row><cell>TSM-18</cell><cell></cell><cell cols="4">50.09 34.66 7.95</cell><cell>33.96</cell></row><row><cell>TSM-50</cell><cell></cell><cell cols="4">50.32 35.07 8.02</cell><cell>34.26</cell></row><row><cell>TSM-50</cell><cell></cell><cell cols="4">50.94 35.61 7.98</cell><cell>34.75</cell></row><row><cell>R(2+1)d-34</cell><cell></cell><cell cols="4">49.57 34.92 8.43</cell><cell>34.05</cell></row><row><cell>R(2+1)d-34</cell><cell></cell><cell cols="4">50.28 35.65 8.06</cell><cell>34.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Evaluations on HACS-1.1 dataset on temporal action localization. "*" indicates RGB-only Kinetics pre-trained TSM feature without fine-tuning.<ref type="bibr" target="#b27">[28]</ref> dataset contains 413 temporally annotated untrimmed videos with 20 action categories. We use the 200 videos in the validation set for training and evaluate on the 213 videos in the testing set. Different from Ac-tivityNet, THUMOS-14 has in average 16 action instances per video for the test set. Thus, the sensitivity to action boundaries plays a more important role for its temporal action localization.</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell>0.75</cell><cell cols="2">0.95 Average</cell></row><row><cell>G-TAD*</cell><cell cols="3">37.50 23.80 7.10</cell><cell>24.25</cell></row><row><cell>G-TAD* +BSP</cell><cell cols="3">38.10 24.73 7.76</cell><cell>25.03</cell></row><row><cell>Gain</cell><cell cols="4">+0.60 +0.93 +0.66 +0.78</cell></row><row><cell cols="4">A.2. Evaluation on smaller dataset</cell></row><row><cell>THUMOS-14</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Evaluation of temporal action localization on THUMOS-14 dataset. "*" indicates RGB-only Kinetics pretrained TSM feature without fine-tuning. TAD* 46.61 39.46 30.14 20.13 12.15 G-TAD* +BSP 52.34 46.28 39.80 30.81 21.14 Gain +5.73 +6.82 +9.66 +10.68 +8.99</figDesc><table><row><cell>Method</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>G-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Comparison to other pre-training methods. We compare BSP to other pre-training strategies. The results show TAL performance of G-TAD on THUMOS-14 dataset. 34.21 25.88 17.82 11.28 Arrow [62] 46.52 40.13 31.30 22.24 14.39 Speed [4] 46.76 39.49 31.92 23.21 15.18 BSP (ours) 48.71 42.78 34.92 27.44 17.60 ensemble 45.67 37.70 28.66 19.55 11.32</figDesc><table><row><cell>Method</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>baseline</cell><cell>41.48</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/JJBOY/BMN-Boundary-Matching-Network</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing Moments in Video With Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boundary Content Graph Neural Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SpeedNet: Learning the speediness in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SST: single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking the faster R-CNN architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporally Grounding Natural Sentence in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning modality interaction for temporal sentence localization and event captioning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic proposal for activity localization in videos via sentence query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical visualtextual graph for temporal activity localization via language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Proposalfree Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez-Opazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edison</forename><surname>Marrese-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh Sadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DAPs: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CTAP: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TALL: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MAC: Mining activity concepts for language-based temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ExCL: Extractive Clip Localization Using Natural Language Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuva</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memoryaugmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selfsupervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking ImageNet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SCC: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayner</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video representation learning by recognizing temporal transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Givi</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The Kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BMN: boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BSN: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-modal moment localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Progressive boundary refinement network for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the Bottom-Up Framework for Query-based Video Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chujie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chilie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chujie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chilie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shuffle and learn: Unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Local-Global Video-Text Interactions for Temporal Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Spatiotemporal contrastive video representation learning. arxiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">CDC: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Untrimmed video classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cuzzolin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">VAL: Visual-attention action localizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In PCM</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Temporal Localization of Moments in Video Collections with Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Escorcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soldan</forename><surname>Mattia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivic</forename><surname>Josef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghanem</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Bryan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning by pace prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">UTS at activitynet 2016. Ac-tivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Languagedriven temporal activity localization: A semantic matching reinforcement learning model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-modal circulant fusion for video-to-language and backward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">R-C3D: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multilevel language and vision integration for text-to-clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. G-Tad</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Video playback rate perception for self-supervised spatio-temporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Combining Local Convolution with Global Self-Attention for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">To find where you talk: Temporal sentence localization in video with attention based location regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Ze-Huan Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dense regression network for video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">MAN: Moment alignment network for natural language moment retrieval via iterative graph adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Span-based localizing network for natural language video localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning 2d temporal adjacent networks for moment localization with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">HACS: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Vision-Language Navigation With Self-Supervised Auxiliary Reasoning Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Crosstask weakly supervised learning from instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramazan</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning actionness via long-range temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
