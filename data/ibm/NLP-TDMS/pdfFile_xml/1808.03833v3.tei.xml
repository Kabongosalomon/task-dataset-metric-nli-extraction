<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Model Adaptation for Multimodal Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
							<email>valada@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
							<email>mohan@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
							<email>burgard@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Research Institute</orgName>
								<address>
									<settlement>Los Altos</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
						</author>
						<title level="a" type="main">Self-Supervised Model Adaptation for Multimodal Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semantic Segmentation · Multimodal Fusion · Scene Understanding · Model Adaptation · Deep Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to reliably perceive and understand the scene is an integral enabler for robots to operate in the realworld. This problem is inherently challenging due to the multitude of object types as well as appearance changes caused by varying illumination and weather conditions. Leveraging complementary modalities can enable learning of semantically richer representations that are resilient to such perturbations. Despite the tremendous progress in recent years, most multimodal convolutional neural network approaches directly concatenate feature maps from individual modality streams rendering the model incapable of focusing only on the relevant complementary information for fusion. To address this limitation, we propose a mutimodal semantic segmentation framework that dynamically adapts the fusion of modalityspecific features while being sensitive to the object category, spatial location and scene context in a self-supervised manner. Specifically, we propose an architecture consisting of two modality-specific encoder streams that fuse intermediate encoder representations into a single decoder using our proposed self-supervised model adaptation fusion mechanism which optimally combines complementary features. As intermediate representations are not aligned across modalities, we introduce an attention scheme for better correlation. In addition, we propose a computationally efficient unimodal segmentation architecture termed AdapNet++ that incorporates a new encoder with multiscale residual units and an efficient atrous spatial pyramid pooling that has a lar-Abhinav Valada ger effective receptive field with more than 10× fewer parameters, complemented with a strong decoder with a multiresolution supervision scheme that recovers high-resolution details. Comprehensive empirical evaluations on Cityscapes, Synthia, SUN RGB-D, ScanNet and Freiburg Forest benchmarks demonstrate that both our unimodal and multimodal architectures achieve state-of-the-art performance while simultaneously being efficient in terms of parameters and inference time as well as demonstrating substantial robustness in adverse perceptual conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans have the remarkable ability to instantaneously recognize and understand a complex visual scene which has piqued the interest of computer vision researches to model this ability since the 1960s <ref type="bibr" target="#b18">(Fei-Fei et al, 2004)</ref>. There are numerous ever-expanding applications to this capability ranging from robotics <ref type="bibr" target="#b54">(Xiang and Fox, 2017)</ref> and remote sensing <ref type="bibr" target="#b3">(Audebert et al, 2018)</ref> to medical diagnostics <ref type="bibr" target="#b47">(Ronneberger et al, 2015)</ref> and content-based image retrieval <ref type="bibr" target="#b41">(Noh et al, 2017)</ref>. However, there are several challenges imposed by the multifaceted nature of this problem including the large variation in types and scales of objects, clutter and occlusions in the scene as well as outdoor appearance changes that take place throughout the day and across seasons.</p><p>Deep Convolutional Neural Network (DCNN) based methods <ref type="bibr">(Long et al, 2015;</ref><ref type="bibr" target="#b7">Chen et al, 2016;</ref><ref type="bibr" target="#b58">Yu and Koltun, 2016)</ref> modelled as a Fully Convolutional Neural Network (FCN) have dramatically increased the performance on several semantic segmentation benchmarks. Nevertheless, they still face challenges due to the diversity of scenes in the realworld that cause mismatched relationship and inconspicuous arXiv:1808.03833v3 [cs.CV] 8 Jul 2019 (a) Input Image (b) Segmented Output <ref type="figure">Figure 1</ref> Example real-world scenarios where current state-of-the-art approaches demonstrate misclassifications. The first row shows an issue of mismatched relationship as well as inconspicuous classes where a decal on the train is falsely predicted as a person and the decal text is falsely predicted as a sign. The second row shows misclassifications caused by overexposure of the camera due to car exiting a tunnel.</p><p>object classes. <ref type="figure">Figure 1</ref> shows two example scenes from realworld scenarios in which misclassifications are produced due to the decal on the train which is falsely predicted as a person and a traffic sign (first row), and overexposure of the camera caused by the vehicle exiting a tunnel (second row). In order to accurately predict the elements of the scene in such situations, features from complementary modalities such as depth and infrared can be leveraged to exploit properties such as geometry and reflectance, respectively. Moreover, the network can exploit complex intra-modal dependencies more effectively by directly learning to fuse visual appearance information from RGB images with learned features from complementary modalities in an end-to-end fashion. This not only enables the network to resolve inherent ambiguities and improve reliability but also obtain a more holistic scene segmentation.</p><p>While most existing work focuses on where to fuse modality-specific streams topologically <ref type="bibr">(Hazirbas et al, 2016;</ref><ref type="bibr" target="#b49">Schneider et al, 2017;</ref><ref type="bibr" target="#b53">Valada et al, 2016c)</ref> and what transformations can be applied on the depth modality to enable better fusion with visual RGB features <ref type="bibr" target="#b21">(Gupta et al, 2014;</ref><ref type="bibr">Eitel et al, 2015)</ref>, it still remains an open question as to how to enable the network to dynamically adapt its fusion strategy based on the nature of the scene such as the types of objects, their spatial location in the world and the present scene context. This is a crucial requirement in applications such as robotics and autonomous driving where these systems run in continually changing environmental contexts. For example, an autonomous car navigating in ideal weather conditions can primarily rely on visual information but when it enters a dark tunnel or exits an underpassage, the cameras might experience under/over exposure, whereas the depth modality will be more informative. Furthermore, the strategy to be employed for fusion also varies with the types of objects in the scene, for instance, infrared might be more useful to detect categories such as people, vehicles, vegetation and boundaries of structures but it does not provide much information on object categories such as the sky. Additionally, the spatial location of objects in the scene also has an influence, for example, the depth modality provides rich information on objects that are at nearby distances but degrades very quickly for objects that are several meters away. More importantly, the approach employed should be robust to sensor failure and noise as constraining the network to always depend on both modalities and use noisy information can worsen the actual performance and lead to disastrous situations.</p><p>Due to these complex interdependencies, naively treating modalities as multi-channel input data or concatenating independently learned modality-specific features does not allow the network to adapt to the aforementioned situations dynamically. Moreover, due to the nature of this dynamicity, the fusion mechanism has to be trained in a self-supervised manner in order to make the adaptivity emergent and to generalize effectively to different real-world scenarios. As a solution to this problem, we present the Self-Supervised Model Adaptation (SSMA) fusion mechanism that adaptively recalibrates and fuses modality-specific feature maps based on the object class, its spatial location and the scene context. The SSMA module takes intermediate encoder representations of modality-specific streams as input and fuses them probabilistically based on the activations of individual modality streams. As we model the SSMA block in a fully convolutional fashion, it yields a probability for each activation in the feature maps which represents the optimal combination to exploit complementary properties. These probabilities are then used to amplify or suppress the representations of the individual modality streams, followed by the fusion. As we base the fusion on modality-specific activations, the fusion is intrinsically tolerant to sensor failure and noise such as missing depth values.</p><p>Our proposed architecture for multimodal segmentation consists of individual modality-specific encoder streams which are fused both at mid-level stages and at the end of the encoder streams using our SSMA blocks. The fused representations are input to the decoder at different stages for upsampling and refining the predictions. Note that only the multimodal SSMA fusion mechanism is self-supervised, the semantic segmentation is trained in a supervised manner. We employ a combination of mid-level and late-fusion as several experiments have demonstrated that fusing semantically meaningful representations yields better performance in comparison to early fusion <ref type="bibr">(Eitel et al, 2015;</ref><ref type="bibr" target="#b53">Valada et al, 2016b;</ref><ref type="bibr">Hazirbas et al, 2016;</ref><ref type="bibr" target="#b54">Xiang and Fox, 2017)</ref>. Moreover, studies of the neural dynamics of the human brain has also shown evidence of late-fusion of modalities for recognition tasks <ref type="bibr" target="#b12">(Cichy et al, 2016)</ref>. However, intermediate network representations are not aligned across modality-specific streams. Hence, integrating fused multimodal mid-level features into high-level features requires explicit prior alignment. Therefore, we propose an attention mechanism that weighs the fused multimodal mid-level skip features with spatially aggregated statistics of the high-level decoder features for better correlation, followed by channel-wise concatenation.</p><p>As our fusion framework necessitates individual modality-specific encoders, the architecture that we employ for the encoder and decoder should be efficient in terms of the number of parameters and computational operations, as well as be able to learn highly discriminative deep features. State-of-the-art semantic segmentation architectures such as DeepLab v3  and <ref type="bibr">PSPnet (Zhao et al, 2017)</ref> employ the ResNet-101 <ref type="bibr">(He et al, 2015a)</ref> architecture which consumes 42.39M parameters and 113.96B FLOPS, as the encoder backbone. Training such architectures requires a large amount of memory and synchronized training across multiple GPUs. Moreover, they have slow run-times rendering them impractical for resource constrained applications such as robotics and augmented reality. More importantly, it is infeasible to employ them in multimodal frameworks that require multiple modality-specific streams as we do in this work.</p><p>With the goal of achieving the right trade-off between performance and computational complexity, we propose the AdapNet++ architecture for unimodal segmentation. We build the encoder of our model based on the full preactivation ResNet-50 <ref type="bibr" target="#b22">(He et al, 2016)</ref> architecture and incorporate our previously proposed multiscale residual units <ref type="bibr" target="#b53">(Valada et al, 2017)</ref> to aggregate multiscale features throughout the network without increasing the number of parameters. The proposed units are more effective in learning multiscale features than the commonly employed multigrid approach introduced in DeepLab v3 . In addition, we propose an efficient variant of the Atrous Spatial Pyramid Pooling (ASPP)  called eASPP that employs cascaded and parallel atrous convolutions to capture long range context with a larger effective receptive field, while simultaneously reducing the number of parameters by 87% in comparison to the originally proposed ASPP. We also propose a new decoder that integrates mid-level features from the encoder using multiple skip refinement stages for high resolution segmentation along the object boundaries. In order to aid the optimization and to accelerate training, we propose a multiresolution supervision strategy that introduces weighted auxiliary losses after each upsampling stage in the decoder. This enables faster convergence, in addition to improving the performance of the model along the object boundaries. Our proposed architecture is compact and trainable with a large mini-batch size on a single consumer grade GPU. Motivated by the recent success of compressing DCNNs by pruning unimportant neurons <ref type="bibr" target="#b39">(Molchanov et al, 2017;</ref><ref type="bibr" target="#b38">Liu et al, 2017;</ref><ref type="bibr" target="#b2">Anwar et al, 2017)</ref>, we explore pruning entire convolutional feature maps of our model to further reduce the number of parameters. Network pruning approaches utilize a cost function to first rank the importance of neurons, followed by removing the least important neurons and fine-tuning the network to recover any loss in accuracy. Thus far, these approaches have only been employed for pruning convolutional layers that do not have an identity or a projection shortcut connection. Pruning residual feature maps (third convolutional layer of a residual unit) also necessitates pruning the projected feature maps in the same configuration in order to maintain the shortcut connection. This leads to a significant drop in accuracy, therefore current approaches omit pruning convolutional filters with shortcut connections. As a solution to this problem, we propose a network-wide holistic pruning approach that employs a simple and yet effective strategy for pruning convolutional filters invariant to the presence of shortcut connections. This enables our network to further reduce the number of parameters and computing operations, making our model efficiently deployable even in resource constrained applications.</p><p>Finally, we present extensive experimental evaluations of our proposed unimodal and multimodal architectures on benchmark scene understanding datasets including Cityscapes <ref type="bibr" target="#b13">(Cordts et al, 2016)</ref>, Synthia <ref type="bibr" target="#b48">(Ros et al, 2016)</ref>, SUN RGB-D <ref type="bibr" target="#b51">(Song et al, 2015)</ref>, ScanNet <ref type="bibr" target="#b16">(Dai et al, 2017)</ref> and Freiburg Forest <ref type="bibr" target="#b53">(Valada et al, 2016b)</ref>. The results demonstrate that our model sets the new state-of-the-art on all these benchmarks considering the computational efficiency and the fast inference time of 72ms on a consumer grade GPU. More importantly, our dynamically adapting multimodal architecture demonstrates exceptional robustness in adverse perceptual conditions such as fog, snow, rain and night-time, thus enabling it to be employed in critical resource constrained applications such as robotics where not only accuracy but robustness, computational efficiency and run-time are equally important. To the best of our knowledge, this is the first multimodal segmentation work to benchmark on these wide range of datasets containing several modalities and diverse environments ranging from urban city driving scenes to indoor environments and unstructured forested scenes.</p><p>In summary, the following are the main contributions of this work: 1. A multimodal fusion framework incorporating our proposed SSMA fusion blocks that adapts the fusion of modality-specific features dynamically according to the object category, its spatial location as well as the scene context and learns in a self-supervised manner. 2. The novel AdapNet++ semantic segmentation architecture that incorporates our multiscale residual units, a new efficient ASPP, a new decoder with skip refinement stages and a multiresolution supervision strategy. 3. The eASPP for efficiently aggregating multiscale features and capturing long range context, while having a larger effective receptive field and over 10× reduction in parameters compared to the standard ASPP. 4. An attention mechanism for effectively correlating fused multimodal mid-level and high-level features for better object boundary refinement. 5. A holistic network-wide pruning approach that enables pruning of convolutional filters invariant to the presence of identity or projection shortcuts. 6. Extensive benchmarking of existing approaches with the same input image size and evaluation setting along with quantitative and qualitative evaluations of our unimodal and multimodal architectures on five different benchmark datasets consisting of multiple modalities. 7. Implementations of our proposed architectures are made publicly available at https://github.com/DeepSceneSeg and a live demo on all the five datasets can be viewed at http://deepscene.cs.uni-freiburg.de.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>In the last decade, there has been a sharp transition in semantic segmentation approaches from employing hand engineered features with flat classifiers such as Support Vector Machines <ref type="bibr" target="#b19">(Fulkerson et al, 2009</ref>), Boosting <ref type="bibr" target="#b52">(Sturgess et al, 2009)</ref> or Random Forests <ref type="bibr" target="#b6">Brostow et al, 2008)</ref>, to end-to-end DCNN-based approaches <ref type="bibr">(Long et al, 2015;</ref><ref type="bibr" target="#b4">Badrinarayanan et al, 2015)</ref>. We first briefly review some of the classical methods before delving into the stateof-the-art techniques.</p><p>Semantic Segmentation: Semantic segmentation is one of the fundamental problems in computer vision. Some of the earlier approaches for semantic segmentation use small patches to classify the center pixel using flat classifiers <ref type="bibr" target="#b52">Sturgess et al, 2009</ref>) followed by smoothing the predictions using Conditional Random Fields (CRFs) <ref type="bibr" target="#b52">(Sturgess et al, 2009)</ref>. Rather than only relying on appearance based features, structure from motion features have also been used with randomized decision forests <ref type="bibr" target="#b6">(Brostow et al, 2008;</ref><ref type="bibr" target="#b52">Sturgess et al, 2009</ref>). View independent 3D features from dense depth maps have been shown to outperform appearance based features, that also enabled classification of all the pixels in an image, as opposed to only the center pixel of a patch <ref type="bibr" target="#b58">(Zhang et al, 2010)</ref>. <ref type="bibr">Plath et al (2009)</ref> propose an approach to combine local and global features using a CRF and an image classification method. However, the performance of these approaches is largely bounded by the expressiveness of handcrafted features which is highly scenario-specific. The remarkable performance achieved by CNNs in classification tasks led to their application for dense prediction problems such as semantic segmentation, depth estimation and optical flow prediction. Initial approaches that employed neural networks for semantic segmentation still relied on patch-wise training <ref type="bibr" target="#b20">(Grangier et al, 2009;</ref><ref type="bibr" target="#b18">Farabet et al, 2012;</ref><ref type="bibr" target="#b43">Pinheiro and Collobert, 2014)</ref>. <ref type="bibr" target="#b43">Pinheiro and Collobert (2014)</ref> use a recurrent CNN to aggregate several low resolution predictions for scene labeling. <ref type="bibr" target="#b18">Farabet et al (2012)</ref> transforms the input image through a Laplacian pyramid followed by feeding each scale to a CNN for hierarchical feature extraction and classification. Although these approaches demonstrated improved performance over handcrafted features, they often yield a grid-like output that does not capture the true object boundaries. One of the first end-to-end approaches that learns to directly map the low resolution representations from a classification network to a dense prediction output was the Fully Convolutional Network (FCN) model <ref type="bibr">(Long et al, 2015)</ref>. FCN proposed an encoder-decoder architecture in which the encoder is built upon the VGG-16 (Simonyan and Zisserman, 2014) architecture with the inner-product layers replaced with convolutional layers. While, the decoder consists of successive deconvolution and convolution layers that upsample and refine the low resolution feature maps by combining them with the encoder feature maps. The last decoder then yields a segmented output with the same resolution as the input image.</p><p>DeconvNet <ref type="bibr" target="#b40">(Noh et al, 2015)</ref> propose an improved architecture containing stacked deconvolution and unpooling layers that perform non-linear upsampling and outperforms FCNs but at the cost of a more complex training procedure. The SegNet <ref type="bibr" target="#b4">(Badrinarayanan et al, 2015)</ref> architecture eliminates the need for learning to upsample by reusing pooling indices from the encoder layers to perform upsampling. <ref type="bibr" target="#b42">Oliveira et al (2016)</ref> propose an architecture that builds upon FCNs and introduces more refinement stages and incorporates spatial dropout to prevent over fitting. The ParseNet <ref type="bibr" target="#b37">(Liu et al, 2015)</ref> architecture models global context directly instead of only relying on the largest receptive field of the network. Recently, there has been more focus on learning multiscale features, which was initially achieved by providing the network with multiple rescaled versions of the image <ref type="bibr" target="#b18">(Farabet et al, 2012)</ref> or by fusing features from multiple parallel branches that take different image resolutions <ref type="bibr">(Long et al, 2015)</ref>. However, these networks still use pooling layers to increase the receptive field, thereby decreasing the spatial resolution, which is not ideal for a segmentation network.</p><p>In order to alleviate this problem, <ref type="bibr" target="#b58">Yu and Koltun (2016)</ref> propose dilated convolutions that allows for exponential increase in the receptive field without decrease in resolution or increase in parameters. DeepLab  and <ref type="bibr">PSPNet (Zhao et al, 2017)</ref> build upon the aforementioned idea and propose pyramid pooling modules that utilize dilated convolutions of different rates to aggregate multiscale global context. DeepLab in addition uses fully connected CRFs in a post processing step for structured prediction. However, a drawback in employing these approaches is the compu-tational complexity and substantially large inference time even using modern GPUs that hinder them from being deployed in robots that often have limited resources. In our previous work <ref type="bibr" target="#b53">(Valada et al, 2017)</ref>, we proposed an architecture that introduces dilated convolutions parallel to the conventional convolution layers and multiscale residual blocks that incorporates them, which enables the model to achieve competitive performance at interactive frame rates. Our proposed multiscale residual blocks are more effective at learning multiscale features compared to the widely employed multigrid approach from DeepLab v3 . While in this work, we propose several new improvements for learning multiscale features, capturing long range context and improving the upsampling in the decoder, while simultaneously reducing the number of parameters and maintaining a fast inference time.</p><p>Multimodal Fusion: The availability of low-cost sensors has encouraged novel approaches to exploit features from alternate modalities in an effort to improve robustness as well as the granularity of segmentation. Silberman et al <ref type="formula" target="#formula_0">(2012)</ref> propose an approach based on SIFT features and MRFs for indoor scene segmentation using RGB-D images. Subsequently, <ref type="bibr" target="#b46">Ren et al (2012)</ref> propose improvements to the feature set by using kernel descriptors and by combining MRF with segmentation trees. Munoz et al (2012) employ modality-specific classifier cascades that hierarchically propagate information and do not require one-to-one correspondence between data across modalities. In addition to incorporating features based on depth images, <ref type="bibr" target="#b23">Hermans et al (2014)</ref> propose an approach that performs joint 3D mapping and semantic segmentation using Randomized Decision Forests. There has also been work on extracting combined RGB and depth features using CNNs <ref type="bibr" target="#b14">(Couprie et al, 2013;</ref><ref type="bibr" target="#b21">Gupta et al, 2014)</ref> for object detection and semantic segmentation. In most of these approaches, hand engineered or learned features are extracted from individual modalities and combined together in a joint feature set which is then used for classification.</p><p>More recently, there has been a series of DCNN-based fusion techniques <ref type="bibr">(Eitel et al, 2015;</ref><ref type="bibr" target="#b28">Kim et al, 2017;</ref><ref type="bibr" target="#b35">Li et al, 2016)</ref> that have been proposed for end-to-end learning of fused representations from multiple modalities. These fusion approaches can be categorized into early, hierarchical and late fusion methods. An intuitive early fusion technique is to stack data from multiple modalities channel-wise and feed it to the network as a four or six channel input. However, experiments have shown that this often does not enable the network to learn complementary features and cross-modal interdependencies <ref type="bibr" target="#b53">(Valada et al, 2016b;</ref><ref type="bibr">Hazirbas et al, 2016)</ref>. Hierarchical fusion approaches combine feature maps from multiple modality-specific encoders at various levels (often at each downsampling stage) and upsample the fused features using a single decoder <ref type="bibr">(Hazirbas et al, 2016;</ref><ref type="bibr" target="#b28">Kim et al, 2017)</ref>. Alternatively, <ref type="bibr" target="#b49">Schneider et al (2017)</ref> propose a mid-level fusion approach in which NiN layers <ref type="bibr" target="#b36">(Lin et al, 2013)</ref> with depth as input are used to fuse feature maps into the RGB encoder in the middle of the network. <ref type="bibr" target="#b35">Li et al (2016)</ref> propose a Long-Short Term Memory (LSTM) context fusion model that captures and fuses contextual information from multiple modalities accounting for the complex interdependencies between them. <ref type="bibr">(Qi et al, 2017)</ref> propose an interesting approach that employs 3D graph neural networks for RGB-D semantic segmentation that accounts for both 2D appearance and 3D geometric relations, while capturing long range dependencies within images.</p><p>In the late fusion approach, identical network streams are first trained individually on a specific modality and the feature maps are fused towards the end of network using concatenation <ref type="bibr">(Eitel et al, 2015)</ref> or element-wise summation <ref type="bibr" target="#b53">(Valada et al, 2016b)</ref>, followed by learning deeper fused representations. However, this does not enable the network to adapt the fusion to changing scene context. In our previous work <ref type="bibr">(Valada et al, 2016a)</ref>, we proposed a mixture-of-experts CMoDE fusion scheme for combining feature maps from late fusion based architectures. Subsequently, in <ref type="bibr" target="#b53">(Valada et al, 2017)</ref> we extended the CMoDE framework for probabilistic fusion accounting for the types of object categories in the dataset which enables more flexibility in learning the optimal combination. Nevertheless, there are several real-world scenarios in which class-wise fusion is not sufficient, especially in outdoor scenes where different modalities perform well in different conditions. Moreover, the CMoDE module employs multiple softmax loss layers for each class to compute the probabilities for fusion which does not scale for datasets such as SUN RGB-D which has 37 object categories. Motivated by this observation, in this work, we propose a multimodal semantic segmentation architecture incorporating our SSMA fusion module that dynamically adapts the fusion of intermediate network representations from multiple modality-specific streams according to the object class, its spatial location and the scene context while learning the fusion in a self-supervised fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AdapNet++ Architecture</head><p>In this section, we first briefly describe the overall topology of the proposed AdapNet++ architecture and our main contributions motivated by our design criteria. We then detail each of the constituting architectural components and our model compression technique.</p><p>Our network follows the general fully convolutional encoder-decoder design principle as shown in <ref type="figure">Figure 2</ref>. The encoder (depicted in blue) is based on the full pre-activation ResNet-50 <ref type="bibr" target="#b22">(He et al, 2016)</ref>   <ref type="figure">Figure 2</ref> Overview of our proposed Adapnet++ architecture. Given an input image, we use the full pre-activation ResNet-50 architecture augmented with our proposed multiscale residual blocks to yield a feature map 16-times downsampled with respect to the input image resolution, then our proposed efficient atrous spatial pyramid (eASPP) module is employed to further learn multiscale features and to capture long range context. Finally, the output of the eASPP is fed into our proposed deep decoder with skip connections for upsampling and refining the semantic pixel-level prediction.</p><p>densities, we incorporate our recently proposed multiscale residual units <ref type="bibr" target="#b53">(Valada et al, 2017)</ref> at varying dilation rates in the last two blocks of the encoder. In addition, to enable our model to capture long-range context and to further learn multiscale representations, we propose an efficient variant of the atrous spatial pyramid pooling module known as eASPP which has a larger effective receptive field and reduces the number of parameters required by over 87% compared to the originally proposed ASPP in DeepLab v3 . We append the proposed eASPP after the last residual block of the encoder, shown as green blocks in <ref type="figure">Figure 2</ref>. In order to recover the segmentation details from the low spatial resolution output of the encoder section, we propose a new deep decoder consisting of multiple deconvolution and convolution layers. Additionally, we employ skip refinement stages that fuse mid-level features from the encoder with the upsampled decoder feature maps for object boundary refinement. Furthermore, we add two auxiliary supervision branches after each upsampling stage to accelerate training and improve the gradient propagation in the network. We depict the decoder as orange blocks and the skip refinement stages as gray blocks in the network architecture shown in <ref type="figure">Figure 2</ref>. In the following sections, we discuss each of the aforementioned network components in detail and elaborate on the design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>Encoders are the foundation of fully convolutional neural network architectures. Therefore, it is essential to build upon a good baseline that has a high representational ability conforming with the computational budget. Our critical requirement is to achieve the right trade-off between the accuracy of segmentation and inference time on a consumer grade GPU, while keeping the number of parameters low. As we also employ the proposed architecture for multimodal fusion, our objective is to design a topology that has a reasonable model size so that two individual modality-specific networks can be trained in a fusion framework and deployed on a single GPU. Therefore, we build upon the ResNet-50 architecture with the full preactivation residual units <ref type="bibr" target="#b22">(He et al, 2016)</ref> instead of the originally proposed residual units <ref type="bibr">(He et al, 2015a)</ref> as they have been shown to reduce overfitting, improve the convergence and also yield better performance. The ResNet-50 architecture has four computational blocks with varying number of residual units. We use the bottleneck residual units in our encoder as they are computationally more efficient than the baseline residual units and they enable us to build more complex models that are easily trainable. The output of the last block of the ResNet-50 architecture is 32-times downsampled with respect to the input image resolution. In order to increase the spatial density of the feature responses and to prevent signal decimation, we set the stride of the convolution layer in the last block (res4a) from two to one which makes the resolution of the output feature maps 1/16-times the input image resolution. We then replace the residual blocks that follow this last downsampling stage with our proposed multiscale residual units that incorporate parallel atrous convolutions <ref type="bibr" target="#b58">(Yu and Koltun, 2016</ref>) at varying dilation rates.</p><p>A naive approach to compute the feature responses at the full image resolution would be to remove the downsampling and replace all the convolutions to atrous convolutions having a dilation rate r ≥ 2 but this would be both computation and memory intensive. Therefore, we propose a novel multiscale residual unit <ref type="bibr" target="#b53">(Valada et al, 2017)</ref> to efficiently enlarge the receptive field and aggregate multiscale features without increasing the number of parameters and the computational burden. Specifically, we replace the 3 × 3 convolution in the full pre-activation residual unit with two parallel 3 × 3 atrous convolutions with different dilation rates and half the num-  <ref type="figure">Figure 3</ref> The proposed encoder is built upon the full pre-activation ResNet-50 architecture. Specifically, we remove the last downsampling stage in ResNet-50 by setting the stride from two to one, therefore the final output of the encoder is 16-times downsampled with respect to the input. We then replace the residual units that follow the last downsampling stage with our proposed multiscale residual units. ber of feature maps each. We then concatenate their outputs before the following 1 × 1 convolution.</p><p>By concatenating their outputs, the network additionally learns to combine the feature maps of different scales. Now, by setting the dilation rate in one of the 3 × 3 convolutional layers to one and another to a rate r ≥ 2, we can preserve the original scale of the features within the block and simultaneously add a larger context. While, by varying the dilation rates in each of the parallel 3 × 3 convolutions, we can enable the network to effectively learn multiscale representations at different stages of the network. The topology of the proposed multiscale residual units and the corresponding original residual units are shown in the legend in <ref type="figure">Figure 3</ref>. The lower left two units show the original configuration, while the lower right two units show the proposed configuration. <ref type="figure">Figure 3</ref> shows our entire encoder structure with the full pre-activation residual units and the multiscale residual units.</p><p>We incorporate the first multiscale residual unit with r 1 = 1, r 2 = 2 before the third block at res3d (unit before the block where we remove the downsampling as mentioned earlier). Subsequently, we replace the units res4c, res4d, res4e, res4f with our proposed multiscale units with rates r 1 = 1 in all the units and r 2 = 2, 4, 8, 16 correspondingly. In addition, we replace the last three units of block four res5a, res5b, res5c with the multiscale units with increasing rates in both 3 × 3 convolutions, as (r 1 = 2, r 2 = 4), (r 1 = 2, r 2 = 8), (r 1 = 2, r 2 = 16) correspondingly. We evaluate our proposed configuration in comparison to the multigrid method of Dee-pLab v3  in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficient Atrous Spatial Pyramid Pooling</head><p>In this section, we first describe the topology of the Atrous Spatial Pyramid Pooling (ASPP) module, followed by the structure of our proposed efficient Atrous Spatial Pyramid Pooling (eASPP). ASPP has become prevalent in most stateof-the-art architectures due to its ability to capture long range context and multiscale information. Inspired by spatial pyramid pooling <ref type="bibr">(He et al, 2015c)</ref>, the initially proposed ASPP in DeepLab v2 <ref type="bibr" target="#b35">(Liang-Chieh et al, 2015)</ref> employs four parallel atrous convolutions with different dilation rates. Concatenating the outputs of multiple parallel atrous convolutions aggregates multi-scale context with different receptive field resolutions. However, as illustrated in the subsequent DeepLab v3 , applying extremely large dilation rates inhibits capturing long range context due to image boundary effects. Therefore, an improved version of ASPP was proposed  to add global context information by incorporating image-level features.</p><p>The resulting ASPP shown in <ref type="figure">Figure 4</ref>(a) consists of five parallel branches: one 1 × 1 convolution and three 3 × 3 convolutions with different dilation rates. Additionally, imagelevel features are introduced by applying global average pooling on the input feature map, followed by a 1 × 1 convolution and bilinear upsampling to yield an output with the same dimensions as the input feature map. All the convolutions have 256 filters and batch normalization layers to improve training. Finally, the resulting feature maps from each of the parallel branches are concatenated and passed through another 1 × 1 convolution with batch normalization to yield 256 output filters. The ASPP module is appended after the last residual block of the encoder where the feature maps are of dimensions 65 × 65 in the DeepLab v3 architecture , therefore dilation rates of 6, 12 and 18 were used in the parallel 3 × 3 atrous convolution layers. However, as we use a smaller input image, the dimensions of the input feature map to the ASPP is 24 × 48, therefore, we reduce the dilation rates to 3, 6 and 12 in the 3 × 3 atrous convolution layers respectively.</p><p>The biggest caveat of employing the ASPP is the extremely large amount of parameters and floating point operations per second (FLOPS) that it consumes. Each of the 3 × 3 convolutions have 256 filters, which in total for the entire ASPP amounts to 15.53 M parameters and 34.58 B FLOPS which is prohibitively expensive. To address this problem, we propose an equivalent structure called eASPP that substantially reduces the computational complexity. Our proposed topology is based on two principles: cascading atrous convolutions and the bottleneck structure. Cascading atrous convolutions effectively enlarges the receptive field as the latter atrous convolution takes the output of the former atrous convolution. The receptive field size F of an atrous convolution is be computed as</p><formula xml:id="formula_0">F = (r − 1) · (N − 1) + N,<label>(1)</label></formula><p>where r is the dilation rate of the atrous convolution and N is the filter size. When two atrous convolutions with the receptive field sizes as F 1 and F 2 are cascaded, the effective receptive field size is computed as</p><formula xml:id="formula_1">F e f f = F 1 + F 2 − 1.<label>(2)</label></formula><p>For example, if two atrous convolutions with filter size F = 3 and dilation r = 3 are cascaded, then each of the con-volutions individually has a receptive field size of 7, while the effective receptive field size of the second atrous convolution is 13. Moreover, cascading atrous convolutions enables denser sampling of pixels in comparison to parallel atrous convolution with a larger receptive field. Therefore, by using both parallel and cascaded atrous convolutions in the ASPP, we can efficiently aggregate dense multiscale features with very large receptive fields.</p><p>In order to reduce the number of parameters in the ASPP topology, we employ a bottleneck structure in the cascaded atrous convolution branches. The topology of our proposed eASPP shown in <ref type="figure">Figure 4</ref>(b) consists of five parallel branches similar to ASPP but the branches with the 3 × 3 atrous convolutions are replaced with our cascaded bottleneck branches. If c is the number of channels in the 3 × 3 atrous convolution, we add a 1 × 1 convolution with c/4 filters before the atrous convolution to squeeze only the most relevant information through the bottleneck. We then replace the 3 × 3 atrous convolution with two cascaded 3 × 3 atrous convolutions with c/4 filters, followed by another 1 × 1 convolution to restore the number of filters to c. The proposed eASPP only has 2.04 M parameters and consumes 3.62 B FLOPS which accounts to a reduction of 87.87% of parameters and 89.53% of FLOPS in comparison to the ASPP. We evaluate our proposed eASPP in comparison to ASPP in the ablation study presented in Section 5.5.2 and show that it achieves improved performance while being more than 10 times efficient in the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder</head><p>The output of the eASPP in our network is 16-times downsampled with respect to the input image and therefore it has  <ref type="figure">Figure 5</ref> Our decoder consists of three upsampling stages that recover segmentation details using deconvolution layers and two skip refinement stages that fuse mid-level features from the encoder to improve the segmentation along object boundaries. Each skip refinement stage consists of concatenation of mid-level features with the upsampled decoder feature maps, followed by two 3 × 3 convolutions to improve the discriminability of the high-level features and the resolution of the refinement.</p><p>to be upsampled back to the full input resolution. In our previous work <ref type="bibr" target="#b53">(Valada et al, 2017)</ref>, we employed a simple decoder with two deconvolution layers and one skip refinement connection. Although the decoder was more effective in recovering the segmentation details in comparison to direct bilinear upsampling, it often produced disconnected segments while recovering the structure of thin objects such as poles and fences. In order to overcome this impediment, we propose a more effective decoder in this work.</p><p>Our decoder shown in <ref type="figure">Figure 5</ref> consists of three stages. In the first stage, the output of the eASPP is upsampled by a factor of two using a deconvolution layer to obtain a coarse segmentation mask. The upsampled coarse mask is then passed through the second stage, where the feature maps are concatenated with the first skip refinement from Res3d. The skip refinement consists of a 1 × 1 convolution layer to reduce the feature depth in order to not outweigh the encoder features. We experiment with varying number of feature channels in the skip refinement in the ablation study presented in Section 5.5.3. The concatenated feature maps are then passed through two 3 × 3 convolutions to improve the resolution of the refinement, followed by a deconvolution layer that again upsamples the feature maps by a factor of two. This upsampled output is fed to the last decoder stage which resembles the previous stage consisting of concatenation with the feature maps from the second skip refinement from Res2c, followed by two 3 × 3 convolution layers. All the convolutional and deconvolutional layers until this stage have 256 feature channels, therefore the output from the two 3 × 3 convolutions in the last stage is fed to a 1 × 1 convolution layer to reduce the number of feature channels to the number of object categories C. This output is finally fed to the last deconvolution layer which upsamples the feature maps by a factor of four to recover the original input resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multiresolution Supervision</head><p>Deep networks often have difficulty in training due to the intrinsic instability associated with learning using gradient descent which leads to exploding or vanishing gradient prob-lems. As our encoder is based on the residual learning framework, shortcut connections in each unit help propagating the gradient more effectively. Another technique that can be used to mitigate this problem to a certain extent is by initializing the layers with pretrained weights, however our proposed eASPP and decoder layers still have to be trained from scratch which could lead to optimization difficulties. Recent deep architectures have proposed employing an auxiliary loss in the middle of encoder network <ref type="bibr" target="#b33">(Lee et al, 2015;</ref><ref type="bibr">Zhao et al, 2017)</ref>, in addition to the main loss towards the end of the network. However, as shown in the ablation study presented in Section 5.5.1 this does not improve the performance of our network although it helps the optimization to converge faster.</p><p>Unlike previous approaches, in this work, we propose a multiresolution supervision strategy to both accelerate the training and improve the resolution of the segmentation. As described in the previous section, our decoder consists of three upsampling stages. We add two auxiliary loss branches at the end of the first and second stage after the deconvolution layer in addition to the main softmax loss L main at the end of the decoder as shown in <ref type="figure">Figure 6</ref>. Each auxiliary loss branch decreases the feature channels to the number of category labels C using a 1 × 1 convolution with batch normalization and upsamples the feature maps to the input resolution using bilinear upsampling. We only use simple bilinear upsampling which does not contain any weights instead of a deconvolution layer in the auxiliary loss branches as our aim is to force the main decoder stream to improve its discriminativeness at each upsampling resolution so that it embeds multiresolution information while learning to upsample. We weigh the two auxiliary losses L aux1 and L aux2 to balance the gradient flow through all the previous layers. While testing, the auxiliary loss branches are discarded and only the main decoder stream is used. We experiment with different loss weightings in the ablation study presented in Section 5.5.3 and in Section 5.5.1 we show that each of the auxiliary loss branches improves the segmentation performance in addition to speeding-up the training.  <ref type="figure">Figure 6</ref> Depiction of the two auxiliary softmax losses that we add before each skip refinement stage in the decoder in addition to the main softmax loss in the end of the decoder. The two auxiliary losses are weighed for balancing the gradient flow through all the previous layers. While testing the auxiliary branches are removed and only the main stream as shown in <ref type="figure">Figure 5</ref> is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network Compression</head><p>As we strive to design an efficient and compact semantic segmentation architecture that can be employed in resource constrained applications, we must ensure that the utilization of convolutional filters in our network is thoroughly optimized. Often, even the most compact networks have abundant neurons in deeper layers that do not significantly contribute to the overall performance of the model. Excessive convolutional filters not only increase the model size but also the inference time and the number of computing operations. These factors critically hinder the deployment of models in resource constrained real-world applications. Pruning of neural networks can be traced back to the 80s when LeCun et al (1990) introduced a technique called Optimal Brain Damage for selectively pruning weights with a theoretically justified measure. Recently, several new techniques have been proposed for pruning weight matrices <ref type="bibr" target="#b53">(Wen et al, 2016;</ref><ref type="bibr" target="#b2">Anwar et al, 2017;</ref><ref type="bibr" target="#b38">Liu et al, 2017;</ref> of convolutional layers as most of the computation during inference is consumed by them.</p><p>These approaches rank neurons based on their contribution and remove the low ranking neurons from the network, followed by fine-tuning of the pruned network. While the simplest neuron ranking method computes the 1 -norm of each convolutional filter , more sophisticated techniques have recently been proposed <ref type="bibr" target="#b2">(Anwar et al, 2017;</ref><ref type="bibr" target="#b38">Liu et al, 2017;</ref><ref type="bibr" target="#b39">Molchanov et al, 2017)</ref>. Some of these approaches are based on sparsity based regularization of network parameters which additionally increases the computational overhead during training <ref type="bibr" target="#b38">(Liu et al, 2017;</ref><ref type="bibr" target="#b53">Wen et al, 2016)</ref>. Techniques have also been proposed for structured pruning of entire kernels with strided sparsity <ref type="bibr" target="#b2">(Anwar et al, 2017</ref>) that demonstrate impressive results for pruning small networks. However, their applicability to complex networks that are to be evaluated on large validation sets has not been explored due its heavy computational processing. Moreover, until a year ago these techniques were only applied to sim-pler architectures such as VGG (Simonyan and Zisserman, 2014) and AlexNet <ref type="bibr" target="#b30">(Krizhevsky et al, 2012)</ref>, as pruning complex deep architectures such as ResNets requires a holistic approach. Thus far, pruning of residual units has only been performed on convolutional layers that do not have an identity or shortcut connection as pruning them additionally requires pruning the added residual maps in the exact same configuration. Attempts to prune them in the same configuration have resulted in a significant drop in performance . Therefore, often only the first and the second convolutional layers of a residual unit are pruned.</p><p>Our proposed AdapNet++ architecture has shortcut and skip connections both in the encoder as well the decoder. Therefore, in order to efficiently maximize the pruning of our network, we propose a holistic network-wide pruning technique that is invariant to the presence of skip or shortcut connections. Our proposed technique first involves pruning all the convolutional layers of a residual unit, followed by masking out the pruned indices of the last convolutional layer of a residual unit with zeros before the addition of the residual maps from the shortcut connection. As masking is performed after the pruning, we efficiently reduce the parameters and computing operations in a holistic fashion, while optimally pruning all the convolutional layers and preserving the shortcut or skip connections. After each pruning iteration, we fine-tune the network to recover any loss in accuracy. We illustrate this strategy adopting a recently proposed greedy criteria-based oracle pruning technique that incorporates a novel ranking method based on a first order Taylor expansion of the network cost function <ref type="bibr" target="#b39">(Molchanov et al, 2017)</ref>. The pruning problem is framed as a combinatorial optimization problem such that when the weights B of the network are pruned, the change in cost value will be minimal.</p><formula xml:id="formula_2">min W |C(T |W ) − C(T |W)| s.t. W 0 ≤ B,<label>(3)</label></formula><p>where T is the training set, W is the network parameters and C(·) is the negative log-likelihood function. Based on Taylor expansion, the change in the loss function from removing a specific parameter can be approximated. Let h i be the output feature maps produced by parameter i and</p><formula xml:id="formula_3">h i = {z 1 0 , z 2 0 , · · · , z C l L }.</formula><p>The output h i can be pruned by setting it to zero and the ranking can be given by</p><formula xml:id="formula_4">|∆ C(h i )| = |C(T , h i = 0) − C(T , h i )|,<label>(4)</label></formula><p>Approximating with Taylor expansion, we can write</p><formula xml:id="formula_5">Θ T E (h i ) = |∆ C(h i )| = |C(T , h i ) − δ C δ h i h i − C(T , h i )| = δ C δ h i h i . (5) Θ T E (z (k) l ) = 1 M ∑ m δ C δ z (k) l,m z (k) l,m ,<label>(6)</label></formula><p>where M is the length of the vectorized feature map. This ranking can be easily computed using the standard backpropagation computation as it requires the gradient of the cost function with respect to the activation and the product of the activation. Furthermore, in order to achieve adequate rescaling across layers, a layer-wise 2 -norm of the rankings is computed aŝ</p><formula xml:id="formula_6">Θ (z (k) l ) = Θ (z (k) l ) ∑ j Θ 2 (z ( j) l ) .<label>(7)</label></formula><p>The entire pruning procedure can be summarized as follows: first the AdapNet++ network is trained until convergence using the training protocol described in Section 5.1. Then the importance of the feature maps is evaluated using the aforementioned ranking method and subsequently the unimportant feature maps are removed. The pruned convolution layers that have shortcut connections are then masked at the indices where the unimportant feature maps are removed to maintain the shortcut connections. The network is then fine-tuned and the pruning process is reiterated until the desired trade-off between accuracy and the number of parameters has been achieved. We present results from pruning our AdapNet++ architecture in Section 5.4, where we perform pruning of both the convolutional and deconvolutional layers of our network in five stages by varying the threshold for the rankings. For each of these stages, we quantitatively evaluate the performance versus number of parameters trade-off obtained using our proposed pruning strategy in comparison to the standard approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Self-Supervised Model Adaptation</head><p>In this section, we describe our approach to multimodal fusion using our proposed self-supervised model adaptation (SSMA) framework. Our framework consists of three components: a modality-specific encoder as described in Section 3.1, a decoder built upon the topology described in Section 3.3 and our proposed SSMA block for adaptively recalibrating and fusing modality-specific feature maps. In the following, we first formulate the problem of semantic segmentation from multimodal data, followed by a detailed description of our proposed SSMA units and finally we describe the overall topology of our fusion architecture.</p><p>We represent the training set for multimodal semantic segmentation as T = {(I n , K n , M n ) | n = 1, . . . , N}, where I n = {u r | r = 1, . . . , ρ} denotes the input frame from modality a, K n = {k r | r = 1, . . . , ρ} denotes the corresponding input frame from modality b and the groundtruth label is given by M n = {m r | r = 1, . . . , ρ}, where m r ∈ {1, ...,C} is the set of semantic classes. The image I n is only shown to the modality-specific encoder E a and similarly, the corresponding image K n from a complementary modality is only shown to the modality-specific encoder E b . This enables each modality-specific encoder to specialize in a particular sub-space learning their own hierarchical representations individually. We assume that the input images I n and K n , as well as the label M n have the same dimensions ρ = H ×W and that the pixels are drawn as i.i.d. samples following a categorical distribution. Let θ be the network parameters consisting of weights and biases. Using the classification scores s j at each pixel u r , we obtain probabilities P = (p 1 , . . . , p C ) with the softmax function such that</p><formula xml:id="formula_7">p j (u r , θ | I n , K n ) = σ (s j (u r , θ )) = exp (s j (u r , θ )) ∑ C k exp (s k (u r , θ ))<label>(8)</label></formula><p>denotes the probability of pixel u r being classified with label j. The optimal θ is estimated by minimizing</p><formula xml:id="formula_8">L seg (T , θ ) = − N ∑ n=1 ρ ∑ r=1 C ∑ j=1 δ m r , j log p j (u r , θ | I n , K n ),<label>(9)</label></formula><p>for (I n , K n , M n ) ∈ T , where δ m r , j is the Kronecker delta.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SSMA Block</head><p>In order to adaptively recalibrate and fuse feature maps from modality-specific networks, we propose a novel architectural unit called the SSMA block. The goal of the SSMA block is to explicitly model the correlation between the two modalityspecific feature maps before fusion so that the network can exploit the complementary features by learning to selectively emphasize more informative features from one modality, while suppressing the less informative features from the other. We construct the topology of the SSMA block in a fullyconvolutional fashion which empowers the network with the ability to emphasize features from a modality-specific network for only certain spatial locations or object categories,  <ref type="figure">Figure 7</ref> The topology of our proposed SSMA unit that adaptively recalibrates and fuses modality-specific feature maps based on the inputs in order to exploit the more informative features from the modalityspecific streams. η denotes the bottleneck compression rate. while emphasizing features from the complementary modality for other locations or object categories. Moreover, the SSMA block dynamically recalibrates the feature maps based on the input scene context.</p><p>The structure of the SSMA block is shown in <ref type="figure">Figure 7</ref>. Let X a ∈ R C×H×W and X b ∈ R C×H×W denote the modalityspecific feature maps from modality A and modality B respectively, where C is the number of feature channels and H × W is the spatial dimension. First, we concatenate the modality-specific feature maps X a and X b to yield X ab ∈ R 2·C×H×W . We then employ a recalibration technique to adapt the concatenated feature maps before fusion. In order to achieve this, we first pass the concatenated feature map X ab through a bottleneck consisting of two 3 × 3 convolutional layers for dimensionality reduction and to improve the representational capacity of the concatenated features. The first convolution has weights W 1 ∈ R 1 η ·C×H×W with a channel reduction ratio η and a non-linearity function δ (·). We use ReLU for the non-linearity, similar to the other activations in the encoders and experiment with different reductions ratios in Section 5.10.2. Note that we omit the bias term to simplify the notation. The subsequent convolutional layer with weights W 2 ∈ R 2·C×H×W increases the dimensionality of the feature channels back to concatenation dimension 2C and a sigmoid function σ (·) scales the dynamic range of the activations to the [0, 1] interval. This can be represented as</p><formula xml:id="formula_9">s = F ssma (X ab ; W) = σ g X ab ; W = σ W 2 δ W 1 X ab .<label>(10)</label></formula><p>The resulting output s is used to recalibrate or emphasize/de-emphasize regions in X ab aŝ</p><formula xml:id="formula_10">X ab = F scale (X ab ; s) = s • X ab ,<label>(11)</label></formula><p>where F scale (X ab , s) denotes Hadamard product of the feature maps X ab and the matrix of scalars s such that each The encoder employs a late fusion technique to fuse feature maps from modality-specific streams using our proposed SSMA block. The SSMA block is employed to fuse the latent features from the eASPP as well as the feature maps from the skip refinements. element x c,i, j in X ab is multiplied with a corresponding activation s c,i, j in s with c ∈ {1, 2, . . . , 2C}, i ∈ {1, 2, . . . , H} and j ∈ {1, 2, . . . ,W }. The activations s adapt to the concatenated input feature map X ab , enabling the network to weigh features element-wise spatially and across the channel depth based on the multimodal inputs I n and K n . With new multimodal inputs, the network dynamically weighs and reweighs the feature maps in order to optimally combine complementary features. Finally, the recalibrated feature mapŝ X ab are passed through a 3 × 3 convolution with weights W 3 ∈ R C×H×W and a batch normalization layer to reduce the feature channel depth and yield the fused output f as</p><formula xml:id="formula_11">f = F f used (X ab ; W) = g(X ab ; W) = W 3X ab .<label>(12)</label></formula><p>As described in the following section, we employ our proposed SSMA block to fuse modality-specific feature maps both at intermediate stages of the network and towards the end of the encoder. Although we utilize a bottleneck structure to conserve the number of parameters consumed, further reduction in the parameters can be achieved by replacing the 3 × 3 convolution layers with 1 × 1 convolutions, which yields comparable performance. We also remark that the SSMA blocks can be used for multimodal fusion in other tasks such as scene classification as shown in Section 5.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fusion Architecture</head><p>We propose a framework for multimodal semantic segmentation using a modified version of our AdapNet++ architecture and the proposed SSMA blocks. For simplicity, we consider the fusion of two modalities, but the framework can be easily extended to arbitrary number of modalities. The encoder of our framework shown in <ref type="figure" target="#fig_0">Figure 8</ref> contains two streams, where each stream is based on the encoder topology described  <ref type="figure">Figure 9</ref> Topology of the modified AdapNet++ decoder used for multimodal fusion. We propose a mechanism to better correlate the fused mid-level skip refinement features with the high-level decoder feature before integrating into the decoder. The correlation mechanism is depicted following the fuse skip connections.</p><p>in Section 3.1. Each encoder stream is modality-specific and specializes in a particular sub-space. In order to fuse the feature maps from both streams, we adopt a combination of mid-level and late fusion strategy in which we fuse the latent representations of both encoders using the SSMA block and pass the fused feature map to the first decoder stage. We denote this as latent SSMA fusion as it takes the output of the eASPP from each modality-specific encoder as input. We set the reduction ratio η = 16 in the latent SSMA. As the AdapNet++ architecture contains skip connections for highresolution refinement, we employ an SSMA block at each skip refinement stage after the 1 × 1 convolution as shown in <ref type="figure" target="#fig_0">Figure 8</ref>. As the 1 × 1 convolutions reduce the feature channel depth to 24, we only use a reduction ratio η = 6 in the two skip SSMAs as identified from the ablation experiments presented in Section 5.10.2.</p><p>In order to upsample the fused predictions, we build upon our decoder described in Section 3.3. The main stream of our decoder resembles the topology of the decoder in our Ad-apNet++ architecture consisting of three upsampling stages. The output of the latent SSMA block is fed to the first upsampling stage of the decoder. Following the AdapNet++ topology, the outputs of the skip SSMA blocks would be concatenated into the decoder at the second and third upsampling stages (skip1 after the first deconvolution and skip2 after the second deconvolution). However, we find that concatenating the fused mid-level features into the decoder does not substantially improve the resolution of the segmentation, as much as in the unimodal AdapNet++ architecture. We hypothesise that directly concatenating the fused midlevel features and fused high-level features causes a feature localization mismatch as each SSMA block adaptively recalibrates at different stages of the network where the resolution of the feature maps and channel depth differ by one half of their dimensions. Moreover, training the fusion network end-to-end from scratch also contributes to this problem as without initializing the encoders with modality-specific pretrained weights, concatenating the uninitialized mid-level fused encoder feature maps into the decoder does not yield any performance gains, rather it hampers the convergence.</p><p>With the goal of mitigating this problem, we propose two strategies. In order to facilitate better fusion, we adopt a multi-stage training protocol where we first initialize each encoder in the fusion architecture with pre-trained weights from the unimodal AdapNet++ model. We describe this procedure in Section 5.1.2. Secondly, we propose a mechanism to better correlate the mid and high-level fused features before concatenation in the decoder. We propose to weigh the fused mid-level skip features with the spatially aggregated statistics of the high-level decoder features before the concatenation. Following the notation convention, we define D ∈ R C×H×W as the high-level decoder feature map before the skip concatenation stage. A feature statistic s ∈ R C is produced by projecting D along the spatial dimensions H × W using a global average pooling layer as</p><formula xml:id="formula_12">s c = F shrink (d c ) = 1 H ×W H ∑ i=1 W ∑ j=1 d c (i, j),<label>(13)</label></formula><p>where s c represents a statistic or a local descriptor of the c th element of D. We then reduce the number of feature channels in s using a 1 × 1 convolution layer with weights W 4 ∈ R C×H×W , batch normalization and an ReLU activation function δ to match the channels of the fused mid-level feature map f, where f is computed as shown in Equation <ref type="formula" target="#formula_0">(12)</ref>. We can represent resulting output as</p><formula xml:id="formula_13">z = F reduce (s; W) = δ (W 4 s).<label>(14)</label></formula><p>Finally, we weigh the fused mid-level feature map f with the reduced aggregated descriptors z using channel-wise multiplication aŝ</p><formula xml:id="formula_14">f = F loc (f c ; z c ) = (z 1 f 1 , z 2 f 2 , . . . , z c f c ) .<label>(15)</label></formula><p>As shown in <ref type="figure">Figure 9</ref>, we employ the aforementioned mechanism to the fused feature maps from skip1 SSMA as well as skip2 SSMA and concatenate their outputs with the decoder feature maps at the second and third upsampling stages respectively. We find that this mechanism guides the fusion of mid-level skip refinement features with the highlevel decoder feature more effectively than direct concatenation and yields a notable improvement in the resolution of the segmentation output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we first describe the datasets that we benchmark on, followed by comprehensive quantitative results for unimodal segmentation using our proposed AdapNet++ architecture in Section 5.3 and the results for model compression in Section 5.4. We then present detailed ablation studies that describe our architectural decisions in Section 5.5, followed by the qualitative unimodal segmentation results in Section 5.6. We present the multimodal fusion benchmarking experiments with the various modalities contained in the datasets in Section 5.7 and the ablation study on our multimodal fusion architecture in Section 5.10. We finally present the qualitative multimodal segmentation results in Section 5.11 and in challenging perceptual conditions in Section 5.12.</p><p>All our models were implemented using the Tensor-Flow <ref type="bibr">(Abadi et al, 2015)</ref> deep learning library and the experiments were carried out on a system with an Intel Xeon E5 with 2.4GHz and an NVIDIA TITAN X GPU. We primarily use the standard Jaccard Index, also known as the intersection-over-union (IoU) metric to quantify the performance. The IoU for each object class is computed as IoU = TP/(TP + FP + FN), where TP, FP and FN correspond to true positives, false positives and false negatives respectively. We report the mean intersection-over-union (mIoU) metric for all the models and also the pixel-wise accuracy (Acc), average precision (AP), global intersectionover-union (gIoU) metric, false positive rate (FPR), false negative rate (FNR) in the detailed analysis. All the metrics are computed as defined in the PASCAL VOC challenge <ref type="bibr" target="#b17">(Everingham et al, 2015)</ref> and additionally, the gIoU metric is computed as gIoU</p><formula xml:id="formula_15">= ∑ C TP C / ∑ C (TP C +FP C +FN C ),</formula><p>where C is the number of object categories. The implementations of our proposed architectures are publicly available at https://github.com/DeepSceneSeg and a live demo can be viewed at http://deepscene.cs.uni-freiburg.de.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Protocol</head><p>In this section, we first describe the procedure that we employ for training our proposed AdapNet++ architecture, followed by the protocol for training the SSMA fusion scheme. We then detail the various data augmentations that we perform on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">AdapNet++ Training</head><p>We train our network with an input image of resolution 768 × 384 pixels, therefore we employ bilinear interpolation for resizing the RGB images and the nearest-neighbor interpolation for the other modalities as well as the groundtruth labels. We initialize the encoder section of the network with weights pre-trained on the ImageNet dataset <ref type="bibr" target="#b17">(Deng et al, 2009</ref>), while we use the He initialization <ref type="bibr">(He et al, 2015b)</ref> for the other convolutional and deconvolutional layers. We use the Adam solver for optimization with β 1 = 0.9, β 2 = 0.999 and ε = 10 −10 . We train our model for 150K iterations using an initial learning rate of λ 0 = 10 −3 with a mini-batch size of 8 and a dropout probability of 0.5. We use the cross-entropy loss function and set the weights λ 1 = 0.6 and λ 2 = 0.5 to balance the auxiliary losses. The final loss function can be given as L = L main + λ 1 L aux1 + λ 2 L aux2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">SSMA Training</head><p>We employ a multi-stage procedure for training the multimodal models using our proposed SSMA fusion scheme. We first train each modality-specific Adapnet++ model individually using the training procedure described in Section 5.1.1. In the second stage, we leverage transfer learning to train the joint fusion model in the SSMA framework by initializing only the encoders with the weights from the individual modality-specific encoders trained in the previous stage. We then set the learning rate of the encoder layers to λ 0 = 10 −4 and the decoder layers to λ 0 = 10 −3 , and train the fusion model with a mini-batch of 7 for a maximum of 100K iterations. This enables the SSMA blocks to learn the optimal combination of multimodal feature maps from the well trained encoders, while slowly adapting the encoder weights to improve the fusion. In the final stage, we fix the learning rate of the encoder layers to λ 0 = 0 while only training the decoder and the SSMA blocks with a learning rate of λ 0 = 10 −5 and a mini-batch size of 12 for 50K iterations. This enables us to train the network with a larger batch size, while focusing more on the upsampling stages to yield the high-resolution segmentation output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Data Augmentation</head><p>The training of deep networks can be significantly improved by expanding the dataset to introduce more variability. In order to achieve this, we apply a series of augmentation strategies randomly on the input data while training. The augmentations that we apply include rotation (−13°to 13°), skewing (0.05 to 0.10), scaling (0.5 to 2.0), vignetting (210 to 300), cropping (0.8 to 0.9), brightness modulation (−40 to 40), contrast modulation (0.5 to 1.5) and flipping. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets</head><p>We evaluate our proposed AdapNet++ architecture on five publicly available diverse scene understanding benchmarks ranging from urban driving scenarios to unstructured forested scenes and cluttered indoor environments. The datasets were particularly chosen based on the criteria of containing scenes with challenging perceptual conditions including rain, snow, fog, night-time, glare, motion blur and other seasonal appearance changes. Each of the datasets contain multiple modalities that we utilize for benchmarking our fusion approach. We briefly describe the datasets and their constituting semantic categories in this section.</p><p>Cityscapes: The Cityscapes dataset <ref type="bibr" target="#b13">(Cordts et al, 2016</ref>) is one of the largest labeled RGB-D dataset for urban scene understanding. Being one of the standard benchmarks, it is highly challenging as it contains images of complex urban scenes, collected from over 50 cities during varying seasons, lighting and weather conditions. The images were captured using a automotive-grade 22cm baseline stereo camera at a resolution of 2048 × 1024 pixels. The dataset contains 5000 finely annotated images, of which 2875 are provided for training, 500 are provided for validation and 1525 are used for testing. As a supplementary training set, 20000 coarse annotations are also provided. The testing images are not publicly released, they are used by the evaluation server for benchmarking on 19 semantic object categories. We report results on the full 19 class label set for both the validation and test sets. Additionally, in order to facilitate comparison with previous fusion approaches we also report results on the reduced 11 class label set consisting of: sky, building, road, sidewalk, fence, vegetation, pole, car/truck/bus, traffic sign, person, rider/bicycle/motorbike and background.</p><p>In our previous work <ref type="bibr" target="#b53">(Valada et al, 2017)</ref>, we directly used the colorized depth image as input to our network. We converted the stereo disparity map to a three-channel colorized depth image by normalizing and applying the standard jet color map <ref type="figure" target="#fig_2">Figure 10</ref> image and the corresponding colorized depth map from the dataset. However, as seen in the figure, the depth maps have considerable amount of noise and missing depth values due to occlusion, which are undesirable especially when utilizing depth maps as an input modality for pixel-wise segmentation. Therefore, in this work, we employ a recently proposed stateof-the-art fast depth completion technique <ref type="bibr" target="#b31">(Ku et al, 2018)</ref> to fill any holes that may be present. The resulting filled depth map is shown in <ref type="figure" target="#fig_2">Figure 10</ref>(d). The depth completion algorithm can easily be incorporated into our pipeline as a preprocessing step as it only requires 11ms while running on the CPU and it can be further parallelized using a GPU implementation. Additionally, <ref type="bibr" target="#b21">Gupta et al (2014)</ref> proposed an alternate representation of the depth map known as the HHA encoding to enable DCNNs to learn more effectively. The authors demonstrate that the HHA representation encodes properties of geocentric pose that emphasizes on complementary discontinuities in the image which are extremely hard for the network to learn, especially from limited training data. This representation also yields a three-channel image consisting of: horizontal disparity, height above ground, and the angle between the local surface normal of a pixel and the inferred gravity direction. The resulting channels are then linearly scaled and mapped to the 0 to 255 range. However, it is still unclear if this representation enables the network to learn features complementary to that learned from visual RGB images as different works show contradicting results <ref type="bibr">(Hazirbas et al, 2016;</ref><ref type="bibr" target="#b21">Gupta et al, 2014;</ref><ref type="bibr">Eitel et al, 2015)</ref>. In this paper, we perform in-depth experiments with both the jet colorized and the HHA encoded depth map on a larger and more challenging dataset than previous works to investigate the utility of these encodings.</p><p>Synthia: The Synthia dataset <ref type="bibr" target="#b48">(Ros et al, 2016</ref>) is a large-scale urban outdoor dataset that contains photo realistic images and depth data rendered from a virtual city built using the Unity engine. It consists of several annotated label sets. In this work, we use the Synthia-Rand-Cityscapes and the video sequences which have images of resolution 1280 × 760 with a 100°horizontal field of view. This dataset is of particular interest for benchmarking the fusion approaches as it contains diverse traffic situations under different weather conditions. Synthia-Rand-Cityscapes consists of 9000 images and the sequences contain 8000 images with groundtruth labels for 12 classes. The categories of object labels are the same as the aforementioned Cityscapes label set.</p><p>SUN RGB-D: The SUN RGB-D dataset <ref type="bibr" target="#b51">(Song et al, 2015)</ref> is one of the most challenging indoor scene understanding benchmarks to date. It contains 10,335 RGB-D images that were captured with four different types of RGB-D cameras (Kinect V1, Kinect V2, Xtion and RealSense) with different resolutions and fields of view. This benchmark also combines several other datasets including 1449 images from the NYU Depth v2 (Silberman et al, 2012), 554 images from the Berkeley B3DO <ref type="bibr" target="#b27">(Janoch et al, 2013)</ref> and 3389 images from the SUN3D <ref type="bibr" target="#b55">(Xiao et al, 2013)</ref>. We use the original train-val split consisting of 5285 images for training and 5050 images for testing. We use the refined in-painted depth images from the dataset that were processed using a multi-view fusion technique. However, some refined depth images still have missing depth values at distances larger than a few meters. Therefore, as mentioned in previous works <ref type="bibr">(Hazirbas et al, 2016)</ref>, we exclude the 587 training images that were captured using the RealSense RGB-D camera as they contain a significant amount of invalid depth measurements that are further intensified due to the in-painting process. This dataset provides pixel-level semantic annotations for 37 categories, namely: wall, <ref type="bibr">floor, cabinet, bed, chair, sofa, table, door, window, bookshelf, picture, counter, blinds, desk, shelves, curtain, dresser, pillow, mirror, floor mat, clothes, ceiling, books, fridge, tv, paper, towel, shower curtain, box, whiteboard, person, night stand, toilet, sink, lamp, bathtub</ref> and bag. Although we benchmark on all the object categories, 16 out of the 37 classes are rarely present in the images and about 0.25% of the pixels are not assigned to any of the classes, making it extremely unbalanced. Moreover, as each scene contains many different types of objects, they are often partially occluded and may appear completely different in the test images.</p><p>ScanNet: The ScanNet RGB-D video dataset <ref type="bibr" target="#b16">(Dai et al, 2017</ref>) is a recently introduced large-scale indoor scene understanding benchmark. It contains 2.5M RGB-D images accounting to 1512 scans acquired in 707 distinct spaces. The data was collected using an iPad Air2 mounted with a depth camera similar to the Microsoft Kinect v1. Both the iPad camera and the depth camera were hardware synchronized and frames were captured at 30Hz. The RGB images were captured at a resolution of 1296 × 968 pixels and the depth frames were captured at 640×480 pixels. The semantic segmentation benchmark contains 16,506 labelled training images and 2537 testing images. From the example depth image shown in <ref type="figure">Figure 13</ref>(b), we can see that there are a number of missing depth values at the object boundaries and at large distances. Therefore, similar to the preprocessing that we perform on the cityscapes dataset, we use a fast depth completion technique <ref type="bibr" target="#b31">(Ku et al, 2018)</ref> to fill the holes. The corresponding filled depth image is shown in <ref type="figure">Figure 13</ref>(c). We also compute the HHA encoding for the depth maps and use them as an additional modality in our experiments.</p><p>The dataset provides pixel-level semantic annotations for 21 object categories, namely: wall, floor, chair, table, desk, bed, bookshelf, sofa, sink, bathtub, toilet, curtain, counter, door, window, shower curtain, refrigerator, picture, cabinet, other furniture and void. Similar to the SUN RGB-D dataset, many object classes are rarely present making the dataset unbalanced. Moreover, the annotations at the object boundaries are often irregular and parts of objects at large distances are unlabelled as shown in <ref type="figure">Figure 13</ref>(e). These factors make the task even more challenging on this dataset.</p><p>Freiburg Forest: In our previous work <ref type="bibr" target="#b53">(Valada et al, 2016b)</ref>, we introduced the Freiburg Multispectral Segmentation benchmark, which is a first-of-a-kind dataset of unstructured forested environments. Unlike urban and indoor scenes which are highly structured with rigid objects that have distinct geometric properties, objects in unstructured forested environments are extremely diverse and moreover, their appearance completely changes from month to month due to seasonal variations. The primary motivation for the introduction of this dataset is to enable robots to discern obstacles that can be driven over such as tall grass and bushes to obstacles that should be avoided such as tall trees and boulders. Therefore, we proposed to exploit the presence of chlorophyll in these objects which can be detected in the Near-InfraRed (NIR) wavelength. NIR images provide a high fidelity description on the presence of vegetation in the scene and as demonstrated in our previous work <ref type="bibr" target="#b53">(Valada et al, 2017)</ref>, it enhances border accuracy for segmentation.</p><p>The dataset was collected over an extended period of time using our Viona autonomous robot equipped with a Bumblebee2 camera to capture stereo images and a modified camera with the NIR-cut filter replaced with a Wratten 25A filter for capturing the NIR wavelength in the blue and green channels. The dataset contains over 15,000 images that were sub-sampled at 1Hz, corresponding to traversing over 4.7km each day. In order to extract consistent spatial and global vegetation information we computed vegetation indices such as Normalized Difference Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI) using the approach presented by <ref type="bibr" target="#b26">Huete et al (1999)</ref>. NDVI is resistant to noise caused due to changing sun angles, topography and shadows but is susceptible to error due to variable atmospheric and canopy background conditions <ref type="bibr" target="#b26">(Huete et al, 1999)</ref>. EVI was proposed to compensate for these defects with improved sensitivity to high biomass regions and improved detection though decoupling of canopy background signal and reduction in atmospheric influences. <ref type="figure">Figure 14</ref> shows an example image from the dataset and the corresponding modalities. The dataset contains hand-annotated segmentation groundtruth for six classes: sky, trail, grass, vegetation, obstacle and void. We use the original train and test splits provided by the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">AdapNet++ Benchmarking</head><p>In this section, we report results comparing the performance of our proposed AdapNet++ architecture against several well adopted state-of-the-art models including DeepLab v3 , ParseNet <ref type="bibr" target="#b37">(Liu et al, 2015)</ref>, <ref type="bibr">FCN-8s (Long et al, 2015)</ref>, SegNet <ref type="bibr" target="#b4">(Badrinarayanan et al, 2015)</ref>, FastNet , DeepLab v2 , Decon-vNet <ref type="bibr" target="#b40">(Noh et al, 2015)</ref> and Adapnet <ref type="bibr" target="#b53">(Valada et al, 2017)</ref>. We use the official implementations of these architectures that are publicly released by the authors to train on the input image resolution of 768 × 384 pixels. For the Cityscapes and ScanNet benchmarking results reported in <ref type="table">Table 2 and  Table 6</ref>, we report results directly from the official benchmark leaderboard. For each of the datasets, we report the mIoU score, as well as the per-class IoU score. In order to have a fair comparison, we also evaluate the models at the same input resolution using the same evaluation settings. We do not apply multiscale inputs or left-right flips during testing as these techniques require each crop of each image to be evaluated several times which significantly increases the computational complexity and runtime (Note: We do not use crops for testing, we evaluate on the full image in a single forward-pass). Moreover, these techniques do not improve the performance of the model in real-time applications. However, we show the potential gains that can be obtained in the evaluation metric utilizing these techniques and with a higher resolution input image in the ablation study presented in Section 5.5.6. Additionally, we report results with full resolution evaluation on the test set of the datasets when available, namely for Cityscapes and ScanNet. <ref type="table" target="#tab_6">Table 1</ref> shows the results on the 11 class Cityscapes validation set. AdapNet++ outperforms all the baselines in each individual object category as well in the mIoU score. Ad-apNet++ outperforms the highest baseline by a margin of 3.24%. Analyzing the individual class IoU scores, we can see that AdapNet++ yields the highest improvement in object categories that contain thin structures such as poles for which it gives a large improvement of 5.42%, a similar improvement of 5.05% for fences and the highest improvement for 7.29% for signs. Most architectures struggle to recover the structure of thin objects due to downsampling by pooling and striding in the network which causes such information to be lost. However, these results show that AdapNet++ efficiently recovers the structure of such objects by learning multiscale features at several stages of the encoder using the proposed multiscale residual units and the eASPP. We further show the improvement in performance due to the incorporation of the multiscale residual units and the eASPP in the ablation study presented in Section 5.5.1. In driving scenarios, information of objects such as pedestrians and cyclists can also be lost when they appear at far away distances. A large improvement can also be seen in categories such as person in which AdapNet++ achieves an improvement of 5.66%. The improvement in larger object categories such as cars and vegetation can be attributed to the new decoder which improves the segmentation performance near object boundaries. This is more evident in the qualitative results presented in Section 5.11. Note that the colors shown below the object category names serve as a legend for the qualitative results.</p><p>We also report results on the full 19 class Cityscapes validation and test sets in <ref type="table">Table 2</ref>. We compare against the top six published models on the leaderboard, namely, PSPNet (Zhao  We benchmark on the Synthia dataset largely due to the variety of seasons and adverse perceptual conditions where the improvement due to multimodal fusion can be seen. However, even for baseline comparison shown in <ref type="table" target="#tab_7">Table 3</ref>, it can be seen that AdapNet++ outperforms all the baselines, both in the overall mIoU score as well as in the score of the individual object categories. It achieves an overall improvement of 3.87% and a similar observation can be made in the improvement of scores for thin structures, reinforcing the utility of our proposed multiscale feature learning configuration. The largest improvement of 13.14% was obtained for the sign class, followed by an improvement of 7.8% for the pole class. In addition a significant improvement of 5.42% can also be seen for the cyclist class.</p><p>Compared to outdoor driving datasets, indoor benchmarks such as SUN RGB-D and ScanNet pose a different challenge. Indoor datasets have vast amounts of object categories in various different configurations and images captured from many different view points compared to driving scenarios where the camera is always parallel to the ground with similar viewpoints from the perspective of the vehicle driving on the road. Moreover, indoor scenes are often extremely cluttered which causes occlusions, in addition to the irregular frequency distribution of the object classes that make the problem even harder. Due to these factors SUN RGB-D is considered one of the hardest datasets to benchmark on. Despite these factors, as shown in <ref type="table" target="#tab_8">Table 4</ref>, Ad-apNet++ outperforms all the baseline networks overall by a margin of 2.66% compared to the highest performing Dee-pLab v3 baseline which took 30, 000 iterations more to reach this score. Unlike the performance in the Cityscapes and Synthia datasets where our previously proposed AdapNet architecture yields the second highest performance, AdapNet is outperformed by DeepLab v3 in the SUN RGB-D dataset. AdapNet++ on the other hand, outperforms the baselines in most categories by a large margin, while it is outperformed in 13 of the 37 classes by small margin. It can also be observed that the classes in which AdapNet++ get outperformed are the most infrequent classes. This can be alleviated by adding supplementary training images containing the low-frequency classes from other datasets or by employing class balancing techniques. However, our initial experiments employing techniques such as median frequency class balancing, inverse median frequency class balancing, normalized inverse frequency balancing, severely affected the performance of our model.</p><p>We report results on the ScanNet validation set in <ref type="table" target="#tab_9">Table 5</ref>. AdapNet++ outperforms the state-of-the-art overall by a margin of 2.83%. The large improvement can be attributed to the proposed eASPP which efficiently captures long range context. Context aggregation plays an important role in such cluttered indoor datasets as different parts of an object are occluded from different viewpoints and across scenes. As objects such as the legs of a chair have thin structures, multiscale learning contributes to recovering such structures. We see a similar trend in the performance as in the SUN RGB-D dataset, where our network outperforms the baselines in most of the object categories (16 of the 20 classes) significantly, while yielding a comparable performance for the other categories. The largest improvement of 5.70% is obtained for the toilet class, followed by an improvement of 5.34% for the bed class which appears as many different variations in the dataset. An interesting observation that can be made is that the highest parametrized network DeconvNet which has 252M parameters has the lowest performance in both SUN RGB-D and ScanNet datasets, while AdapNet++ which has about 1/9th of the parameters, outperforms it by more than twice the margin. However, this is only observed in the indoor datasets, while in the outdoor datasets DeconvNet performs comparable to the other networks. This is primarily due to the fact that indoor datasets have more number of small classes and the predictions of DeconvNet do not retain them. <ref type="table">Table 6</ref> shows the results on the ScanNet test set. We compare against the top performing models on the leaderboard, namely, FuseNet (Hazirbas et al, 2016), 3DMV (2d proj) <ref type="bibr" target="#b15">(Dai and Nießner, 2018</ref><ref type="bibr">), PSPNet (Zhao et al, 2017</ref><ref type="bibr">), and Enet (Paszke et al, 2016</ref>. Note that 3DMV and FuseNet are multimodal fusion methods. Our proposed AdapNet++ model outperforms all the unimodal networks and achieves state-of-the-art performance for unimodal semantic segmentation on the ScanNet benchmark.</p><p>Finally, we also benchmark on the Freiburg Forest dataset as it contains several modalities and it is the largest dataset to provide labeled training data for unstructured forested environments. We show the results on the Freiburg Forest dataset in <ref type="table" target="#tab_10">Table 7</ref>, where our proposed AdapNet++ outperforms the state-of-the-art by 0.82%. Note that this dataset contains large objects such trees and it does not contain thin structures or objects in multiple scales. Therefore, the improvement produced by AdapNet++ is mostly due to the proposed decoder which yields an improved resolution of segmentation along the object boundaries. The actual utility of this dataset is seen in the qualitative multimodal fusion results, where the fusion helps to improve the segmentation in the presence of disturbances such as glare on the optics and snow. Nevertheless, we see the highest improvement of 3.52% in the obstacle class, which is the hardest to segment in this dataset as it contains many different types of objects in one category and it has comparatively fewer examples in the dataset Moreover, we also compare the number of parameters and the inference time with the baseline networks in <ref type="table" target="#tab_10">Table 7</ref>. Our proposed AdapNet++ architecture performs inference in 72.77ms on an NVIDIA TITAN X which is substantially faster than the top performing architectures in all the benchmarks. Most of them consume more than twice the amount of time and the number of parameters making them unsuitable for real-world resource constrained applications. Our critical design choices enable AdapNet++ to consume only 10.98ms more than our previously proposed AdapNet, while exceeding its performance in each of the benchmarks by a large margin. This shows that AdapNet++ achieves the right performance vs. compactness trade-off which enables it to be employed in not only resource critical applications, but also in applications that demand efficiency and a fast inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">AdapNet++ Compression</head><p>In this section, we present empirical evaluations of our proposed pruning strategy that is invariant to shortcut connections in <ref type="table">Table 8</ref>. We experiment with pruning entire convolutional filters which results in the removal of its corresponding feature map and the related kernels in the following layer. Most existing approaches only prune the first and the second convolution layer of each residual block, or in addition, equally prune the third convolution layer similar to the shortcut connection. However, this equal pruning strategy al-ways leads to a significant drop in the accuracy of the model that is not recoverable . Therefore, recent approaches have resorted to omitting pruning of these connections. Contrarily, our proposed technique is invariant to the presence of identity or projection shortcut connections, thereby making the pruning more effective and flexible. We employ a greedy pruning approach but rather than pruning layer by layer and fine-tuning the model after each step, we perform pruning of entire residual blocks at once and then perform the fine-tuning. As our network has a total of 75 convolutional and deconvolutional layers, pruning and fine-tuning each layer will be extremely cumbersome. Nevertheless, we expect a higher performance employing a fully greedy approach.</p><p>We compare our strategy with a baseline approach ) that uses the 1 -norm of the convolutional filters to compute their importance as well as the approach that we build upon that uses the Taylor expansion criteria (Molchanov et al, 2017) for the ranking as described in Section 3.5. We denote the approach of <ref type="bibr" target="#b39">Molchanov et al (2017)</ref> as Oracle in our results. In the first stage, we start by pruning only the Res5 block of our model as it contains the most number of filters, therefore, a substantial amount of parameters can be reduced without any loss in accuracy. As shown in <ref type="table">Table 8</ref>, our approach enables a reduction of 6.82% of the parameters and 3.3B FLOPS with a slight increase in the mIoU metric. Similar to our approach the original Oracle approach does not cause a drop in the mIoU metric but achieves a lower reduction in parameters. Whereas, the baseline approach achieves a smaller reduction in the parameters and simultaneously causes a drop in the mIoU score. Our aim for pruning in the first stage was to compress the model without causing a drop in the segmentation performance, while in the following stages, we aggressively prune the model to achieve the best parameter to performance ratio. Results from this experiment are shown as the percentage in reduction of parameters in comparison to the change in mIoU in <ref type="figure" target="#fig_6">Figure 15</ref>. In the second stage, we prune the convolutional feature maps of Res2, Res3, Res4 and Res5 layers. Using our proposed method, we achieve a reduction of 23.31% of parameters with minor drop of 0.19% in the mIoU score. Whereas, the Oracle approach yields a lower reduction in parameters as well as a larger drop in performance. A similar trend can also be seen for the other pruning stages where our proposed approach yields a higher reduction in parameters and FLOPS with a minor reduction in the mIoU score. This shows that pruning convolutional feature maps with regularity leads to a better compression ratio than selectively pruning layers at different stages of the network. In the third stage, we perform pruning of the deconvolutional feature maps, while in the fourth and fifth stages we further prune all the layers of the network by varying the threshold for the rankings. In the final stage we obtain a reduction of 41.62% of the parameters and 42.60% of FLOPS with a drop of 2.72% in the mIoU score. Considering the compression that can be achieved, this minor drop in the mIoU score is acceptable to enable efficient deployment in resource constrained applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">AdapNet++ Ablation Studies</head><p>In order to evaluate the various components of our Ad-apNet++ architecture, we performed several experiments in different settings. In this section, we study the improvement obtained due to the proposed encoder with the multiscale residual units, a detailed analysis of the proposed eASPP, comparisons with different base encoder network topologies, the improvement that can be obtained by using higher resolution images as input and using multiscale testing. For each of these components, we also study the effect of different parameter configurations. All the ablation studies presented in this section were performed on models trained on the Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Detailed study on the AdapNet++ Architecture</head><p>We first study the major contributions made to the encoder as well as the decoder in our proposed AdapNet++ architecture. <ref type="table">Table 9</ref> shows results from this experiment and subsequent improvement due to each of the configurations. The simple base model M1 consisting of the standard ResNet-50 for the encoder and a single deconvolution layer for upsampling achieves a mIoU of 75.22%. The model M2 that incorporates our multiscale residual units achieves an improvement of 1.7% without any increase in the memory consumption. Whereas, the multigrid approach from DeepLab v3  in the same configuration achieves only 0.38% of improvement in the mIoU score. This shows the novelty in employing our multiscale residual units for efficiently learning multiscale features throughout the network. In the M3 model, we study the effect of incorporating skip connections for refinement. Skip connections that were initially introduced in the FCN architecture are still widely used for improving the resolution of the segmentation by incorporating low or mid-level features from the encoder into the decoder while upsampling. The ResNet-50 architecture contains the most discriminative features in the middle of the network. In our M3 model, we first upsample the encoder <ref type="table">Table 9</ref> Effect of the various contributions proposed in the AdapNet++ architecture. The performance is shown for the model trained on the Cityscapes dataset and evaluated on the validation set. ↑ k f refers to a deconvolution layer and c k f refers to a convolution layer with f number of filters, k × k kernel size and n is the number of classes. PA ResNet-50 refers to the full preactivation ResNet-50 architecture. The weights for the two auxilary losses were set to L 1 = 0.5 and L 2 = 0.6 for this experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><formula xml:id="formula_16">M1 ResNet-50 - c 1 n ↑ 16 n - - - - 75.22 M2 ResNet-50 c 1 n ↑ 16 n - - - - 76.92 M3 ResNet-50 c 1 n ↑ 2 2n ⊕ ↑ 8 n 3d - - - 77.78 M4 PA ResNet-50 c 1 n ↑ 2 2n ⊕ ↑ 8 n 3d - - - 78.44 M5 PA ResNet-50 c 1 n ↑ 2 2n ⊕ ↑ 8 n 3d - - 78.93 M6 PA ResNet-50 c 1 n ↑ 2 2n ⊕ ↑ 2 2n ⊕ ↑ 8 n 3d,2c - - 79.19 M7 PA ResNet-50 ↑ 2 c 3 c 3 ↑ 2 c 3 c 3 c 1 n ↑ 8 n 3d,2c - - 79.82 M8 PA ResNet-50 ↑ 2 c 3 c 3 ↑ 2 c 3 c 3 c 1 n ↑ 8 n 3d,2c - 80.34 M9 PA ResNet-50 ↑ 2 c 3 c 3 ↑ 2 c 3 c 3 c 1 n ↑ 8 n 3d,2c 80.67</formula><p>output by a factor two, followed by fusing the features from Res3d block of the encoder to refinement and subsequently using another deconvolutional layer to upsample back to input resolution. This model achieves a further improvement of 0.86%.</p><p>In the M4 model, we replace the standard residual units with the full pre-activation residual units which yields an improvement of 0.66%. As mentioned in the work by <ref type="bibr" target="#b22">He et al (2016)</ref>, the results corroborate that pre-activation residual units yields a lower error than standard residual units due to the ease of training and improved generalization capability. Aggregating multiscale context using ASPP has become standard practice in most classification and segmentation networks. In the M5 model, we add the ASPP module to the end of the encoder segment. This model demonstrates an improved mIoU of 78.93% due to the ability of the ASPP to capture long range context. In the subsequent M6 model, we study if adding another skip refinement connection from the encoder yields a better performance. This was challenging as most combinations along with the Res3d skip connection did not demonstrate any improvement. However, adding a skip connection from Res2c showed a slight improvement.</p><p>In all the models upto this stage, we fused the mid-level encoder features into the decoder using element-wise addition. In order to make the decoder stronger, we experiment with improving the learned decoder representations with additional convolutions after concatenation of the mid-level features. Specifically, the M7 model has three upsampling stages, the first two stages consist of a deconvolution layer that upsamples by a factor of two, followed by concatenation of the mid-level features and two following 3 × 3 convolutions that learn highly discriminative fused features. This model shows an improvement of 0.63% which is primarily due to the improved segmentation along the object boundaries as demonstrated in the qualitative results in Section 5.11. Our M7 model contains a total of 75 convolutional and deconvolutional layers, making the optimization challenging. In order to accelerate the training and to further improve the segmentation along object boundaries, we propose a multiresolution supervision scheme in which we add a weighted auxiliary loss to each of the first two upsampling stages. This model denoted as M8 achieves an improved mIoU of 80.34%. In comparison to aforementioned scheme, we also experimented with adding a weighted auxiliary loss at the end of the encoder of the M7 model, however it did not improve the performance, although it accelerated the training. Finally we also experimented with initializing the layers with the He initialization <ref type="bibr">(He et al, 2015b</ref>) scheme (also known as MSRA) in the M9 model which further boosts the mIoU to 80.67%. The following section further builds upon the M9 model to yield the topology of our proposed AdapNet++ architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Detailed study on the eASPP</head><p>In this section, we quantitatively and qualitatively evaluate the performance of our proposed eASPP configuration and the new decoder topology. We perform all the experiments in this section using the best performing M9 model described in Section 5.5.1 and report the results on the Cityscapes validation set in <ref type="table" target="#tab_6">Table 10</ref>. In the first configuration of the M91 model, we employ a single 3 × 3 atrous convolution in the ASPP, similar the configuration proposed in DeepLab v3  and use a single 1 × 1 convolution in the place of the two 3 × 3 convolutions in the decoder of the M9 model. This model achieves an mIoU score of 80.06% with 41.3M parameters and consumes 115.99B FLOPS. In order to better fuse the concatenated mid-level <ref type="table" target="#tab_6">Table 10</ref> Evaluation of various atrous spatial pyramid pooling configurations. The performance is shown for the model trained on the Cityscapes dataset and evaluated on the validation set. ↑ k f refers to a deconvolution layer and c k f refers to a convolution layer with f number of filters and k × k kernel size. The weights for the two auxilary losses were set to L 1 = 0.5 and L 2 = 0.6 for this experiment. features with the decoder and to improve its discriminability, we replace the 1 × 1 convolution layer with a 3 × 3 convolution in the M92 model and with two 3 × 3 convolutions in the M93 model. Both these models demonstrate an increase in performance corroborating that a simple 1 × 1 convolution is insufficient for object boundary refinement using fusion of mid-level encoder features.</p><p>In an effort to reduce the number of parameters, we employ a bottleneck architecture in the ASPP of the M94 model by replacing the 3 × 3 atrous convolution with a structure consisting of a 1 × 1 convolution with half the number of filters, followed by a 3 × 3 atrous convolution with half the number of filters and another 1 × 1 convolution with the original amount of filters. This model achieves an mIoU score of 80.42% which accounts to a reduction of 0.25% in comparison to the M93 model, however, it reduces computational requirement by 13.6M parameters and 31.41B FLOPS which makes the model very efficient. Nevertheless, this drop in performance is not ideal. Therefore, in order to compensate for this drop, we leverage the idea of cascading atrous convolutions that enables an increase in the size of the effective receptive field and the density of the pixel sampling. Specifically, in the M95 model, we add a cascaded 3 × 3 atrous convolution in place of the single 3 × 3 atrous convolution in the M94 model. This model achieves a mIoU score of 80.77% which is an increase of 0.35% in the mIoU with only a minor increase of 0.1M parameters in comparison to our M94 model. The originally proposed ASPP module consumes 15.53M parameters and 34.58B FLOPS, where the cascaded bottleneck structure in the M95 model only consumes 2.04M parameters and 3.62B FLOPS which is over 10 times more computationally efficient. We denote this cascaded bottleneck structure as eASPP.</p><p>Furthermore, we present detailed experimental comparisons of our proposed eASPP with other ASPP configurations in <ref type="table" target="#tab_6">Table 11</ref>. Specifically, we compare against the initial ASPP configuration proposed in DeepLab v2  which we denote as ASPP v2, the improved ASPP configuration that also incorporates image-level features as proposed in DeepLab v3  which we denote as ASPP v3, the ASPP configuration with separable convolutions and the more recently proposed DenseASPP <ref type="bibr" target="#b57">(Yang et al, 2018)</ref> configuration. In order to have a fair comparison, we use the same AdapNet++ architecture with the different ASPP configurations for this experiment and present the results on the Cityscapes validation set. The four parallel atrous convolution layers in the ASPP v2 configuration of DeepLab v2 have the number of feature channels equal to the number of object classes in the dataset, while the ASPP v3 configuration of DeepLab v3 has the number of feature channels equal to 256 in the three parallel atrous convolution layers. The ASPP v2 model with the convolution feature channels set to the number of object classes achieves a mIoU of 79.22% and increasing the number of convolution feature channels to 256 yields a mIoU of 80.25%. By incorporating image-level features using a global pooling layer and removing the fourth parallel atrous convolution in ASPP v2, the ASPP v3 model achieves an improved performance of 80.67% with a minor decrease in the parameters and FLOPs. Recently, separable convolutions are being employed in place of the standard convolution layer as an efficient alternative to reduce the model size. Employing atrous separable convolutions in the ASPP v3 configuration significantly reduces the number of parameters and FLOPs consumed by the model to 3M and 5.56B respectively. However, this also reduces the mIoU of the model to 80.27% which is comparable to the ASPP v2 configuration with 256 convolutional filters. The model with the DenseASPP configuration achieves a mIoU of 80.62% which is still lower than the ASPP v3 configuration but it reduces the number of parameters and FLOPs to 4.23M and 9.74B respectively. It should be noted that in the work of <ref type="bibr" target="#b57">Yang et al (2018)</ref>, DenseASPP was only compared to ASPP v2 with the number of convolutional feature channels equal to the number of object classes (mIoU of 79.22%). In comparison to the aforementioned ASPP topologies, our proposed eASPP achieves the highest mIoU score of 80.77% with the lowest consumption of parameters and FLOPs. This accounts to a reduction of 86.86% of the number of parameters and 89.53% of FLOPs with a increase in the mIoU compared to the previously best performing ASPP v3 topology.</p><p>In order to illustrate the phenomenon caused by cascading atrous convolutions in our proposed eASPP, we visualize the empirical receptive field using the approach proposed by Zhou et al <ref type="bibr">(2014)</ref>. First, for each feature vector representing an image patch, we use a 8 × 8 mean image to occlude the patch at different locations using a sliding window. We then record the change in the activation by measuring the Euclidean distances as a heat map which indicates which regions are sensitive to the feature vector. Although the size of the empirical receptive fields is smaller than theoretical receptive fields, they are better localized and more representative of the information they capture <ref type="bibr">(Zhou et al, 2014)</ref>. In <ref type="figure" target="#fig_7">Figure 16</ref>, we show visualizations of the empirical receptive field size of the convolution layer of the ASPP that has one 3 × 3 atrous convolution in each branch in comparison to our M95 model that has cascaded 3 × 3 atrous convolutions. <ref type="figure" target="#fig_7">Figure 16(b)</ref> and (c) show the receptive field at the annotated yellow dot for the atrous convolution with the largest dilation rate in ASPP and in our eASPP correspondingly. It can be seen that our eASPP has a much larger receptive field that enables capturing large contexts. Moreover, it can be seen that the pixels are sampled much denser in our eASPP in comparison to the ASPP. In <ref type="figure" target="#fig_7">Figure 16(d)</ref> and (h), we show the aggregated receptive fields of the entire module in which it can be observed that our eASPP has much lesser isolated points of focus and a cleaner sensitive area than the ASPP. We evaluated the generalization of our proposed eASPP by incorporating the module into our AdapNet++ architecture and benchmarking its performance in comparison to DeepLab which incorporates the ASPP. The results presented in Section 5.3 demonstrate that our eASPP effectively generalizes to a wide range of datasets containing diverse environments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Improving Granularity of Segmentation</head><p>In our AdapNet++ architecture, we propose two strategies to improve the segmentation along object boundaries in addition to the new decoder architecture. The first being the two skip refinement stages that fuse mid-level encoder features from Res3d and Res2c into the decoder for object boundary refinement. However, as the low and mid-level features have a large number of filters (512 in Res3d and 256 in Res2c) in comparison to the decoder filters that only have 256 feature channels, they will outweigh the high level features and decrease the performance. Therefore, we employ a 1 × 1 convolution to reduce the number of feature channels in the low and mid-level representations before fusing them into the decoder. In <ref type="table" target="#tab_6">Table 12</ref>, we show results varying the number of feature channels in the 1 × 1 skip refinement convolutions in the M95 model from Section 5.5.2. We obtain the best results by reducing the number of mid-level encoder feature channels to 24 using the 1 × 1 convolution layer.</p><p>The second strategy that we employ for improving the segmentation along object boundaries is using our proposed multiresolution supervision scheme. As described in Section 3.4, we employ auxiliary loss branches after each of the first two upsampling stages in the decoder to improve the resolution of the segmentation and to accelerate training. Weighing the two auxiliary losses is critical to balance the gradient flow through all the previous layers of the network. We experiment with different loss weightings and report results for the same M95 model in <ref type="table" target="#tab_6">Table 13</ref>. The network achieves the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 17</head><p>Influence of the proposed decoder, skip refinement and multiresolution supervision on the segmentation along objects boundaries using the trimap experiment. The plot shows the mIoU score as a function of the trimap band width along the object boundaries. The results are shown on the Cityscapes dataset and evaluated on the validation set.</p><p>highest performance for auxiliary loss weightings λ 1 = 0.6 and λ 2 = 0.5 for L aux1 and L aux2 respectively.</p><p>In order to quantify the improvement specifically along the object boundaries, we evaluate the performance of our architecture using the trimap experiment <ref type="bibr" target="#b29">(Kohli et al, 2009</ref>). The mIoU score for the pixels that are within the trimap band of the void class labels (255) are computed by applying the morphological dilation on the void labels. Results from this experiment shown in <ref type="figure">Figure 17</ref> demonstrates that our new decoder improves the performance along object boundaries compared to the decoder in AdapNet <ref type="bibr" target="#b53">(Valada et al, 2017)</ref>, while the M7 model with the new decoder and the skip refinement further improves the performance. Finally, the M8 model consisting of our new decoder with the skip refinement stages and our multiresolution supervision scheme for training significantly improves the segmentation along the boundaries which is more evident when the trimap band is narrow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Encoder Topology</head><p>In recent years, several efficient network architectures have been proposed for image classification that are computationally inexpensive and have fast inference times. In order to study the trade-off between accuracy and computational requirement, we performed experiments using five widely employed architectures as the encoder backbone. Specifically, we evaluate the performance using ResNet-50 (He et al, 2015a), full pre-activation ResNet-50 <ref type="bibr" target="#b22">(He et al, 2016)</ref>, Res-NeXt <ref type="bibr" target="#b56">(Xie et al, 2017)</ref>, SEnet <ref type="bibr" target="#b24">(Hu et al, 2017)</ref> and Xception <ref type="bibr" target="#b11">(Chollet, 2016)</ref> architectures for the encoder topology and augmented them with our proposed modules similar to the M95 model described in Section 5.5.2. Results from this experiment are shown in <ref type="table" target="#tab_6">Table 14</ref>. Note that in the comparisons presented in this section, no model compression has been performed. It can be seen that the full pre-activation ResNet-50 model achieves the highest mIoU score, closely followed by the ResNeXt model. However the ResNeXt model has an additional 7.34M parameters with a slightly lesser number of FLOPS. While, the standard ResNet-50 architecture has 3.19M parameters lesser than the full pre-activation ResNet-50 model, it achieves a lower mIoU score of 79.32%. Therefore, we choose the full-preactivation ResNet-50 architecture as the encoder backbone in our proposed AdapNet++ architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.5">Decoder Topology</head><p>In this section, we compare the performance and computational efficiency of the new strong decoder that we introduce in our proposed AdapNet++ architecture with other existing progressive upsampling decoders. For a fair comparison, we employ the same AdapNet++ encoder in all the models in this experiment and only replace the decoder with the topologies proposed in LRR (Ghiasi and Fowlkes, 2016) and RefineNet <ref type="bibr" target="#b36">(Lin et al, 2017)</ref>. All these decoder topologies that we compare with utilize similar stage-wise upsampling with deconvolution layers and refinement with skip connections from higher resolution encoder feature maps. As a reference, we also compare with a model that employs the AdapNet++ encoder and direct bilinear upsampling for the decoder. Therefore, this reference model does not consume any parameters and FLOPs for the decoder section.  <ref type="table" target="#tab_6">Table 15</ref> shows the results from this experiment in which we see that the reference model with direct bilinear upsampling achieves a mIoU score of 74.83%. The LRR decoder model outperforms the reference model and the Re-fineNet decoder model further outperforms the LRR decoder model. However, the RefineNet decoder consumes a significant amount of parameters and FLOPs compared to the LRR decoder. Nevertheless, our proposed decoder in AdapNet++ achieves the highest mIoU score of 80.77%, thereby outperforming both the other competing progressive upsampling decoders while consuming the lowest amount of FLOPs and still maintaining a good parameter efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.6">Image Resolution and Testing Strategies</head><p>We further performed experiments using input images with larger resolutions as well as with left-right flipped inputs and multiscale inputs while testing. In all our benchmarking experiments, we use an input image with a resolution of 768 × 384 pixels in order enable training of the multimodal fusion model that has two encoder streams on a single GPU. State-of-the-art semantic segmentation architectures use multiple crops from the full resolution of the image as input. For example for the Cityscapes dataset, eight crops of 720 × 720 pixels from each full resolution image of 2048 × 1024 pixels are often used. This yields a downsampled output with a larger resolution at the end of the encoder, thereby leading to a lesser loss of information due to downsampling and more boundary delineation. Employing a larger resolution image as input also enables better segmentation of small objects that are at far away distances, especially in urban driving datasets such as Cityscapes. However, the caveat being that it requires multi-GPU training with synchronized batch normalization in order to utilize a large enough mini-batch size, which makes the training more cumbersome. Moreover, using large crops of the full resolution image significantly increases the inference time of the model as the inference time for one image is the sum of the inference time consumed for each of the crops. Nevertheless, we present experimental results with input images of resolution 896×448 pixels, 1024×512 pixels, and eight crops of 720 × 720 pixels from the full resolution of 2048 × 1024 pixels, in addition to the resolution of 768 × 384 pixels that we use. In addition to the varying input image resolutions, we also test with left-right flips and multiscale inputs. However, although this increases the mIoU score it substantially increases the computation complexity and runtime, therefore rendering it not useful in real-world applications. A summary of the results from this experiment are shown in <ref type="table" target="#tab_6">Table 16</ref>. It can be seen that with each higher resolution image, the model yields an increased mIoU score and simultaneously consumes a larger inference time. Similarly, left-right flips and multiscale inputs also yield an improvement in the mIoU score. For the input image resolution of 768 × 384 pixels that we employ in the benchmarking experiments, left-right flips yields an increase of 0.58% in the mIoU, while multiscale inputs in addition, yields a further improvement of 0.9%. The corresponding pixel accuracy and and average precision also shows an improvement. The model trained with eights crops of 720 × 720 pixels from each full resolution image of 2048 × 1024 pixels demonstrates an improvement of 2.33% in the mIoU score in comparison to the model with a lower resolution image that we use for benchmarking. Furthermore, using left-right flips and multiscale inputs yields an overall improvement of 3.77% in the mIoU and additional improvements in the other metrics in comparison to the benchmarking model. However, it can be seen that the inference time full resolution model is 494.98ms and multiscale testing with left-right flips further increases it to 12188.57ms, while the inference time of the model that uses a image resolution of 768 × 384 pixels is only 72.77ms demonstrating that using full resolution of the image and multiscale testing with left-right flips for real-world robotics applications is impractical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Qualitative Results of Unimodal Segmentation</head><p>In this section, we qualitatively evaluate the semantic segmentation performance of our AdapNet++ architecture in comparison to the best performing state-of-the-art model for each dataset according to the quantitative results presented in Section 5.3. We utilize this best performing model as a baseline for the qualitative comparisons presented in this section. <ref type="figure" target="#fig_0">Figure 18</ref> shows two examples for each dataset that we benchmark on. The colors for the segmented labels shown correspond to the colors and the object categories mentioned in the benchmarking tables shown in Section 5.3. <ref type="figure" target="#fig_0">Figure 18(a)</ref> and <ref type="formula">(b)</ref> show examples from the Cityscapes dataset in which the improvement over the baseline output (AdapNet) can be seen in the better differentiation between inconspicuous classes such as sidewalk and road as well as pole and sign. This can be primarily attributed to the eASPP which has a large receptive field and thus captures larger object context which helps to discern the differences between the inconspicuous classes. The improvement due to better boundary segmentation of thin object classes such as poles can be seen in the images. <ref type="figure" target="#fig_0">Figure 18(c) and (d)</ref> show examples from the Synthia dataset, where objects such as bicycles, cars and people are better segmented. The baseline output (AdapNet) shows several missing cars, people and bicycles, whereas the Ad-apNet++ output accurately captures these objects. Moreover, it can also be seen that the pole-like structures and trees are often discontinuous in the baseline output, while they are more well defined in the AdapNet++ output. In <ref type="figure" target="#fig_0">Figure 18(d)</ref>, an interesting observation is made where an entire fence is segmented in the baseline output but is absent in the scene. This is due to the fact that the intersection of the sidewalk and the road gives an appearance of a fence which is then misclassified. In the same image, it can also be observed that a small building-like structure on the right is not captured, whereas our AdapNet++ model accurately segments the structure. <ref type="figure" target="#fig_0">Figure 18</ref>(e) and (f) show examples from the indoor SUN RGB-D dataset. Examples from this dataset show significant misclassification due to inconspicuous objects. Often scenes in indoor datasets have large objects that require the network to have very large receptive fields to be able to accurately distinguish between them. <ref type="figure" target="#fig_0">Figure 18</ref>(e) shows a scene in which parts of the chair and the table are incorrectly classified as a desk in the output of the baseline model (DeepLab v3). These two classes have very similar structure and appearance which makes distinguishing between them extremely challenging. In <ref type="figure" target="#fig_0">Figure 18</ref>(f), we can see parts of the sofa incorrectly classified in the baseline model output, whereas the entire object is accurately predicted in the AdapNet++ output. In the baseline output, misclassification can also be seen for the picture on the wall which is precisely segmented in the AdapNet++ output.</p><p>In <ref type="figure" target="#fig_0">Figure 18(g)</ref> and (h), we show examples from the indoor ScanNet dataset. <ref type="figure" target="#fig_0">Figure 18(g)</ref> shows misclassification in the output of the baseline model (DeepLab v3) in the boundary where the wall meets the floor and for parts of the desk that is misclassified as other furniture. <ref type="figure" target="#fig_0">Figure 18(h)</ref> shows a significant improvement in the segmentation of AdapNet++ in comparison to the baseline model. The cabinet and counter are entirely misclassified as a desk and other furniture correspondingly in the output of the baseline model, whereas they are accurately predicted by our AdapNet++ mode.</p><p>Finally, <ref type="figure" target="#fig_0">Figure 18</ref>(i) and <ref type="formula">(j)</ref> show examples from the unstructured Freiburg Forest dataset where the improvement can largely be seen in discerning the object boundaries of classes such as grass and vegetation, as well as trail and grass. By observing these images, we can see that even for us humans it is difficult to estimate the boundaries between these classes. Our AdapNet++ architecture predicts the boundaries comparatively better than the baseline model (DeepLab v3). The improvement in the segmentation can also been seen in the finer segmentation of the vegetation and the trail path in the AdapNet++ output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Multimodal Fusion Benchmarking</head><p>In this section, we present comprehensive results on the performance of our proposed multimodal SSMA fusion architecture in comparison to state-of-the-art multimodal fusion methods, namely, LFC <ref type="bibr" target="#b53">(Valada et al, 2016b</ref><ref type="bibr">), FuseNet (Hazirbas et al, 2016</ref> and CMoDE <ref type="bibr" target="#b53">(Valada et al, 2017)</ref>. We employ the same AdapNet++ network backbone for all the fusion models including the competing methods. Therefore, we use the official implementation from the authors as a reference and append the fusion mechanism to our backbone. We also compare with baseline fusion approaches with the AdapNet++ topology as the backbone such as Late Fusion: a 1 × 1 convolution layer appended after individual modality-specific networks and the outputs are merged by adding the feature maps before the softmax, Stacking: channel-wise concatenation of modalities before input to the network, Average: averaging prediction probabilities of individual modality-specific networks followed by argmax, and Maximum: maximum of the prediction probabilities of individual modality-specific networks followed by argmax. Additionally, we also compare against the performance of the unimodal AdapNet++ architecture for each of the modalities in the dataset for reference. We denote our proposed multimodal model as SSMA and  <ref type="figure" target="#fig_0">Figure 18</ref> Qualitative segmentation results of our unimodal AdapNet++ architecture in comparison to the best performing state-of-the-art model on different datasets. In addition to the segmentation output, we also show the improvement/error map which denotes the misclassified pixels in red and the pixels that are misclassified by the best performing state-of-the-art model but correctly predicted by AdapNet++ in green. The legend for the segmented labels correspond to the colors shown in the benchmarking tables in Section 5.3. the model with left-right flips as well as multiscale testing as SSMA msf in our experiments.</p><p>In <ref type="table" target="#tab_6">Table 17</ref>, we show the results on the Cityscapes validation set considering visual images (RGB), depth and the HHA encoding of the depth as modalities for the fusion. As hypothesised, the visual RGB images perform the best among the other modalities achieving a mIoU of 80.80%. This is especially observed in outdoor scene understanding datasets containing stereo depth images that quickly degrade the information contained, with increasing distance from the camera. Among the baseline fusion approaches, Stacking achieves the highest performance for both RGB-D and RGB-HHA fusion, however, their performance is still lower than the unimodal visual RGB segmentation. This can be attributed to the fact that the baseline approaches are not able to exploit the complementary features from the modalities due to the naive fusion. CMoDE fusion with RGB-HHA achieves the highest performance among state-of-the-art approaches, surpassing the performance of unimodal segmentation. While, our proposed SSMA model for RGB-HHA fusion achieves a mIoU of 83.29% outperforming all the other approaches and setting the new state-of-the-art. The SSMA msf model further improves upon the performance of the SMMA model by 1.3%. As the Cityscapes dataset does not contain harsh environments, the improvement that can be achieved using fusion is limited to scenes that contain inconspicuous object classes or mismatched relationship. However, the additional robustness that it demonstrates due to multimodal fusion is still notable as shown in the qualitative results in the following sections. Additionally, the benchmarking results on the Cityscapes test set is shown in <ref type="table">Table 2</ref>. The results demonstrate that our SSMA fusion architecture with the AdapNet++ network backbone achieves a comparable performance as the top performing DPC and DRN architectures, while outperforming the other networks on the leaderboard. We benchmark on the Synthia dataset to demonstrate the utility of fusion when both modalities contain rich information. It consists of scenes with adverse perceptual conditions including rain, snow, fog and night, therefore the benefit of multimodal fusion for outdoor environments is most evident on this dataset. As the Synthia dataset does not provide camera calibration parameters, we cannot compute the HHA encoding, therefore we benchmark using visual RGB and depth images. Results from benchmarking on this dataset are shown in <ref type="table" target="#tab_6">Table 18</ref>. Due to the high-resolution depth information, the unimodal depth model achieves a mIoU of 87.87%, outperforming segmentation using visual RGB images by 1.17%. This demonstrates that accurate segmentation can be obtained using only depth images as input provided that the depth sensor gives accurate long range information. Among the baseline fusion approaches and the state-of-theart techniques, the CMoDE RGB-D fusion approach achieves the highest mIoU, outperforming the unimodal depth model by 1.7%. While our proposed SSMA architecture demonstrates state-of-the-art performance of 91.25% and further improves the mIoU to 92.10% using the SSMA msf model. This accounts to a large improvement of 5.4% over the best performing unimodal segmentation model. Other metrics such as the pixel accuracy and average precision also show similar improvement.</p><p>One of our main motivations to benchmark on this dataset is to evaluate our SSMA fusion model on a diverse set of scenes with adverse perpetual conditions. For this exper-  <ref type="figure">Figure 19</ref> Evaluation of our proposed SMMA fusion technique on the Synthia-Sequences dataset containing a variety of seasons and weather conditions. We use the models trained on the Synthia-Rand-Cityscapes dataset and only test on the individual conditions in the Synthia-Sequences dataset to quantify its robustness. Our model that performs RGB-D fusion consistently outperforms the unimodal models which can be more prominently seen qualitatively in <ref type="figure">Figure 21(c,d)</ref>.</p><p>iment, we trained our SSMA fusion model on the Synthia-Rand-Cityscapes training set and evaluated the performance on each of the conditions contained in the Synthia-Sequences dataset. The Synthia-Sequences dataset contains individual video sequences in different conditions such as summer, fall, winter, spring, dawn, sunset, night, rain, soft rain, fog, night rain and winter night. Results from this experiment are shown in <ref type="figure">Figure 19</ref>. The unimodal visual RGB model achieves an overall mIoU score of 49.27% ± 4.04% across the 12 sequences. Whereas, the model trained on the depth maps achieves a mIoU score of 67.07% ± 1.12%, thereby substantially outperforming the model trained using visual RGB images.</p><p>As this is a synthetic dataset, the depth maps provided are accurate and dense even for structures that are several hundreds of meters away from the camera. Therefore, this enables the unimodal depth model to learn representations that accurately encode the structure of the scene and these structural representations are proven to be invariant to the change in perceptual conditions. It can also be observed that the unimodal depth model performs consistently well in all the conditions with a variance of 1.24%, demonstrating the generalization to different conditions. However, the visual RGB model with a variance of 16.30% performs inconsistently across different conditions. Nevertheless, we observe that our RGB-D SSMA fusion model outperforms the unimodal visual RGB model by achieving a mIoU score of 76.51% ± 0.49% across the 12 conditions, accounting to an improvement of 27.24%. Moreover, the SSMA fusion model has a variance of 0.24%, demonstrating better generalization ability across varying adverse perceptual conditions.</p><p>We also benchmark on the indoor SUN RGB-D dataset which poses a different set of challenges than the outdoor datasets. The improvement from multimodal fusion is more evident here as indoor scenes are often smaller confined spaces with several cluttered objects and the depth modality provides valuable structural information that can be exploited. Results from RGB-D and RGB-HHA multimodal fusion is shown in <ref type="table" target="#tab_6">Table 19</ref>. Among the individual modalities, segmentation using visual RGB images yields the highest mIoU of 38.40%. The model trained on depth images performs 4.13% lower than the visual RGB model. This can be attributed to the fact that the depth images are extremely noisy with numerous missing depth values in the SUN RGB-D dataset. However, our proposed SSMA fusion on RGB-HHA achieves state-of-the-art performance with a mIoU of 44.43%, constituting a substantial improvement of 6.03% over the unimodal visual RGB model. Moreover, our SSMA msf model further improves upon the mIoU by 1.3%. Similar to the performance in other datasets, RGB-HHA achieves a higher mIoU than RGB-D fusion corroborating the fact that CNNs learn more effectively from the HHA encoding but with a small additional preprocessing time. <ref type="table" target="#tab_23">Table 20</ref> shows the results on the ScanNet validation set. ScanNet is the largest indoor RGB-D dataset to date with over 1513 different scenes and 2.5M views. Unlike the SUN RGB-D dataset, ScanNet contains depth maps of better quality with lesser number of missing depth values. The unimodal visual RGB model achieves an mIoU of 52.92% with a pixel accuracy of 77.70%, while the unimodal HHA model achieves an mIoU of 54.19% with an accuracy of 80.20%. For multimodal fusion, CMoDE using RGB-HAA demonstrates the highest performance among state-of-the- art architectures achieving a mIoU of 64.07%. While our proposed SSMA RGB-HHA model outperforms CMoDE by yielding a mIoU of 66.34%, which is a significant improvement of 13.42% over the unimodal visual RGB model. Moreover, the SSMA msf model further improves the mIoU score to 67.52%. To the best of our knowledge, this is the largest improvement due to multimodal fusion obtained thus far. An interesting observation that can be made from the results on SUN RGB-D and ScanNet is that the lowest multimodal fusion performance is obtained using the Stacking fusion approach, reaffirming our hypothesis that fusing more semantically mature features enables the model to exploit complementary properties from the modalities more effectively. We also benchmark on the ScanNet test set and report the results in <ref type="table">Table 6</ref>. Our proposed SSMA fusion architecture with the AdapNet++ network backbone sets the new state-of-the-art on the ScanNet benchmark.</p><p>Finally, we present benchmarking results on the Freiburg Forest dataset that contains three inherently different modalities including visual RGB images, Depth data and EVI. EVI or Enhanced Vegetation Index was designed to enhance the vegetation signal in high biomass regions and it is computed from the information contained in three bands, namely, Near-InfraRed, Red and Blue channels <ref type="bibr">(Running et al, 1999)</ref>. As this dataset contains scenes in unstructured forested environments, EVI provides valuable information to discern between inconspicuous classes such as vegetation and grass.  <ref type="table" target="#tab_6">Table 21</ref> shows the results on this dataset for multimodal fusion of RGB-D and RGB-EVI. For unimodal segmentation, the RGB model yields the highest performance, closely followed by the model trained on EVI. While for multimodal segmentation, our SSMA model trained on RGB-EVI yields the highest mIoU of 83.90% and our SMMA msf model further improves upon the performance and achieves a mIoU of 84.18%. Both these models outperform existing multimodal fusion methods and set the new state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Multimodal Fusion Discussion</head><p>To summarize, the models trained on visual RGB images perform the best in general in comparison to unimodal segmentation with other modalities. However, when the depth data is less noisy and the environment is a confined indoor space, the model trained on depth or HHA encoded depth outperforms visual RGB models. Among the multimodal fusion baselines, late fusion and Stacking, each perform well in different environments. Stacking performs better in outdoor environments, while late fusion performs better indoors. This can be attributed to the fact that the late fusion method fuses semantically mature representations. Therefore, in indoor environments, modalities such as depth maps from stereo cameras are less noisy than in outdoors and as the environment is confined, all the objects in the scene are well represented with dense depth values. This enables the late fusion architecture to leverage semantically rich representations for fusion. However in outdoor environments, depth values are very noisy and no information is present for objects at far away distances. Therefore, the semantic representations from the depth stream are considerably less informative for certain parts of the scene which does not allow the Late Fusion network to fully exploit complementary features and hence it does not provide significant gains. On the other hand, in indoor or synthetic scenes where the depth modality is dense and rich with information, late fusion generally outperforms the stacking approach. Among the current state-of-the-art methods, CMoDE outperforms all other approaches in most of the diverse environments. To recapitulate, CMoDE employs a class-wise probabilistic late fusion technique that adaptively weighs the modalities based on the scene condition. However, our proposed SSMA fusion techniques outperforms CMoDE in all the datasets and sets the new state-of-the-art in multimodal semantic segmentation. This demonstrates that fusion of modalities is an inherently complex problem that depends on several factors such as the object class of interest, the spatial location of the object and the environmental scene context. Our proposed SSMA fusion approach dynamically adapts the fusion of semantically mature representations based on the aforementioned factors, thereby enabling our model to effectively exploit complementary properties from the modalities. Moreover, as the dynamicity is learned in a self-supervised fashion, it efficiently generalizes to different diverse environments, perceptual conditions and the types of modalities employed for fusion. Another advantage of this dynamic adaptation property of our multimodal SSMA fusion mechanism is the intrinsic tolerance to sensor failure. In case one of the modalities becomes unavailable, the SSMA module can be trained to switch the output of the unavailable modality-specific encoder off by generating gating probabilities as zeros. This enables the multimodal model to still yield a valid segmentation output using only the modality that is available and the performance is comparable to that of the unimodal model with the remaining modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Generalization of SSMA fusion to Other Tasks</head><p>In order to demonstrate the generalization of our proposed SSMA module for multimodal fusion in other tasks, we report results for the scene type classification task on the Scan-Net benchmark. The goal of the scene type classification task is to classify scans of indoor scenes into 13 distinct categories, namely, apartment, bathroom, bedroom/hotel, bookstore/library, conference room, copy/mail room, hallway, kitchen, laundry room, living room/lounge, misc, office, storage/basement/garage. The benchmark ranks the methods according to recall and the intersection-over-union metric (IoU). For our approach, we employ the top-down 2D projection of the textured scans as one modality and the jet-colorized depth map of the top-down 2D projection of the scans as another modality. We utilize the SE-ResNetXt-101 <ref type="bibr" target="#b25">(Hu et al, 2018)</ref> architecture for the unimodal model and for the multimodal network backbone. Our multimodal architecture has a late fusion topology with two individual modality-specific SE-ResNetXt-101 streams that are fused after block 5 using our SSMA module. The output of the SSMA module is fed to a fully connected layer that has number of output units equal to the number of scene classes in the dataset. We evaluate the performance of our multimodal SSMA fusion model against the individual modality-specific networks, as well as the multimodal fusion baselines such as Average, Maximum, Stacking and Late Fusion, as described in Section 5.7.</p><p>Results from this experiment on the ScanNet validation set are shown in <ref type="table" target="#tab_25">Table 22</ref>. It can be seen that the unimodal depth model outperforms the RGB model in both the mean IoU (mIoU) score and the mean recall (mRecall). Among the multimodal fusion baselines, only the Late Fusion network outperforms the unimodal depth model by a small margin in the mIoU score but it achieves a lower mean recall. However, our multimodal SSMA fusion model achieves the state-ofthe-art performance with a mIoU score of 37.45% and a mean recall of 54.28%. This accounts for an improvement of 2.28% in the mIoU score and 9.8% in the mean recall over the Late Fusion model, and a larger improvement over the performance of the unimodal depth model. Since the only difference in the Late Fusion architecture and the SSMA architecture is how the multimodal fusion is carried out, the improvement achieved by the SSMA model can be solely attributed to the dynamic fusion mechanism of our SSMA module. We also benchmarked on the ScanNet test set for scene classification, in which our multimodal SSMA model achieves a mIoU score of 35.5% and a mean recall of 49.8%, thereby setting the state-of-the-art for scene type classification on this benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Multimodal Fusion Ablation Studies</head><p>In this section, we study the influence of various contributions that we make for multimodal fusion. Specifically, we evaluate the performance by comparing the fusion at different intermediate network stages. We then evaluate the utility of our proposed channel attention scheme for better correlation of mid-level encoder and high-level decoder features. Subsequently, we experiment with different SSMA bottleneck downsampling rates and qualitatively analyze the convolution activation maps of our fusion model at various intermediate network stages to study the effect of multimodal fusion on the learned network representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10.1">Experiments with Real-Time Backbone Networks</head><p>In the interest of real-time performance, we additionally trained multimodal models in our proposed SSMA fusion framework with different real-time-intended backbone networks. Specifically, we performed experiments using two different backbone networks: ERFnet <ref type="bibr">(Romera et al, 2018)</ref> and MobileNet v2 <ref type="bibr" target="#b48">(Sandler et al, 2018)</ref>. For the ERFnet fusion model, we replace the two modality-specific encoders in our multimodal fusion configuration with the ERFnet encoder and we replace our decoder with the ERFnet decoder.</p><p>While for the fusion model with the MobileNet v2 backbone, we employ the MobileNet v2 topology for the two modalityspecific encoders in our multimodal fusion configuration and we append our eASPP as well as the decoder from our AdapNet++ architecture. Note that ERFnet is a semantic segmentation architecture with both an encoder and decoder, while MobileNet v2 is only a classification architecture and therefore it only has an encoder topology. Results from this experiment for RGB-HHA fusion along with the unimodal RGB and unimodal HHA performance are shown in <ref type="table" target="#tab_7">Table 23</ref>. The ERFnet model in our multimodal SSMA fusion configuration achieves a mIoU score of 64.60% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10.2">Detailed Study on the Fusion Architecture</head><p>In our proposed multimodal fusion architecture, we employ a combination of mid-level fusion and late-fusion. Results from fusion at each of these stages is shown in <ref type="table" target="#tab_8">Table 24</ref>. First, we employ the main SSMA fusion module at the end of the two modality-specific encoders, after the eASPPs and we denote this model as F1. The F1 model achieves a mIoU of 81.55%, which constitutes to an improvement of 0.78% over the unimodal F0 model. We then employ an SSMA module at each skip refinement stage to fuse the mid-level skip features from each modality-specific stream. The fused skip features are then integrated into the decoder for refinement of high-level decoder features. The F2 model that performs multimodal fusion at both stages, yields a mIoU of 81.75%, which is not significant compared to the improvement that we achieve in fusion of the mid-level features into the decoder in our unimodal AdapNet++ architecture. As described in Section 4.2, we hypothesise that this is due to the fact that the mid-level representations learned by a network do not align across different modality-specific streams. Therefore, we employ our channel attention mechanism to better correlate these features using the spatially aggregated statistics of the high-level decoder features. The model that incorporates this proposed attention mechanism achieves an improved mIoU of 82.64%, which is an improvement of 1.09% in comparison to 0.2% without the channel attention mechanism. Note that the increase in quantitative performance due to multimodal fusion is more apparent in the indoor or synthetic datasets as shown in Section 5.7. The experiment shown in <ref type="table" target="#tab_8">Table 24</ref> shows a correspondingly larger increase for the contributions presented in this table. However, as we present the unimodal ablation studies on the Cityscapes dataset, we continue to show the multimodal ablation studies on the same dataset. The proposed SSMA module has a bottleneck structure in which the middle convolution layer downsamples the number of feature channels according to a rate η as described in Section 4.1. As we perform fusion both at the mid-level and at the end of the encoder section, we have to estimate the downsampling rates individually for each of the SSMA blocks. We start by using values from a geometric sequence for the main encoder SSMA downsampling rate η enc and correspondingly vary the values for the skip SSMA downsampling rates η skip . Results from this experiment shown in <ref type="table" target="#tab_9">Table 25</ref> demonstrates that the best performance is obtained for η enc = 16 and η skip = 6 which also increases the parameter efficiency compared to lower downsampling rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10.3">Fusion Stage and Reliance on Modalities</head><p>In this section, we study the SSMA fusion configuration in terms of learning dependent or independent probability weightings that are used to recalibrate the modality-specific feature maps dynamically. The SSMA configuration that we depict in <ref type="figure">Figure 7</ref> learns independent probability weighting where the activations at a specific location in the feature maps from modality A can be independently enhanced or suppressed regardless of whether the activations at the corresponding location in the feature maps from modality B are going to be enhanced or suppressed. An alternative dependent configuration can be employed by replacing the sigmoid with a softmax, where the softmax takes the activation at a specific location from a feature map from modality A and the activation in the corresponding location from the feature map from modality B, and outputs dependent probabilities that are used to weigh the modality-specific activations. This dependent configuration acts as punishing the modality that makes the mistake while rewarding the other. While the independent configuration also considers if a modality is making a mistake, it does not necessarily punish one and reward the other, it has the ability to punish both or reward both in addition. We study the performance of the multimodal fusion in these two dependent and independent configurations in this section. Additionally, we study the effect of learning the fusion in a fully supervised manner by employing an explicit loss function at the output of each SSMA module after the fusion. We also study the overall configuration on where the SSMA module is to be placed, at the end of the encoder stage where the features are highly discriminative or at the end of the decoder stage where the features are more high-level and semantically mature. <ref type="table" target="#tab_29">Table 26</ref> shows the results from this experiment where we present the multimodal fusion performance for the model in the dependent SSMA and independent SSMA configuration, as well as when the SSMA module is placed at the encoder stages as in our standard configuration or at the decoder stage, and with and without an explicit supervision for the fusion. The results are presented for RGB-HHA fusion on the Cityscapes validation set. It can be seen that the models S1 to S4 without the explicit supervision outperform the corresponding models S5 to S8 with the explicit supervision demonstrating that learning the fusion in self-supervised manner is more beneficial. Comparing the performance of models that employ the SSMA fusion at the encoder stages S1, S2, S5, S6, with the corresponding models at employ the SSMA fusion at the decoder stage S3, S4, S7, S8, we see that the encoder fusion models substantially outperform the decoder fusion models. Finally, comparing the models with the dependent SSMA configuration S1, S3, S5, S7 with the corresponding models the the independent SSMA configuration, we observe that independent configuration always outperforms the dependent configuration. In summary, these results demonstrate that our multimodal SSMA fusion scheme that learns independent probability weightings in a self-supervised manner, when employed at the encoder stages outperforms the other configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10.4">Influence of Multimodal Fusion on Activation Maps</head><p>In an effort to present visual explanations for the improvement in performance due to multimodal fusion, we study the activation maps at various intermediate network stages before and after the multimodal fusion using the GradCam++ technique <ref type="bibr">(Chattopadhyay et al, 2017)</ref>. The approach introduces pixel-wise weighting of the gradients of the output with respect to a particular spatial location in the convolutional feature map to generate a score. The score provides a measure of the importance of each location in feature map towards the overall prediction of the network. We apply a colormap over the obtained scores to generate a heat map as shown in <ref type="figure">Figure 20</ref>. We visualize the activation maps at five different stages of the network. Firstly, at the output of each modality-specific encoder X a and X b which is the input to the SSMA fusion block. Secondly, after recalibrating the individual modality-specific feature maps inside the SSMA blockX a andX b , and finally after the fusion with the 3 × 3 convolution inside the SSMA block f . <ref type="figure">Figure 20</ref> illustrates one example for each dataset that we benchmark on with the activation maps, the input modalities and the corresponding segmentation output for the particular object category.</p><p>For the Cityscapes dataset, we show the activation maps for the person category. It can be seen that the activation map X a from the visual RGB stream is well defined for the person class but it does not show high activations centered on the objects, whereas the activation map from the depth stream X b is more noisy but high activations are shown on the objects. For the locations in the input depth map that show noisy depth data, the activation map correspondingly shows prominent activations in these regions. After the recalibration of the feature maps, bothX a andX b are less noisy and maintaining the structure with high activations. Furthermore, the activation map of the fused convolution f shows very well defined high activations that almost correspond to the segmented output.</p><p>The second column in <ref type="figure">Figure 20</ref> shows the activations for the pole class in the Synthia dataset. As the scene was captured during rainfall, the objects in the visual RGB image are indistinguishable. However, the depth map still maintains some structure of the scene. Studying both the modalityspecific activation maps at the input to the SSMA module shows substantial amount of noisy activations spread over the scene. Therefore, the unimodal visual RGB model only achieves an IoU of 74.94% for the pole class. Whereas, after the recalibration of the feature maps, the activation maps show significantly reduced noise. It can be seen the recalibrated activation mapX b of the depth stream shows more defined high activations on the pole, whereasX a of the visual RGB stream shows less amount of activations indicating that the network suppresses the noisy RGB activations in order to better leverage the well defined features from the depth stream. Activations of the final fused convolution layer show higher activations on the pole than either of the recalibrated activation maps demonstrating the utility of multimodal fusion. This enables the fusion model to achieve an improvement of 8.4% for the pole class.</p><p>The third column of <ref type="figure">Figure 20</ref> shows the activation maps for the table class in the SUN RGB-D dataset. Interestingly, both the modality-specific activation maps at the input show high activations at different locations indicating the complementary nature of the features in this particular scene. However, the activation map X b from the HHA stream also shows high activations on the couch in the background which would cause misclassifications. After the recalibration of the HHA feature maps, the activation mapX b no longer has high activations on the couch but it retains the high activations on the table. While, the recalibrated activation mapX a of the visual RGB stream shows significantly lesser noisy activations. The activation map of the fused convolution f shows well defined high activations on the table, more than the modality-specific input activation maps. The enables the SSMA fusion model to achieve an improvement of 4.32% in the IoU for the table category.</p><p>For the ScanNet dataset, we show the activation maps for the bathtub category in the fourth column in <ref type="figure">Figure 20</ref>. It can be seen that the modality specific activation maps at the input of the SSMA module shows high activations at complementary locations, corroborating the utility of exploiting features from both modalities. Moreover, the activation map X b from the HHA stream shows significantly higher activations on the object of interest than the RGB stream. This also aligns with the quantitative results, where the unimodal HHA model outperforms the model trained on visual RGB images. After the recalibration of the feature maps inside the SSMA block, the activation maps show considerably lesser noise while maintaining the high activations at complementary locations. The activation map of the fused convolution f shows only high activations on the bathtub and resembles the actual structure of the segmented output.</p><p>The last column of <ref type="figure">Figure 20</ref> shows the activation maps for the trail category in the Freiburg Forest dataset. Here we show the fusion with visual RGB and EVI. The EVI modality does not provide substantial complementary information for the trail class in comparison to the RGB images. This is also evident in the visualization of the activations at the input of the SSMA module. The activation maps of the EVI modality after the recalibration show significantly lesser noise but also  <ref type="figure">Figure 20</ref> Visualization of the activation maps with respect to a particular class at various stages of the network before and after multimodal fusion. X a and X b are at the outputs of the modality-specific encoder which is input to the SSMA fusion module,X a andX b are the at the feature maps after recalibration inside the SSMA block, and f is after the fusion of both modalities inside the SSMA block. Both the input modalities and the corresponding segmentation output for the particular object category is also shown.</p><p>lesser amount of high activation regions than the recalibrated activation maps of the visual RGB stream. Nevertheless, the activation map after the fusion f shows more defined structure of the trail than either of the modality-specific activation maps of the input to the SSMA module. <ref type="figure">Figure 21</ref> illustrates the visualized comparisons of multimodal semantic segmentation for each of the five benchmark datasets. We compare with the output of unimodal AdapNet++ architecture and show the improvement\error map which denotes the improvement over the unimodal Ad-apNet++ output in green and the misclassifications in red. <ref type="figure">Figure 21</ref>(a) and (b) show interesting examples from the Cityscapes dataset. In both these examples, we can see a significant improvement in the segmentation of cyclists. As cyclists constitute to a person riding a bike, often models assign a part of the pixels on the person riding a bike as the person class, instead of the cyclist class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11">Qualitative Results of Multimodal Segmentation</head><p>Another common scenario is when there is a person standing a few meters behind a parked bike, the model misclas-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Modality1</head><p>Input <ref type="formula" target="#formula_1">Modality2</ref>  Qualitative multimodal fusion results in comparison to the output of the unimodal visual RGB model on each of the five datasets that we benchmark on. The last two rows show failure modes. In addition to the segmentation output, we also show the improvement/error map which denotes the misclassified pixels in red and the pixels that are misclassified by the best performing state-of-the-art model but correctly predicted by AdapNet++ model in green. The legend for the segmented labels correspond to the colors shown in the benchmarking tables in Section 5.3. sifies the person as a cyclist but since he is not on the bike, the right classification would be the person category. In these examples, we can see that by leveraging the features from the depth modality our network makes accurate predictions in these situations. In <ref type="figure">Figure 21(a)</ref>, we can also see that parts of the car at several meters away is not completely segmented in the unimodal output but it is accurately captured in the multimodal output. Furthermore, in the unimodal output of <ref type="figure">Figure 21(b)</ref>, we see parts of the sidewalk behind the people is misclassified as road and parts of the fence that is several meters away is misclassified as a sidewalk. As the distinction between these object categories can clearly be seen in the depth images, our fusion model accurately identifies these boundaries. <ref type="figure">Figure 21(c) and (d)</ref> show examples on the Synthia dataset. Here we show the first scene during rainfall and the second scene during night-time. In the unimodal output of first scene, we can see significant misclassifications in all the object categories, except building and vegetation that are substantially large in size. Whereas, the multimodal fusion model is able to leverage the depth features to identify the objects in the scene. In <ref type="figure">Figure 21(d)</ref>, even for us humans it is impossible to see the people on the road due to the darkness in the scene. As predicted, the unimodal visual RGB model misclassifies the entire road with people as a car, which could lead to disastrous situations if it occurred in the real-world. The multimodal model is able to accurately predict the scene with almost no error in the predictions.</p><p>In <ref type="figure">Figure 21</ref>(e) and (f), we show examples on the indoor SUN RGB-D dataset. Due to the large number of object categories in this dataset, several inconspicuous classes exist. Leveraging structural properties of objects from the HHA encoded depth can enable better discrimination between them. <ref type="figure">Figure 21</ref>(e) shows a scene where the unimodal model misclassifies parts of the wooden bed as a chair and parts of the pillow as the bed. We can see that the multimodal output significantly improves upon the unimodal counterpart. <ref type="figure">Figure 21</ref>(f) shows a complex indoor scene with substantial clutter. The unimodal model misclassifies the table as a desk and a hatch in the wall is misclassified as a door. Moreover, partly occluded chairs are not entirely segmented in the unimodal output. The HHA encoded depth shows well defined structure of these objects, which enables the fusion approach to precisely segment the scene. Note that the window in the top left corner of <ref type="figure">Figure 21</ref>(f) is mislabeled as a desk in the groundtruth.  <ref type="figure">Figure 21(g)</ref>, overexposure of the image near the windows causes misclassification of parts of the window as a picture and crumpled bedsheets as well as the bookshelf is missclassified as a desk. While the multimodal segmentation output does not demonstrate these errors. <ref type="figure">Figure 21(h)</ref> shows an image with motion-blur due to camera motion. The motion blur causes a significant percentage of the image to be misclassified as the largest object in the scene and this case as a bookshelf. Analyzing the HHA encoded depth map, we can see that it does not contain overexposed sections or motionblur, rather it strongly emphasizes the structure of the objects in the scene. By leveraging features from the HHA encoded depth stream, our multimodal model is able to accurately predict the object classes in the presence of these perceptual disturbances.</p><p>In <ref type="figure">Figure 21</ref>(i) and (j), we show results on the unstructured Freiburg Forest dataset. <ref type="figure">Figure 21(i)</ref> shows an oversaturated image due to sunlight which causes boulders on the grass to be completely absent in the unimodal segmentation output. Oversaturation causes boulders to appear with a similar texture as the trail or vegetation class. However, the RGB-EVI multimodal model is able to leverage the complementary EVI features to segment these structures. <ref type="bibr">Figure 21(j)</ref> shows an example scene with glare on the camera optics and snow on the ground. In the unimodal semantic segmentation output, the presence of these disturbances often causes localized misclassifications in the areas where they are present. Whereas, the multimodal semantic segmentation model compensates for these disturbances exploiting the complementary modalities.</p><p>The final two rows in <ref type="figure">Figure 21</ref> show interesting failure modes where the multimodal fusion model demonstrates incorrect predictions. In <ref type="figure">Figure 21</ref>(k), we show an example from the Cityscapes dataset which contains an extremely thin fence connected by wires along the median of the road. The thin wires are not captured by the depth modality and it is visually infeasible to detect it from the RGB image. Moreover due its thin structure, the vehicles on the opposite lane are clearly visible. This causes both the unimodal and multimodal models to partially segment the vehicles behind the fence which causes incorrect predictions according to the groundtruth. However, we can see that the multimodal model still captures more of the fence structure than the unimodal model. In <ref type="figure">Figure 21</ref>(l), we show an example from the SUN RGB-D dataset in which misclassifications are produced due to inconspicuous classes. The scene contains two object categories that have very similar appearance in some scenes, namely, chair and sofa. The chair class is denoted in dark green, while the sofa class is denoted in light green. As we see in this scene, a single-person sofa is considered to be a chair according to the groundtruth, whereas only the longer sofa in the middle is considered to be in the sofa class. In this scene, the single person sofa is adjacent to the longer sofa which causes the network to predict the pixels on both of them as the sofa class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Modality1</head><p>Input <ref type="formula" target="#formula_1">Modality2</ref>  In addition to the segmentation output, we also show the improvement/error map which denotes the misclassified pixels in red and the pixels that are misclassified by the best performing state-of-the-art model but correctly predicted by AdapNet++ in green. The legend for the segmented labels correspond to the colors shown in the benchmarking tables in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.12">Visualizations Across Seasons and Weather Conditions</head><p>In this section, we present qualitative results on the Synthia-Sequences dataset that contains video sequences of 12 different seasons and weather conditions. We visualize the segmentation output of the multimodal and unimodal models for which the qualitative results are shown in <ref type="table" target="#tab_6">Table 19</ref>. For this experiment, the models were trained on the Synthia-Rand-Cityscapes dataset and only evaluated on the Synthia-Sequences dataset. The Synthia-Sequences dataset contains a diverse set of conditions such as summer, fall, winter, spring, dawn, sunset, night, rain, soft rain, fog, night rain and winter night. We show qualitative evaluations on each of these conditions by comparing the multimodal segmentation performance with the output obtained from the unimodal visual RGB model in <ref type="figure" target="#fig_11">Figure 22</ref>. This aim of this experiment is twofold: to study the robustness of the model to adverse perceptual conditions such as rain, snow, fog and nightime; Secondly, to evaluate the generalization of the model to unseen scenarios.</p><p>From the examples shown in <ref type="figure" target="#fig_11">Figure 22</ref>, we can see the diverse nature of the scenes containing environments such as highway driving, inner-city with skyscrapers and small sized cities. The visual RGB images in all of the scenes show the changing weather conditions that cause vegetation to change color in <ref type="figure" target="#fig_11">Figure 22(b)</ref>, snow on the ground and leaf-less trees in <ref type="figure" target="#fig_11">Figure 22</ref>(c), glaring light due to sunrise in <ref type="figure" target="#fig_11">Figure 22</ref>(c), orange hue due to sunset in <ref type="figure" target="#fig_11">Figure 22</ref>(f), dark scene with isolated lights in <ref type="figure" target="#fig_11">Figure 22</ref>(g), noisy visibility due to rain in <ref type="figure" target="#fig_11">Figure 22</ref>(h) and blurred visibility due to fog in <ref type="figure" target="#fig_11">Figure 22</ref>(j). Even for humans it is extremely hard to identify objects in some of these environments. The third column shows the output obtained from the unimodal visual RGB model. The output shows significant misclassifications in scenes that contain rain, fog, snow or nighttime, whereas the multimodal RGB-D model precisely segments the scene by leveraging the more stable depth features. The improvement map in green shown in the last column of <ref type="figure" target="#fig_11">Figure 22</ref>, demonstrates substantial improvement over unimodal segmentation and minimal error for multimodal segmentation. The error is noticeable only along the boundaries of objects that are far away, which can be remedied using a higher resolution input image. <ref type="figure" target="#fig_11">Figure 22</ref>(e) and (j) show partial failure cases. In the first example in <ref type="figure" target="#fig_11">Figure 22</ref>(e), the occluded bus on the left is misclassified as a fence due to its location beyond the sidewalk, where often fences appear in the same configuration. While, in <ref type="figure" target="#fig_11">Figure 22</ref>(j), a segment of the vegetation several meters away is misclassified as a part of the building behind. However, overall the multimodal network is able to generalize to unseen environments and visibility conditions demonstrating the efficacy of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed an architecture for multimodal semantic segmentation that incorporates our self-supervised model adaptation blocks which dynamically adapt the fusion of features from modality-specific streams at various intermediate network stages in order to optimally exploit complementary features. Our fusion mechanism is simultaneously sensitive to critical factors that influence the fusion, including the object category, its spatial location and the environmental scene context in order to fuse only the relevant complementary information. We also introduced a channel attention mechanism for better correlating the fused mid-level modalityspecific encoder features with the high-level decoder features for object boundary refinement. Moreover, as the fusion mechanism is self-supervised, we demonstrated that it effectively generalizes to the fusion of different modalities, beyond the commonly employed RGB-D data and across different environments ranging from urban driving scenarios to indoor scenes and unstructured forested environments.</p><p>In addition, we presented a computationally efficient unimodal semantic segmentation architecture that consists of an encoder with our multiscale residual units and an efficient atrous spatial pyramid module, complemented by a strong decoder with skip refinement stages. Our proposed multiscale residual units outperform the commonly employed multigrid method and our proposed efficient atrous spatial pyramid pooling achieves a 10× reduction in the number of parameters with a simultaneous increase in performance compared to the standard atrous spatial pyramid pooling. Additionally, we proposed a holistic network-wide pruning approach to further compress our model to enable efficient deployment. We presented exhaustive theoretical analysis, visualizations, quantitative and qualitative results on Cityscapes, Synthia, SUN RGB-D, ScanNet and Freiburg Forest datasets. The results demonstrate that our unimodal AdapNet++ architecture achieves state-of-the-art performance on Synthia, ScanNet and Freiburg Forest benchmarks while demonstrating comparable performance on Cityscapes and SUN RGB-D benchmarks with a significantly lesser number of parameters and a substantially faster inference time in comparison to other state-of-the-art models. More importantly, our multimodal semantic segmentation architecture sets the new state-of-the-art on all the aforementioned benchmarks, while demonstrating exceptional robustness in adverse perceptual conditions. Zhao H, Shi J, Qi X, Wang X, Jia J <ref type="formula" target="#formula_0">(2017)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 8</head><label>8</label><figDesc>Topology of our Adapnet++ encoder for multimodal fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 10</head><label>10</label><figDesc>Example image from the Cityscapes dataset showing a complex urban scene with many dynamic objects and the corresponding depth map representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) and Figure 10(c) show an example (a) RGB (b) Depth (c) HHA Figure 11 Example image from the Synthia dataset showing an outdoor urban scene and the corresponding depth map representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 12</head><label>12</label><figDesc>Example image from the SUN RGB-D dataset showing an indoor scene and the corresponding depth map representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 13 Figure 14</head><label>1314</label><figDesc>Example image from the Scannet dataset showing a complex indoor scene, the corresponding depth map representations and the ground truth semantic segmentation mask. Example image from the Freiburg Forest dataset showing the various spectra and modalities: Near-InfraRed (NIR), Normalized Difference Vegetation Index (NDVI), Near-InfraRed-Red-Green (NRG), Enhanced Vegetation Index (EVI) and Depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 15</head><label>15</label><figDesc>Evaluation of network compression approaches shown as the percentage of reduction in the number of parameters with the corresponding decrease in the mIoU score for various baseline approaches versus our proposed technique. The results are shown for the AdapNet++ model trained on the Cityscapes dataset and evaluated on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 16</head><label>16</label><figDesc>Comparison of the receptive field of ASPP and our proposed eASPP. The receptive field is visualized for the annotated yellow dot. Our proposed eASPP has larger receptive field size and denser pixel sampling in comparison to the ASPP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 21 Qualitative multimodal fusion results in comparison to the output of the unimodal visual RGB model on each of the five datasets that we benchmark on. The last two rows show failure modes. In addition to the segmentation output, we also show the improvement/error map which denotes the misclassified pixels in red and the pixels that are misclassified by the best performing state-of-the-art model but correctly predicted by AdapNet++ model in green. The legend for the segmented labels correspond to the colors shown in the benchmarking tables in Section 5.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 21(g) and (h) show examples of indoor scenes from the ScanNet dataset. In the unimodal output of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 22</head><label>22</label><figDesc>Qualitative multimodal semantic segmentation results in comparison the model trained on visual RGB images on the Synthia-Seasons dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>model as it offers a good trade-off between learning highly discriminative deep features and the computational complexity required. In order to effectively compute high resolution feature responses at different spatial</figDesc><table><row><cell>Input</cell><cell>Pre-activation ResNet</cell><cell>eASPP</cell><cell>Deep Decoder with</cell><cell>Segmented</cell></row><row><cell>Image</cell><cell>with Multiscale Blocks</cell><cell></cell><cell>Skip Connections</cell><cell>Prediction</cell></row><row><cell></cell><cell></cell><cell>256x 24x48 256x 24x48 256x 256x 24x48 24x48</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Skip1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Skip2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1</head><label>1</label><figDesc>Performance comparison of AdapNet++ with baseline models on the Cityscapes validation set with 11 semantic class labels (input image dim: 768 × 384). Note that no left-right flips or multiscale testing is performed. Note: Corresponding multimodal results are reported inTable 17. Benchmarking results on the Cityscapes dataset with full resolution evaluation on 19 semantic class labels. Only the eight top performing published models in the leaderboard are listed in this table. The inference time is reported for an input image resolution of 768 × 384 pixels and it was computed on an NVIDIA TITAN X (PASCAL) GPU using the official implementation of each method.</figDesc><table><row><cell>Network</cell><cell>Sky</cell><cell cols="5">Building Road Sidewalk Fence Vegetation Pole</cell><cell>Car</cell><cell>Sign</cell><cell>Person Cyclist</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(%)</cell></row><row><cell>FCN-8s</cell><cell cols="2">76.51 83.97</cell><cell>93.82 67.67</cell><cell cols="2">24.91 86.38</cell><cell cols="2">31.71 84.80 50.92 59.89 59.11</cell><cell>59.97</cell></row><row><cell>SegNet</cell><cell cols="2">73.74 79.29</cell><cell>92.70 59.88</cell><cell cols="2">13.63 81.89</cell><cell cols="2">26.18 78.83 31.44 45.03 43.46</cell><cell>52.17</cell></row><row><cell>FastNet</cell><cell cols="2">77.69 86.25</cell><cell>94.97 72.99</cell><cell cols="2">31.02 88.06</cell><cell cols="2">38.34 88.42 52.34 61.76 61.83</cell><cell>68.52</cell></row><row><cell>ParseNet</cell><cell cols="2">77.57 86.81</cell><cell>95.27 74.02</cell><cell cols="2">33.31 87.37</cell><cell cols="2">38.24 88.99 53.34 63.25 63.87</cell><cell>69.28</cell></row><row><cell>DeconvNet</cell><cell cols="2">89.38 83.08</cell><cell>95.26 68.07</cell><cell cols="2">27.58 85.80</cell><cell cols="2">34.20 85.01 27.62 45.11 41.11</cell><cell>62.02</cell></row><row><cell>DeepLab v2</cell><cell cols="2">74.28 81.66</cell><cell>90.86 63.3</cell><cell cols="2">26.29 84.33</cell><cell cols="2">27.96 86.24 44.79 58.89 60.92</cell><cell>63.59</cell></row><row><cell>AdapNet</cell><cell cols="2">92.45 89.98</cell><cell>97.43 81.43</cell><cell cols="2">49.93 91.44</cell><cell cols="2">53.43 92.23 65.32 69.86 69.62</cell><cell>77.56</cell></row><row><cell>DeepLab v3</cell><cell cols="2">92.82 89.02</cell><cell>96.74 78.13</cell><cell cols="2">41.00 90.81</cell><cell cols="2">49.74 91.02 64.48 66.52 66.98</cell><cell>75.21</cell></row><row><cell>AdapNet++ (ours)</cell><cell cols="2">94.18 91.49</cell><cell>97.93 84.40</cell><cell cols="2">54.98 92.09</cell><cell cols="2">58.85 93.86 72.61 75.52 72.90</cell><cell>80.80</cell></row><row><cell cols="2">Table 2 Network</cell><cell></cell><cell>Backbone</cell><cell></cell><cell cols="2">mIoU (%)</cell><cell>Parms.</cell><cell>Time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>val</cell><cell>test</cell><cell>(M)</cell><cell>(ms)</cell></row><row><cell></cell><cell>PSPNet</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell>80.91</cell><cell>81.19</cell><cell>56.27</cell><cell>172.42</cell></row><row><cell></cell><cell cols="2">DeepLab v3</cell><cell>ResNet-101</cell><cell></cell><cell>79.30</cell><cell>81.34</cell><cell>58.16</cell><cell>79.90</cell></row><row><cell></cell><cell>Mapillary</cell><cell></cell><cell>WideResNet-38</cell><cell></cell><cell>78.31</cell><cell>82.03</cell><cell>135.86</cell><cell>214.46</cell></row><row><cell></cell><cell cols="2">DeepLab v3+</cell><cell cols="2">Modified Xception</cell><cell>79.55</cell><cell>82.14</cell><cell>43.48</cell><cell>127.97</cell></row><row><cell></cell><cell>DPC</cell><cell></cell><cell cols="2">Modified Xception</cell><cell>80.85</cell><cell>82.66</cell><cell>41.82</cell><cell>144.41</cell></row><row><cell></cell><cell>DRN</cell><cell></cell><cell>WideResNet-38</cell><cell></cell><cell>79.69</cell><cell>82.82</cell><cell>129.16</cell><cell>1259.67</cell></row><row><cell></cell><cell cols="2">AdapNet++ (ours)</cell><cell>ResNet50</cell><cell></cell><cell>81.24</cell><cell>81.34</cell><cell>30.20</cell><cell>72.92</cell></row><row><cell></cell><cell cols="2">SSMA (ours)</cell><cell>ResNet50</cell><cell></cell><cell>82.19</cell><cell>82.31</cell><cell>56.44</cell><cell>101.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc>Zhuang et al, 2018). The results of the competing methods reported in this table are directly taken from the benchmark leaderboard for the test set and from the corresponding manuscripts of the methods for the validation set. We trained our models on 768 × 768 crops from the full image resolution for benchmarking on the leaderboard. Our AdapNet++ model with a much smaller network backbone achieves a comparable performance as other top performing models on the leaderboard. Moreover, our network is the most efficient architecture in terms of both the number of parameters that it consumes as well as the inference time compared to other networks on the entire first page of the Cityscapes leaderboard.</figDesc><table><row><cell>Network</cell><cell>Sky</cell><cell cols="4">Building Road Sidewalk Fence Vegetation Pole</cell><cell>Car</cell><cell>Sign</cell><cell>Person Cyclist</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(%)</cell></row><row><cell>FCN-8s</cell><cell cols="2">92.36 91.92</cell><cell>88.94 86.46</cell><cell>48.22 77.41</cell><cell cols="3">36.02 82.63 30.37 57.10 46.84</cell><cell>67.11</cell></row><row><cell>SegNet</cell><cell cols="2">91.90 87.19</cell><cell>83.72 80.94</cell><cell>50.02 71.63</cell><cell cols="3">26.12 71.31 1.01</cell><cell>52.34 32.64</cell><cell>58.98</cell></row><row><cell>FastNet</cell><cell cols="2">92.21 92.41</cell><cell>91.85 89.89</cell><cell>56.64 78.59</cell><cell cols="3">51.17 84.75 32.03 69.87 55.65</cell><cell>72.28</cell></row><row><cell>ParseNet</cell><cell cols="2">93.80 93.09</cell><cell>91.05 88.98</cell><cell>53.22 79.48</cell><cell cols="3">46.15 85.37 36.00 63.30 50.82</cell><cell>71.02</cell></row><row><cell>DeconvNet</cell><cell cols="2">95.88 93.83</cell><cell>92.85 90.79</cell><cell>66.40 81.04</cell><cell cols="3">48.23 84.65 0.00</cell><cell>69.46 52.79</cell><cell>70.54</cell></row><row><cell>DeepLab v2</cell><cell cols="2">94.07 93.34</cell><cell>88.07 88.93</cell><cell>55.57 80.22</cell><cell cols="3">45.97 85.87 38.73 64.40 52.54</cell><cell>71.61</cell></row><row><cell>AdapNet</cell><cell cols="2">96.95 95.88</cell><cell>95.60 94.46</cell><cell>76.30 86.59</cell><cell cols="3">67.14 92.20 58.85 80.18 66.89</cell><cell>82.83</cell></row><row><cell>DeepLab v3</cell><cell cols="2">95.30 92.75</cell><cell>93.58 91.56</cell><cell>73.37 80.71</cell><cell cols="3">55.83 88.09 44.17 75.65 60.15</cell><cell>77.38</cell></row><row><cell>AdapNet++ (ours)</cell><cell cols="2">97.77 96.98</cell><cell>96.60 95.70</cell><cell>79.87 89.63</cell><cell cols="3">74.94 94.22 71.99 83.64 72.31</cell><cell>86.70</cell></row><row><cell cols="5">et al, 2017), DeepLab v3 (Chen et al, 2017), Mapilary (Bulò</cell><cell></cell><cell></cell></row><row><cell cols="5">et al, 2018), DeepLab v3+ (Chen et al, 2018b), DPC (Chen</cell><cell></cell><cell></cell></row><row><cell cols="2">et al, 2018a), and DRN (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Performance comparison of AdapNet++ with baseline models on the Synthia validation set (input image dim: 768 × 384). Note that no left-right flips or multiscale testing is performed. Note: Corresponding multimodal results are reported in Table 18.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>Performance comparison of AdapNet++ with baseline models on the SUN RGB-D validation set (input image dim: 768 × 384). Note that no left-right flips or multiscale testing is performed. Note: Corresponding multimodal results are reported in Table 19. 79.94 37.27 52.85 58.42 42.79 43.69 27.58 43.49 28.46 42.21 27.08 28.34 11.38 SegNet 70.18 82.01 37.62 45.77 57.22 35.86 40.60 30.32 39.37 29.67 40.67 22.48 32.13 14.79 FastNet 68.39 79.73 34.98 44.93 55.26 36.68 38.57 26.88 38.28 26.02 39.72 20.55 22.47 11.71 ParseNet 70.83 81.85 40.25 55.64 61.12 45.06 45.86 30.41 41.73 34.45 44.11 28.63 32.59 12.78 DeconvNet 61.53 75.52 26.97 38.52 48.46 30.94 37.12 21.99 27.74.07 31.15 47.00 55.46 40.81 40.60 29.10 37.95 18.67 35.67 23.20 25.08 13.52 AdapNet 72.82 86.61 41.90 56.63 64.99 46.14 46.74 36.14 44.43 28.40 46.53 25.17 31.07 13.41 DeepLab v3 74.99 85.44 39.64 54.14 64.23 45.84 47.35 38.30 45.50 30.70 45.30 15.21 24.92 18.18 AdapNet++ (ours) 73.81 84.79 47.45 64.31 65.76 52.15 49.97 41.66 46.99 32.83 45.19 32.71 34.72 21.16</figDesc><table><row><cell>Network</cell><cell>Wall</cell><cell cols="2">Floor Cabinet Bed</cell><cell cols="2">Chair Sofa</cell><cell cols="8">Table Door Window Bshelf Picture Counter Blinds Desk</cell></row><row><cell>FCN-8s</cell><cell cols="9">68.57 48 0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>9.40</cell></row><row><cell cols="11">DeepLab v2 63.68 Network Shelves Curtain Dresser Pillow Mirror FMat Clothes Ceiling Books Fridge Tv</cell><cell cols="3">Paper Towel ShwrC</cell></row><row><cell>FCN-8s</cell><cell>6.01</cell><cell cols="4">46.96 28.80 27.73 18.97 0.00</cell><cell cols="4">20.34 51.98 30.98 9.06</cell><cell cols="4">34.23 15.95 12.79 0.00</cell></row><row><cell>SegNet</cell><cell>5.15</cell><cell cols="4">39.69 27.46 26.05 15.64 0.00</cell><cell cols="8">16.01 53.89 26.21 12.95 24.23 14.57 10.88 0.00</cell></row><row><cell>FastNet</cell><cell>5.67</cell><cell cols="4">41.98 25.08 22.76 11.76 0.10</cell><cell cols="8">13.37 49.14 29.38 17.49 19.80 17.95 13.55 2.93</cell></row><row><cell>ParseNet</cell><cell>8.58</cell><cell cols="4">46.51 34.05 28.07 24.82 0.02</cell><cell cols="8">19.10 52.28 34.36 27.05 31.49 21.65 21.49 6.46</cell></row><row><cell>DeconvNet</cell><cell>0.00</cell><cell>29.59 0.00</cell><cell>0.00</cell><cell>8.32</cell><cell>0.00</cell><cell>0.00</cell><cell cols="2">46.40 0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>DeepLab v2</cell><cell>2.55</cell><cell cols="4">42.91 24.56 29.81 27.16 0.00</cell><cell cols="8">15.85 43.47 25.39 35.55 23.21 16.53 22.00 0.34</cell></row><row><cell>AdapNet</cell><cell>3.98</cell><cell cols="4">47.90 28.58 32.90 19.25 0.00</cell><cell cols="8">17.30 52.60 30.63 20.32 16.56 18.21 13.69 0.00</cell></row><row><cell>DeepLab v3</cell><cell cols="5">66.37 48.84 28.20 32.87 31.23 0.00</cell><cell cols="8">16.98 60.09 31.74 37.86 33.80 17.84 20.51 0.00</cell></row><row><cell>AdapNet++ (ours)</cell><cell>5.03</cell><cell cols="4">50.32 37.45 32.35 25.89 0.12</cell><cell cols="8">21.68 60.65 32.41 46.96 18.81 20.79 27.95 1.32</cell></row><row><cell>Network</cell><cell>Box</cell><cell cols="4">Wboard Person NStand Toilet Sink</cell><cell cols="3">Lamp Bathtub Bag</cell><cell>mIoU</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN-8s</cell><cell cols="2">18.24 41.38 8.92</cell><cell>0.97</cell><cell cols="5">59.64 44.21 22.83 26.20 8.81</cell><cell>30.46</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SegNet</cell><cell cols="2">13.50 38.02 3.83</cell><cell>0.76</cell><cell cols="5">60.98 45.97 22.82 31.94 8.97</cell><cell>29.14</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FastNet</cell><cell cols="3">15.33 38.12 15.82 7.93</cell><cell cols="5">54.08 39.59 22.20 23.71 9.80</cell><cell>28.15</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ParseNet</cell><cell cols="8">20.77 44.48 34.59 15.37 66.96 47.15 31.49 28.14 13.19</cell><cell>34.68</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeconvNet</cell><cell>4.43</cell><cell>32.06 0.00</cell><cell>0.00</cell><cell cols="3">47.35 32.85 0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>15.64</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepLab v2</cell><cell cols="8">13.56 22.32 46.18 11.45 55.94 43.33 29.52 32.73 7.63</cell><cell>30.06</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdapNet</cell><cell cols="3">15.72 43.02 17.48 9.44</cell><cell cols="5">61.11 49.43 31.32 21.56 9.38</cell><cell>32.47</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepLab v3</cell><cell cols="8">19.58 44.10 31.36 18.15 68.76 52.01 35.36 45.97 10.37</cell><cell>35.74</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdapNet++ (ours)</cell><cell cols="8">22.70 51.08 33.77 20.59 71.28 55.31 36.74 38.60 15.34</cell><cell>38.40</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>Performance comparison of AdapNet++ with baseline models on the ScanNet validation set (input image dim: 768 × 384). Note that no left-right flips or multiscale testing is performed. Note: Corresponding multimodal results are reported inTable 20.</figDesc><table><row><cell>Network</cell><cell>Wall</cell><cell>Floor</cell><cell cols="2">Cabinet Bed</cell><cell cols="2">Chair Sofa</cell><cell cols="2">Table Door Window Bshelf Picture Counter Desk Curtain</cell></row><row><cell>FCN-8s</cell><cell cols="8">70.00 74.70 39.31 66.23 51.96 48.21 45.19 42.61 42.15 62.59 27.66 24.69 33.25 48.29</cell></row><row><cell>SegNet</cell><cell cols="8">64.03 63.46 28.44 30.22 39.84 27.09 35.68 31.86 29.36 35.23 21.19 12.08 16.45 14.44</cell></row><row><cell>FastNet</cell><cell cols="8">66.31 71.35 34.22 56.32 47.86 44.87 39.34 33.20 34.32 50.11 23.27 24.48 28.87 42.56</cell></row><row><cell>ParseNet</cell><cell cols="8">71.29 76.42 39.92 71.11 52.72 51.07 48.74 43.06 45.25 58.81 31.33 27.84 38.46 54.81</cell></row><row><cell>DeconvNet</cell><cell cols="7">66.41 69.83 30.57 51.30 38.27 42.98 38.74 32.51 28.86 22.31 0.00</cell><cell>0.00</cell><cell>19.70 31.62</cell></row><row><cell>DeepLab v2</cell><cell cols="8">59.80 74.21 39.23 50.21 48.95 46.25 47.39 41.27 34.32 48.27 23.36 29.26 35.13 33.89</cell></row><row><cell>AdapNet</cell><cell cols="8">70.62 76.73 43.89 54.10 50.21 48.58 51.95 49.33 35.75 50.05 26.41 30.93 37.23 34.64</cell></row><row><cell>DeepLab v3</cell><cell cols="8">70.83 77.65 44.21 55.38 54.36 50.23 54.26 51.13 36.14 51.48 25.78 31.59 38.11 36.20</cell></row><row><cell>AdapNet++ (ours)</cell><cell cols="8">71.21 80.41 46.48 60.72 58.89 55.12 58.42 55.15 37.18 52.15 26.73 32.79 39.66 37.12</cell></row><row><cell>Network</cell><cell cols="4">Fridge SCurtain Toilet Sink</cell><cell cols="3">Bathtub OtherF mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(%)</cell></row><row><cell>FCN-8s</cell><cell cols="6">51.96 32.15 55.67 35.80 38.27 26.72</cell><cell>45.87</cell></row><row><cell>SegNet</cell><cell>0.09</cell><cell cols="4">24.34 30.70 32.50 3.47</cell><cell>9.73</cell><cell>27.51</cell></row><row><cell>FastNet</cell><cell cols="6">37.77 20.27 49.55 27.27 31.99 24.39</cell><cell>39.42</cell></row><row><cell>ParseNet</cell><cell cols="6">52.40 28.60 58.53 36.27 41.91 25.85</cell><cell>47.72</cell></row><row><cell>DeconvNet</cell><cell>1.31</cell><cell>0.12</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>16.66</cell><cell>24.56</cell></row><row><cell>DeepLab v2</cell><cell cols="6">43.69 42.57 62.26 39.97 55.40 22.36</cell><cell>43.89</cell></row><row><cell>AdapNet</cell><cell cols="6">43.27 45.83 68.22 43.45 60.37 24.04</cell><cell>47.28</cell></row><row><cell>DeepLab v3</cell><cell cols="6">51.76 51.46 76.22 51.24 64.36 29.32</cell><cell>50.09</cell></row><row><cell>AdapNet++ (ours)</cell><cell cols="6">54.54 54.88 81.92 54.91 68.65 31.46</cell><cell>52.92</cell></row><row><cell cols="7">Table 6 Bechmarking results on the ScanNet test set with full resolu-</cell><cell></cell></row><row><cell cols="7">tion evaluation. Results were obtained from the ScanNet benchmark</cell><cell></cell></row><row><cell>leaderboard.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell></cell><cell>Multimodal</cell><cell></cell><cell cols="2">mIoU (%)</cell><cell></cell><cell></cell></row><row><cell>Enet</cell><cell></cell><cell>-</cell><cell></cell><cell>37.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSPNet</cell><cell></cell><cell>-</cell><cell></cell><cell>47.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DMV (2d proj)</cell><cell></cell><cell></cell><cell></cell><cell>49.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FuseNet</cell><cell></cell><cell></cell><cell></cell><cell>52.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">AdapNet++ (ours)</cell><cell>-</cell><cell></cell><cell>50.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSMA (ours)</cell><cell></cell><cell></cell><cell></cell><cell>57.7</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>Performance comparison of AdapNet++ with baseline models on the Freiburg Forest validation set (input image dim: 768 × 384). Note that no left-right flips or multiscale testing is performed. Note: Corresponding multimodal results are reported inTable 21.</figDesc><table><row><cell cols="2">Network</cell><cell></cell><cell>Trail</cell><cell cols="2">Grass Veg.</cell><cell>Sky</cell><cell>Obst.</cell><cell cols="2">mIoU gIoU</cell><cell>FPR</cell><cell>FNR Params.</cell><cell>Time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(M)</cell><cell>(ms)</cell></row><row><cell>FCN-8s</cell><cell></cell><cell></cell><cell cols="5">82.60 85.69 88.78 89.97 40.40</cell><cell cols="2">77.49 86.64 6.21</cell><cell>7.15</cell><cell>134.0 101.99</cell></row><row><cell>SegNet</cell><cell></cell><cell></cell><cell cols="5">82.12 84.99 88.64 89.90 27.23</cell><cell cols="2">74.58 86.24 6.77</cell><cell>6.98</cell><cell>29.4</cell><cell>79.13</cell></row><row><cell cols="2">ParseNet</cell><cell></cell><cell cols="5">85.67 87.33 89.63 89.17 43.08</cell><cell cols="2">78.97 87.65 6.34</cell><cell>6.01</cell><cell>20.5 286.54</cell></row><row><cell>FastNet</cell><cell></cell><cell></cell><cell cols="5">85.70 87.53 90.23 90.73 43.76</cell><cell cols="2">79.67 88.18 6.09</cell><cell>5.72</cell><cell>21.0</cell><cell>49.31</cell></row><row><cell cols="2">DeconvNet</cell><cell></cell><cell cols="5">86.37 87.17 89.63 92.20 34.80</cell><cell cols="2">78.04 89.06 6.53</cell><cell>6.78</cell><cell>252.0 168.54</cell></row><row><cell cols="2">DeepLab v2</cell><cell></cell><cell cols="5">88.75 88.87 90.29 91.65 49.55</cell><cell cols="2">81.82 89.98 5.94</cell><cell>5.79</cell><cell>43.7 128.90</cell></row><row><cell cols="2">AdapNet</cell><cell></cell><cell cols="5">88.99 88.99 91.18 92.89 48.80</cell><cell cols="2">82.17 90.67 4.89</cell><cell>5.42</cell><cell>24.4</cell><cell>61.81</cell></row><row><cell cols="2">DeepLab v3</cell><cell></cell><cell cols="5">88.62 88.84 90.55 91.75 51.61</cell><cell cols="2">82.28 90.08 5.21</cell><cell>5.82</cell><cell>58.16</cell><cell>82.83</cell></row><row><cell cols="3">AdapNet++ (ours)</cell><cell cols="5">89.27 89.41 91.43 93.01 52.32</cell><cell cols="2">83.09 90.75 4.84</cell><cell>5.37</cell><cell>28.1</cell><cell>72.77</cell></row><row><cell cols="8">Table 8 Comparison of network compression approaches on our Ad-</cell><cell></cell></row><row><cell cols="8">apNet++ model trained on the Cityscapes dataset and evaluated on the</cell><cell></cell></row><row><cell>validation set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Technique</cell><cell>mIoU</cell><cell cols="3">Param. FLOPS</cell><cell cols="2">Reduction % of</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(%)</cell><cell>(M)</cell><cell></cell><cell cols="3">(B) Param. FLOPS</cell><cell></cell><cell></cell></row><row><cell>Original</cell><cell>80.77</cell><cell cols="2">30.20</cell><cell>138.47</cell><cell>−</cell><cell>−</cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>80.67</cell><cell cols="2">28.57</cell><cell>136.05</cell><cell>-5.40</cell><cell>-1.75</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80.80</cell><cell cols="2">28.34</cell><cell>135.64</cell><cell>-6.15</cell><cell>-2.04</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80.56</cell><cell cols="2">23.67</cell><cell>125.33</cell><cell>-21.62</cell><cell>-9.49</cell><cell></cell><cell></cell></row><row><cell>Oracle</cell><cell>80.18</cell><cell cols="2">21.66</cell><cell>83.84</cell><cell>-28.28</cell><cell>-39.45</cell><cell></cell><cell></cell></row><row><cell></cell><cell>79.65</cell><cell cols="2">19.91</cell><cell>81.72</cell><cell>-34.07</cell><cell>-40.98</cell><cell></cell><cell></cell></row><row><cell></cell><cell>77.95</cell><cell cols="2">17.79</cell><cell>79.84</cell><cell>-41.09</cell><cell>-42.34</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80.80</cell><cell cols="2">28.14</cell><cell>135.17</cell><cell>-6.82</cell><cell>-2.38</cell><cell></cell><cell></cell></row><row><cell>Oracle with</cell><cell>80.58</cell><cell cols="2">23.16</cell><cell>124.14</cell><cell>-23.31</cell><cell>-10.34</cell><cell></cell><cell></cell></row><row><cell>skip (Ours)</cell><cell>80.21</cell><cell cols="2">21.11</cell><cell>83.01</cell><cell>-30.10</cell><cell>-40.05</cell><cell></cell><cell></cell></row><row><cell></cell><cell>79.68</cell><cell cols="2">19.75</cell><cell>81.53</cell><cell>-34.60</cell><cell>-41.12</cell><cell></cell><cell></cell></row><row><cell></cell><cell>78.05</cell><cell cols="2">17.63</cell><cell>79.48</cell><cell>-41.62</cell><cell>-42.60</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11</head><label>11</label><figDesc>Performance comparison of our proposed eASPP with various other ASPP configurations. The results are reported for the models trained on the Cityscapes dataset and evaluated on the validation set.</figDesc><table><row><cell>ASPP Topology</cell><cell>mIoU</cell><cell>Param</cell><cell>FLOPS</cell></row><row><cell></cell><cell>(%)</cell><cell>(M)</cell><cell>(B)</cell></row><row><cell>ASPP v2</cell><cell>80.25</cell><cell>18.87</cell><cell>50.96</cell></row><row><cell>ASPP v3</cell><cell>80.67</cell><cell>15.53</cell><cell>34.58</cell></row><row><cell>ASPP v3 with Separable Conv.</cell><cell>80.27</cell><cell>3.00</cell><cell>5.56</cell></row><row><cell>DenseASPP</cell><cell>80.62</cell><cell>4.23</cell><cell>9.74</cell></row><row><cell>eASPP (Ours)</cell><cell>80.77</cell><cell>2.04</cell><cell>3.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12</head><label>12</label><figDesc>Effect on varying the number of filters in the skip refinement connections in the M95 model. The performance is shown for the model trained on the Cityscapes dataset and evaluated on the validation set .</figDesc><table><row><cell>Skip Channels</cell><cell>12</cell><cell>24</cell><cell>36</cell><cell>48</cell><cell>60</cell></row><row><cell>mIoU (%)</cell><cell>80.50</cell><cell>80.77</cell><cell>80.67</cell><cell>80.59</cell><cell>80.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13</head><label>13</label><figDesc>Effect on varying the weighting factor of the auxiliary losses in the M95 model. The performance is shown for the model trained on the Cityscapes dataset and evaluated on the validation set.</figDesc><table><row><cell></cell><cell cols="2">Aux 1 Weight</cell><cell>0.3</cell><cell>0.4</cell><cell>0.4</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.6</cell><cell>0.6</cell><cell>0.6</cell><cell>0.7</cell><cell>0.7</cell><cell>1.0</cell></row><row><cell></cell><cell cols="2">Aux 2 Weight</cell><cell>0.2</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.6</cell><cell>0.7</cell><cell>0.4</cell><cell>0.5</cell><cell>0.7</cell><cell>0.5</cell><cell>0.6</cell><cell>1.0</cell></row><row><cell></cell><cell>mIoU (%)</cell><cell></cell><cell cols="11">80.51 80.55 80.68 80.60 80.55 80.49 80.53 80.77 80.69 80.55 80.71 80.37</cell></row><row><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>45 50 55 60 65 mean IoU [%]</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Old decoder New decoder New dec. + Skip refinement New dec. + Skip ref. + Multires.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>10</cell><cell cols="3">20 Trimap Width [pixels] 30</cell><cell>40</cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14</head><label>14</label><figDesc>Effect of various encoder topologies in the M95 model. The performance is shown for the model trained on the Cityscapes dataset and evaluated on the validation set.</figDesc><table><row><cell>Encoder</cell><cell cols="5">ResNet PA ResNet ResNeXt SEnet Xception</cell></row><row><cell>mIoU (%)</cell><cell>79.32</cell><cell>80.77</cell><cell>80.30</cell><cell cols="2">78.31 78.70</cell></row><row><cell>Param (M)</cell><cell>30.2</cell><cell>30.2</cell><cell>29.7</cell><cell>32.7</cell><cell>27.5</cell></row><row><cell>FLOPS (B)</cell><cell cols="2">135.28 138.47</cell><cell cols="3">145.81 145.34 137.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15</head><label>15</label><figDesc>Performance of the strong decoder that we introduce in our AdapNet++ architecture in comparison with other progressive upsampling decoder topologies. All the models employ the same Ad-apNet++ encoder with only the decoder replaced with alternative topologies and the results are reported on the Cityscapes validation set.</figDesc><table><row><cell>Decoder Topology</cell><cell>mIoU</cell><cell>Param</cell><cell>FLOPs</cell></row><row><cell></cell><cell>(%)</cell><cell>(M)</cell><cell>(B)</cell></row><row><cell>Bilinear upsampling</cell><cell>74.83</cell><cell>−</cell><cell>−</cell></row><row><cell>LRR</cell><cell>79.38</cell><cell>2.01</cell><cell>75.88</cell></row><row><cell>RefineNet</cell><cell>80.44</cell><cell>14.29</cell><cell>265.33</cell></row><row><cell>AdapNet++ (Ours)</cell><cell>80.77</cell><cell>4.59</cell><cell>69.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16</head><label>16</label><figDesc>Effect on using a higher resolution input image and employing left-right flip as well as multiscale inputs during testing. The performance is shown for the model trained on the Cityscapes dataset and evaluated on the validation set.</figDesc><table><row><cell>Image Size</cell><cell cols="4">Flip MS mIoU Acc.</cell><cell>AP</cell><cell>Time</cell></row><row><cell>(pixels)</cell><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(ms)</cell></row><row><cell>768 × 384</cell><cell>-</cell><cell>-</cell><cell cols="3">80.77 96.04 90.97 72.77</cell></row><row><cell>768 × 384</cell><cell></cell><cell>-</cell><cell cols="3">81.35 96.18 90.76 148.93</cell></row><row><cell>768 × 384</cell><cell></cell><cell></cell><cell cols="3">82.25 96.36 91.86 1775.96</cell></row><row><cell>896 × 448</cell><cell>-</cell><cell>-</cell><cell cols="3">81.69 96.06 89.96 88.89</cell></row><row><cell>896 × 448</cell><cell></cell><cell>-</cell><cell cols="3">82.28 96.21 90.53 183.57</cell></row><row><cell>896 × 448</cell><cell></cell><cell></cell><cell cols="3">83.19 96.48 91.52 2342.31</cell></row><row><cell>1024 × 512</cell><cell>-</cell><cell>-</cell><cell cols="3">82.47 96.13 90.63 105.94</cell></row><row><cell>1024 × 512</cell><cell></cell><cell>-</cell><cell cols="3">83.07 96.28 91.17 219.28</cell></row><row><cell>1024 × 512</cell><cell></cell><cell></cell><cell cols="3">84.27 96.66 92.36 3061.11</cell></row><row><cell cols="2">2048 × 1024 -</cell><cell>-</cell><cell cols="3">83.10 96.25 90.87 494.97</cell></row><row><cell>2048 × 1024</cell><cell></cell><cell>-</cell><cell cols="3">83.58 96.37 91.14 1022.12</cell></row><row><cell>2048 × 1024</cell><cell></cell><cell></cell><cell cols="3">84.54 96.74 92.48 12188.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17</head><label>17</label><figDesc>Comparison of multimodal fusion approaches on the Cityscapes validation set (input image dim: 768 × 384). All the fusion models have the same unimodal AdapNet++ network backbone. Results on the test set are shown inTable 2.</figDesc><table><row><cell>Network</cell><cell>Approach</cell><cell>mIoU</cell><cell>Acc.</cell><cell>AP</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>RGB</cell><cell>Unimodal</cell><cell>80.80</cell><cell>96.04</cell><cell>90.97</cell></row><row><cell>Depth</cell><cell>Unimodal</cell><cell>66.36</cell><cell>91.21</cell><cell>80.23</cell></row><row><cell>HHA</cell><cell>Unimodal</cell><cell>67.66</cell><cell>91.66</cell><cell>81.81</cell></row><row><cell></cell><cell>Average</cell><cell>78.84</cell><cell>95.58</cell><cell>90.49</cell></row><row><cell></cell><cell>Maximum</cell><cell>78.81</cell><cell>95.58</cell><cell>90.37</cell></row><row><cell></cell><cell>Stacking</cell><cell>80.21</cell><cell>95.96</cell><cell>90.05</cell></row><row><cell></cell><cell>Late Fusion</cell><cell>78.75</cell><cell>95.57</cell><cell>90.48</cell></row><row><cell>RGB-D</cell><cell>LFC</cell><cell>81.04</cell><cell>96.11</cell><cell>91.10</cell></row><row><cell></cell><cell>CMoDE</cell><cell>81.33</cell><cell>96.12</cell><cell>90.29</cell></row><row><cell></cell><cell>FuseNet</cell><cell>81.54</cell><cell>96.23</cell><cell>90.24</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>82.29</cell><cell>96.36</cell><cell>90.77</cell></row><row><cell></cell><cell>SSMA msf (Ours)</cell><cell>83.44</cell><cell>96.59</cell><cell>92.21</cell></row><row><cell></cell><cell>Average</cell><cell>79.44</cell><cell>95.56</cell><cell>90.27</cell></row><row><cell></cell><cell>Maximum</cell><cell>79.40</cell><cell>95.55</cell><cell>90.09</cell></row><row><cell></cell><cell>Stacking</cell><cell>80.62</cell><cell>96.01</cell><cell>90.09</cell></row><row><cell>RGB-HHA</cell><cell>Late Fusion LFC</cell><cell>79.01 81.13</cell><cell>95.49 96.14</cell><cell>90.25 91.32</cell></row><row><cell></cell><cell>CMoDE</cell><cell>81.40</cell><cell>96.12</cell><cell>90.29</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>82.64</cell><cell>96.41</cell><cell>90.65</cell></row><row><cell></cell><cell>SSMA msf (Ours)</cell><cell>83.94</cell><cell>96.68</cell><cell>91.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 18</head><label>18</label><figDesc>Comparison of multimodal fusion approaches on the Synthia validation set (input image dim: 768 × 384). All the fusion models have the same unimodal AdapNet++ network backbone.</figDesc><table><row><cell>Network</cell><cell>Approach</cell><cell>mIoU</cell><cell>Acc.</cell><cell>AP</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>RGB</cell><cell>Unimodal</cell><cell>86.70</cell><cell>97.18</cell><cell>93.17</cell></row><row><cell>Depth</cell><cell>Unimodal</cell><cell>87.87</cell><cell>97.78</cell><cell>94.23</cell></row><row><cell></cell><cell>Average</cell><cell>89.22</cell><cell>98.03</cell><cell>95.04</cell></row><row><cell></cell><cell>Maximum</cell><cell>89.13</cell><cell>98.01</cell><cell>94.97</cell></row><row><cell></cell><cell>Stacking</cell><cell>88.95</cell><cell>98.03</cell><cell>94.41</cell></row><row><cell></cell><cell>Late Fusion</cell><cell>89.13</cell><cell>98.01</cell><cell>94.66</cell></row><row><cell>RGB-D</cell><cell>LFC</cell><cell>89.48</cell><cell>98.09</cell><cell>94.96</cell></row><row><cell></cell><cell>CMoDE</cell><cell>89.57</cell><cell>98.13</cell><cell>94.58</cell></row><row><cell></cell><cell>FuseNet</cell><cell>89.62</cell><cell>98.34</cell><cell>94.67</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>91.25</cell><cell>98.48</cell><cell>95.68</cell></row><row><cell></cell><cell>SSMA msf (Ours)</cell><cell>92.10</cell><cell>98.64</cell><cell>96.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 19</head><label>19</label><figDesc>Comparison of multimodal fusion approaches on the SUN RGB-D validation set (input image dim: 768 × 384). All the fusion models have the same unimodal AdapNet++ network backbone.</figDesc><table><row><cell>Network</cell><cell>Approach</cell><cell>mIoU</cell><cell>Acc.</cell><cell>AP</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>RGB</cell><cell>Unimodal</cell><cell>38.40</cell><cell>76.90</cell><cell>62.78</cell></row><row><cell>Depth</cell><cell>Unimodal</cell><cell>34.27</cell><cell>73.83</cell><cell>74.39</cell></row><row><cell>HHA</cell><cell>Unimodal</cell><cell>34.59</cell><cell>74.39</cell><cell>57.18</cell></row><row><cell></cell><cell>Average</cell><cell>40.70</cell><cell>78.58</cell><cell>64.54</cell></row><row><cell></cell><cell>Maximum</cell><cell>40.58</cell><cell>78.50</cell><cell>64.04</cell></row><row><cell></cell><cell>Stacking</cell><cell>36.48</cell><cell>76.68</cell><cell>57.92</cell></row><row><cell></cell><cell>Late Fusion</cell><cell>41.68</cell><cell>79.27</cell><cell>66.63</cell></row><row><cell>RGB-D</cell><cell>LFC</cell><cell>41.82</cell><cell>79.36</cell><cell>66.75</cell></row><row><cell></cell><cell>CMoDE</cell><cell>41.87</cell><cell>79.84</cell><cell>66.81</cell></row><row><cell></cell><cell>FuseNet</cell><cell>41.85</cell><cell>79.56</cell><cell>66.87</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>43.90</cell><cell>80.16</cell><cell>66.11</cell></row><row><cell></cell><cell>SSMA msf (Ours)</cell><cell>44.52</cell><cell>80.67</cell><cell>67.92</cell></row><row><cell></cell><cell>Average</cell><cell>41.01</cell><cell>78.54</cell><cell>64.93</cell></row><row><cell></cell><cell>Maximum</cell><cell>40.91</cell><cell>78.49</cell><cell>64.78</cell></row><row><cell></cell><cell>Stacking</cell><cell>37.49</cell><cell>76.42</cell><cell>57.88</cell></row><row><cell>RGB-HHA</cell><cell>Late Fusion LFC</cell><cell>41.91 42.42</cell><cell>79.49 79.55</cell><cell>67.31 67.41</cell></row><row><cell></cell><cell>CMoDE</cell><cell>42.55</cell><cell>79.94</cell><cell>65.38</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>44.43</cell><cell>80.21</cell><cell>64.94</cell></row><row><cell></cell><cell>SSMA msf (Ours)</cell><cell>45.73</cell><cell>80.97</cell><cell>67.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 20</head><label>20</label><figDesc>Comparison of multimodal fusion approaches on the ScanNet validation set (input image dim: 768 × 384). All the fusion models have the same unimodal AdapNet++ network backbone. Results on the test set are shown inTable 6.</figDesc><table><row><cell>Network</cell><cell>Approach</cell><cell>mIoU</cell><cell>Acc.</cell><cell>AP</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>RGB</cell><cell>Unimodal</cell><cell>52.92</cell><cell>77.70</cell><cell>77.28</cell></row><row><cell>Depth</cell><cell>Unimodal</cell><cell>53.80</cell><cell>80.63</cell><cell>74.46</cell></row><row><cell>HHA</cell><cell>Unimodal</cell><cell>54.19</cell><cell>80.20</cell><cell>73.90</cell></row><row><cell></cell><cell>Average</cell><cell>58.20</cell><cell>82.31</cell><cell>79.45</cell></row><row><cell></cell><cell>Maximum</cell><cell>57.68</cell><cell>82.12</cell><cell>78.35</cell></row><row><cell></cell><cell>Stacking</cell><cell>55.89</cell><cell>79.04</cell><cell>77.08</cell></row><row><cell></cell><cell>Late Fusion</cell><cell>61.37</cell><cell>82.48</cell><cell>80.15</cell></row><row><cell>RGB-D</cell><cell>LFC</cell><cell>62.97</cell><cell>83.70</cell><cell>80.93</cell></row><row><cell></cell><cell>CMoDE</cell><cell>64.00</cell><cell>84.94</cell><cell>81.27</cell></row><row><cell></cell><cell>FuseNet</cell><cell>63.83</cell><cell>84.24</cell><cell>81.02</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>66.29</cell><cell>86.11</cell><cell>81.81</cell></row><row><cell></cell><cell>SSMA msf (Ours)</cell><cell>67.38</cell><cell>86.24</cell><cell>81.93</cell></row><row><cell></cell><cell>Average</cell><cell>58.39</cell><cell>82.36</cell><cell>78.39</cell></row><row><cell></cell><cell>Maximum</cell><cell>57.88</cell><cell>82.12</cell><cell>77.34</cell></row><row><cell></cell><cell>Stacking</cell><cell>56.48</cell><cell>80.33</cell><cell>77.47</cell></row><row><cell>RGB-HHA</cell><cell>Late Fusion LFC</cell><cell>61.45 63.09</cell><cell>82.68 83.67</cell><cell>80.12 80.67</cell></row><row><cell></cell><cell>CMoDE</cell><cell>64.07</cell><cell>84.84</cell><cell>80.80</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>66.34</cell><cell>86.02</cell><cell>81.49</cell></row><row><cell></cell><cell>SSMA msf (Ours)</cell><cell>67.52</cell><cell>86.38</cell><cell>82.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 21</head><label>21</label><figDesc>Comparison of multimodal fusion approaches on the Freiburg Forest validation set (input image dim: 768×384). All the fusion models have the same unimodal AdapNet++ network backbone.</figDesc><table><row><cell>Network</cell><cell>Approach</cell><cell>mIoU</cell><cell>Acc.</cell><cell>AP</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>RGB</cell><cell>Unimodal</cell><cell>83.09</cell><cell>95.15</cell><cell>89.83</cell></row><row><cell>Depth</cell><cell>Unimodal</cell><cell>73.93</cell><cell>91.42</cell><cell>85.36</cell></row><row><cell>EVI</cell><cell>Unimodal</cell><cell>80.96</cell><cell>94.20</cell><cell>88.88</cell></row><row><cell></cell><cell>Average</cell><cell>79.51</cell><cell>92.87</cell><cell>90.99</cell></row><row><cell></cell><cell>Maximum</cell><cell>81.62</cell><cell>93.93</cell><cell>90.87</cell></row><row><cell></cell><cell>Stacking</cell><cell>83.13</cell><cell>95.19</cell><cell>89.95</cell></row><row><cell></cell><cell>Late Fusion</cell><cell>82.11</cell><cell>93.95</cell><cell>90.85</cell></row><row><cell>RGB-D</cell><cell>LFC</cell><cell>82.53</cell><cell>94.99</cell><cell>90.96</cell></row><row><cell></cell><cell>CMoDE</cell><cell>83.21</cell><cell>95.19</cell><cell>90.19</cell></row><row><cell></cell><cell>FuseNet</cell><cell>83.10</cell><cell>95.07</cell><cell>90.03</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>83.81</cell><cell>95.62</cell><cell>92.78</cell></row><row><cell></cell><cell>SSMA msf (Ours)</cell><cell>83.99</cell><cell>95.70</cell><cell>93.08</cell></row><row><cell></cell><cell>Average</cell><cell>83.00</cell><cell>95.10</cell><cell>90.19</cell></row><row><cell></cell><cell>Maximum</cell><cell>83.00</cell><cell>95.10</cell><cell>90.17</cell></row><row><cell></cell><cell>Stacking</cell><cell>83.18</cell><cell>95.21</cell><cell>90.11</cell></row><row><cell>RGB-EVI</cell><cell>Late Fusion LFC</cell><cell>82.80 83.00</cell><cell>95.01 95.13</cell><cell>90.07 90.28</cell></row><row><cell></cell><cell>CMoDE</cell><cell>83.31</cell><cell>95.22</cell><cell>90.19</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>83.90</cell><cell>95.56</cell><cell>92.28</cell></row><row><cell></cell><cell>SSMA msf (Ours)</cell><cell>84.18</cell><cell>95.64</cell><cell>92.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 22</head><label>22</label><figDesc>Performance of multimodal SSMA fusion for the scene type classification task on the ScanNet benchmark. Results are reported on the validation set.</figDesc><table><row><cell>Network</cell><cell>Approach</cell><cell>mIoU</cell><cell>mRecall</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>RGB</cell><cell>Unimodal</cell><cell>33.16</cell><cell>43.37</cell></row><row><cell>Depth</cell><cell>Unimodal</cell><cell>35.07</cell><cell>44.82</cell></row><row><cell></cell><cell>Average</cell><cell>34.92</cell><cell>43.99</cell></row><row><cell></cell><cell>Maximum</cell><cell>34.75</cell><cell>43.79</cell></row><row><cell>RGB-D</cell><cell>Stacking</cell><cell>32.47</cell><cell>41.89</cell></row><row><cell></cell><cell>Late Fusion</cell><cell>35.17</cell><cell>44.48</cell></row><row><cell></cell><cell>SSMA (Ours)</cell><cell>37.45</cell><cell>54.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 23</head><label>23</label><figDesc>Performance of multimodal SSMA fusion technique with different real-time backbone networks. Results are shown on the Cityscapes validation set (input image dim: 768 × 384).</figDesc><table><row><cell>Network</cell><cell>Modalities</cell><cell>mIoU</cell><cell>Time</cell></row><row><cell>Backbone</cell><cell></cell><cell>(%)</cell><cell>(ms)</cell></row><row><cell></cell><cell>RGB</cell><cell>62.71</cell><cell>43.87</cell></row><row><cell>ERFnet</cell><cell>HHA</cell><cell>51.84</cell><cell>43.87</cell></row><row><cell></cell><cell>RGB-HHA SSMA</cell><cell>64.60</cell><cell>66.06</cell></row><row><cell></cell><cell>RGB</cell><cell>74.78</cell><cell>47.43</cell></row><row><cell>MobileNet v2</cell><cell>HHA</cell><cell>61.89</cell><cell>47.43</cell></row><row><cell></cell><cell>RGB-HHA SSMA</cell><cell>77.17</cell><cell>73.62</cell></row><row><cell></cell><cell>RGB</cell><cell>80.80</cell><cell>72.77</cell></row><row><cell>AdapNet++</cell><cell>HHA</cell><cell>67.66</cell><cell>72.77</cell></row><row><cell></cell><cell>RGB-HHA SSMA</cell><cell>82.64</cell><cell>99.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 24</head><label>24</label><figDesc>Effect of the various contributions proposed for multimodal fusion using AdapNet++ and the SMMA architecture. The performance is shown for RGB-HHA fusion on the Cityscapes dataset and evaluated on the validation set. 84% with an inference time of 99.96ms. Each of these multimodal fusion models outperform their unimodal counterparts and have an inference time in the range of 66ms − 99ms. This demonstrates the modularity of our fusion framework that enables the selection of an appropriate network backbone according to the desired frame rate.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">SSMA Fusion</cell><cell cols="2">mIoU Acc.</cell><cell>AP</cell></row><row><cell></cell><cell cols="3">ASPP Skip Ch. Agg.</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>F0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">80.77 96.04 90.97</cell></row><row><cell>F1</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">81.55 96.19 91.15</cell></row><row><cell>F2</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">81.75 96.25 91.09</cell></row><row><cell>F3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">82.64 96.41 90.65</cell></row><row><cell cols="6">with an inference time of 66.06ms, thereby outperforming</cell></row><row><cell cols="6">both the unimodal RGB ERFnet model and the unimodal</cell></row><row><cell cols="6">HHA ERFnet model. The MobileNet v2 model in our mul-</cell></row><row><cell cols="6">timodal SSMA fusion configuration further outperforms the</cell></row><row><cell cols="6">ERFnet fusion model by 12.57% in the mIoU score with an</cell></row><row><cell cols="6">inference time of 73.62ms. In comparison to these network</cell></row><row><cell cols="6">backbones, our AdapNet++ fusion model achieves a mIoU</cell></row><row><cell cols="2">score of 82.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 25</head><label>25</label><figDesc>Effect of varying the SSMA bottleneck downsampling rate η on the RGB-HHA fusion performance. Results are shown for the model trained on the Cityscapes dataset and evaluated on the validation set.</figDesc><table><row><cell>Encoder</cell><cell>Skip</cell><cell>mIoU</cell><cell>Acc.</cell><cell>AP</cell></row><row><cell>SSMA</cell><cell>SSMA</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>η enc = 2</cell><cell>η skip = 2</cell><cell>82.15</cell><cell>96.32</cell><cell>91.17</cell></row><row><cell>η enc = 4</cell><cell>η skip = 4</cell><cell>82.11</cell><cell>96.27</cell><cell>91.34</cell></row><row><cell>η enc = 8</cell><cell>η skip = 4</cell><cell>82.21</cell><cell>96.32</cell><cell>91.61</cell></row><row><cell>η enc = 16</cell><cell>η skip = 4</cell><cell>82.25</cell><cell>96.31</cell><cell>91.12</cell></row><row><cell>η enc = 16</cell><cell>η skip = 6</cell><cell>82.64</cell><cell>96.41</cell><cell>90.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 26</head><label>26</label><figDesc>Comparison of multimodal SSMA fusion at different network stages and by employing a dynamic dependent probability weighting or independent probability weighting configuration. Results are shown for RGB-HHA fusion on the Cityscapes validation set with (supervised) and without (self-supervised) an explicit loss for the SSMA module.</figDesc><table><row><cell cols="2">Model SSMA</cell><cell cols="2">Encoder Stage</cell><cell cols="2">Decoder Stage</cell><cell>IoU</cell></row><row><cell></cell><cell>Loss</cell><cell>Dep.</cell><cell cols="2">Indep. Dep.</cell><cell cols="2">Indep. (%)</cell></row><row><cell>S1</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.39</cell></row><row><cell>S2</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>82.64</cell></row><row><cell>S3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>80.96</cell></row><row><cell>S4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>81.14</cell></row><row><cell>S5</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.32</cell></row><row><cell>S6</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>82.52</cell></row><row><cell>S7</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>80.86</cell></row><row><cell>S8</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>81.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head></head><label></label><figDesc>Pyramid scene parsing network. In: Proceedings of the Conference on Computer Vision and Pattern Recognition Zhou B, Khosla A, Lapedriza A, Oliva A, Torralba A (2014) Object detectors emerge in deep scene cnns. arXiv preprint arXiv:14126856 Zhuang Y, Yang F, Tao L, Ma C, Zhang Z, Li Y, Jia H, Xie X, Gao W (2018) Dense relation network: Learning consistent and contextaware representation for semantic image segmentation. In: 2018 25th IEEE International Conference on Image Processing (ICIP), pp 3698-3702</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wicke</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured pruning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Journal on Emerging Technologies in Computing Systems (JETC)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lefèvre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>arXiv: 151100561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Robot localization in floor plans using a room layout edge extraction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boniardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Caselitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno>arXiv:190301804</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno>arXiv:171011063</idno>
	</analytic>
	<monogr>
		<title level="m">Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<editor>Forsyth D, Torr P, Zisserman A</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the Conference on Computer Vision and Pattern Recognition Chattopadhyay A</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>arXiv: 160600915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>arXiv:170605587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8713" to="8724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>arXiv:180202611</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno>arXiv:161002357</idno>
		<title level="m">Xception: Deep learning with depthwise separable convolutions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Similarity-based fusion of meg and fmri reveals spatio-temporal dynamics in human cortex during visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Cichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pantazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3563" to="3579" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>arXiv:13013572</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<idno>arXiv:180310409</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for robust rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
		</imprint>
	</monogr>
	<note>The pascal visual object classes challenge: A retrospective</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene parsing with multiscale feature learning, purity trees, and optimal covers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning Fei</title>
		<meeting>the International Conference on Machine Learning Fei</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="863" to="863" />
		</imprint>
	</monogr>
	<note>What do we see when we glance at a scene</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Ghiasi G, Fowlkes CC</title>
		<meeting>the International Conference on Computer Vision Ghiasi G, Fowlkes CC</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional networks for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J ;</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D ;</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J ;</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J ;</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition He K</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition He K</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
	<note>Spatial pyramid pooling in deep convolutional networks for visual recognition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense 3d semantic mapping of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv:170901507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Squeeze-and-excitation networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Algorithm theoretical basis document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Justice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">213</biblScope>
		</imprint>
	</monogr>
	<note>Modis vegetation index (mod13)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A category-level 3d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Consumer Depth Cameras for Computer Vision</title>
		<meeting>the IEEE International Conference on Consumer Depth Cameras for Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="141" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Season-invariant semantic segmentation with a deep multimodal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uenoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and Service Robotics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno>arXiv:180200036</idno>
		<title level="m">defense of classical image processing: Fast depth completion on the cpu</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="volume">562</biblScope>
			<biblScope unit="page">570</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Liang</title>
		<meeting>the European Conference on Computer Vision Liang</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Id</surname></persName>
		</author>
		<idno>arXiv:13124400</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<editor>Lin M, Chen Q, Yan S</editor>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>arXiv: 150604579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shelhamer E, Darrell T (2015) Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference on Computer Vision Long J</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename><forename type="middle">T</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representation Munoz D</title>
		<meeting>the International Conference on Learning Representation Munoz D</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Largescale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3456" to="3465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno>arXiv:160602147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep learning for human part discovery in images</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-class image segmentation using conditional random fields and global classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S ;</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference on Machine Learning Plath N</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multimodal interaction-aware motion prediction for autonomous street crossing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno>arXiv:180806887</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Vlocnet++: Deep multitask learning for semantic visual localization and odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4407" to="4414" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition Romera E, Alvarez JM, Bergasa LM</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition Romera E, Alvarez JM, Bergasa LM</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
	<note>Arroyo R</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Modis daily photosynthesis (psn) and annual net primary production (npp) product (mod17) algorithm theoretical basis document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Glassy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pe ;</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition Running SW</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition Running SW</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
		<respStmt>
			<orgName>University of Montana</orgName>
		</respStmt>
	</monogr>
	<note>Proceedings of the Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multimodal neural networks: Rgb-d for semantic segmentation and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rätsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Analysis</title>
		<editor>Sharma P, Bianchi FM</editor>
		<imprint>
			<biblScope unit="page" from="98" to="109" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic texton forests for image categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>arXiv:14091556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale image recognition</title>
		<meeting>the European Conference on Computer Vision Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale image recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Combining Appearance and Structure from Motion Features for Road Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Workshop, State Estimation and Terrain Perception for All Terrain Mobile Robots</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Proceedings of the British Machine Vision Conference Valada A, Dhall A, Burgard W</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep multispectral semantic scene understanding of forested environments using multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W ;</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W ;</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS 2016) Workshop, Are the Sceptics Right? Limits and Potentials of Deep Learning in Robotics Valada A</title>
		<meeting><address><addrLine>Vertens J, Dhall A, Burgard</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno>arXiv:170303098</idno>
		<title level="m">Semantic mapping with data associated recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes using dense depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V ;</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<editor>Daniilidis K, Maragos P, Paragios N</editor>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
