<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty Inspired RGB-D Saliency Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Uncertainty Inspired RGB-D Saliency Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Uncertainty</term>
					<term>RGB-D Saliency Prediction</term>
					<term>Conditional Variational Autoencoders</term>
					<term>Alternating Back-Propagation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the first stochastic framework to employ uncertainty for RGB-D saliency detection by learning from the data labeling process. Existing RGB-D saliency detection models treat this task as a point estimation problem by predicting a single saliency map following a deterministic learning pipeline. We argue that, however, the deterministic solution is relatively ill-posed. Inspired by the saliency data labeling process, we propose a generative architecture to achieve probabilistic RGB-D saliency detection which utilizes a latent variable to model the labeling variations. Our framework includes two main models: 1) a generator model, which maps the input image and latent variable to stochastic saliency prediction, and 2) an inference model, which gradually updates the latent variable by sampling it from the true or approximate posterior distribution. The generator model is an encoder-decoder saliency network. To infer the latent variable, we introduce two different solutions: i) a Conditional Variational Auto-encoder with an extra encoder to approximate the posterior distribution of the latent variable; and ii) an Alternating Back-Propagation technique, which directly samples the latent variable from the true posterior distribution. Qualitative and quantitative results on six challenging RGB-D benchmark datasets show our approach's superior performance in learning the distribution of saliency maps. The source code is publicly available via our project page: https://github.com/JingZhang617/UCNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O BJECT-level saliency detection (i.e., salient object detection) involves separating the most conspicuous objects that attract human attention from the background <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b8">[9]</ref>. Recently, visual saliency detection from RGB-D images has attracted lots of interests due to the importance of depth information in the human vision system and the popularity of depth sensing technologies <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b14">[15]</ref>. With the extra depth data, conventional RGB-D saliency detection models focus on predicting one single saliency map for the RGB-D input by exploring the complementary information between the RGB image and the depth data.</p><p>The standard practice for RGB-D saliency detection is to train a deep neural network using ground-truth (GT) saliency maps provided by the corresponding benchmark datasets, thus formulating saliency detection as a point estimation problem by learning a mapping function Y = f (X; θ), where θ represents network parameter set, and X and Y are input RGB-D image pair and corresponding GT saliency map. Usually, the GT saliency maps are obtained through human consensus or by the dataset creators <ref type="bibr" target="#b15">[16]</ref>. Building upon large scale RGB-D datasets, deep convolutional neural network-based RGB-D saliency detection models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> have made profound progress. We argue that the way RGB-D saliency detection progresses (Email: saeed.anwar@data61.csiro.au) • Fatemeh Saleh is with the Australian National University, ACRV. (Email:</p><p>fatemehsadat.saleh@anu.edu.au) • Sadegh Aliakbarian is with the Australian National University, ACRV.</p><p>(Email: sadegh.aliakbarian@anu.edu.au) • Nick Barnes is with Research School of Engineering, the Australian National University. (Email: nick.barnes@anu.edu.au) • A preliminary version of this work appeared at CVPR 2020 <ref type="bibr" target="#b0">[1]</ref>. • Corresponding author: Deng-Ping <ref type="bibr">Fan.</ref> through the conventional pipelines <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> fails to capture the uncertainty in labeling the GT saliency maps.</p><p>According to research in human visual perception <ref type="bibr" target="#b18">[19]</ref>, visual saliency detection is subjective to some extent. Each person could have specific preferences <ref type="bibr" target="#b19">[20]</ref> in labeling the saliency map (which has been discussed in user-specific saliency detection <ref type="bibr" target="#b20">[21]</ref>). More precisely speaking, the GT labeling process is never a deterministic process, which is different from category-aware tasks, such as semantic segmentation <ref type="bibr" target="#b21">[22]</ref>, as a " <ref type="table">Table"</ref> will never be ambiguously labeled as "Cat", while the salient foreground for one annotator may be defined as background by other annotators as shown in the second row of <ref type="figure">Fig. 1</ref>.</p><p>In <ref type="figure">Fig. 1</ref>, we present the GT saliency map and other candidate salient regions (produced by our CVAE-based method, which will be introduced in Section 3.2) that may attract human attention. <ref type="figure">Fig. 1</ref> shows that the deterministic mapping (from "Image" to "GT") may lead to an "over-confident" model, as the provided "GT" may be biased as shown in the second row of <ref type="figure">Fig. 1</ref>. To overcome this, instead of performing point estimation, we are interested in how the network achieves distribution estimation with diverse saliency maps produced 1 , capturing the uncertainty of human annotation. Furthermore, in practice, it is more desirable to have multiple saliency maps produced to reflect human uncertainty instead of a single saliency map prediction for subsequent tasks.</p><p>Inspired by human perceptual uncertainty, as well as the labeling process of saliency maps, we propose a generative architecture to achieve probabilistic RGB-D saliency detection with a latent variable z modeling human uncertainty in the annotation. Two main models are included in this framework: 1) a generator (i.e., encoder-decoder) model, which maps the input RGB-D data and latent variable to stochastic saliency prediction; and 2) an inference model, which progressively refreshes the latent variable. <ref type="figure">Fig. 1</ref>. GT compared with our predicted saliency maps. For simple context image (first row), we can produce consistent predictions. When dealing with complex scenarios where there exists uncertainties in salient regions (second row), our model can produce diverse predictions ("Our CVAE Samples"), where "Our CVAE" is our deterministic prediction after the saliency consensus module, which will be introduced in Section 3.3.</p><p>To infer the latent variable, we introduce two different strategies:</p><p>• A Conditional Variational Auto-encoder (CVAE) <ref type="bibr" target="#b22">[23]</ref> based model with an additional encoder to approximate the posterior distribution of the latent variable.</p><p>• The Alternating Back-Propagation (ABP) <ref type="bibr" target="#b23">[24]</ref> based technique, which directly samples the latent variable from the true posterior distribution via Langevin Dynamics based Markov chain Monte Carlo (MCMC) sampling <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. This paper is an extended version of our conference paper, UC-Net <ref type="bibr" target="#b0">[1]</ref>. In particular, UC-Net focuses on generating saliency maps via CVAE and augmented ground-truth to model diversity and to avoid posterior collapse problem <ref type="bibr" target="#b26">[27]</ref>. While UC-Net showed promising performance by modeling such variations, it still has a number of shortcomings. Firstly, UC-Net requires engineering efforts (ground-truth augmentation) to model diversity and achieve stabilized training (mitigating posterior collapse). Here, we use a simpler technique to achieve the same goal, by using the standard KL-annealing strategy <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> with less human intervention. Experimental results in <ref type="figure" target="#fig_9">Fig. 13</ref> clearly illustrate the effectiveness of the KL-annealing strategy. Secondly, we improve the quality of the generated saliency maps by designing a more expressive decoder that benefits from spatial and channel attention mechanisms <ref type="bibr" target="#b29">[30]</ref>. Thirdly, inspired by <ref type="bibr" target="#b22">[23]</ref> we modify the cost function of UC-Net to reduce the discrepancy in encoding the latent variable at training and test time, which is elaborated in Section 3.</p><p>Moreover, CVAE-based methods approximate the posterior distribution via an inference model (or an encoder) and optimize the evidence lower bound (ELBO). The lower bound is simply the composition of the reconstruction loss and the divergence between the approximate posterior and prior distribution. If the model focuses more on optimizing the reconstruction quality, the latent space may fail to learn meaningful representation. On the other hand, if the model focuses more on reducing the divergence between the approximate posterior and prior distribution, the model may sacrifice the reconstruction quality. Additionally, since the model approximates the posterior distribution rather than modeling the true posterior, it may lose expressivity in general. Here, we propose to use Alternating Back-Propagation (ABP) technique <ref type="bibr" target="#b23">[24]</ref> that directly samples latent variables from the true posterior. While it is much simpler, our experimental results show ABP leads to impressive result for generating saliency maps. Note that both CVAE-based and ABP-based solutions can produce stochastic saliency predictions by modeling output space distribution as a generative model conditioned on the input RGB-D image pair. Similar to UC-Net, during the testing phase, a saliency consensus module is introduced to mimic the majority voting mechanism for GT saliency map generation, and generate one single saliency map in the end for performance evaluation. Finally, in addition to producing state-of-the-art results, our experiments provide a thorough evaluation of the different components of our model as well as an extensive study on the diversity of the generated saliency maps.</p><p>Our main contributions are summarized as: 1) We propose the first uncertainty inspired probabilistic RGB-D saliency prediction model with a latent variable z introduced to the network to represent human uncertainty in annotation; 2) We introduce two different schemes to infer the latent variable, including a CVAE <ref type="bibr" target="#b22">[23]</ref> framework with an additional encoder to approximate the posterior distribution of z and an ABP <ref type="bibr" target="#b23">[24]</ref> pipeline, which samples the latent variable directly from its true posterior distribution via Langevin dynamics based Markov chain Monte Carlo (MCMC) sampling <ref type="bibr" target="#b25">[26]</ref>. Each of them can model the conditional distribution of output, and lead to diverse predictions during testing; 3) Extensive experimental results on six RGB-D saliency detection benchmark datasets demonstrate the effectiveness of our proposed solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we first briefly review existing RGB-D saliency detection models. We then investigate existing generative models, including Variational Auto-encoder (VAE) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and Generative Adversarial Networks (GAN) <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. We also highlight the uniqueness of the proposed solutions in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RGB-D Saliency Detection</head><p>Depending on how the complementary information of RGB images and depth data is fused, existing RGB-D saliency detection models can be roughly classified into three categories: earlyfusion models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b33">[34]</ref>, late-fusion models <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b34">[35]</ref> and crosslevel fusion models <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b42">[43]</ref>. The first solution directly concatenates the RGB image with its depth information, forming a four-channel input, and feed it to the network to obtain both the appearance information and geometric information. <ref type="bibr" target="#b33">[34]</ref> proposed an early-fusion model to generate features for each superpixel of the RGB-D pair, which was then fed to a CNN to produce saliency of each superpixel. The second approach treats each modality independently, and predictions from both modalities are fused at the end of the network. <ref type="bibr" target="#b34">[35]</ref> introduced a late-fusion network (i.e., AFNet) to fuse predictions from the RGB and depth branch adaptively. In a similar pipeline, <ref type="bibr" target="#b17">[18]</ref> fused the RGB and depth information through fully connected layers. The third one fuses intermediate features of each modality by considering correlations of the above two modalities. To achieve this, <ref type="bibr" target="#b35">[36]</ref> presented a complementary-aware fusion block. <ref type="bibr" target="#b16">[17]</ref> designed attention-aware cross-level combination blocks to obtain complementary information of each modality. <ref type="bibr" target="#b10">[11]</ref> employed a fluid pyramid integration framework to achieve multi-scale crossmodal feature fusion. <ref type="bibr" target="#b12">[13]</ref> designed a self-mutual attention model to effectively fuse RGB and depth information. Similarly, <ref type="bibr" target="#b11">[12]</ref> presented a complimentary interaction module (CIM) to select complementary representation from the RGB and depth data. <ref type="bibr" target="#b13">[14]</ref> provided joint learning and densely-cooperative fusion framework for complementary feature discovery. <ref type="bibr" target="#b14">[15]</ref> introduced a depth distiller to transfer the depth knowledge from the depth stream to the RGB stream to achieve a lightweight architecture without use of depth data at test time. A comprehensive survey can be found in <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">VAE or CVAE-based Deep Probabilistic Models</head><p>Ever since the seminal work by Kingma et al. <ref type="bibr" target="#b30">[31]</ref> and Rezende et al. <ref type="bibr" target="#b44">[45]</ref>, VAE and its conditional counterpart CVAE <ref type="bibr" target="#b22">[23]</ref> have been widely applied in various computer vision problems. A typical VAE-based model consists of an encoder, a decoder, and a loss function. The encoder is a neural network with weights and biases θ, which maps the input datapoint X to a latent (hidden) representation z. The decoder is another neural network with weights and biases φ, which reconstructs the datapoint X from z. To train a VAE, a reconstruction loss and a regularizer are needed to penalize the disagreement of the latent representation's prior and posterior distribution. Instead of defining the prior distribution of the latent representation as a standard Gaussian distribution, CVAE-based networks utilize the input observation to modulate the prior on Gaussian latent variables to generate the output.</p><p>In low-level vision, VAE and CVAE have been applied to tasks such as latent representations with sharp samples <ref type="bibr" target="#b45">[46]</ref>, difference of motion modes <ref type="bibr" target="#b46">[47]</ref>, medical image segmentation models <ref type="bibr" target="#b47">[48]</ref>, and modeling inherent ambiguities of an image <ref type="bibr" target="#b48">[49]</ref>. Meanwhile, VAE and CVAE have been explored in more complex vision tasks such as uncertain future forecast <ref type="bibr" target="#b49">[50]</ref>, salient feature enhancement <ref type="bibr" target="#b50">[51]</ref>, human motion prediction <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, and shape-guided image generation <ref type="bibr" target="#b53">[54]</ref>. Recently, VAE and CVAE have been extended to 3D domain targeting applications such as 3D meshes deformation <ref type="bibr" target="#b54">[55]</ref>, and point cloud instance segmentation <ref type="bibr" target="#b55">[56]</ref>. For saliency detection, <ref type="bibr" target="#b56">[57]</ref> adopted VAE to model image background, and separated salient objects from the background through the reconstruction residuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GAN or CGAN-based Dense Models</head><p>GAN <ref type="bibr" target="#b31">[32]</ref> and its conditional counterparts <ref type="bibr" target="#b32">[33]</ref> have also been used in dense prediction tasks. Existing GAN-based dense prediction models mainly focus on two directions: 1) using GANs in a fully supervised manner <ref type="bibr" target="#b57">[58]</ref>- <ref type="bibr" target="#b61">[62]</ref> and treat the discriminator loss as a higher-order regularizer for dense prediction; or 2) apply GANs to 'semi-supervised scenarios <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, where the output of the discriminator serves as guidance to evaluate the degree of the unsupervised sample participating in network training. In saliency detection, following the first direction, <ref type="bibr" target="#b64">[65]</ref> introduced a discriminator in the fixation prediction network to distinguish predicted fixation map and ground-truth. Different from the above two directions, <ref type="bibr" target="#b65">[66]</ref> adopted GAN in a RGB-D saliency detection network to explore the intra-modality (RGB, depth) and crossmodality simultaneously. <ref type="bibr">[</ref> distinguish real saliency map (group truth) and fake saliency map (prediction), thus structural information can be learned without CRF <ref type="bibr" target="#b67">[68]</ref> as post-processing technique. <ref type="bibr" target="#b68">[69]</ref> adopted CycleGAN <ref type="bibr" target="#b69">[70]</ref> as an domain adaption technique to generate pseudo-NIR image for existing RGB saliency dataset and achieve multispectral image salient object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Uniqueness of Our Solutions</head><p>To the best of our knowledge, generative models have not been exploited in saliency detection to model annotation uncertainty, except for our preliminary version <ref type="bibr" target="#b0">[1]</ref>. As a conditional latent variable model, two different solutions can be used to infer the latent variable. One is CVAE-based <ref type="bibr" target="#b22">[23]</ref> method (the one we used in the preliminary version <ref type="bibr" target="#b0">[1]</ref>), which infers the latent variable using Variational Inference, and another one is MCMC based method, which we propose to use in this work. Specifically, we present a new latent variable inference solution with less parameter load based on the alternating back-propagation technique <ref type="bibr" target="#b23">[24]</ref>. CVAE-based models infer the latent variable through finding the ELBO of the log-likelihood to avoid MCMC as it was too slow in the non-deep-learning era. In other words, CVAEs approximates Maximum Likelihood Estimation (MLE) by finding the ELBO with an extra encoder. The main issue of this strategy is "posterior collapse" <ref type="bibr" target="#b26">[27]</ref>, where the latent variable is independent of network prediction, making it unable to represent the uncertainty of human annotation. We introduced the "New Label Generation" strategy in our preliminary version <ref type="bibr" target="#b0">[1]</ref> as an effective way to avoid posterior collapse problem. In this extended version, we propose a much simpler strategy using the KL annealing strategy <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, which slowly introduces the KL loss term to the loss function with an extra weight. The experimental results show that this simple strategy can avoid the posterior collapse problem with the provided single GT saliency map.</p><p>Besides the KL annealing term, we introduce ABP <ref type="bibr" target="#b23">[24]</ref> as an alternative solution to prevent posterior collapse in the network. ABP introduces gradient-based MCMC and updates the latent variable with gradient descent back-propagation to directly train the network targeting MLE. Compared with CVAE, ABP samples latent variables directly from its true posterior distribution, making it more accurate in inferring the latent variable. Furthermore, no assistant network (the additional encoder in CVAE) used in ABP, which leads to smaller network parameter load.</p><p>We introduce ABP-based inference model as an extension to the CVAE-based pipeline <ref type="bibr" target="#b0">[1]</ref>. Experimental results show that both solutions can effectively estimate the latent variable, leading to stochastic saliency predictions. Details of the two inference models are introduced in Section 3.2. <ref type="figure">Fig. 3</ref>. Details of the "Generator Model", which takes image X and latent variable z as input, and produce stochastic saliency map S, where "S1-S4" represent the four convolutional blocks of our backbone network. "DASPP" is the DenseASPP module <ref type="bibr" target="#b70">[71]</ref>, "PAM" and "CAM" are position attention and channel attention module <ref type="bibr" target="#b29">[30]</ref>, "RCA" is the Residual Channel Attention operation from <ref type="bibr" target="#b71">[72]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR MODEL</head><p>In this section, we present our probabilistic RGB-D saliency detection model, which learns the underlying conditional distribution of saliency maps rather than a mapping function from RGB-D input to a single saliency map.</p><formula xml:id="formula_0">Let D = {X i , Y i } N i=1</formula><p>be the training dataset, where X i denotes the RGB-D input, Y i denotes the GT saliency map, and N denotes the total number of images in the dataset. We intend to model P ω (Y |X, z), where z is a latent variable representing the inherent uncertainty in salient regions which can be also seen in how a human annotates salient objects. Our framework utilizes two main components during training: 1) a generator model, which maps input RGB-D X and latent variable z to conditional prediction P ω (Y |X, z); and 2) an inference model, which infers the latent variable z. During testing, we can sample multiple latent variables from the learned prior distribution P θ (z|X) to produce stochastic saliency prediction. The whole pipeline of our model during training and testing is illustrated in <ref type="figure">Fig. 2</ref> (a) and (b) respectively. Specifically, during training, the model learns saliency from the "Generator Model", and updates the latent variable with the "Inference Model". During testing, we sample from the "Prior" distribution of the latent variable to obtain stochastic saliency predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generator Model</head><p>The Generator Model takes X and latent variable z as input, and produces stochastic prediction S = P ω (Y |X, z), where ω is the parameter set of the generator model. We choose ResNet50 <ref type="bibr" target="#b72">[73]</ref> as our backbone, which contains four convolutional blocks. To enlarge the receptive field, we follow DenseASPP <ref type="bibr" target="#b70">[71]</ref> to obtain a feature map with the receptive field of the whole image on each stage of the backbone network. We then gradually concatenate the two adjacent feature maps in a top-down manner and feed it to a "Residual Channel Attention" module <ref type="bibr" target="#b71">[72]</ref> to obtain stochastic saliency map S. As illustrated in <ref type="figure">Fig. 3</ref>, our generator model follows the recent progress in dense prediction problems such as semantic segmentation <ref type="bibr" target="#b21">[22]</ref>, via a proper use of a hybrid attention mechanism. To this end, our generator model benefits from two types of attention: a Position Attention Module <ref type="bibr" target="#b29">[30]</ref> and a Channel Attention Module <ref type="bibr" target="#b29">[30]</ref>. The former aims to capture the spatial dependencies between any two locations of the feature map, while the latter aims to capture the channel dependencies between any two channel in the feature map. We follow <ref type="bibr" target="#b29">[30]</ref> to aggregate and fuse the outputs of these two attention modules to further enhance the feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference Model</head><p>We propose two different solutions to infer or update the latent variable z: 1) A CVAE-based <ref type="bibr" target="#b22">[23]</ref> pipeline, in which we approximate the posterior distribution via a neural network (i.e., the encoder); and 2) An ABP <ref type="bibr" target="#b23">[24]</ref> based strategy to sample directly from the true posterior distribution of z via Langevin Dynamics based MCMC <ref type="bibr" target="#b24">[25]</ref>.</p><p>Infer z with CVAE: The Variational Auto-encoder <ref type="bibr" target="#b30">[31]</ref> is a directed graphical model and typically comprise of two fundamental components, an encoder that maps the input variable X to the latent space Q φ (z|X), where z is a low dimensional Gaussian variable and a decoder that reconstructs X from z to get P ω (X|z). To train the VAE, a reconstruction loss and a regularizer to penalize the disagreement of the prior and the approximate posterior distribution of z are utilized as:</p><formula xml:id="formula_1">L VAE = E z∼Q φ (z|X) [− log P ω (X|z)] +D KL (Q φ (z|X)||P (z)),<label>(1)</label></formula><p>where the first term is the reconstruction loss, or the expected negative log-likelihood, and the second term is a regularizer, which is Kullback-Leibler divergence D KL (Q φ (z|X)||P (z)) to reduce the gap between the normally distributed prior P (z) and the approximate posterior Q φ (z|X). The expectation E z∼Q φ (z|X) is taken with the latent variable z generated from the approximate posterior distribution Q φ (z|X). Different from the VAE, which model marginal likelihood (P (X) in particular) with a latent variable generated from the standard normal distribution, the CVAE <ref type="bibr" target="#b22">[23]</ref> modulates the prior of latent variable z as a Gaussian distribution with parameters conditioned on the input data X. There are three types of variables in the conditional generative model: conditioning variable, latent variable, and output variable. In our saliency detection scenario, we define output as the saliency prediction Y , and latent variable as z. As our output Y is conditioned on the input RGB-D data X, we then define the input X as the conditioning variable. For the latent variable z drawn from the Gaussian distribution P θ (z|X), the output variable Y is generated from P ω (Y |X, z), then the posterior of z is formulated as Q φ (z|X, Y ), representing feature embedding of the given input-output pair (X, Y ).</p><p>The loss of CVAE is defined as:</p><formula xml:id="formula_2">L CVAE = E z∼Q φ (z|X,Y ) [− log P ω (Y |X, z)] +λ kl * D KL (Q φ (z|X, Y )||P θ (z|X)),<label>(2)</label></formula><p>where P ω (Y |X, z) is the likelihood of P (Y ) given latent variable z and conditioning variable X, the Kullback-Leibler divergence D KL (Q φ (z|X, Y )||P θ (z|X)) works as a regularization loss to reduce the gap between the prior P θ (z|X) and the auxiliary posterior Q φ (z|X, Y ). Furthermore, to prevent the possible posterior collapse problem as mentioned in Section 2.4, we introduce a linear KL annealing <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> term λ kl as weight for the KL loss term D KL , which is defined as λ kl = ep/N ep , where ep is current epoch, and N ep is the maximum epoch The "Generator Model" is shown in <ref type="figure">Fig. 3</ref>. During training, we sample from both posterior net z ∼ Q φ (z|X, Y ) and prior net z ∼ P θ (z|X) to obtain predictions S CV AE and S GSN N respectively. During testing, S GSN N is our prediction.</p><p>number. In this way, during training, the CVAE aims to model the conditional log likelihood of prediction under encoding error</p><formula xml:id="formula_3">D KL (Q φ (z|X, Y )||P θ (z|X))</formula><p>. During testing, we can sample from the prior network P θ (z|X) to obtain stochastic predictions. As explained in <ref type="bibr" target="#b22">[23]</ref>, the conditional auto-encoding of output variables at training may not be optimal to make predictions at test time, as the CVAE uses a posterior of z (z ∼ Q φ (z|X, Y )) for the reconstruction loss in the training stage, while it uses the prior of z (z ∼ P θ (z|X)) during testing. One solution to mitigate the discrepancy in encoding the latent variable at training and testing is to allocate more weights to the KL loss term (e.g., λ kl ). Another solution is setting the posterior network the same as the prior network, i.e., Q φ (z|X, Y ) = P θ (z|X), and we can sample the latent variable z directly from prior network in both training and testing stages. We call this model the "Gaussian Stochastic Neural Network" (GSNN) <ref type="bibr" target="#b22">[23]</ref>, and the objective function is:</p><formula xml:id="formula_4">L GSNN = E z∼P θ (z|X) [− log P ω (Y |X, z)].<label>(3)</label></formula><p>We can combine the two objective functions introduced above (L CVAE and L GSNN ) to obtain a hybrid objective function:</p><formula xml:id="formula_5">L Hybrid = αL CVAE + (1 − α)L GSNN<label>(4)</label></formula><p>Following the standard practice of CVAE <ref type="bibr" target="#b22">[23]</ref>, we design a CVAE-based RGB-D saliency detection pipeline as shown in <ref type="figure" target="#fig_1">Fig.  4</ref>. The two inference models (Q φ (z|X, Y ) and P θ (z|X)) share same structure as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, except for Q φ (z|X, Y ), we have concatenation of X and Y as input, while P θ (z|X) takes X as input. Let's define P θ (z|X) as PriorNet, which maps the input RGB-D data X to a low-dimensional latent feature space, where θ is the parameter set of PriorNet. With the provided GT saliency map Y , we define Q φ (z|X, Y ) as PosteriorNet, with φ being the network parameter set. We use five convolutional layers and two fully connected layers to map the input RGB-D image X (or concatenation of X and Y for PosteriorNet) to the statistics of the latent space: (µ prior , σ prior ) for PriorNet and (µ post , σ post ) for PosteriorNet respectively. Then the corresponding latent vector z can be achieved with the reparameteration trick: z = µ + σ · , where ∼ N (0, I).</p><p>According to Eq. 4, the KL-divergence in L CVAE is used to measure the distribution mismatch between the P θ (z|X) and Q φ (z|X, Y ), or how much information is lost when using Q φ (z|X, Y ) to represent P θ (z|X). The GSNN loss term L GSNN , on the other hand, can mitigate the discrepancy in encoding the latent variable during training and testing. The hybrid loss in Eq. 4 can achieve structured outputs with hyper-parameter α to balance the two objective functions in Eq. 2 and Eq. 3. . Detailed structure of inference models, where K is dimension of the latent space, "c1 4K" represents a 1×1 convolutional layer of output channel size 4 × K, "fc" represents the fully connected layer. Inferential back-propagation: For each i, run l steps of Langevin Dynamics to sample zi ∼ Pω(zi|Yi, Xi) following Eq. 8, with zi initialized as Gaussian white noise (first iteration) or obtained from previous iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learning Stochastic Saliency via Alternating Backpropagation</head><formula xml:id="formula_6">Input: Training dataset D = {(Xi, Yi)} N i=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Learning back-propagation: Update model parameters via:</p><formula xml:id="formula_7">ω ← ω + γ ∂L(ω)</formula><p>∂ω , where the gradient of L(ω) can be obtained through stochastic gradient descent. <ref type="bibr" target="#b4">5</ref>: end for Infer z with ABP: As mentioned earlier, one drawback of CVAE-based models is the posterior collapse problem <ref type="bibr" target="#b26">[27]</ref>, where the model learns to ignore the latent variable, thus it becomes independent of the prediction Y , as Q φ (z|X, Y ) will simply collapse to P θ (z|X), and z embeds no information about the prediction. In our scenario, the "Posterior Collapse" phenomenon can be interpreted as the fact that the latent variable z fails to capture the inherent human uncertainty in the annotations. To this end, we propose another alternative solution based on alternating back-propagation <ref type="bibr" target="#b23">[24]</ref>. Instead of approximating the posterior of z with an encoder network as in a CVAE, we directly sample z from its true posterior distribution via gradient based MCMC.</p><p>Alternating Back-Propagation <ref type="bibr" target="#b23">[24]</ref> was introduced for learning the generator network model. It updates the latent variable and network parameters in an EM-manner. Firstly, given network prediction with the current parameter set, it infers the latent variable by Langevin dynamics based MCMC, which they call "Inferential back-propagation" <ref type="bibr" target="#b23">[24]</ref>. Secondly, given the updated latent variable, the network parameter set is updated with gradient descent, and they call it "Learning back-propagation" <ref type="bibr" target="#b23">[24]</ref>. Following the previous variable definitions, given the training example (X, Y ), we intend to infer z and learn the network parameter ω to minimize the reconstruction error as well as a regularization term that corresponds to the prior on z.</p><p>As a non-linear generalization of factor analysis, the conditional generative model aims to generalize the mapping from continuous latent variable z to the prediction Y conditioned on the input image X. As in traditional factor analysis, we define our generative model as: where P (z) is the prior distribution of z. The conditional distribution of Y given X is P ω (Y |X) = p(z)P ω (Y |X, z)dz with the latent variable z integrated out. We define the observed-data loglikelihood as</p><formula xml:id="formula_8">z ∼ P (z) = N (0, I), (5) Y = f ω (X, z) + , ∼ N (0, diag(σ) 2 ),<label>(6)</label></formula><formula xml:id="formula_9">L(ω) = n i=1 log P ω (Y i |X i ), where the gradient of P ω (Y |X) is defined as: ∂ ∂ω log P ω (Y |X) = 1 P ω (Y |X) ∂ ∂ω P ω (Y |X) = E Pω(z|X,Y ) ∂ ∂ω log P ω (Y, z|X) .<label>(7)</label></formula><p>The expectation term E Pω(z|X,Y ) can be approximated by drawing samples from P ω (z|X, Y ), and then computing the Monte Carlo average. This step corresponds to inferring the latent variable z. Following ABP <ref type="bibr" target="#b23">[24]</ref>, we use Langevin Dynamics based MCMC (a gradient-based Monte Carlo method) to sample z, which iterates:</p><formula xml:id="formula_10">z t+1 = z t + s 2 2 ∂ ∂z log P ω (Y, z t |X) + sN (0, I d ), (8) with ∂ ∂z log P ω (Y, z|X) = 1 σ 2 (Y −f ω (X, z)) ∂ ∂z f ω (X, z)−z,<label>(9)</label></formula><p>where t is the time step for Langevin sampling, and s is the step size. The whole pipeline of inferring latent variable z via ABP is shown in Algorithm 1. Analysis of two inference models: Both the CVAE-based <ref type="bibr" target="#b22">[23]</ref> inference model and ABP-based <ref type="bibr" target="#b23">[24]</ref> strategy can infer latent variable z, where the former one approximates the posterior distribution of z with an extra encoder, while the latter solution targets at MLE by directly sampling from the true posterior distribution. As mentioned above, the CVAE-based solution may suffer from posterior collapse <ref type="bibr" target="#b26">[27]</ref>, where the latent variable z is independent of the prediction, making it unable to represent the uncertainty of labeling. To prevent posterior collapse, we adopt the KL annealing strategy <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and let the KL loss term in Eq. 2 gradually contribute to the CVAE loss function. On the contrary, the ABP-based solution suffers no posterior collapse problem, which leads to simpler and more stable training, where the latent variable z is updated based on the current prediction. In both of our proposed solutions, with the inferred Gaussian random variable z, our model can lead to stochastic prediction, with z representing labeling variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Output Estimation</head><p>Once the generative model parameters are learned, our model can produce prediction from input X following the generative process of the conditional generative model. With multiple iterations of sampling, we can obtain multiple saliency maps from the same input X. To evaluate performance of the generative network, we need to estimate the deterministic prediction of the structured outputs. Inspired by <ref type="bibr" target="#b22">[23]</ref>, our first solution is to simply average the multiple predictions. Alternatively, we can obtain multiple z from the prior distribution, and define the deterministic prediction as Y = f ω (X, E(z)), where E(z) is the mean of the multiple latent variable. Inspired by how the GT saliency map is obtained (e.g., Majority Voting), we introduce a third solution, namely "Saliency Consensus Module", which is introduced in detail. Saliency Consensus Module: To prepare a training dataset for saliency detection, multiple annotators are asked to label one image, and the majority <ref type="bibr" target="#b15">[16]</ref> of saliency regions is defined as being salient in the final GT saliency map.</p><p>Although the way in which the GT is acquired is well known in the saliency detection community yet, there exists no research on embedding this mechanism into deep saliency frameworks. The main reason is that current models define saliency detection as a point estimation problem instead of a distribution estimation problem, and the final single saliency map can not be further processed to achieve "majority voting". We, instead, design a stochastic learning pipeline to obtain the conditional distributions of prediction, which makes it possible to perform a similar strategy as preparing the training data to generate deterministic prediction for performance evaluation. Thus, we introduce the saliency consensus module to compute the majority of different predictions in the testing stage as shown in <ref type="figure">Fig. 2 (b)</ref>.</p><p>During testing, we sample z from PriorNet (for the CVAEbased inference model) or directly sample it from a standard Gaussian distribution N (0, I), and feed it to the "Generator Model" to produce stochastic saliency prediction as shown in <ref type="figure">Fig.  2 (b)</ref>. With C different samplings, we can obtain C predictions P 1 , ..., P C . We simultaneously feed these multiple predictions to the saliency consensus module to obtain the consensus of predictions for performance evaluation.</p><p>Given multiple predictions {P c } C c=1 , where P c ∈ [0, 1], we first compute the binary 2 version P c b of the predictions by performing adaptive thresholding <ref type="bibr" target="#b73">[74]</ref> on P c . For each pixel (u, v), we obtain a C dimensional feature vector P u,v ∈ {0, 1}. We define P mjv b ∈ {0, 1} as a one-channel saliency map representing the majority of P u,v , which is defined as:</p><formula xml:id="formula_11">P mjv b (u, v) =            1, C c=1 P c b (u, v)/C ≥ 0.5, 0, C c=1 P c b (u, v)/C &lt; 0.5.<label>(10)</label></formula><p>We define an indicator</p><formula xml:id="formula_12">1 c (u, v) = 1(P c b (u, v) = P mjv b</formula><p>(u, v)) representing whether the binary prediction is consistent with the majority of the predictions.</p><formula xml:id="formula_13">If P c b (u, v) = P mjv b (u, v), then 1 c (u, v) = 1. Otherwise, 1 c (u, v) = 0.</formula><p>We obtain one gray saliency map after saliency consensus as:</p><formula xml:id="formula_14">P mjv g (u, v) = C c=1 (P c b (u, v) × 1 c (u, v)) C c=1 1 c (u, v) .<label>(11)</label></formula><p>We show one toy example with C = 3 in <ref type="figure" target="#fig_3">Fig. 6</ref> to illustrate how the saliency consensus module works. As shown in <ref type="figure">Fig.   2</ref>. As the GT map Y ∈ {0, 1}, we produce a series of binary predictions with each one representing annotation from one saliency annotator. 6, given three gray-scale predictions (illustrated in blue), we perform adaptive thresholding to obtain three different binary predictions (illustrated in orange). Then we compute a majority matrix (illustrated in purple), which is also binary, with each pixel representing majority prediction of the specific coordinate. Finally, after the saliency consensus module, our final gray-scale prediction is computed based on mean of those pixels agreed (when P c b (u, v) = P mjv b (u, v), we mean in location u, v, the prediction agrees with the majority) with the majority matrix, and ignore others. For example, the majority of saliency in coordinate (1, 1) is 1, we obtain the gray prediction after the saliency consensus module as (0.9 + 0.7)/2 = 0.8, where 0.9 and 0.7 are predictions in (1, 1) of the first and third predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss function</head><p>We introduce two different inference models to update the latent variable z: a CVAE-based model as shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, and an ABP-based strategy as shown in Algorithm 1. To further highlight structure accuracy of the prediction, we introduce smoothness loss based on the assumption that pixels inside a salient object should have a similar saliency value, and sharp distinction happens along object edges.</p><p>As an edge-aware loss, smoothness loss was initially introduced in <ref type="bibr" target="#b74">[75]</ref> to encourage disparities to be locally smooth with an L1 penalty on the disparity gradients. It was then adopted in <ref type="bibr" target="#b75">[76]</ref> to recover optical flow in the occluded area by using an image prior. We adopt smoothness loss to achieve a saliency map of high intraclass similarity, with consistent saliency prediction inside salient objects, and distinction happens along object edges. Following <ref type="bibr" target="#b75">[76]</ref>, we define first-order derivatives of the saliency map in the smoothness term as</p><formula xml:id="formula_15">L Smooth = u,v d∈ − → x , − → y Ψ(|∂ d P u,v |e −α|∂ d Ig(u,v)| ),<label>(12)</label></formula><p>where Ψ is defined as Ψ(s) = √ s 2 + 1e −6 , P u,v is the predicted saliency map at position (u, v), and Ig(u, v) is the image intensity, d indexes over partial derivative in − → x and − → y directions. We set α = 10 in our experiments following the setting in <ref type="bibr" target="#b75">[76]</ref>. We need to compute intensity Ig of the image in the smoothness loss, as shown in Eq. <ref type="bibr" target="#b11">(12)</ref>. To achieve this, we follow a saliency-preserving <ref type="bibr" target="#b76">[77]</ref> color image transformation strategy and convert the RGB image I to a gray-scale intensity image Ig as:</p><formula xml:id="formula_16">Ig = 0.2126 × I lr + 0.7152 × I lg + 0.0722 × I lb ,<label>(13)</label></formula><p>where I lr , I lg , and I lb represent the color components in the linear color space after Gamma function be removed from the original color space. I lr is achieved via: </p><formula xml:id="formula_17">I lr =        I</formula><p>where I r is the original red channel of image I, and we compute I g and I b in the same way as Eq. <ref type="formula" target="#formula_1">(14)</ref>. CVAE Inference Model based Loss Function: For the CVAEbased inference model, we show its loss function in Eq. 4, where the negative log-likelihood loss measures the reconstruction error.</p><p>To preserve structure information and penalize wrong predictions along object boundaries, we adopt the structure-aware loss in <ref type="bibr" target="#b6">[7]</ref>. The structure-aware loss is a weighted extension of cross-entropy loss, which integrates the boundary IOU loss <ref type="bibr" target="#b77">[78]</ref> to highlight the accuracy of boundary prediction. With smoothness loss L Smooth and CVAE loss L Hybrid , our final loss function for the CVAE-based framework is defined as:</p><formula xml:id="formula_19">L CV AE sal = L Hybrid + λ 1 L Smooth .<label>(15)</label></formula><p>We tested λ 1 in the range of [0.1, 0.2, . . . , 0.9, 1.0], and found ralatively better performance with λ 1 = 0.3. ABP Inference Model based Loss Function: As there exists no extra encoder for the posterior distribution estimation, the loss function for the ABP inference model is simply the negative observed-data log-likelihood:</p><formula xml:id="formula_20">L ABP = − n i=1 log P ω (Y i |X i ),<label>(16)</label></formula><p>which can be the same structure-aware loss as in <ref type="bibr" target="#b6">[7]</ref> similar to CVAE-based inference model. Integrated with the above smoothness loss, we obtain the loss function for the ABP-based saliency detection model as:</p><formula xml:id="formula_21">L ABP sal = L ABP + λ 2 L Smooth .<label>(17)</label></formula><p>Similarly, we also empirically set λ 2 = 0.3 in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Datasets:</p><p>We perform experiments on six datasets including five widely used RGB-D saliency detection datasets (namely NJU2K <ref type="bibr" target="#b84">[85]</ref>, NLPR <ref type="bibr" target="#b79">[80]</ref>, SSB <ref type="bibr" target="#b89">[90]</ref>, LFSD <ref type="bibr" target="#b90">[91]</ref>, DES <ref type="bibr" target="#b81">[82]</ref>) and one newly released dataset (SIP <ref type="bibr" target="#b15">[16]</ref>).</p><p>Competing Methods: We compare our method with 18 algorithms, including ten handcrafted conventional methods and eight deep RGB-D saliency detection models. Evaluation Metrics: Four evaluation metrics are used to evaluate the deterministic predictions, including two widely used: 1) Mean Absolute Error (MAE M); 2) mean F-measure (F β ) and two recently proposed: 3) Structure measure (S-measure, S α ) [98] and 4) mean Enhanced alignment measure (E-measure, E ξ ) <ref type="bibr" target="#b78">[79]</ref>.</p><p>• MAE M: The MAE estimates the approximation degree between the saliency map Sal and the ground-truth G. It provides a direct estimate of conformity between estimated and GT map. MAE is defined as:</p><formula xml:id="formula_22">MAE = 1 N |Sal − G|,<label>(18)</label></formula><p>where N is the total number of pixels.</p><p>• S-measure S α : Both MAE and F-measure metrics ignore the important structure information evaluation, whereas behavioral vision studies have shown that the human visual system is highly sensitive to structures in scenes <ref type="bibr" target="#b97">[98]</ref>. Thus, we additionally include the structure measure (Smeasure <ref type="bibr" target="#b97">[98]</ref>). The S-measure combines the region-aware (S r ) and object-aware (S o ) structural similarity as their final structure metric:</p><formula xml:id="formula_23">S α = α * S o + (1 − α) * S r ,<label>(19)</label></formula><p>where α ∈ [0, 1] is a balance parameter and set to 0.5 as default.</p><p>• E-measure E ξ : E-measure is the recent proposed Enhanced alignment measure <ref type="bibr" target="#b78">[79]</ref> in the binary map evaluation field. This measure is based on cognitive vision  studies, which combines local pixel values with the imagelevel mean value in one term, jointly capturing image-level statistics and local pixel matching information. Here, we introduce it to provide a more comprehensive evaluation.</p><p>• F-measure F β : It is essentially a region based similarity metric. We provide the mean F-measure using varying 255  fixed (0-255) thresholds as shown in <ref type="figure" target="#fig_4">Fig. 7</ref>.</p><p>Implementation Details: We train our model using PyTorch, and initialized the encoder of the "Generator Model" with ResNet50 <ref type="bibr" target="#b72">[73]</ref> parameters pre-trained on ImageNet. Inside the "DASPP" module of the "Generator Model" in <ref type="figure">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to State-of-the-art Methods</head><p>Quantitative Comparison: We report the performance of our method (with both inference models) and competing methods in <ref type="table" target="#tab_3">Table 1</ref>, where "CVAE" is our framework with CVAE as inference model, and "ABP" represents the model that updates latent variable z with alternating back-propagation. Results in <ref type="table" target="#tab_3">Table 1</ref> demonstrate the benefits of both CVAE and ABP which consistently achieve the best performance on all datasets. Specifically, on SSB <ref type="bibr" target="#b89">[90]</ref> and SIP <ref type="bibr" target="#b15">[16]</ref>, our method achieves around a 2.5% S-measure, E-measure and F-measure performance boost and a decrease in MAE by 1.5% compared with the "Deep Models" in <ref type="table" target="#tab_3">Table 1</ref>. Moreover, compared with our preliminary version "UC-Net" <ref type="bibr" target="#b0">[1]</ref>, we observe improved performance, which indicates the effectiveness of the proposed structure. We also show E-measure and F-measure curves of competing methods and ours in <ref type="figure" target="#fig_4">Fig. 7</ref>. We observe that our method produces not only stable E-measure and F-measure but also the best performance. To further evaluate the proposed method, we compute performance of eight cutting-edge RGB saliency detection models on the RGB-D testing dataset 3 and compared with our "CVAE" based model. The results are shown in <ref type="table" target="#tab_6">Table 3</ref>, which further illustrates the superior performance of the proposed framework. Qualitative Comparisons: In <ref type="figure" target="#fig_5">Fig. 8, we</ref> show five examples comparing our method with six RGB-D saliency detection models. Salient objects in these images can be large (fifth row), small (second row) or in complex backgrounds (first, third, fourth and fifth rows). Especially for the example in the first row, the background is complex, part of the background shares similar color and texture as the salient foreground. Most of those competing methods (AFNet <ref type="bibr" target="#b34">[35]</ref>, CPFP <ref type="bibr" target="#b10">[11]</ref> and DMRA <ref type="bibr" target="#b9">[10]</ref>) failed to correctly segment the precise salient foreground, while our approach achieves better salient object detection with each of the proposed two inference models. For the image in the last row, there exists an object (i.e., green toy) that strongly stands out from its background, while the depth map can to some extent decrease the salience of such high-contrast region. All of the competing methods (DCMC <ref type="bibr" target="#b86">[87]</ref>, SE <ref type="bibr" target="#b88">[89]</ref>, AFNet <ref type="bibr" target="#b34">[35]</ref>, CPFP <ref type="bibr" target="#b10">[11]</ref> in particular) falsely detect part of the background region as being salient, whereas our accurate predictions further indicate the effectiveness of our solutions. With all the results in <ref type="figure" target="#fig_5">Fig. 8</ref>, we can see evidence of the superiority of our approach. Probabilistic Distribution Evaluation: As a probabilistic network, our models can produce a distribution of plausible saliency maps instead of a single, deterministic prediction for each input image. We argue that, for images with simple background, consistent predictions should be produced, whereas for complex images with cluttered background, we expect our model to capture the uncertainty in the saliency maps, and thus can generate diverse predictions. To evaluate performance of our model, following the active learning pipeline <ref type="bibr" target="#b98">[99]</ref>, we first generate B = 100 easy and difficult samples. To achieve this, we first adopt three different conventional saliency models (RBD <ref type="bibr" target="#b99">[100]</ref>, MR <ref type="bibr" target="#b100">[101]</ref> and GS <ref type="bibr" target="#b101">[102]</ref>, which rank among the top six conventional handcrafted feature based RGB saliency models <ref type="bibr" target="#b73">[74]</ref>), and define them as f 1, f 2 and f 3 respectively. Given image X i 4 in training dataset D, we compute its corresponding saliency map f 1(X i ), f 2(X i ) and f 3(X i ). We choose entropy as measure for image complexity. Then, we define mean saliency map of X i as P i = (f 1(X i )+f 2(X i )+f 3(X i ))/3. We define the complexity of the image as task driven (for saliency detection). Then given a ground-truth saliency map Y i and mean saliency map P i , we define foreground entropy as: −P i log P i .</p><p>We then define mean entropy as a complexity measure, and choose B images with the smallest entropy as the easy samples <ref type="bibr" target="#b3">4</ref>. We use the RGB data only. and B images with the largest entropy as the difficult samples (with B = 100). We sample Sn = 5 times from the prior distribution and compute the variance of each group. Specifically, for image pair X i , with Sn iterations of sampling, we obtain its prediction {S j i } Sn j=1 . We compute the similarity of these Sn different predictions, and treat it as prediction diversity evaluation. We show entropy and standard deviation of images in <ref type="figure" target="#fig_7">Fig. 10</ref>. Inference Time 5 Comparison: We summarize basic information of competing methods in <ref type="table" target="#tab_4">Table 2</ref> for clear comparison, including their code type and inference time. <ref type="table" target="#tab_4">Table 2</ref> shows that the inference time <ref type="bibr" target="#b5">6</ref> of our method is comparable with competing methods, which further illustrates that our model can achieve probabilistic predictions with no inference time sacrificed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Structured Output Generation</head><p>As a generative network, we introduce a latent variable z modeling uncertainty of human annotation. We further show examples of our model generating structured outputs as shown in <ref type="figure" target="#fig_6">Fig. 9</ref>. The "Our CVAE Samples" in <ref type="figure" target="#fig_6">Fig. 9</ref> represents three random samples of our method with the CVAE inference model, and "Our ABP Samples" are samples with the ABP strategy. "Our CVAE" and "Our ABP" are the deterministic predictions of our frameworks with the above two inference models obtained via our "Saliency Consensus Module". <ref type="figure" target="#fig_6">Fig. 9</ref> shows that both the two inference models can produce reasonable stochastic predictions, and the final deterministic prediction after the "Saliency Consensus Module" ("Our CVAE" and "Our ABP") is consistent with the provided GT, which verifies effectiveness of both our latent variable and the "Saliency Consensus Module".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>We further analyse the proposed framework in this section, including the generative network related strategies, the loss functions, the alternative depth data (HHA <ref type="bibr" target="#b102">[103]</ref> in particular), and the solution to prevent network from posterior collapse. We show the performance in <ref type="table">Table 4</ref>. Note that unless otherwise stated, we use the CVAE-based inference model in the following experiments. Different Fusion Schemes: The latent variable z can be fused to the network in three different ways: early fusion (in the input layer), middle fusion (in bottleneck network), or late fusion (before the output layer). We propose an early fusion model as shown in <ref type="figure">Fig. 11 (a)</ref>. We further design a middle fusion models and a late fusion model as shown in <ref type="figure">Fig. 11 (b)</ref> and (c) respectively. The performance of each model is shown in <ref type="table">Table 4</ref> "Middle" and "Late". For the middle fusion model, last convolutional layer of the fourth group (e.g., S4) of the backbone network is fed to a 1 × 1 convolutional layer to obtain a M = 32 dimensional feature map, which is then map to a K (dimension of the latent variable z) dimensional feature vector with a fully connected layer ("fc"). To avoid posterior collapse <ref type="bibr" target="#b26">[27]</ref>, inspired by <ref type="bibr" target="#b51">[52]</ref>, we mix ("Mixup") the feature vector and z channel-wise; thus, the network cannot distinguish between features of the deterministic branch and the probabilistic branch. We then expand the mixed feature vector in the spatial dimension, and feed it to another 1 × 1 convolutional layer to achieve feature map S4' of the same dimension as S4, <ref type="bibr" target="#b4">5</ref>. Conventional handcrafted-feature based methods are implemented on CPU, and deep RGB-D saliency prediction models are based on GPU, thus we report CPU time for the former and GPU time for the later.</p><p>6. The inference time we report represents prediction with one random sampling from the PriorNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>Evaluation of the effect of different components in our models, and alternative structures. We present mean F β and mean E ξ .   and replace S4 with S4' in <ref type="figure">Fig. 3</ref>. For the late fusion model, the "Generator Model" represents the generator model in <ref type="figure">Fig. 3</ref> before the last "RCA" module. We expand z in spatial dimension and concatenate it with the deterministic feature. We also perform "Mixup" here similar to the middle fusion model. We then feed the mixed feature map to one "RCA" module and "DASPP" model to achieve prediction S. We observe slightly worse performance of the middle fusion model ("Middle") and late fusion model ("Late"). The main reason is that strong non-linear representation can be obtained when the latent variable is fed to the beginning of the network, which is also consistent with the result that "Middle" is better than "Late". Analysing the Effect of the Dimension of z: The scale of z may influence both network performance and diversity of predictions. In this paper, we set dimension of z to 3. We further carry out experiments with dimension of z in the range of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>, and show mean absolution error of our model on six benchmark RGB-D saliency dataset in <ref type="figure">Fig. 12</ref>. We observe relatively stable performance for different dimension of z. The relatively stable performance regardless of dimension of z shows that the capacity of the network is large enough to take different degree of stochasticity in the input. Meanwhile, as there exists only a few quite difficult samples, and lower dimension of z is enough to capture variants of labeling. Deterministic Prediction Generation: As introduced in Section 3.3, three different solutions can be used to generate a deterministic prediction for performance evaluation, including 1) averaging multiple predictions; 2) averaging multiple latent variables; and 3) the proposed saliency consensus module. We evaluate performance of other deterministic inference solutions and show performance in <ref type="table">Table 4</ref> "AveP" and "AveZ", representing the average-prediction solution and average-z solution respectively. We observe similar performance of "AveP" and "AveZ" compared with the proposed saliency consensus module. The similar performance of "AveP" and "AveZ" illustrates that both conventional deterministic prediction generation solutions work well for the saliency detection task. The better performance of "Ours" indicates effectiveness of the proposed solution.</p><formula xml:id="formula_24">NJU2K SSB DES NLPR LFSD SIP Method Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M</formula><p>Effectiveness of Loss Functions: Due to the inconsistency of Q φ (z|X, Y ) and P θ (z|X) used in the training and testing stage respectively, the model may behave differently during training and testing. To mitigate the discrepancy in encoding the latent variable, and achieve similar network behavior during training and testing, we introduce Gaussian Stochastic Neural Network (GSNN) and a hybrid loss function as shown in Eq. 4. To test how our network performs with only the CVAE loss in Eq. 2 or GSNN loss in Eq. 3, we train two extra models and show performance as "CVAE S" and "GSNN" respectively. We see clear performance decreased with each loss used solely. Meanwhile, although the two models perform worse than the proposed solution, we still observe consistent better performance compared with competing methods. Both the performance drop of "CVAE S" and "GSNN" compared with "Ours", and better performance of "CVAE S" and "GSNN" compared with competing methods, indicate effectiveness of the proposed generative model for saliency detection.</p><p>Smoothness Loss: We introduce the smoothness loss to our loss function to set constraints on the structure of the prediction. To evaluate the contribution of the smoothness loss, we remove it from our loss function and show the performance as "NoS". The lower performance indicates the effectiveness of the smoothness loss. Moreover, as shown in Eq. 12, the smoothness loss takes saliency prediction and gray-scale image as input, which can also be interpreted as a self-supervised regularizer. Structure-aware Loss vs. Cross-entropy Loss: Similar to <ref type="bibr" target="#b6">[7]</ref>, we use structure-aware loss instead of the widely used crossentropy loss to penalize prediction along object edges, thus we can achieve structure-preserving saliency prediction. To prove that our model can also works well with basic cross-entropy loss, we designed another model with cross-entropy loss used instead of the structure-aware loss, and show performance as "CE". We notice clear decreased performance of "CE" on "LFSD" and "SIP" dataset. For both "LFSD" and "SIP" dataset, there exists salient foreground regions that share similar color as the background, which makes the cross-entropy based model ineffective in those scenarios. While the structure-aware loss can penalize prediction with wrong structure information, making it effective for those difficult images. HHA vs. Depth: HHA <ref type="bibr" target="#b102">[103]</ref> is a widely used technique that encodes the depth data to three channels: horizontal disparity, height above ground, and the angle the pixels local surface normal makes with the inferred gravity direction. HHA is widely used in RGB-D dense models <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b103">[104]</ref> to obtain better feature representation. To test if HHA also works in our scenario, we replace depth with HHA, and performance is shown in "HHA". We observe similar performance achieved with HHA instead of the raw depth data. Those models using HHA aim to obtain better depth representation, as the raw depth is not usually in lowquality. The proposed stochastic model introduces randomness to the network, which can also serve as denoising technique to improve robustness of the model, and this is also consistent with the observation in <ref type="bibr" target="#b104">[105]</ref>.</p><p>Training without KL Annealing: As discussed in Section 2.4, we introduce KL annealing strategy to prevent the possible posterior collapse problems of the CVAE-based model. To test contribution of this strategy, we simply remove the KL annealing term, and set weight of the KL loss term in Eq. 2 as 1 from the first epoch. Performance of this experiment is shown as "w/o KLA". Although the performance on the six benchmark RGB-D saliency datasets does not show effect of KL annealing clearly (as we generate a deterministic prediction), we observed that it highly affects the diversity of the prediction as shown in <ref type="figure" target="#fig_9">Fig. 13</ref>, which presents the mean variance of multiple predictions on the RGB-D testing sets. Specifically, we perform five iterations of random sampling during testing, and compute variance of those five different predictions. We show mean of the variance maps in <ref type="figure" target="#fig_9">Fig. 13</ref>. Meanwhile, we show the mean variance of our CVAE-based and ABP-based models as "CVAE" and "ABP" respectively. <ref type="figure" target="#fig_9">Fig. 13</ref> clearly shows that both of our proposed solutions can generate more diverse predictions than "w/o KLA", leading to larger variance than "w/o KLA".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Probabilistic RGB Saliency Detection</head><p>We propose a generative model based RGB-D saliency detection network, and we extend it to RGB saliency detection to test flexibility of the proposed framework, and show performance in <ref type="table" target="#tab_8">Table  5</ref>. We train our model ("Ours CVAE" and "Ours ABP") with DUTS training dataset <ref type="bibr" target="#b108">[109]</ref>, and evaluate performance of our methods and competing methods on six widely-used benchmarks:</p><p>(1) DUTS testing dataset; (2) ECSSD <ref type="bibr" target="#b109">[110]</ref>; (3) DUT <ref type="bibr" target="#b100">[101]</ref>; (4) HKU-IS <ref type="bibr" target="#b110">[111]</ref>; (5) THUR <ref type="bibr" target="#b111">[112]</ref> and (6) SOC <ref type="bibr" target="#b112">[113]</ref>. Note that, similar to the RGB-D based framework, we use the same network structure, except that the input image X is RGB data instead of the RGB-D image pair. The consistent better performance of our network ("Ours CVAE" or "Ours ABP") illustrates flexibility of our model, which can be lead to new benchmark performance for both RGB-D saliency detection and RGB saliency detection.   </p><formula xml:id="formula_25">Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M ↓ Sα ↑ F β ↑ E ξ ↑ M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Inspired by human uncertainty in ground-truth annotation, we proposed the first uncertainty inspired RGB-D saliency detection model. Different from existing methods, which generally treat saliency detection as a point estimation problem, we propose to learn the distribution of saliency maps, and proposed a generative learning pipeline to produce stochastic saliency predictions. Meanwhile, we introduce two different inference models: 1) a CVAEbased inference model, where an extra encoder to approximate true posterior distribution of the latent variable z; and 2) an ABP-based inference model to sample z directly from its true posterior distribution with gradient based MCMC. Under our formulation, our model is able to generate multiple predictions, representing uncertainty of human annotation. With the proposed saliency consensus module, we are able to produce accurate saliency prediction following the similar pipeline as the groundtruth annotation generation process. Quantitative and qualitative evaluations on six standard and challenging benchmark RGB-D datasets demonstrated the superiority of our approach in learning the distribution of saliency maps.</p><p>Meanwhile, we thoroughly investigate the generative model and include analysis of both the latent variable, the loss function and the different fusion schemes to introduce z to the network. Furthermore, we extend our solutions to RGB saliency detection. Without changing network structure (we only change the input from RGB-D data to RGB data), we achieve state-of-the-art performance compared with the last RGB saliency models.</p><p>Two different inference models are introduced to learn the proposed generative network as shown in <ref type="figure">Fig. 2 (a)</ref>. From our experience, both the CVAE-based and ABP-based inference models can lead to diverse saliency predictions as shown in <ref type="figure" target="#fig_9">Fig. 13</ref>. While, as extra encoder used in the CVAE model, it leads to more network parameters than the ABP-based solution. On the other hand, as we update the latent variable by running several steps of Langevin Dynamics based MCMC as shown in Eq. 8, which leads to relatively longer training time.</p><p>In the future, we would like to extend our approach to other saliency detection problems. Also, we plan to capture new datasets with multiple human annotations to further model the statistics of human uncertainty in saliency perception.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Jing Zhang is with Research School of Engineering, the Australian National University, ACRV, DATA61-CSIRO. (Email: zjnwpu@gmail.com) • Deng-Ping Fan is with the CS, Nankai University, China. (Email: dengp-fan@gmail.com) • Yuchao Dai is with School of Electronics and Information, Northwestern Polytechnical University, China. (Email: daiyuchao@gmail.com) • Saeed Anwar is with the Australian National University, DATA61-CSIRO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>RGB-D saliency detection via CAVE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5. Detailed structure of inference models, where K is dimension of the latent space, "c1 4K" represents a 1×1 convolutional layer of output channel size 4 × K, "fc" represents the fully connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Example showing how the saliency consensus module works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>E-measure and F-measure curves on six testing datasets (NJU2K, SSB, DES, NLPR, LFSD and SIP). Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Visual comparison of predictions of our methods and competing methods. Note that, our final prediction is generated with the proposed "Saliency Consencus Module" (see Section 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Structured outputs generation, where "Our CVAE Samples" and "Our CVAE" are samples and the deterministic prediction respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Image distribution by analysing entropy and standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Detail network structures of different fusion schemes: the early fusion model (a), the middle fusion model (b) and the late fusion model (c). Dimension analysis of the latent variable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Mean variance of multiple predictions using our CAVE-based model ("CVAE"), ABP-based model ("ABP"), and the CAVE-based model without KL annealing term ("w/o KLA"). Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 2. Training and testing pipeline. During training, the inferred latent variable z and input image X are fed to the "Generator Model" for stochastic saliency prediction. During testing, we sample from the prior distribution of z to produce diverse predictions for each input image.</figDesc><table><row><cell>(a) Training pipeline</cell><cell>(b) Testing pipeline</cell></row><row><cell>67] used GAN as a denoising technique</cell><cell></cell></row><row><cell>to clear up the noisy input images. [62] designed a discriminator to</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Network Setup: Maximal epoch Nep, number of Langvin steps l, step size s, learning rate γ Output: Network parameter set ω and the inferred latent variable : for t = 1, ..., Nep do</figDesc><table><row><cell>{zi} N i=1</cell></row><row><cell>1: Initialize backbone of the "Generator Model" with ResNet50</cell></row><row><cell>[73] for image classification, and other new added layers with</cell></row><row><cell>a truncated Gaussian distribution. Initialize zi with standard</cell></row><row><cell>Gaussian distribution.</cell></row><row><cell>3:</cell></row></table><note>2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1</head><label>1</label><figDesc>Benchmarking results of ten leading handcrafted feature-based models and eight deep models on six RGBD saliency datasets. ↑ &amp; ↓ denote larger and smaller is better, respectively. Here, we adopt mean F β and mean E ξ<ref type="bibr" target="#b78">[79]</ref>.Evaluation tool: https://github.com/DengPingFan/D3NetBenchmark. Feature based Models Deep Models Ours Metric LHM CDB DESM GP CDCP ACSD LBE DCMC MDSF SE DF AFNet CTMF MMCI PCF TANet CPFP DMRA UC-Net CVAE ABP Sα ↑ .514 .632 .665 .527 .669 .699 .695 .686 .748 .664 .763 .822 .849 .858 .877 .879 .878 .886 .897 .902 .900 F β ↑ .328 .498 .550 .357 .595 .512 .606 .556 .628 .583 .653 .827 .779 .793 .840 .841 .850 .873 .886 .893 .889 E ξ ↑ .447 .572 .590 .466 .706 .594 .655 .619 .677 .624 .700 .867 .846 .851 .895 .895 .910 .920 .930 .937 .937 M ↓ .205 .199 .283 .211 .180 .202 .153 .172 .157 .169 .140 .077 .085 .079 .059 .061 .053 .051 Sα ↑ .562 .615 .642 .588 .713 .692 .660 .731 .728 .708 .757 .825 .848 .873 .875 .871 .879 .835 .903 .898 .904 F β ↑ .378 .489 .519 .405 .638 .478 .501 .590 .527 .611 .617 .806 .758 .813 .818 .828 .841 .837 .884 .878 .886 E ξ ↑ .484 .561 .579 .508 .751 .592 .601 .655 .614 .664 .692 .872 .841 .873 .887 .893 .911 .879 .938 .935 .939 M ↓ .172 .166 .295 .182 .149 .200 .250 .148 .176 .143 .141 .075 .086 .068 .064 .060 .051 .066 Sα ↑ .578 .645 .622 .636 .709 .728 .703 .707 .741 .741 .752 .770 .863 .848 .842 .858 .872 .900 .934 .937 .940 F β ↑ .345 .502 .483 .412 .585 .513 .576 .542 .523 .618 .604 .713 .756 .735 .765 .790 .824 .873 .919 .929 .928 E ξ ↑ .477 .572 .566 .503 .748 .613 .650 .631 .621 .706 .684 .809 .826 .825 .838 .863 .888 .933 .967 .975 .975 M ↓ .114 .100 .299 .168 .115 .169 .208 .111 .122 .090 .093 .068 .055 .065 .049 .046 .038 .030 Sα ↑ .630 .632 .572 .655 .727 .673 .762 .724 .805 .756 .806 .799 .860 .856 .874 .886 .888 .899 .920 .917 .919 F β ↑ .427 .421 .430 .451 .609 .429 .636 .542 .649 .624 .664 .755 .740 .737 .802 .819 .840 .865 .891 .893 .891 E ξ ↑ .560 .567 .542 .571 .782 .579 .719 .684 .745 .742 .757 .851 .840 .841 .887 .902 .918 .940 .951 .952 .852 M ↓ .108 .108 .312 .146 .112 .179 .081 .117 .095 .091 .079 .058 .056 .059 .044 .041 .036 .031 Sα ↑ .557 .520 .722 .640 .717 .734 .736 .753 .700 .698 .791 .738 .796 .787 .794 .801 .828 .847 .864 .868 .866 F β ↑ .396 .376 .612 .519 .680 .566 .612 .655 .521 .640 .679 .736 .756 .722 .761 .771 .811 .845 .855 .857 .859 E ξ ↑ .491 .465 .638 .584 .754 .625 .670 .682 .588 .653 .725 .796 .810 .775 .818 .821 .863 .893 .901 .904 .903 M ↓ .211 .218 .248 .183 .167 .188 .208 .155 .190 .167 .138 .134 .119 .132 .112 .111 .088 .075 Sα ↑ .511 .557 .616 .588 .595 .732 .727 .683 .717 .628 .653 .720 .716 .833 .842 .835 .850 .806 .875 .883 .876 F β ↑ .287 .341 .496 .411 .482 .542 .572 .500 .568 .515 .465 .702 .608 .771 .814 .803 .821 .811 .867 .877 .863 E ξ ↑ .437 .455 .564 .511 .683 .614 .651 .598 .645 .592 .565 .793 .704 .845 .878 .870 .893 .844 .914 .927 .921 M ↓ .184 .192 .298 .173 .224 .172 .200 .186 .167 .164 .185 .118 .139 .086 .071 .075 .064 .085 .051 .045 .049</figDesc><table><row><cell>Handcrafted [80] [81] [82] [83] [84] [85] [86] [87]</cell><cell>[88] [89] [34] [35]</cell><cell>[18]</cell><cell>[37] [36] [17] [11] [10]</cell><cell>[1]</cell></row><row><cell>NJU2K [85]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>.043</cell><cell>.039 .039</cell></row><row><cell>SSB [90]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>.039</cell><cell>.039 .037</cell></row><row><cell>DES [82]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>.019</cell><cell>.016 .016</cell></row><row><cell>NLPR [80]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>.025</cell><cell>.025 .024</cell></row><row><cell>LFSD [91]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>.066</cell><cell>.065 .065</cell></row><row><cell>SIP [16]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>The code type and inference time of existing approaches. M = Matlab. Pt = PyTorch. Tf = Tensorflow.</figDesc><table><row><cell>Method</cell><cell cols="3">LHM [80] CDB [81] DESM [82]</cell><cell>GP [83]</cell><cell cols="5">CDCP [84] ACSD [92] LBE [86] DCMC [87] MDSF [88]</cell></row><row><cell>Time (s)</cell><cell>2.13</cell><cell>0.60</cell><cell>7.79</cell><cell>12.98</cell><cell>60.00</cell><cell>0.72</cell><cell>3.11</cell><cell>1.20</cell><cell>60.00</cell></row><row><cell>Code Type</cell><cell>M</cell><cell>M</cell><cell>M</cell><cell>M&amp;C++</cell><cell>M&amp;C++</cell><cell>C++</cell><cell>M&amp;C++</cell><cell>M</cell><cell>C++</cell></row><row><cell>Method</cell><cell>SE [89]</cell><cell>DF [34]</cell><cell cols="3">AFNet [35] CTMF [18] MMCI [37]</cell><cell>PCF [36]</cell><cell>CPFP [11]</cell><cell>Our ABP</cell><cell>Our CVAE</cell></row><row><cell>Time (s)</cell><cell>1.57</cell><cell>10.36</cell><cell>0.03</cell><cell>0.63</cell><cell>0.05</cell><cell>0.06</cell><cell>0.17</cell><cell>0.05</cell><cell>0.06</cell></row><row><cell cols="2">Code Type M&amp;C++</cell><cell>M&amp;C++</cell><cell>Tf</cell><cell>Caffe</cell><cell>Caffe</cell><cell>Caffe</cell><cell>Caffe</cell><cell>Pt</cell><cell>Pt</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, we use four different scales of dilation rate: 6, 12, 18, 24 same as<ref type="bibr" target="#b70">[71]</ref>, and set all intermediate channel size as M = 32. For both inference models, we set the dimension of the latent variable as K = 3. Weights of new layers are initialized with N (0, 0.01), and bias is set as constant. We use the Adam method with momentum 0.9 and decrease the learning rate 10% after 80% of the maximum epoch.The base learning rate is initialized as 5e-5. The whole training takes around 9 hours with training batch size 5, and maximum epoch 100 on a PC with an NVIDIA GeForce RTX GPU. For input image size 352 × 352, the inference time of our CVAE model and ABP model are 0.06s and 0.05s on average respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3</head><label>3</label><figDesc>Performance of competing RGB saliency detection models and ours on RGBD saliency datasets, where depth data is not used while testing using the RGB saliency models. We adopt mean F β and mean E ξ . Sα ↑ .862 .813 .864 .754 .767 .875 .879 .861 .902 F β ↑ .835 .783 .818 .744 .716 .852 .863 .837 .893</figDesc><table><row><cell></cell><cell cols="4">Metric AFBNet NLDF PiCANet RAS DGRL CPD SCRN F3Net CAVE</cell></row><row><cell></cell><cell>[93]</cell><cell>[78]</cell><cell>[94] [95] [96] [97] [9]</cell><cell>[7] Ours</cell></row><row><cell>NJU2K [92]</cell><cell>E ξ ↑ .888</cell><cell>.848</cell><cell cols="2">.869 .800 .804 .903 .912 .890 .937</cell></row><row><cell></cell><cell>M ↓ .064</cell><cell>.091</cell><cell cols="2">.072 .115 .107 .056 .052 .061 .039</cell></row><row><cell></cell><cell>Sα ↑ .893</cell><cell>.859</cell><cell cols="2">.896 .828 .824 .902 .902 .891 .898</cell></row><row><cell>SSB [90]</cell><cell>F β ↑ .865 E ξ ↑ .918</cell><cell>.831 .893</cell><cell cols="2">.844 .820 .781 .880 .881 .868 .878 .899 .871 .865 .928 .928 .921 .935</cell></row><row><cell></cell><cell>M ↓ .045</cell><cell>.062</cell><cell cols="2">.053 .076 .073 .040 .041 .043 .039</cell></row><row><cell></cell><cell>Sα ↑ .879</cell><cell>.828</cell><cell cols="2">.883 .806 .833 .894 .907 .880 .937</cell></row><row><cell>DES [82]</cell><cell>F β ↑ .845 E ξ ↑ .893</cell><cell>.758 .831</cell><cell cols="2">.822 .762 .753 .870 .885 .845 .929 .872 .823 .849 .907 .927 .892 .975</cell></row><row><cell></cell><cell>M ↓ .035</cell><cell>.058</cell><cell cols="2">.039 .060 .054 .029 .026 .030 .016</cell></row><row><cell></cell><cell>Sα ↑ .881</cell><cell>.847</cell><cell cols="2">.876 .853 .840 .893 .894 .884 .917</cell></row><row><cell>NLPR [80]</cell><cell>F β ↑ .816 E ξ ↑ .896</cell><cell>.782 .876</cell><cell cols="2">.789 .810 .767 .844 .846 .838 .893 .870 .888 .873 .914 .920 .912 .952</cell></row><row><cell></cell><cell>M ↓ .042</cell><cell>.052</cell><cell cols="2">.051 .049 .053 .034 .036 .035 .025</cell></row><row><cell></cell><cell>Sα ↑ .817</cell><cell>.777</cell><cell cols="2">.827 .673 .782 .836 .827 .835 .868</cell></row><row><cell>LFSD [91]</cell><cell>F β ↑ .784 E ξ ↑ .838</cell><cell>.756 .806</cell><cell cols="2">.778 .672 .759 .811 .800 .810 .857 .825 .727 .817 .856 .847 .857 .904</cell></row><row><cell></cell><cell>M ↓ .094</cell><cell>.121</cell><cell cols="2">.103 .162 .117 .088 .088 .089 .065</cell></row><row><cell></cell><cell>Sα ↑ .876</cell><cell>.795</cell><cell cols="2">.851 .718 .682 .870 .866 .866 .883</cell></row><row><cell>SIP [16]</cell><cell>F β ↑ .847 E ξ ↑ .911</cell><cell>.752 .840</cell><cell cols="2">.806 .696 .606 .859 .861 .850 .877 .866 .766 .744 .910 .903 .905 .927</cell></row><row><cell></cell><cell>M ↓ .055</cell><cell>.100</cell><cell cols="2">.073 .121 .138 .053 .057 .055 .045</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>↓</head><label></label><figDesc>Middle .897 .888 .933 .042 .895 .880 .934 .041 .931 .920 .968 .018 .916 .887 .950 .026 .854 .843 .888 .073 .873 .863 .914 .048 Late .890 .875 .929 .046 .891 .866 .931 .042 .929 .909 .970 .020 .907 .877 .947 .028 .839 .828 .887 .076 .870 .853 .916 .051 AveP .900 .892 .936 .040 .897 .877 .934 .040 .935 .924 .970 .017 .914 .890 .951 .025 .857 .842 .899 .067 .880 .876 .926 .046 AveZ .901 .890 .927 .040 .892 .875 .930 .040 .929 .921 .971 .018 .914 .884 .950 .026 .855 .843 .892 .068 .880 .874 .926 .046 GSNN .900 .887 .935 .040 .894 .873 .930 .041 .931 .919 .971 .018 .913 .885 .949 .026 .852 .834 .894 .070 .871 .864 .916 .051 CVAE S .900 .890 .932 .040 .894 .876 .931 .041 .936 .927 .974 .016 .914 .891 .949 .026 .856 .843 .897 .068 .877 .867 .920 .048 NoS .893 .881 .933 .042 .885 .876 .930 .044 .931 .921 .966 .017 .914 .878 .950 .027 .853 .845 .898 .069 .882 .868 .924 .047 CE .900 .891 .936 .041 .894 .876 .930 .040 .935 .921 .970 .018 .913 .891 .950 .025 .851 .833 .887 .075 .876 .856 .916 .051 HHA .897 .886 .934 .042 .902 .882 .937 .038 .930 .917 .970 .019 .919 .892 .950 .024 .850 .834 .888 .074 .870 .856 .915 .052 w/o KLA .900 .890 .932 .041 .893 .870 .931 .040 .932 .923 .972 .017 .913 .887 .948 .027 .854 .841 .893 .069 .881 .872 .923 .046 Our CAVE .902 .893 .937 .039 .898 .878 .935 .039 .937 .929 .975 .016 .917 .893 .952 .025 .868 .857 .904 .065 .883 .877 .927 .045 Our ABP .900 .889 .937 .039 .904 .886 .939 .037 .940 .928 .975 .016 .919 .891 .852 .024 .866 .859 .903 .065 .876 .863 .921 .049</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>Comparison with the state-of-the-art RGB saliency detection models on six benchmark RGB saliency datasets. We adopt mean F β and mean E ξ .</figDesc><table><row><cell>DUTS</cell><cell>ECSSD</cell><cell>DUT</cell><cell>HKU-IS</cell><cell>THUR</cell><cell>SOC</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>↓</head><label></label><figDesc>DGRL [96] .846 .790 .887 .051 .902 .898 .934 .045 .809 .726 .845 .063 .897 .884 .939 .037 .816 .727 .838 .077 ----PiCAN [94] .842 .757 .853 .062 .898 .872 .909 .054 .817 .711 .823 .072 .895 .854 .910 .046 .818 .710 .821 .084 .801 .332 .810 .133 NLDF [78] .816 .757 .851 .065 .870 .871 .896 .066 .770 .683 .798 .080 .879 .871 .914 .048 .801 .711 .827 .081 .816 .319 .837 .106 BASN [106] .876 .823 .896 .048 .910 .913 .938 .040 .836 .767 .865 .057 .909 .903 .943 .032 .823 .737 .841 .073 .841 .359 .864 .092 AFNet [93] .867 .812 .893 .046 .907 .901 .929 .045 .826 .743 .846 .057 .905 .888 .934 .036 .825 .733 .840 .072 .700 .062 .684 .115 MSNet [107] .862 .792 .883 .049 .905 .886 .922 .048 .809 .710 .831 .064 .907 .878 .930 .039 .819 .718 .829 .079 ----SCRN [9] .885 .833 .900 .040 .920 .910 .933 .041 .837 .749 .847 .056 .916 .894 .935 .034 .845 .758 .858 .066 .838 .363 .859 .099 LDF [108] .890 .861 .925 .034 .919 .923 .943 .036 .839 .770 .865 .052 .920 .913 .953 .028 .842 .768 .863 .064 ----Ours CVAE .888 .860 .927 .034 .921 .926 .947 .035 .839 .773 .869 .051 .921 .919 .957 .026 .848 .765 .862 .064 .849 .369 .872 .089 Ours ABP .890 .864 .931 .034 .915 .918 .941 .037 .843 .770 .864 .050 .917 .913 .949 .027 .849 .773 .869 .066 .842 .365 .868 .091</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. The RGB saliency models are trained on RGB saliency training set, and testing on RGB-D testing set, where the depth is not used.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS.</head><p>This research was supported in part by NSFC (61871325, 61671387, 61620106008, 61572264), National Key Research and Development Program of China (2018AAA0102803), and Tianjin Natural Science Foundation (17JCJQJC43700).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An iterative and cooperative top-down and bottom-up inference network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep unsupervised saliency detection: A multiple noisy labeling perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9029" to="9038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Employing deep part-object relationships for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">F3net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Art. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weaklysupervised salient object detection via scribble annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Select, supplement and focus for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning selective self-mutual attention for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jl-dcf: Joint learning and densely-cooperative fusion framework for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A2dele: Adaptive and attentive depth distiller for efficient rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Three-stream attention-aware network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE T. Image Process</publisher>
			<biblScope unit="page" from="2825" to="2835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">CNNs-based RGB-D saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3171" to="3183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Methods for comparing scanpaths and saliency maps: strengths and weaknesses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baccino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meaning-based guidance of attention in scenes as revealed by meaning maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="743" to="747" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A saliency-based search mechanism for overt and covert shifts of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1489" to="1506" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="3483" to="3491" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Alternating back-propagation for generator network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Art. Intell</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MCMC Using Hamiltonian Dynamics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="113" to="162" />
			<date type="published" when="2011" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Handbook of Markov Chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="113" to="162" />
		</imprint>
	</monogr>
	<note>MCMC using hamiltonian dynamics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lagging inference networks and posterior collapse in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spokoyny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ladder variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sø Nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maalø E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Sø</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<editor>Adv. Neural Inform. Process. Syst. (D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3738" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno>abs/1411.1784</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RGBD salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adaptive fusion for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01369</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multiscale multi-path and cross-modal interactions for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Siamese network for rgb-d salient object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12134</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BBS-Net: RGB-D Salient Object Detection with a Bifurcated Backbone Strategy Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Bifurcated backbone strategy for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2007</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Accurate rgb-d salient object detection via collaborative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical dynamic filtering network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Bilateral attention network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14582</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">RGB-D Salient Object Detection: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00230</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pixelvae: A latent variable model for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MT-VAE: learning motion transformations to generate multimodal human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="276" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Phiseg: Capturing uncertainty in medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Tezcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Hötker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">J</forename><surname>Muehlematter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schawkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Donati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<editor>MICCAI</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="119" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A probabilistic u-net for segmentation of ambiguous images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="6965" to="6975" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. Worksh</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Contrastive variational autoencoder enhances salient features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A stochastic conditioning scheme for diverse human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sampling good latent variables via cpp-vaes: Vaes with condition posterior as prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08521</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A variational u-net for conditional appearance and shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8857" to="8865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Variational autoencoders for deforming 3d mesh models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Supervae: Superpixelwise variational autoencoder for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Art. Intell</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8569" to="8576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst. Worksh</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Seggan: Semantic segmentation with generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Segan: Adversarial network with multi-scale l 1 loss for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Saliency detection by conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Graphic and Image Processing</title>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page">253</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Salient object detection using cascaded convolutional neural networks and adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2237" to="2247" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5689" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Salgan: Visual saliency prediction with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">A</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">cmsalgan: Rgb-d salient object detection with cross-view generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE T. Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">DSAL-GAN: denoising based saliency prediction with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Makwana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>Adv. Neural Inform. Process. Syst. (J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multi-spectral salient object detection by adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Art. Intell</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12023" to="12030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Saliency preservation in low-resolution grayscale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A C</forename><surname>Yohanandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Nonlocal deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6609" to="6617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Jt. Conf. Artif. Intell</title>
		<imprint>
			<biblScope unit="page" from="698" to="704" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Stereoscopic saliency model using contrast and depth-guided-background prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="2227" to="2238" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICIMCS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Exploiting global priors for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1115" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Local background enclosure for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Salient object detection for rgb-d image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2806" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1115" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">PiCANet: Learning Pixel-wise Contextual Attention for Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">tech. rep</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Translate-to-recognize networks for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11836" to="11845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Training with noise is equivalent to tikhonov regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A mutual learning method for salient object detection with intertwined multisupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Label decoupling framework for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Salientshape: group saliency in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="202" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
