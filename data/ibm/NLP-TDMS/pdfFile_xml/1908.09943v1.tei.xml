<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fashion Image Retrieval with Capsule Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furkan</forename><surname>Kınlı</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ozyegin University</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barışözcan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ozyegin University</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furkan</forename><surname>Kıraç</surname></persName>
							<email>3furkan.kirac@ozyegin.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">Ozyegin University</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fashion Image Retrieval with Capsule Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, we investigate in-shop clothing retrieval performance of densely-connected Capsule Networks with dynamic routing. To achieve this, we propose Triplet-based design of Capsule Network architecture with two different feature extraction methods. In our design, Stackedconvolutional (SC) and Residual-connected (RC) blocks are used to form the input of capsule layers. Experimental results show that both of our designs outperform all variants of the baseline study, namely FashionNet, without relying on the landmark information. Moreover, when compared to the SOTA architectures on clothing retrieval, our proposed Triplet Capsule Networks achieve comparable recall rates only with half of parameters used in the SOTA architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fashion has recently become one of the most featured topics of interdisciplinary studies in Computer Science. With the emergence of deep learning based solutions, fashion-related researches start to get promising results on various subjects including clothing recognition, attribute prediction, clothing retrieval, body segmentation, and style prediction. Retrieving the desired clothing image from a collection is one of the most challenging tasks in fashion domain, and it is attacked by such a mechanism that learns to capture different notions of the similarities between the images in a common subspace.</p><p>There has been numerous studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref> to employ Convolutional Neural Networks (CNNs) to their solutions. However, CNNs, by their nature, have some limitations such as losing the hierarchical spatial information of the objects and not being robust to affine transformations. Recently, an alternative deep learning architecture, namely Capsule Networks, and a novel dynamic routing algorithm have been proposed by Sabour and Hinton et al. <ref type="bibr" target="#b9">[10]</ref>. In this design, with the help of the routing-by- <ref type="figure">Figure 1</ref>. Some examples of retrieved images by our architectures. Blue: query, Green: correct, Red: wrong. agreement algorithm, it is possible to learn more descriptive information about the objects without losing the intrinsic spatial relationship between the object and its parts. Therefore, Capsule Networks have the capacity for recognizing the images regardless of the visual angle and without requiring different transformations, since this architecture can inherently learn higher dimensional pose configuration of the images.</p><p>In this study, we employ Capsule Networks to clothing retrieval problem by extending their capabilities with some improvements. First, we extract the features of larger-sized clothing images by more powerful methods (i.e. stacked or residual-connected convolutional layers), and forward these features to fully-connected capsules. Next, we introduce a Triplet-based design of Capsule Networks that learns the similarity between triplets. Lastly, we train our proposed architectures on in-shop partition of DeepFashion data set <ref type="bibr" target="#b6">[7]</ref>, and compare our results with the baseline study, namely FashionNet <ref type="bibr" target="#b6">[7]</ref> and the other SOTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Clothing retrieval has become more important after some major developments in Computer Science and the emer-  gence of e-commerce. Recent studies generally attack to this task by using deep convolutional networks. <ref type="bibr" target="#b2">[3]</ref> introduces an excessively challenging task, namely Exact Street to Shop, where the goal is to match the exact same item in the photos captured by users to online shopping photos. <ref type="bibr" target="#b3">[4]</ref> proposes Dual Attribute-aware Network (DARN) to address the cross-domain image matching problem. <ref type="bibr" target="#b6">[7]</ref> introduces a new data set, namely DeepFashion, which has a vast amount of large-scale clothing images annotated with numerous attributes, landmark information and cross-domain image correspondences. <ref type="bibr" target="#b0">[1]</ref> demonstrates that integrating bag-of-words approach to weakly-supervised learning process can achieve promising results on clothing retrieval task. <ref type="bibr" target="#b11">[12]</ref> proposes a Visual Attention Model (VAM), and introduces a novel Dropout-like connection after attention layers. <ref type="bibr" target="#b12">[13]</ref> addresses the issues of defining a model with right complexity and choosing hard samples carefully during training. <ref type="bibr" target="#b8">[9]</ref> shows how to improve the robustness of the feature embeddings by exploiting the independence within ensembles. <ref type="bibr" target="#b1">[2]</ref> introduces hierarchical triplet loss (HTL) to address the random sampling issue during training a triplet loss. <ref type="bibr" target="#b5">[6]</ref> proposes multiple-way attention-based ensemble architecture that learns the feature embeddings with multiple attention masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Capsules</head><p>Capsules are groups of neurons that convey higher dimensional information throughout the network in more refined way. This information is interpreted as the pose con-figuration and the existence probability of an instance. Each capsule in a higher level is formed by the routing of incoming votes from the capsules in lower level. At this point, these votes are calculated by the linear transformation of the pose configuration. During dynamic routing <ref type="bibr" target="#b9">[10]</ref>, the linear combination of incoming votes weighted by their coefficients (i.e. coupling coefficients) forms the non-activated outputs in higher level capsules. For each iteration, the weights of these votes are updated with respect to the dot product of the incoming votes and the outputs in higher level capsules. This is called agreement between capsules. Finally, the output of each capsule in lower level is determined by squashing function as proposed in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Architectures</head><p>In our design, we adjust the original Capsule Network structure to a Triplet-based version, so that the network can learn the similarity between two images by feeding the objective function with the embedded representations extracted by capsules. At this point, our Capsule Network design aims to minimize the Triplet loss shown in Equation 1, where d is the Euclidean distance metric, α is distance margin, l, l + , l − are the latent capsule embeddings extracted from the anchor image x, positive image x + and negative image x − respectively. During forming these embeddings, we normalize latent capsules by L2-norm, and then we mask all capsules but the one that belongs to the correct class to zero.</p><formula xml:id="formula_0">i [d(l i , l + i ) − d(l i , l − i ) + α]<label>(1)</label></formula><p>As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, Capsule Networks essentially contain two main blocks: feature extraction block and capsule layers. There is only one feature extraction block that has a single convolutional layer with 64 filters in the original design proposed by Sabour and Hinton et al. <ref type="bibr" target="#b9">[10]</ref>. Extracting the features by such a shallow structure may be enough for one-channel handwritten digit images with the size of 28 × 28 <ref type="bibr" target="#b9">[10]</ref>. However, fully-connected capsules need more complex features to achieve better results on more complicated image-related problems. Therefore, we design two different feature extraction blocks to form more powerful features as the input of capsules. First, a number of convolutional layers are stacked without using any pooling operation between them, and the latter is to connect these layers as residual. In both of our designs, leaky form of linear rectifier <ref type="bibr" target="#b7">[8]</ref> is used as activation function, and batch normalization <ref type="bibr" target="#b4">[5]</ref> is applied between convolutional layers.</p><p>Furthermore, capsule layers are kept identical in both designs. There are two fully-connected capsule layers, namely Primary Capsule and Class Capsule. Primary Capsule is the layer where the extracted features are grouped with respect to the capsule dimensionality. In our designs, this layer has 32 channels of 16-dimensional capsules that are fully-connected to Class Capsule. Next, there are c number of 16-dimensional capsules in Class Capsule layer, where c is the number of classes in the data set. Activations and the latent capsule vectors of Class Capsule are calculated via dynamic routing with 3 iterations. Any kind of reconstruction methods (e.g. as in <ref type="bibr" target="#b9">[10]</ref>) is not applied to our Capsule Network designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The experiments for proposed Stacked-convolutional (SCCapsNet) and Residual-connected (RCCapsNet) architectures are conducted on in-shop partition of DeepFashion data set <ref type="bibr" target="#b6">[7]</ref>. Both are trained on 25k training images, and tests are performed by using 14k query and 12k gallery images. Since this task is an information retrieval task, the performance is measured by Recall@K metric, where K is 1 or multiplies of 10 up to 50. Moreover, as mentioned in Schroff et al. <ref type="bibr" target="#b10">[11]</ref>, negative hard sampling strategy improves the convergence behavior of the model significantly. Based on this strategy, the negative images are picked as the closest image to the anchor provided that they are of different categories; whereas we pick each possible positive image in the data set as the positive one.</p><p>As shown in <ref type="table">Table 1</ref>, SCCapsNet and RCCapsNet achieve better retrieval performance than all variants of the baseline study (i.e. FashionNet) by a wide margin. It is important to note that both of our proposed architectures use only images during training in contrast to the baseline study where the network is supported by different number of attributes and the landmark information. These experiments <ref type="table">Table 1</ref>. Recall@K performance of the variants of the baseline study <ref type="bibr" target="#b6">[7]</ref> and our proposed model. FashionNet has different building blocks where the model has different numbers of attributes (A) (i.e. 100, 500 and 1000), or fashion landmarks (L) are replaced with human joints (J) or poselets (P). SCCapsNet and RCCapsNet do not use any extra side information during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Top demonstrate that our Capsule Network designs can inherently learn pose configuration of the objects without any requirement of recovering pose information. <ref type="table" target="#tab_1">Table 2</ref> summarizes in-shop clothing retrieval results of SCCapsNet, RCCapsNet, and the SOTA methods. These figures indicate how successful our proposed designs are, and what the main limitations of them are when compared to the SOTA CNN-based architectures. First, both of our designs outperform the earlier methods (i.e. WTBI <ref type="bibr" target="#b2">[3]</ref> and DARN <ref type="bibr" target="#b3">[4]</ref>) which both disparately use semantic attributes to improve the overall performance, but neglect pose configurations of the images during training. According to Top-20 Recall@K scores, while SCCapsNet improves the scores of the best FashionNet variant by 31% and 14%, RCCap-sNet has even better performance with a margin of 34% and 17% respectively. The other approach whose performance falls behind in ours is the method of leveraging weaklyannotated textual descriptors of the images proposed by Corbiére et al. <ref type="bibr" target="#b0">[1]</ref>. In this design, these textual descriptors (i.e. bag-of-words) represent different coarse semantic concepts such as texture information, color and shape. Capsules can directly learn these concepts from the images in a sophisticated way, and hence, SCCapsNet and RCCapsNet can achieve higher Recall@K scores than this approach without taking advantage of bag-of-words descriptors.</p><p>In addition to all these, our proposed architectures cannot achieve the performances of more advanced CNN-based architectures. In these designs, there are various techniques applied to CNNs to boost the overall performance, which are alternative hard sampling strategies <ref type="bibr" target="#b12">[13]</ref>, more advanced objective functions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>, network ensembling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6]</ref> and attention-based mechanisms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>. Although these techniques may significantly improve the overall performance in CNNs, in principle, they increase the model complexities by a wide margin, or increase training time considerably. At this point, the numbers of trainable parameters in SC- CapsNet and RCCapsNet are respectively ∼2.5 and ∼4.5 million, while the SOTA methods have twice as many trainable parameters in their models. Capsule Networks need more time for training than CNNs since dynamic routing algorithm is a relatively slow routing mechanism when compared to the pooling variants. Therefore, within limited computational resources, these techniques are not yet applied to our models to boost the overall performance of our Capsule Network designs, and left as future research ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this study, we present two different Triplet-based designs of Capsule Networks with more powerful feature extraction blocks, and employ them to clothing retrieval task. Experiments show promising results where both of our designs outperform all FashionNet variants without any extra information besides to the images. Moreover, when compared to the SOTA methods, our designs perform comparably well with only the half of the number of parameters as in the SOTA methods, and it shows the potential of Capsule idea in case of the computational burdens are lightened.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:1908.09943v1 [cs.CV] 26 Aug 2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of our proposed architectures containing different feature extraction blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental results of in-shop clothing retrieval task on DeepFashion data set. "-": not reported.</figDesc><table><row><cell>Models</cell><cell># of</cell><cell cols="6">Top-1 Top-10 Top-20 Top-30 Top-40 Top-50</cell></row><row><cell></cell><cell>Params (M)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>WTBI [3]</cell><cell>60</cell><cell>35.0</cell><cell>47.0</cell><cell>50.6</cell><cell>51.5</cell><cell>53.0</cell><cell>54.5</cell></row><row><cell>DARN [4]</cell><cell>105</cell><cell>38.0</cell><cell>56.0</cell><cell>67.5</cell><cell>70.0</cell><cell>72.0</cell><cell>72.5</cell></row><row><cell>FashionNet [7]</cell><cell>134</cell><cell>53.2</cell><cell>72.5</cell><cell>76.4</cell><cell>77.0</cell><cell>79.0</cell><cell>80.0</cell></row><row><cell>Corbiére et al. [1]</cell><cell>25</cell><cell>39.0</cell><cell>71.8</cell><cell>78.1</cell><cell>81.6</cell><cell>83.8</cell><cell>85.6</cell></row><row><cell>SCCapsNet (ours)</cell><cell>2.5</cell><cell>32.1</cell><cell>72.4</cell><cell>81.8</cell><cell>86.3</cell><cell>89.2</cell><cell>90.9</cell></row><row><cell>RCCapsNet (ours)</cell><cell>4.5</cell><cell>33.9</cell><cell>75.2</cell><cell>84.6</cell><cell>88.6</cell><cell>91.0</cell><cell>92.6</cell></row><row><cell>HDC [13]</cell><cell>5</cell><cell>62.1</cell><cell>84.9</cell><cell>89.0</cell><cell>91.2</cell><cell>92.3</cell><cell>93.1</cell></row><row><cell>VAM [12]</cell><cell>6</cell><cell>66.6</cell><cell>88.7</cell><cell>92.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BIER [9]</cell><cell>5</cell><cell>76.9</cell><cell>92.8</cell><cell>95.2</cell><cell>96.2</cell><cell>96.7</cell><cell>97.1</cell></row><row><cell>HTL [2]</cell><cell>5</cell><cell>80.9</cell><cell>94.3</cell><cell>95.8</cell><cell>97.2</cell><cell>97.4</cell><cell>97.8</cell></row><row><cell>A-BIER [9]</cell><cell>5</cell><cell>83.1</cell><cell>95.1</cell><cell>96.9</cell><cell>97.5</cell><cell>97.8</cell><cell>98.0</cell></row><row><cell>ABE [6]</cell><cell>10</cell><cell>87.3</cell><cell>96.7</cell><cell>97.9</cell><cell>98.2</cell><cell>98.5</cell><cell>98.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Leveraging weakly annotated data for fashion image retrieval and label prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Corbière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ollion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2268" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Metric Learning with Hierarchical Triplet Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Where to Buy It: Matching Street Clothing Photos in Online Shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3343" to="3351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-domain image retrieval with a dual attribute-aware ranking network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BIER : Boosting Independent Embeddings Robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clothing retrieval with visual attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VCIP</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hard-Aware Deeply Cascaded Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="814" to="823" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
