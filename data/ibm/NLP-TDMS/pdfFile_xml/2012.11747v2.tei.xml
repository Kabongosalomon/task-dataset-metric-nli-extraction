<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RealFormer: Transformer Likes Residual Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
							<email>ruininghe@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargav</forename><surname>Kanagal</surname></persName>
							<email>bhargav@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
							<email>jainslie@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">RealFormer: Transformer Likes Residual Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer is the backbone of modern NLP models. In this paper, we propose Real-Former, a simple Residual Attention Layer Transformer architecture that significantly outperforms canonical Transformers on a spectrum of tasks including Masked Language Modeling, GLUE, and SQuAD. Qualitatively, RealFormer is easy to implement and requires minimal hyper-parameter tuning. It also stabilizes training and leads to models with sparser attentions. Code will be open-sourced upon paper acceptance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> architectures are the backbone of numerous state-of-the-art NLP models such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, GPT , and Meena <ref type="bibr" target="#b0">(Adiwardana et al., 2020)</ref>, and have seen wide successes across both academia and industry. Typically, a Transformer network consists of a stack of residual layers. The original design follows a "Post-LN" structure which adds Layer Norm (LN) as a "postprocessing" step for each sub-layer, as shown in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. It has been adopted by various stateof-the-art models including BERT, XLNet , RoBERTa , AL-BERT <ref type="bibr" target="#b11">(Lan et al., 2019)</ref>, and Transformer-XL . Another design is to reorganize the order of sub-layers to create a "direct" / clean path to propagate embeddings of tokens in the input sequence through the whole network, as shown in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>. 1 This design adds LN as a "preprocessing" step for each sub-layer, and is often referred to as "Pre-LN" and used by some wellknown extra large models such as GPT-2  and Megatron <ref type="bibr" target="#b20">(Shoeybi et al., 2019)</ref>.</p><p>In some sense, Post-LN and Pre-LN are analogous to ResNet v1 <ref type="bibr" target="#b7">(He et al., 2016a)</ref> and ResNet v2 <ref type="bibr" target="#b8">(He et al., 2016b)</ref> respectively in the Computer Vision literature. Although ResNet v2 is almost always preferable to v1 for Computer Vision, it might not have been the case for Pre-LN Transformer in the NLP literature. It is likely that the particularities of self-attention modules and Transformer architectures potentially favor (at least slightly) different designs compared to traditional convolutional neural networks.</p><p>In this paper, we propose a simple Transformerbased architecture to show that it is beneficial to create a "direct" path to propagate raw attention scores through the whole network. Our architecture is called Residual Attention Layer Transformer, or RealFormer in short. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (c), each RealFormer layer takes the raw attention scores of all attention heads from the previous layer and adds "residual scores" (computed the same way as attention scores in regular Transformers) on top. The sum of the two scores is then used to compute attention weights via softmax (again, as in regular Transformers). In other words, RealFormer can be seen as adding a simple skip connection to the Post-LN Transformer. Note that it does not add any multiplication ops to the computational graph and therefore the performance is expected to be comparable.</p><p>Specifically, our main contributions include:</p><p>• We present RealFormer, a novel and simple model based on original Transformer (with no more than a few lines of code changes and minimal hyper-parameter tuning).</p><p>• We demonstrate that RealFormer can be used as a drop-in replacement of Transformer in BERT, outperforming both Post-LN and Pre-LN Transformers across a wide spectrum of model sizes (from small to extra large) Pre-LN layer used by (e.g.) GPT-2 that creates a "direct" path to propagate token embeddings; (c) Our RealFormer layer that creates a "direct" path to propagate attention scores (by adding a simple skip edge on top of (a)).</p><p>for Masked Language Modeling (i.e., pretraining).</p><p>• We also demonstrate that RealFormer can consistently improve accuracy of downstream tasks including GLUE <ref type="bibr" target="#b24">(Wang et al., 2018)</ref>, SQuAD v1.1 <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref> and SQuAD v2.0 <ref type="bibr" target="#b16">(Rajpurkar et al., 2018)</ref>. Furthermore, it even achieves competitive downstream results when pre-trained with only half the number of epochs of the strongest baseline.</p><p>• Finally, we demonstrate qualitatively that attention in RealFormer tends to be sparser and more correlated across layers compared to baselines, which we believe may have some regularization effects that could stabilize training and benefit fine-tuning.</p><p>2 Related Work <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref> proposed Transformer initially for machine translation task in 2017 and it has profoundly changed the natural language processing field ever since. <ref type="bibr" target="#b13">Radford et al. (2018)</ref> demonstrated that generative pre-training of a Transformer-based language model (GPT) on a diverse corpus of unlabeled text can give large gains to downstream NLP tasks that suffer from scarce labeled data. Following this thread, <ref type="bibr" target="#b6">Devlin et al. (2019)</ref> proposed to pretrain a bidirectional Transformer encoder (BERT) with a novel Masked Language Modeling as the main optimization objective. Since then, advances on many NLP tasks have been dominated by the self-supervised general-purpose pre-training, taskspecific fine-tuning paradigm. Following BERT, there has been a large stream of work that explores better self-supervision objectives (e.g., ; <ref type="bibr" target="#b4">Clark et al. (2020)</ref>), larger pre-training data and better hyper-parameters (e.g., , model parameter sharing (e.g., <ref type="bibr" target="#b11">Lan et al. (2019)</ref>, multi-task pre-training (e.g., <ref type="bibr" target="#b21">Sun et al. (2019)</ref>; <ref type="bibr" target="#b15">Raffel et al. (2019)</ref>. These efforts typically employ a Post-LN Transformer at their core. In this paper we adopt BERT to test different Transformer architectures because it is widely used and representative of this thread of work.</p><p>Another notable thread of work focuses on improving the efficiency/scalability of Transformer. Typically, they try to reduce the quadratic complexity of the self-attention mechanism with respect to sequence length via low-rank methods (e.g., ), fixed strided attention patterns (e.g., ), learnable attention patterns (e.g., <ref type="bibr" target="#b10">Kitaev et al. (2020)</ref>; <ref type="bibr" target="#b19">Roy et al. (2020)</ref>), memory-based global &amp; local attention (e.g., ; <ref type="bibr" target="#b2">Beltagy et al. (2020);</ref><ref type="bibr" target="#b28">Zaheer et al. (2020)</ref>), and so on. These methods are particularly useful when dealing with long documents that go beyond the capacity of standard Transformer models. We would refer the reader to <ref type="bibr" target="#b22">Tay et al. (2020)</ref> for a detailed survey. RealFormer is orthogonal to these methods as it focuses on improving standard Transformer with an universal technique which can apply to these models as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RealFormer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Standard Transformer</head><p>There is an encoder and a decoder in Transformer <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>. Since they work in a similar way, here we only introduce the encoder and refer the reader to the original paper for complete details.</p><p>There are two sub-layers inside each layer of a Transformer encoder. The first sub-layer contains a Multi-Head Attention module that computes output embeddings of a set of queries (Q) by aggregating the embeddings (V ) of a set of keys (K):</p><formula xml:id="formula_0">MultiHead (Q, K, V ) = Concat (head 1 , ..., head h ) W O ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">head i = Attention (QW Q i , KW K i , V W V i ). Q and K are matrices with dimension d k and V is a matrix with dimension d v . W Q i , W K i , and W V</formula><p>i are matrices that linearly project queries, keys, and values into the "attention space" of the i-th head. W O is a matrix that linearly transforms the concatenation of the outputs of all heads.</p><p>Attention function is typically implemented with a Scaled Dot-Product Attention module <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> which computes a weighted sum of the values:</p><formula xml:id="formula_2">Attention (Q , K , V ) = Softmax ( Q K T √ d k ) V ,<label>(2)</label></formula><formula xml:id="formula_3">where matrix Q K T √ d k</formula><p>is the raw attention scores for each (query, key) pair. These scores are normalized via the Softmax function for each query and then act as weights for the corresponding vectors in V .</p><p>The second sub-layer contains a fully-connected Feed-Forward Network (FFN) module with one hidden layer:</p><formula xml:id="formula_4">FFN (x) = σ (x W 1 + b 1 ) W 2 + b 2 ,<label>(3)</label></formula><p>where σ is an activation function usually implemented with ReLU or GeLU (e.g., <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>). FFN is applied to each position in the sequence separately and identically. Finally, there are Layer Norm (LN) modules inserted into the above two sub-layers to stabilize training. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, there are two canonical designs of the Transformer network which only differ in the ways they organize the modules. Post-LN is the original architecture proposed by <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref> which normalizes the outputs at the end of each sub-layer. In contrast, Pre-LN normalizes sub-layer inputs instead and creates a direct path (without LN in the way) to propagate embeddings of the tokens in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Residual Attention Layer Transformer</head><p>RealFormer closely follows the Post-LN design and simply adds a skip edge to connect Multi-Head Attention modules in adjacent layers, as shown in <ref type="figure" target="#fig_0">Figure 1 (c)</ref>. Formally, it adds P rev, the pre-softmax attention scores from the previous layer with shape (#heads, f rom seq len, to seq len), 2 as one additional input to the Multi-Head Attention module in the current layer:</p><formula xml:id="formula_5">ResidualMultiHead (Q, K, V, P rev) = Concat (head 1 , ..., head h ) W O ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">head i = ResidualAttention (QW Q i , KW K i , V W V i , P rev i )</formula><p>and P rev i is the slice of P rev with shape (f rom seq len, to seq len) corresponding to head i . ResidualAttention adds "residual scores" on top of P rev i and then computes the weighted sum as usual:</p><formula xml:id="formula_7">ResidualAttention (Q , K , V , P rev ) = Softmax ( Q K T √ d k + P rev ) V .<label>(5)</label></formula><p>Finally, new attention scores Q K T √ d k + P rev are passed over to the next layer.</p><p>Implementing RealFormer takes no more than adding a few lines of code to the "backbone" Transformer. The same idea can be straightforwardly applied even when there are more than one type of attention modules in the network. For example, there are encoder-encoder self-attention, encoder-decoder attention, and decoder-decoder self-attention modules for machine translation. In such cases, RealFormer simply creates multiple direct paths, one for each type of attention module, as long as the attention pattern / mask is the same across the layers along the path (which is almost always the case).</p><p>As we will show in Section 4, Post-LN Transformer tends to outperform Pre-LN Transformer for a variety of setups (given a reasonable computing budget). Therefore in this paper we choose Post-LN as the backbone for RealFormer for simplicity. Note however that it should be straightforward to switch the backbone to different Transformer variants in settings that favor different trade-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct comprehensive empirical studies on RealFormer comparing against two canonical Transformer architectures: Post-LN and Pre-LN. We evaluate the strength of RealFormer in terms of both pre-training and fine-tuning accuracy on downstream tasks with minimal (if any) hyper-parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BERT</head><p>BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> has been the standard way of transferring knowledge from large unlabeled text corpora by pre-training a bidirectional Transformer encoder. Numerous downstream NLP tasks suffering from scarcity of supervised data have benefited considerably by fine-tuning a pretrained BERT model. This drives us to adopt BERT as the main evaluation setup for RealFormer.</p><p>Experiment setup. We follow the standard BERT pre-training setup (dataset: Wikipedia + BookCorpus, vocab: uncased 30K, max sequence length: 512 3 , dropout: 10%, learning rate: 1e-4, learning rate schedule: warm up and then linearly decay to 0, weight decay: 0.01, optimizer: AdamW, objective: Masked Language Modeling + Next Sentence Prediction, etc.) to compare all three Transformer models: Post-LN, Pre-LN, and RealFormer. We experiment with Transformer architectures with a wide spectrum of sizes, from Small and Base all the way to xLarge, as detailed in <ref type="table" target="#tab_0">Table 1</ref>. For simplicity, all models are pre-trained 1M steps with a mini-batch size of 512 (except that xLarge uses 256 to avoid TPU OOM). Note that we use a larger mini-batch size than <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>, i.e., doubling the amount of pre-training epochs, to show more complete behavior of different models.</p><p>We use exactly the same setup for all three Transformer architectures except that for the Pre-LN Transformer we follow the initialization strategy suggested by  and . 4 Note that for simplicity RealFormer inher-3 Unlike BERT which uses a reduced sequence length for the first 90% of steps, we always use 512 for simplicity. <ref type="bibr">4</ref> We also experimented with the initialization strategy used by BERT but with similar results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Pre-training Results</head><p>To evaluate pre-trained models, we report Masked Language Modeling (MLM) accuracy 6 on a randomly held-out development set. As shown in Table 2, RealFormer outperforms the two baseline Transformers considerably with the gap increasing with model size. Our hypothesis is that larger models are inherently harder to train (we did observe that BERT with Post-LN is unstable and sometimes even diverges for xLarge) and RealFormer can help regularize the model and stabilize training.</p><p>We also report the pre-training curves in <ref type="figure" target="#fig_1">Figure 2</ref>. One interesting finding is that the Pre-LN Transformer seems to favor the combination of extra large models and a small number of steps, though it is consistently outperformed by the other two Transformers in "regular-sized" settings or given enough budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Downstream Evaluation Results</head><p>We fine-tune the above BERT-Large models with the three Transformers on a variety of downstream tasks including GLUE, SQuAD v1.1 and SQuAD   v2.0 to evaluate the performance of RealFormer on both sentence-level (i.e., GLUE) and token-level (i.e., SQuAD) NLP tasks.</p><p>GLUE. General Language Understanding Evaluation (GLUE) is a canonical benchmark proposed by <ref type="bibr" target="#b24">Wang et al. (2018)</ref> for evaluating the performance of models across a diverse set of NLU tasks. Following the fine-tuning recipe in <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>, we use a mini-batch size of 32 for all models on all GLUE tasks. For each (task, model) pair, we select number of epochs in {2, 3, 4} and learning rate in {6e-6, 8e-6, 1e-5, 2e-5, 3e-5, 4e-5, 5e-5}. 7 For each setup, we run the experiment <ref type="bibr">7</ref> We use a slightly wider range than <ref type="bibr" target="#b6">Devlin et al. (2019)</ref> to better accommodate all three models.   <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>. Numbers in smaller font are standard deviations. All numbers are scaled by 100. 5 times and report the best median performance and the corresponding standard deviation on the development set.</p><p>Detailed results are tabulated in <ref type="table" target="#tab_3">Table 3</ref>. We exclude the problematic WNLI task following <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>. For each task, we report metric(s) that are suggested by the GLUE benchmark <ref type="bibr" target="#b24">(Wang et al., 2018)</ref>. RealFormer achieves the best overall performance and outperforms both Post-LN and Pre-LN Transformers significantly on most tasks, testifying its strength at tackling sentence-level tasks.</p><p>SQuAD. The Stanford Question Answering Dataset (SQuAD v1.1) is a reading comprehension dataset consisting of 100K crowd-sourced questionanswer pairs, where the answer to each question is a segment of text from the corresponding reading passage <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref>. SQuAD v2.0, a later version, further extends with over 50K unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.</p><p>We follow the fine-tuning recipe in <ref type="bibr" target="#b6">Devlin et al. (2019)</ref> for all three Transformer models on these two datasets without using any additional data such as TriviaQA <ref type="bibr" target="#b9">(Joshi et al., 2017)</ref>. For both v1.  and v2.0, we select mini-batch size in {32, 48}, number of epochs in {2, 3, 4}, and learning rate in {2e-5, 3e-5, 4e-5, 5e-5}. For each setup, we run the experiment 5 times and report the best median performance and the corresponding standard deviation on the development set. As we can see from <ref type="table" target="#tab_5">Table 4</ref>, RealFormer outperforms both Post-LN and Pre-LN Transformers considerably, attesting its strength at tackling token-level tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Research Questions</head><p>How well does RealFormer perform with half the pre-training budget? Although RealFormer has outperformed both Post-LN and Pre-LN Transformers considerably when pre-training 1M steps, we are also interested in investigating its potential when the pre-training budget is more limited. For this purpose, we experiment with BERT-Large models. In particular, we take the 500K step checkpoint of the pre-trained RealFormer in <ref type="table" target="#tab_1">Table 2</ref> and fine-tune it on GLUE and SQuAD datasets using exactly the same procedure as described above. Comparison results against the strongest baseline, Post-LN Transformer pre-trained 500K (checkpoint) and 1M steps respectively, are collected in <ref type="table" target="#tab_7">Table 5</ref>. We can see that RealFormer with merely half the amount of pre-training epochs can beat Post-LN (1M) on GLUE with a significant margin, and almost match its performance on SQuAD.</p><p>How well does RealFormer perform with a larger learning rate? As suggested by some recent works (e.g., <ref type="bibr" target="#b26">Xiong et al. (2020)</ref>), Pre-LN Transformer may benefit from using larger learning rates compared to Post-LN. To this end, we follow the pre-training procedure detailed earlier and switch to a larger learning rate, 2e-4, to pretrain BERT-Large with the three Transformer models. MLM accuracy on the development set with training steps is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We can see that</p><p>• Both Pre-LN and RealFormer can reap some benefits of using larger learning rates;</p><p>• RealFormer seems to benefit a bit more in this case <ref type="formula">(</ref>  <ref type="table" target="#tab_1">Table 2</ref>. In particular, for each (token, layer, head) triplet, we compute the entropy of the attention weights (probabilities) as the "sparsity measure" of attentions. Intuitively, as entropy gets lower, the attention weight distribution becomes more skewed and therefore attention is sparser. In a similar fashion to <ref type="bibr" target="#b18">Ramsauer et al. (2020)</ref>, we use violin plots to show the entropy distributions of RealFormer (see <ref type="figure">Figure 4)</ref>. Plots for Post- <ref type="figure">Figure 4</ref>: Distribution of entropies of the attention probabilities of the tokens of 8,192 held-out examples using the pre-trained BERT-Base with RealFormer (see Section 4.1.1). Attention heads in each layer are ordered by their medians of entropies for better legibility. Distributions are re color-coded based on the median of entropies: RED (median &gt; 4.5), YELLOW (1.5 ≤ median ≤ 4.5), BLUE (median &lt; 1.5). I.e., colder colors mean sparser attentions. There is a clear trend that higher layers tend to have sparser attentions.</p><p>LN and Pre-LN Transformers are included in Appendix ( <ref type="figure">Figure 6 and Figure 7)</ref>. Each row is a layer in BERT-Base and each column is an attention head. For better legibility, (1) for each layer, we sort the attention heads in ascending order of the median of entropies; and (2) we also color code these plots to help distinguish heads with relatively sparse attentions (BLUE: median &lt; 1.5) and relatively dense attentions (RED: median &gt; 4.5) from the rest (YELLOW: 1.5 ≤ median ≤ 4.5). From the three figures we can see that attention tends to get sparser for later (upper) layers for all three Transformers. However, RealFormer differs from the other two in the following two ways:</p><p>• RealFormer has significantly sparser attentions for top layers (layer 9-11);</p><p>• RealFormer tends to have lower variance across all layers, which means that attention density is less input-dependent.</p><p>We hypothesize that the above two properties might be a sign of stableness and could benefit finetuning.</p><p>How much do attention heads in layer L resemble those in layer L − 1? Since RealFormer uses a residual attention scheme, it is interesting to show to what extent an attention head is "relying on" the corresponding head in the previous layer. To this end, we take each of the three pre-trained BERT-Base models in <ref type="table" target="#tab_1">Table 2</ref> and compute the Jensen-Shannon Divergence (JSD) between attention probabilities in each pair of vertically adjacent heads, i.e., JSD (head L i , head L−1 i ), for 1 ≤ L &lt; 12 and 0 ≤ i &lt; 12. Instead of computing one scalar value for each head pair, we show the full distribution based on the tokens in 8,192 held-out examples, i.e., each data point is the JSD between the attention probabilities of a token at these two heads. <ref type="figure" target="#fig_3">Figure 5</ref> and <ref type="figure">Figure 8</ref> in Appendix demonstrate the JSD distributions for RealFormer and Post-LN Transformer 8 respectively via violin plots. We can see that RealFormer tends to have significantly lower JSD values, especially for heads in middle layers. This might mean that RealFormer has some regularization advantages and provides one hypothesis for why it tends to outperform Post-LN more 8 Note that JSD results from Post-LN are used only as a reference. We expect them to be "random" because there is no correspondence between heads in adjacent layers for Post-/Pre-LN. Proof: An equivalent Post-/Pre-LN can be constructed by permuting the order of attention heads in a layer (and the corresponding variables).  <ref type="table">Table 6</ref>: Masked Language Modeling (MLM) accuracy on development set of BERT-Large with different dropout rates. When dropout rate is 0%, we report the best possible number with early-stop because all models start to overfit at around 500K steps.</p><p>for larger models. Note that head L i can still be useful even if it has exactly the same attention probabilities with head L−1 i because of the existence of the FFN sub-layer and the potential differences in value matrices (i.e., V in Eq. 2).</p><p>Can dropout outperform residual attention for regularizing large models? One may wonder whether increasing dropout rate can already regularize large models well so that residual attention is not necessary. To this end, we experiment with different dropout rates for pre-training BERT-Large with different Transformers (following the procedures in Section 4.1.1). Results are collected in Table 6, from which we can see that (1) RealFormer outperforms the two baselines across all dropout settings, and (2) simply increasing dropout rate can not regularize Transformer models as well as what residual attention appears to be doing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose RealFormer, a novel and simple Transformer architecture based on the idea of residual attention. We show that it can be used as a drop-in replacement of Transformer in BERT. Quantitatively, it considerably outperforms two canonical Transformer architectures, Post-LN and Pre-LN, on both pre-training and downstream tasks including GLUE and SQuAD. Furthermore, on downstream tasks, RealFormer even outperforms baselines pre-trained with twice the amount of epochs. Qualitatively, we show that RealFormer tends to have comparatively sparser attentions, both within heads and across heads in adjacent layers. Finally, we show that RealFormer can benefit significantly from hyper-parameter tuning, though the main results in this paper are not based on it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of different Transformer layers: (a) The prevalent Post-LN layer used by (e.g.) BERT; (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Masked Language Modeling (MLM) accuracy on development set (best viewed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Masked Language Modeling (MLM) accuracy on development set of BERT-Large with different learning rates (best viewed in color). RealFormer seems to benefit more from using a larger learning rate compared to Pre-LN. Note that Post-LN diverged with 2e-4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Distribution of Jensen-Shannon Divergence (JSD) of attention probabilities in (vertically) adjacent attention heads, i.e., JSD(head L i , head L−1 i ). Based on 8,192 held-out examples using the pre-trained BERT-Base with RealFormer (see Section 4.1.1). Distributions are color-coded based on the median of JSD: RED (median &gt; 0.75), YELLOW (0.25 ≤ median ≤ 0.75), BLUE (median &lt; 0.25). I.e., colder color means more "similar" attention heads across adjacent layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Model architectures for BERT evaluation. L: #layers, H: hidden size, A: #heads, I: intermediate size.</figDesc><table><row><cell>Model</cell><cell></cell><cell>L</cell><cell>H</cell><cell>A</cell><cell>I</cell></row><row><cell cols="2">BERT-Small</cell><cell>4</cell><cell>512</cell><cell cols="2">8 2,048</cell></row><row><cell>BERT-Base</cell><cell></cell><cell cols="4">12 768 12 3,072</cell></row><row><cell cols="2">BERT-Large</cell><cell cols="4">24 1,024 16 4,096</cell></row><row><cell cols="6">BERT-xLarge 36 1,536 24 6,144</cell></row><row><cell>Model</cell><cell cols="5">Post-LN Pre-LN RealFormer</cell></row><row><cell>BERT-Small</cell><cell></cell><cell cols="2">61.88% 61.67%</cell><cell></cell><cell>62.02%</cell></row><row><cell>BERT-Base</cell><cell></cell><cell cols="2">70.20% 69.74%</cell><cell></cell><cell>70.42%</cell></row><row><cell>BERT-Large</cell><cell></cell><cell cols="2">73.64% 73.21%</cell><cell></cell><cell>73.94%</cell></row><row><cell cols="4">BERT-xLarge 73.72% 73.53%</cell><cell></cell><cell>74.76%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Masked Language Modeling accuracy on development set after pre-training 1M steps.</figDesc><table /><note>5 RealFormer outperforms baselines more as model gets larger.its all hyper-parameter setups from Post-LN Trans- former unless otherwise specified. All experiments are performed on 128 or 256 TPU v3 cores depending on model sizes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Improvement gap of RealFormer over the best baseline tends to increase with model size. Note that these are with zero hyperparameter tuning for RealFormer. (As we will show in Section 4.1.3, RealFormer can be improved considerably by simply using a larger learning rate and thereby even double the gap size over Post-LN.)</figDesc><table><row><cell>Task</cell><cell cols="3">Post-LN Pre-LN RealFormer</cell></row><row><cell>MNLI-m</cell><cell cols="3">85.96±0.11 85.03±0.12 86.28±0.14</cell></row><row><cell cols="4">MNLI-nm 85.98±0.14 85.05±0.19 86.34±0.30</cell></row><row><cell>QQP</cell><cell cols="3">91.29±0.10 91.29±0.16 91.34±0.03</cell></row><row><cell>QQP (F1)</cell><cell cols="3">88.34±0.15 88.33±0.26 88.28±0.08</cell></row><row><cell>QNLI</cell><cell cols="3">92.26±0.15 92.35±0.26 91.89±0.17</cell></row><row><cell>SST-2</cell><cell cols="3">92.89±0.17 93.81±0.13 94.04±0.24</cell></row><row><cell cols="4">CoLA (MC) 58.85±1.31 58.04±1.50 59.83±1.06</cell></row><row><cell cols="4">STS-B (PC) 90.08±0.27 90.06±0.33 90.11±0.56</cell></row><row><cell cols="4">STS-B (SC) 89.77±0.26 89.62±0.28 89.88±0.54</cell></row><row><cell>MRPC</cell><cell cols="3">87.50±0.67 86.76±5.64 87.01±0.91</cell></row><row><cell cols="4">MRPC (F1) 91.16±0.45 90.69±3.16 90.91±0.65</cell></row><row><cell>RTE</cell><cell cols="3">71.12±2.52 68.59±1.52 73.65±0.90</cell></row><row><cell>Overall</cell><cell>84.01</cell><cell>83.47</cell><cell>84.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>GLUE development set results of fine-tuning BERT-Large models in Table 2. Default metric: ac- curacy, MC: Matthews correlation, PC: Pearson corre- lation, SC: Spearman correlation. Overall: first aver- age metrics within each task (if there are 1+) and then across tasks. Numbers in smaller font are standard de- viations. All numbers are scaled by 100.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>SQuAD Public Post-LN Pre-LN RealFormer v1.1 (F1) 90.9 91.68±0.12 91.06±0.09 91.93±0.12 v1.1 (EM) 84.1 85.15±0.13 83.98±0.24 85.58±0.15 v2.0 (F1) 81.9 82.51±0.12 80.30±0.12 82.93±0.05 v2.0 (EM) 78.7 79.57±0.12 77.35±0.16 79.95±0.08</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>SQuAD development set results of fine-tuning BERT-Large models in Table 2. EM: exact match. Public: Post-LN results from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>F1) 91.46±0.18 91.68±0.12 91.56±0.09 v1.1 (EM) 84.87±0.24 85.15±0.13 85.06±0.12 v2.0 (F1) 81.44±0.50 82.51±0.12</figDesc><table><row><cell>Task</cell><cell>Post-LN</cell><cell>Post-LN</cell><cell>RealFormer</cell></row><row><cell></cell><cell>(500K)</cell><cell>(1M)</cell><cell>(500K)</cell></row><row><cell>GLUE</cell><cell>83.84</cell><cell>84.01</cell><cell>84.34</cell></row><row><cell cols="4">v1.1 (82.52±0.55</cell></row><row><cell cols="3">v2.0 (EM) 78.64±0.48 79.57±0.12</cell><cell>79.54±0.54</cell></row><row><cell>Overall</cell><cell>83.97</cell><cell>84.37</cell><cell>84.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Downstream development set results of fine-</cell></row><row><cell>tuning BERT-Large with Post-LN and RealFormer pre-</cell></row><row><cell>trained with different number of steps. v*.*: SQuAD</cell></row><row><cell>version, EM: exact match. Overall: First average</cell></row><row><cell>across SQuAD and then GLUE. Numbers in smaller</cell></row><row><cell>font are standard deviations. All numbers are scaled by</cell></row><row><cell>100.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that a final LN module is usually added at the very top of the whole network.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Batch dimension is omitted for brevity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We found it to be beneficial for RealFormer to set attention scores at each layer to be the running mean (instead of running sum) for models deeper than 24 layers and therefore used this setup for RealFormer xLarge.6  All methods achieved similar (and great) results on the Next Sentence Prediction task presumably because it is much easier.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Appendices <ref type="figure">Figure 6</ref>: Distribution of entropies of the attention probabilities of the tokens of 8,192 held-out examples using the pre-trained BERT-Base with Post-LN Transformer (see Section 4.1.1). Attention heads in each layer are ordered by their medians of entropies for better legibility. Distributions are color-coded based on the median of entropies: RED (median &gt; 4.5), YELLOW (1.5 ≤ median ≤ 4.5), BLUE (median &lt; 1.5). I.e., colder colors mean sparser attentions. Note that here top layers (layer 9-11) tend to have larger entropies compared to RealFormer, which means that attentions are relatively denser.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Towards a human-like open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08483</idno>
		<title level="m">Etc: Encoding long and structured data in transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A robustly optimized bert pretraining approach. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>OpenAI Blog</publisher>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<title level="m">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schäfl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milena</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geir</forename><surname>Kjetil Sandve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Greiff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<title level="m">Hopfield networks is all you need</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ernie 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12412</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Linformer: Selfattention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04745</idno>
		<title level="m">On layer normalization in the transformer architecture</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<title level="m">Big bird: Transformers for longer sequences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
