<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-View Image Synthesis using Conditional GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Regmi</surname></persName>
							<email>krishna.regmi7@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
							<email>aborji@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-View Image Synthesis using Conditional GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to generate natural scenes has always been a challenging task in computer vision. It is even more painstaking when the generation is conditioned on images with drastically different views. This is mainly because understanding, corresponding, and transforming appearance and semantic information across the views is not trivial. In this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cGAN). Two new architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq) are proposed to generate scenes with resolutions of 64×64 and 256×256 pixels. X-Fork architecture has a single discriminator and a single generator. The generator hallucinates both the image and its semantic segmentation in the target view. X-Seq architecture utilizes two cGANs. The first one generates the target image which is subsequently fed to the second cGAN for generating its corresponding semantic segmentation map. The feedback from the second cGAN helps the first cGAN generate sharper images. Both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. The proposed methods show that they are able to capture and maintain the true semantics of objects in source and target views better than the traditional image-to-image translation method which considers only the visual appearance of the scene. Extensive qualitative and quantitative evaluations support the effectiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work, we address the problem of synthesizing ground-level images from overhead imagery and vice versa using conditional Generative Adversarial Networks <ref type="bibr" target="#b19">[20]</ref>. Primarily, such models try to generate new images from conditioning variables as input. Preliminary works in GANs utilize unsupervised learning to generate samples from latent representations or from a random noise vector <ref type="bibr" target="#b8">[9]</ref>.</p><p>View synthesis is a long-standing problem in computer ground-level/street-view (right). The images reflect the great diversity and richness of features in two views implying that the network needs to learn a lot for meaningful cross-view generation. We propose to use cGANs to solve this problem.</p><p>vision. This task is more challenging when views are drastically different, fields of views have little or no overlap, and objects are occluded. Furthermore, two objects that are similar in one view may look quite different in another (i.e., the view-invariance problem). For example, the aerial view of a building (i.e., the roof) tells very little about the color and design of the building seen from the street-view. The generation process is generally easier when the image contains a single object in a uniform background. In contrast, when the scene contains multiple objects, generating other view becomes much more challenging. This is due to the increase in underlying parameters that contribute to the variations (e.g., occlusions, shadows, etc). An example scenario, addressed here, is generating street-view (a.k.a ground level) image of a location from its aerial (a.k.a overhead) imagery. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates some corresponding images in two views. Isola et al. <ref type="bibr" target="#b11">[12]</ref> put forward a general-purpose framework to solve multiple image translation tasks. Their work translates images of objects or scenes which are represented by RGB images, gradient fields, edge maps, aerial images, sketches, etcetera between these representations. Thus, their method operates on representations in a single view. Formulating our problem as an image translation task between the views, we use their method as a baseline and extend it for cross-view image generation.</p><p>Inspired by recent works of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>, we formulate the cross-view image synthesis problem as an image-to-image translation problem and solve it using the conditional generative adversarial network. Previous works in view synthesis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31]</ref> have generated images with single objects in them or natural scenes with very little variation in viewing angles between the input and target images. The network learns to copy large parts of image content from the input. Images in each view of our work contain high degree of details and clutter (e.g., trees, cars, roads, buildings, etcetra) along with variations between the corresponding image pairs. Thus, the network needs to learn that the corresponding images in each view need to contain all details and place them in correct positions with proper orientations and inclinations. Perhaps the closest work to ours is the one by Zhai et al. <ref type="bibr" target="#b34">[35]</ref>. They generate ground-level panorama from aerial imagery by predicting ground image features from aerial image features and use them to synthesize images.</p><p>In general, some of the challenges pertaining to crossview synthesis task are as follows. First, aerial images cover wider regions of the ground than the street-view images whereas street-view images contain more details about objects (e.g., house, road, trees) than aerial images. So, not only the information in aerial images is too noisy, but also less informative for street-view image synthesis. Similarly, a network needs to estimate a lot regions to synthesize aerial images. Second, transient objects like cars (also people) are not present at the corresponding locations in image pairs since they are taken at different times. Third, houses that are different in street-view look similar from aerial view. This causes synthesized street-view images to contain buildings with similar color or texture, prohibiting diversity in generated buildings. Fourth challenge regards variation among roads in two views due to perspective and occlusions. While the road edges are nearly linear and visible in street-view, they are often occluded by dense vegetations and contorted in aerial view. Fifth, when using model generated segmentation maps as ground truth to improve the quality of generated images, as done here, label noise and model errors introduce some artifacts in the results.</p><p>To address the above challenges, we propose the following methods. We start with a simple image-to-image translation network of <ref type="bibr" target="#b11">[12]</ref> as a baseline. We then propose two new cGAN architectures that generate images as well as segmentation maps in target view. Addition of semantic segmentation generation to the architectures helps improve the generation of images. The first architecture, called X-Fork, is a slight modification of the baseline, forking at the penultimate block to generate two outputs, target view image and segmentation map. The second architecture, called X-Seq, has a sequence of two baseline networks connected. The target view image generated by the first network is fed to the second network to generate its corresponding segmentation map. Once trained, both architectures are able to generate better images than the baseline that learns to generate the images only. This implies that learning to generate segmentation map along with the image indeed improves the quality of generated image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Relating Aerial and Ground-level Images</head><p>Zhai et al. <ref type="bibr" target="#b34">[35]</ref> explored to predict the semantic layout of ground image from its corresponding aerial image. They used the predicted layout to synthesize ground-level panorama. Prior works relating the aerial and ground imageries have addressed problems such as cross-view colocalization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>, ground-to-aerial geo-localization <ref type="bibr" target="#b16">[17]</ref> and geo-tagging the cross-view images <ref type="bibr" target="#b33">[34]</ref>.</p><p>Cross-view relations have also been studied between egocentric (first person) and exocentric (surveillance or third-person) domains for different purposes. Human reidentification by matching viewers in top-view and egocentric cameras have been tackled by establishing the correspondences between the views in <ref type="bibr" target="#b0">[1]</ref>. Soran et al. <ref type="bibr" target="#b28">[29]</ref> utilize the information from one egocentric camera and multiple exocentric cameras to solve the action recognition task. Ardeshir et al. <ref type="bibr" target="#b1">[2]</ref> learn motion features of actions performed in ego-and exocentric domains to transfer motion information across the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning View Transformations</head><p>Existing works on viewpoint transformation have been conducted to synthesize novel views of the same objects <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>. Zhou et al. <ref type="bibr" target="#b37">[38]</ref> proposed models that learn to copy the pixel information from input view and utilize them to preserve the identity and structure of the objects to generate new views. Tatarchenko et al. <ref type="bibr" target="#b30">[31]</ref> trained an encodedecoder network to obtain 3D representation models of cars and chairs which they later used to generate different views of an unseen car or chair image. Dosovitskiy et al. <ref type="bibr" target="#b7">[8]</ref> learned generative models by training on 3D renderings of cars, chairs and tables and synthesized intermediate views and objects by interpolating between views and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">GAN and cGAN</head><p>Goodfellow et al. <ref type="bibr" target="#b8">[9]</ref> are the pioneers of Generative Adversarial Networks that is very successful at generating sharp and unblurred images, much better compared to existing methods such as Restricted Boltzmann Machines <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> or deep Boltzmann Machines <ref type="bibr" target="#b24">[25]</ref>.</p><p>Conditional GANs are used to synthesize images conditioned on different parameters during both training and testing. Examples include conditioning on labels of MNIST to generate digits by Mirza et al. <ref type="bibr" target="#b19">[20]</ref>, conditioning on image representations to translate an image between different representations <ref type="bibr" target="#b11">[12]</ref>, and generating panoramic groundlevel scenes from aerial images of the same location <ref type="bibr" target="#b34">[35]</ref>. Pathak et al. <ref type="bibr" target="#b22">[23]</ref> generated missing parts in images (i.e., inpainting) using networks trained jointly with adversarial and reconstruction losses and produced sharp and coherent images. Reed et al. <ref type="bibr" target="#b23">[24]</ref> synthesized images conditioned on detailed textual descriptions of the objects in the scene, and Zhang et al. <ref type="bibr" target="#b35">[36]</ref> improved on that by using a two-stage Stacked GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Cross-Domain Transformations using GANs</head><p>Kim et al. <ref type="bibr" target="#b12">[13]</ref> utilized the GAN networks to learn the relation between images in two different domains such that these learned relations can be transferred between the domains. Similar work by Zhu et al. <ref type="bibr" target="#b38">[39]</ref> learned mappings between unpaired images using cycle-consistency loss. They assume that a mapping from one domain to the other and back to the first should generate the original image. Both works exploited large unpaired datasets to learn the relation between domains and formulated the mapping task between images in different domains as a generation problem. Zhu et al. <ref type="bibr" target="#b38">[39]</ref> compare their generation task with previous works on paired datasets by Isola et al. <ref type="bibr" target="#b11">[12]</ref>. They conclude that the results with paired images is the upperbound for their unpaired examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background on GANs</head><p>Generative Adversarial Network architecture <ref type="bibr" target="#b8">[9]</ref> consists of two adversarial networks: a generator and a discriminator that are trained simultaneously based on the min-max game theory. The generator G is optimized to map a ddimensional noise vector (usually d=100) to an image (i.e., synthesizing) that is close to the true data distribution. The discriminator D, on the other hand, is optimized to accurately distinguish between the synthesized images coming from the generator and the real images from the true data distribution. The objective function of such a network is</p><formula xml:id="formula_0">min G max D L GAN (G, D) = E x ∼ pdata(x) [logD(x)]+ E z ∼ pz(z) [log(1 − D(G(z)))],<label>(1)</label></formula><p>where, x is real data sampled from data distribution p data and z is a d-dimensional noise vector sampled from a Gaussian distribution p z . Conditional GANs synthesize images looking into some auxiliary variable which may be labels <ref type="bibr" target="#b19">[20]</ref>, text embeddings <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref> or images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b12">13]</ref>. In conditional GANs, both the discriminator and the generator networks receive the conditioning variable represented by c in Eqn. <ref type="bibr" target="#b1">(2)</ref>. The generator uses this additional information during image synthesis while the discriminator makes its decision by looking at the pair of conditioning variable and the image it receives. Real pair input to the discriminator consists of true image from distribution and its corresponding label while the fake pair consists of synthesized image and the label. For conditional GAN, the objective function is</p><formula xml:id="formula_1">min G max D L cGAN (G, D) = E x,c ∼ pdata(x,c) [logD(x, c)] +E x ,c ∼ pdata(x ,c) [log(1 − D(x , c))],<label>(2)</label></formula><p>where x = G(z, c) is the generated image. In addition to the GAN loss, previous works (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b22">23]</ref>) have tried to minimize the L1 or L2 distances between real and generated image pairs. This step aids the generator to synthesize images very similar to the ground truth. Minimizing L1 distance generates less blurred images than minimizing the L2 distance. That is, using the L1 distance increases image sharpness in generation tasks. Therefore, we use the L1 distance in our method. The expression to minimize the L1 distance is</p><formula xml:id="formula_2">min G L L1 (G) = E x,x ∼ pdata(x,x ) [|| x − x || 1 ],<label>(3)</label></formula><p>The objective function for such conditional GAN network is the sum of Eqns. <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>.</p><p>Considering the synthesis of the ground level imagery (I g ) from aerial image (I a ), the conditional GAN loss and L1 loss are represented as in Eqns. <ref type="formula" target="#formula_3">(4)</ref> and <ref type="formula" target="#formula_4">(5)</ref>, respectively. For ground to aerial synthesis, the roles of I a and I g are reversed.</p><formula xml:id="formula_3">min G max D L cGAN (G, D) = E Ig,Ia ∼ pdata(Ig,Ia) [logD(I g , I a )] +E Ia,I g ∼ pdata(Ia,I g ) [log(1 − D(I g , I a ))],<label>(4)</label></formula><formula xml:id="formula_4">min G L L1 (G) = E Ig,I g ∼ pdata(Ig,I g ) [|| I g − I g || 1 ],<label>(5)</label></formula><p>where, I g = G(I a ). We employ the network of <ref type="bibr" target="#b11">[12]</ref> as our baseline architecture. The objective function for the baseline is the sum of conditional GAN loss in Eqn. (4) and L1 loss in Eqn. <ref type="bibr" target="#b4">(5)</ref>, as represented in Eqn. <ref type="formula" target="#formula_5">(6)</ref>:</p><formula xml:id="formula_5">L network = L cGAN (G, D) + λL L1 (G),<label>(6)</label></formula><p>where, λ is the balancing factor between the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed cGAN-based Approaches</head><p>In this section, we propose two architectures for the task of cross-view image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Crossview Fork (X-Fork)</head><p>Our first architecture, known as Crossview Fork, is shown in <ref type="figure" target="#fig_2">Figure 2a</ref>. The discriminator architecture is taken from the baseline <ref type="bibr" target="#b11">[12]</ref> but the generator network is forked to synthesize images as well as segmentation maps. The fork-generator architecture is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. The first six blocks of decoder share the weights. This is because the image and segmentation map contain a lot of shared features. The number of kernels used in each layer (block) of the generator are shown below the blocks.</p><p>Even though the X-Fork architecture generates the crossview image and its segmentation map, the discriminator receives only the real/fake image pairs but not the segmentation pairs during the training. In other words, the generated segmentation map serves as an auxiliary output. Notice that here we are primarily interested in generating higher quality images rather than the segmentation maps. Thus, the conditional GAN loss for this network is still the same as in Eqn. <ref type="bibr" target="#b3">(4)</ref>. To use the segmentation information, in addition to the L1 distance between the generated image and the real image, we also include the L1 distance between the ground-truth segmentation and the generated segmentation map into the loss.  lar to baseline architecture except that G forks to synthesize image and segmentation map in target view, and b) X-Seq: a sequence of two cGANs, G1 synthesizes target view image that is used by G2 for segmentation map synthesis in corresponding view. In both architectures, Ia and Ig are real images in aerial and ground views, respectively. Sg is the ground-truth segmentation map in streetview obtained using pre-trained RefineNet <ref type="bibr" target="#b15">[16]</ref>. I g and S g are synthesized image and segmentation map in ground view.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Crossview Sequential (X-Seq)</head><p>Our second architecture uses a sequence of two cGAN networks as shown in <ref type="figure" target="#fig_2">Figure 2b</ref>. The first network generates cross-view images similar to the baseline. The second network receives images from the first generator as conditioning input to synthesize the segmentation map in the same view. Thus, the first network is a cross-view cGAN while the second one is an image-to-segmentation cGAN. The whole architecture is trained end-to-end so that both cGANs learn simultaneously. Intuitively, the input-output dependency between the cGANs constrains the generated images and the segmentation maps, and in effect improves the quality of the generated outputs. Training the first network to generate better cross-view images enhances generation of better segmentation maps by the second generator. At the same time, the feedback from the better trained second network forces the first network to improve its generation. Thus, when both networks are trained in tandem, better quality images are generated compared to the baseline.</p><p>Replacing G and D in Eqns. <ref type="formula" target="#formula_3">(4)</ref> and <ref type="formula" target="#formula_4">(5)</ref> by G 1 and D 1 , respectively, we obtain the equivalent expressions for losses of cross-view cGAN network in this architecture. For the image-to-segmentation cGAN network, the images generated by G 1 are considered as conditioning inputs. We now express the cGAN loss for this network as</p><formula xml:id="formula_6">min G 2 max D 2 L cGAN (G 2 , D 2 ) = E I g ,Sg ∼ pdata(I g ,Sg) [logD 2 (S g , I g )]+ E S g ,I g ∼ pdata(S g ,I g ) [log(1 − D 2 (S g , I g ))],<label>(7)</label></formula><p>where, I g = G 1 (I a ) and S g = G 2 (I g ). The L1 loss for the image-to-segmentation network is</p><formula xml:id="formula_7">min G 2 L L1 (G 2 ) = E Sg,S g ∼ pdata(Sg,S g ) [|| S g − S g ) || 1 ],<label>(8)</label></formula><p>The overall objective function for the X-Seq network is</p><formula xml:id="formula_8">LX−Seq = LcGAN (G1, D1) + λLL1(G1) + LcGAN (G2, D2) + λLL1(G2).<label>(9)</label></formula><p>Eqn. <ref type="formula" target="#formula_8">(9)</ref> is optimized during the training to learn the parameters G 1 , D 1 , G 2 and D 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>For the experiments in this work, we use the cross-view image dataset provided by Vo et al. <ref type="bibr" target="#b32">[33]</ref>. This dataset consists of more than one million pairs of street-view and overhead view images collected from 11 different cities in the US. We select 76,048 image pairs from Dayton and create a train/test split of 55,000/21,048 pairs. We call it Dayton Dataset. The images in the original dataset have resolution of 354×354. We resize them to 256×256. Some example images are shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We also recruit the CVUSA dataset <ref type="bibr" target="#b33">[34]</ref> for direct comparison of our work with Zhai et al. <ref type="bibr" target="#b34">[35]</ref>. This dataset consists of 35,532/8,884 train/test split of image pairs. Following Zhai et al., the aerial images are center-cropped to 224 × 224 and then resized to 256 × 256. We only generate a single camera-angle image rather than the panorama. To do so, we take the first quarter of the ground level images and segmentations from the dataset and resize them to 256 × 256 in our experiments. Please see <ref type="figure" target="#fig_7">Figure 7</ref> for some images from the CVUSA dataset.</p><p>The two networks, X-Fork and X-Seq, learn to generate the target view images and segmentation maps conditioned on source view image. Training procedure requires the images as well as their semantic segmentation maps. The CVUSA dataset has annotated segmentation maps for ground view images, but for Dayton dataset such information is not available. To compensate, we use one of the leading semantic segmentation methods, known as the Re-fineNet <ref type="bibr" target="#b15">[16]</ref>. This network is pre-trained on outdoor scenes of the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref> and is used to generate the segmentation maps that are utilized as ground truth maps. These semantic maps have pixel labels from 20 classes (e.g., road, sidewalk, building, vegetation, sky, void, etc). <ref type="figure" target="#fig_4">Figure  4</ref> shows image pairs from the dataset and their segmentation masks overlaid in both views. As can be seen, the segmentation mask (label) generation process is far from perfect since it is unable to segment parts of buildings, roads, cars, etcetera in images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We use the conditional GAN architecture of <ref type="bibr" target="#b11">[12]</ref> as the baseline and call it Pix2pix. The generator is an encoderdecoder network with blocks of Convolution, Batch Normalization <ref type="bibr" target="#b10">[11]</ref> and activation layers. Leaky ReLU with a slope of 0.2 is used as the activation function in the encoder, whereas the decoder has ReLU activation except for its final layer where Tanh is used. The first three blocks of the decoder have a Dropout layer in between Batch normalization and activation layer, with dropout rate of 50%. The discriminator is similar to the encoder of the generator. The only difference is that the final layer uses sigmoid non-linearity that gives the probability of its input being real.</p><p>The used convolutional kernels are 4×4 with a stride of 2. The upconvolution in the decoder is Torch <ref type="bibr" target="#b4">[5]</ref> implementation of SpatialF ullConvolution, and upsamples the input by 2. For the encoder and the discriminator, convolutional operation downsamples the images by 2. No pooling operation is used in the networks. The λ used in Eqns. <ref type="bibr" target="#b5">(6)</ref> and <ref type="formula" target="#formula_8">(9)</ref> is the balancing factor between the GAN loss and L1 loss. Its value is fixed at 100. Following the idea to smooth the labels by <ref type="bibr" target="#b29">[30]</ref> and demonstration of its effectiveness by Salimans et al. <ref type="bibr" target="#b25">[26]</ref>, we use one-sided label smoothing to stabilize the training process, replacing 1 by 0.9 for real labels. During the training, we utilized different data augmentation methods like random jitter and horizontal flipping of images. The network is trained end-to-end with weights initialized with a random Gaussian distribution with zero mean and 0.02 standard deviation. It is implemented in Torch <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>Our experiments are conducted in a2g (aerial-to-ground) and g2a (ground-to-aerial) directions on Dayton dataset and a2g direction only on CVUSA dataset. We consider image resolutions of 64×64 and 256×256 on Dayton dataset while for experiments on CVUSA dataset, 256×256 resolution images are used.</p><p>First, we run experiments on lower resolution images (64×64) for proof of concept. Encouraging qualitative and quantitative results in this resolution motivated us to apply our methods to higher resolution (256×256) images. The lower resolution experiments are carried out for 100 epochs with batch size of 16, whereas the higher resolution experiments are conducted for 35 epochs with batch size of 4.</p><p>We conduct experiments on CVUSA dataset for comparison with Zhai et al.'s work <ref type="bibr" target="#b34">[35]</ref>. Following their setup, we train our architectures for 30 epochs, using the Adam optimizer and moment parameters β1 = 0.5 and β2 = 0.999.</p><p>It is not straightforward to evaluate the quality of synthesized images <ref type="bibr" target="#b2">[3]</ref>. In fact, evaluation of GAN methods continues to be an open problem <ref type="bibr" target="#b31">[32]</ref>. A common evaluation method is to show the generated images to human observers and ask their opinion about the images. Human judgment is based on the response to the question: Is this image (imagepair) real or fake? Alternatively, the images generated by different generative models can be pitted against each other and the observer is asked to select the image that looks more real. But in experiments involving natural scenes, such evaluation methods are more challenging as multiple factors often affect the quality of the generated images. For example, the observer may not be sure whether to base his judgment on better visual quality, higher sharpness at object boundaries, or more semantic information present in the image (e.g., multiple objects in the images, more details on objects, etc). Therefore, instead of behavioral experiments, we illustrate qualitative results in <ref type="figure" target="#fig_5">Figures 5, 6 and 7</ref> and conduct an in-depth quantitative evaluation on test images of two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Qualitative Evaluation</head><p>For 64×64 resolution experiments, the networks are modified by removing the last two blocks of CBL from discriminator and encoder, and the first two blocks of UBDR from decoder of the generator. We run experiments on all three methods. Qualitative results are depicted in <ref type="figure" target="#fig_5">Figure 5</ref>. The results affirm that the networks have learned to transfer the image representations across the views. Generated ground level images clearly show details about road, trees, sky, clouds, and pedestrian lanes. Trees, grass, road, house roofs are well rendered in the synthesized aerial images.</p><p>For 256×256 resolution synthesis, we conduct experiments on all three architectures and illustrate the qualitative results on Dayton and CVUSA datasets in  respectively. For Dayton dataset, we observe that the images generated in higher resolution contain more details of objects in both views and are less granulated than those in lower resolution. Houses, trees, pedestrian lanes, and roads look more natural. Test results on CVUSA dataset show that images generated by proposed methods are visually better compared to Zhai et al. <ref type="bibr" target="#b34">[35]</ref> and Pix2pix <ref type="bibr" target="#b11">[12]</ref> methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Quantitative Evaluation</head><p>The quantitative results of our experiments on both datasets are presented in <ref type="table" target="#tab_1">Tables 1-4</ref>. 64×64 and 256×256 in column headers of the tables refer to results obtained for two resolutions of Dayton dataset. Next, we discuss the quantitative measures used to evaluate our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Inception Score</head><p>A common quantitative GAN evaluation measure is the Inception Score <ref type="bibr" target="#b25">[26]</ref>. The core idea behind the inception score is to assess how diverse the generated samples are within a class while being meaningfully representative of the class at the same time.</p><p>InceptionScore, I = exp(E x D KL (p(y|x)||p(y))), <ref type="bibr" target="#b9">(10)</ref> where, x is a generated sample and y is its predicted label. We can not use the Inception model because the datasets that we use include natural outdoor images that do not fit into ImageNet classes <ref type="bibr" target="#b6">[7]</ref>. To solve this, we use the AlexNet model <ref type="bibr" target="#b13">[14]</ref> trained on Places dataset <ref type="bibr" target="#b36">[37]</ref> with 365 categories to compute the inception score. The Places dataset has images similar to those in our datasets. The scores are reported in <ref type="table" target="#tab_1">Table 1</ref>. The scores for X-Fork generated images are closest to that of real data distribution for Dayton dataset in lower resolution in both directions and also in higher resolution in a2g direction. The X-Seq method works best for CVUSA dataset and for g2a synthesis in higher resolution over Dayton dataset.</p><p>We observe that the confidence scores predicted by the pre-trained model on our dataset are dispersed between classes for many samples and not all the categories are represented by the images. Therefore, we compute inception scores on Top-1 and Top-5 classes, where "Top-k" means that top k predictions for each image are unchanged while the remaining predictions are smoothed by an epsilon equal to (1 -(top-k predictions))/(n-k classes). Results on top-k classes follow a similar pattern as in all classes (except for Top-1 class on lower resolution in g2a over Dayton dataset).</p><p>In addition to inception score, we compute the top-k prediction accuracy between real and generated images. We use the same pre-trained Alexnet model to obtain annotations for real images and class predictions for generated images. We compute top-1 and top-5 accuracies. Results are shown in <ref type="table" target="#tab_2">Table 2</ref>. For each setting, accuracies are computed in two ways: 1) considering all images, and 2) considering real images whose top-1 (highest) prediction is greater than 0.5. Below each accuracy heading, the first column considers all images whereas the second column computes accu-   racies the second way. For lower resolution images on Dayton dataset and for experiments on CVUSA dataset, X-Fork method outperforms the remaining methods. For higher resolution images, our methods show dramatic improvements over Pix2pix in the a2g direction, whereas X-Seq works best in the g2a direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">KL(model data)</head><p>We next compute the KL divergence between the model generated images and the real data distribution for quantitative analysis of our work, similar to some generative works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>. We again use the same pre-trained Alexnet as in the previous subsection. The lower KL score implies that the generated samples are closer to the real data distribution. The scores are provided in <ref type="table" target="#tab_3">Table 3</ref>. As it can be seen, our proposed methods generate much better results than existing generative methods on both datasets. X-Fork generates images very similar to real distribution in all ex-periments except on the higher resolution a2g experiment where X-Seq is slightly better than X-Fork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">SSIM, PSNR and Sharpness Difference</head><p>As in some generative works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref>, we also employ Structural-Similarity (SSIM), Peak Signal-to-Noise Ratio (PSNR) and Sharpness Difference measures to evaluate our methods.</p><p>SSIM measures the similarity between the images based on their luminance, contrast and structural aspects. SSIM value ranges between -1 and +1. A higher value means greater similarity between the images being compared. It is computed as</p><formula xml:id="formula_9">SSIM (I g , I g ) = (2µ Ig µ I g + c 1 )(2σ IgI g + c 2 ) (µ 2 Ig + µ 2 I g + c 1 )(σ 2 Ig + σ 2 I g + c 2 )<label>(11)</label></formula><p>PSNR measures the peak signal-to-noise ratio between two images to assess the quality of a transformed (generated) image compared to its original version. The higher the PSNR, the better is the quality of generated image. It is computed as P SN R(I g , I g ) = 10log 10 (</p><formula xml:id="formula_10">max 2 I g mse )<label>(12)</label></formula><p>where, mse(I g , I g ) = 1 n n i=1 (I g [i] − I g [i]) 2 , and max I g = 255 (maximum pixel intensity value). Sharpness difference measures the loss of sharpness during image generation. To compute the sharpness difference between the generated image and the true image, we follow <ref type="bibr" target="#b18">[19]</ref> and compute the difference of gradients between the images as SharpDif f.(I g , I g ) = 10log 10 (</p><formula xml:id="formula_11">max 2 I g grads ),<label>(13)</label></formula><p>where, grads = 1</p><formula xml:id="formula_12">N i j (|( i I g + j I g )-( i I g + j I g )|) and, i I = |I i,j − I i−1,j | , j I = |I i,j − I i,j−1 |.</formula><p>Sharpness difference in Eqn. <ref type="formula" target="#formula_0">(13)</ref> is inverse of grads. We would like the grads to be small, so the higher the overall score the better.</p><p>The scores are reported in <ref type="table" target="#tab_4">Table 4</ref>. Over Dayton dataset, X-Seq model works the best in a2g direction while X-Fork outperforms the rest in the g2a direction. On CVUSA, X-Fork improves over Zhai et al. by 5.03% in SSIM, 8.93% in PSNR, and 12.35% in Sharpness difference.</p><p>Because there is no consensus in evaluation of GANs, we had to use several scores. Theis et al. <ref type="bibr" target="#b31">[32]</ref> show that these scores often do not agree with each other and this was observed in our evaluations as well. So, it is difficult to infer whether X-Fork or X-Seq is better. We find that the proposed methods are consistently superior to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Generated Segmentation Maps</head><p>Our methods generate semantic segmentation maps along with the real images in cross-view. The overlay of segmentation maps generated by X-Fork network (pretrained RefineNet) on Dayton images are presented in the last (second) column of <ref type="figure" target="#fig_4">Figure 4</ref>. Please see supplementary materials for more qualitative results on semantic segmentation. The overlaid images show that the network is able to learn the semantic representations of object classes. For quantitative analysis, segmentation maps generated by our methods are compared against the segmentation maps obtained by applying RefineNet <ref type="bibr" target="#b15">[16]</ref> to the target images. We compute per-class accuracies and mean IOU for the most common classes in our datasets: 'vegetation', 'road' and 'building' in aerial segmentation maps plus the 'sky' in ground segmentations. The scores are reported in <ref type="table" target="#tab_5">Table 5</ref>. Even though X-Fork does better than X-Seq, we find that both methods achieve good scores for segmentation.  responding street-view image synthesized by X-Seq method and three nearest images in the training set retrieved by computing L1 distance between generated image and training set images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">kNN</head><p>Here, we test whether our proposed architectures have actually learned the representations between images in two views rather than just memorizing the blocks from training images to generate new ones. For this, we pick three images from the training set that are closest to the generated images in terms of L1 distance. As shown in <ref type="figure" target="#fig_8">Figure 8</ref>, the generated images have subtle differences with the training set images implying that our network has indeed learned important semantic representations in input view needed to transform the source image to target view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Conclusion</head><p>We explored image generation using conditional GANs between two drastically different views. Generating semantic segmentations together with images in target view helps the networks learn better images compared to the baselines. Extensive qualitative and quantitative evaluations testify the effectiveness of our methods. Using higher resolution images provided significant improvements in visual quality and added more details to synthesized images. The challenging nature of the problem leaves room for further improvements. Code and data will be shared.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example images in overhead/aerial view (left) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>X-Seq architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Our proposed network architectures. a) X-Fork: Simi-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Generator of X-Fork architecture inFigure 2a. BN means batch-normalization layer. The first six blocks of decoder share weights, forking at the penultimate block. The number of channels in each convolution layer are shown below each blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Original image pairs from training set (left), images with segmentation masks from pre-trained RefineNet [16] overlaid on original images (middle) and images with segmentation masks generated by X-Fork network overlaid on original images (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FiguresFigure 5 :</head><label>5</label><figDesc>Example images generated by different methods in lower (64 × 64) resolution in a2g and g2a directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Example images generated by different methods in higher (256 × 256) resolution in a2g and g2a directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of our methods and baselines on CVUSA dataset in a2g direction. First two columns show true image pairs, next four columns show images generated by Zhai et al.<ref type="bibr" target="#b34">[35]</ref>, Pix2pix<ref type="bibr" target="#b11">[12]</ref>, X-Fork and X-Seq methods, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Along the columns, we show real image pairs, cor-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Aerial Image Street view Image Street view Segmentation 64 128 256 512 512 512 512 512 Upconvolution + BN + Dropout + ReLU(UBDR) 512 512 512 512 512 256 128 64 64 128 256 512 512 512 512 512 Convolution + BN + lReLU (CBL) Convolution + lReLU (CL)</head><label></label><figDesc></figDesc><table><row><cell>&amp;</cell><cell>&amp;</cell><cell>Upconvolution + BN</cell></row><row><cell></cell><cell></cell><cell>+ ReLU (UBR)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>KL divergence scores between conditional and marginal probabilities (Inception Score).</figDesc><table><row><cell>Dir.</cell><cell>Methods</cell><cell></cell><cell>64×64</cell><cell></cell><cell></cell><cell>256×256</cell><cell></cell><cell></cell><cell>CVUSA</cell><cell></cell></row><row><cell></cell><cell></cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell></cell><cell></cell><cell>classes</cell><cell>class</cell><cell>classes</cell><cell>classes</cell><cell>class</cell><cell>classes</cell><cell>classes</cell><cell>class</cell><cell>classes</cell></row><row><cell></cell><cell>Zhai et al. [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.8434</cell><cell>1.5171</cell><cell>1.8666</cell></row><row><cell></cell><cell>Pix2pix [12]</cell><cell>1.8029</cell><cell>1.5014</cell><cell>1.9300</cell><cell>2.8515</cell><cell>1.9342</cell><cell>2.9083</cell><cell>3.2771</cell><cell>2.2219</cell><cell>3.4312</cell></row><row><cell>a2g</cell><cell>X-Fork</cell><cell>1.9600</cell><cell>1.5908</cell><cell>2.0348</cell><cell>3.0720</cell><cell>2.2402</cell><cell>3.0932</cell><cell>3.4432</cell><cell>2.5447</cell><cell>3.5567</cell></row><row><cell></cell><cell>X-Seq</cell><cell>1.8503</cell><cell>1.4850</cell><cell>1.9623</cell><cell>2.7384</cell><cell>2.1304</cell><cell>2.7674</cell><cell>3.8151</cell><cell>2.6738</cell><cell>4.0077</cell></row><row><cell></cell><cell>Real Data</cell><cell>2.2096</cell><cell>1.6961</cell><cell>2.3008</cell><cell>3.7090</cell><cell>2.5590</cell><cell>3.7900</cell><cell>4.9971</cell><cell>3.4122</cell><cell>5.1150</cell></row><row><cell></cell><cell>Pix2pix [12]</cell><cell>1.7970</cell><cell>1.3029</cell><cell>1.6101</cell><cell>3.5676</cell><cell>2.0325</cell><cell>2.8141</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>g2a</cell><cell>X-Fork</cell><cell>1.8557</cell><cell>1.3162</cell><cell>1.6521</cell><cell>3.1342</cell><cell>1.8656</cell><cell>2.5599</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-Seq</cell><cell>1.7854</cell><cell>1.3189</cell><cell>1.6219</cell><cell>3.5849</cell><cell>2.0489</cell><cell>2.8414</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Real Data</cell><cell>2.1408</cell><cell>1.4302</cell><cell>1.8606</cell><cell>3.8979</cell><cell>2.3146</cell><cell>3.1682</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracies: Top-1 and Top-5.</figDesc><table><row><cell>Dir.</cell><cell>Methods</cell><cell></cell><cell cols="2">64×64</cell><cell></cell><cell></cell><cell cols="2">256×256</cell><cell></cell><cell></cell><cell cols="2">CVUSA</cell></row><row><cell></cell><cell></cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell></row><row><cell></cell><cell>Zhai et al. [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.97</cell><cell>14.03</cell><cell>42.09</cell><cell>52.29</cell></row><row><cell></cell><cell>Pix2pix [12]</cell><cell>7.90</cell><cell>15.33</cell><cell>27.61</cell><cell>39.07</cell><cell>6.8</cell><cell>9.15</cell><cell>23.55</cell><cell>27.00</cell><cell>7.33</cell><cell>9.25</cell><cell>25.81</cell><cell>32.67</cell></row><row><cell cols="2">a2g X-Fork</cell><cell>16.63</cell><cell>34.73</cell><cell>46.35</cell><cell>70.01</cell><cell>30.00</cell><cell>48.68</cell><cell>61.57</cell><cell>78.84</cell><cell>20.58</cell><cell>31.24</cell><cell>50.51</cell><cell>63.66</cell></row><row><cell></cell><cell>X-Seq</cell><cell>4.83</cell><cell>5.56</cell><cell>19.55</cell><cell>24.96</cell><cell>30.16</cell><cell>49.85</cell><cell>62.59</cell><cell>80.70</cell><cell>15.98</cell><cell>24.14</cell><cell>42.91</cell><cell>54.41</cell></row><row><cell></cell><cell>Pix2pix [12]</cell><cell>1.65</cell><cell>2.24</cell><cell>7.49</cell><cell>12.68</cell><cell>10.23</cell><cell>16.02</cell><cell>30.90</cell><cell>40.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">g2a X-Fork</cell><cell>4.00</cell><cell>16.41</cell><cell>15.42</cell><cell>35.82</cell><cell>10.54</cell><cell>15.29</cell><cell>30.76</cell><cell>37.32</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-Seq</cell><cell>1.55</cell><cell>2.99</cell><cell>6.27</cell><cell>8.96</cell><cell>12.30</cell><cell>19.62</cell><cell>35.95</cell><cell>45.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>KL Divergence between model and data distributions.</figDesc><table><row><cell>Dir. Method</cell><cell>64×64</cell><cell>256×256</cell><cell>CVUSA</cell></row><row><cell>Zhai et al. [35]</cell><cell>-</cell><cell>-</cell><cell>27.43 ± 1.63</cell></row><row><cell>Pix2pix [12]</cell><cell cols="3">6.29 ± 0.8 38.26 ± 1.88 59.81 ± 2.12</cell></row><row><cell>a2g X-Fork</cell><cell cols="3">3.42 ± 0.72 6.00 ± 1.28 11.71 ± 1.55</cell></row><row><cell>X-Seq</cell><cell cols="3">6.22 ± 0.87 5.93 ± 1.32 15.52 ± 1.73</cell></row><row><cell>Pix2pix [12]</cell><cell cols="2">6.39 ± 0.90 7.88 ± 1.24</cell><cell>-</cell></row><row><cell>g2a X-Fork</cell><cell cols="2">4.45 ± 0.84 6.92 ± 1.15</cell><cell>-</cell></row><row><cell>X-Seq</cell><cell cols="2">7.20 ± 0.92 7.07 ± 1.19</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>SSIM, PSNR and Sharpness Difference between real data and samples generated using different methods.</figDesc><table><row><cell>Dir.</cell><cell>Methods</cell><cell></cell><cell>64×64</cell><cell></cell><cell></cell><cell>256×256</cell><cell></cell><cell></cell><cell>CVUSA</cell></row><row><cell></cell><cell></cell><cell>SSIM</cell><cell>PSNR</cell><cell>Sharp Diff</cell><cell>SSIM</cell><cell>PSNR</cell><cell>Sharp Diff</cell><cell>SSIM</cell><cell>PSNR</cell><cell>Sharp Diff</cell></row><row><cell></cell><cell>Zhai et al. [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.4147</cell><cell>17.4886</cell><cell>16.6184</cell></row><row><cell></cell><cell>Pix2pix [12]</cell><cell>0.4808</cell><cell>19.4919</cell><cell>16.4489</cell><cell>0.4180</cell><cell>17.6291</cell><cell>19.2821</cell><cell>0.3923</cell><cell>17.6578</cell><cell>18.5239</cell></row><row><cell cols="2">a2g X-Fork</cell><cell>0.4921</cell><cell>19.6273</cell><cell>16.4928</cell><cell>0.4963</cell><cell>19.8928</cell><cell>19.4533</cell><cell>0.4356</cell><cell>19.0509</cell><cell>18.6706</cell></row><row><cell></cell><cell>X-Seq</cell><cell>0.5171</cell><cell>20.1049</cell><cell>16.6836</cell><cell>0.5031</cell><cell>20.2803</cell><cell>19.5258</cell><cell>0.4231</cell><cell>18.8067</cell><cell>18.4378</cell></row><row><cell></cell><cell>Pix2pix [12]</cell><cell>0.3675</cell><cell>20.5135</cell><cell>14.7813</cell><cell>0.2693</cell><cell>20.2177</cell><cell>16.9477</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">g2a X-Fork</cell><cell>0.3682</cell><cell>20.6933</cell><cell>14.7984</cell><cell>0.2763</cell><cell>20.5978</cell><cell>16.9962</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-Seq</cell><cell>0.3663</cell><cell>20.4239</cell><cell>14.7657</cell><cell>0.2725</cell><cell>20.2925</cell><cell>16.9285</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Evaluation Scores for segmentation maps.</figDesc><table><row><cell>Methods</cell><cell>a2g</cell><cell></cell><cell>g2a</cell><cell></cell></row><row><cell></cell><cell cols="4">Per-class acc. mIOU Per-class acc. mIOU</cell></row><row><cell>X-Fork</cell><cell>0.6262</cell><cell>0.4163</cell><cell>0.5473</cell><cell>0.2157</cell></row><row><cell>X-Seq</cell><cell>0.4783</cell><cell>0.3187</cell><cell>0.4990</cell><cell>0.2139</cell></row><row><cell>Real pairs</cell><cell>X-Seq</cell><cell></cell><cell cols="2">Closest images from training set</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We would like to thank NVIDIA for the donation of the Titan-X GPUs used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ego2top: Matching viewers in egocentric and top-view videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ardeshir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Egotransfer: Transferring motion across egocentric and exocentric domains using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ardeshir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Regmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno>abs/1612.05836</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pros and cons of gan evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03446</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1612.02136</idno>
		<title level="m">Mode regularized generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="692" to="705" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks. Commun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM</publisher>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RefineNet: Multipath refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="5007" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-view image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep multiscale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transformation-grounded image generation network for novel 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1703.02921</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>M. F. Balcan and K. Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="194" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition in the presence of one egocentric and multiple static cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Soran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2014 -12th Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">9007</biblScope>
			<biblScope unit="page" from="178" to="193" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-view 3d models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016-04" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Localizing and Orienting Street Views Using Overhead Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Predicting ground-level scene layout from aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bessinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
