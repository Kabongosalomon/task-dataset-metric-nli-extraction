<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Self-Supervised Single View Depth Estimation by Masking Occlusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Schellevis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Radboud University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Self-Supervised Single View Depth Estimation by Masking Occlusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single view depth estimation models can be trained from video footage using a self-supervised end-to-end approach with view synthesis as the supervisory signal. This is achieved with a framework that predicts depth and camera motion, with a loss based on reconstructing a target video frame from temporally adjacent frames. In this context, occlusion relates to parts of a scene that can be observed in the target frame but not in a frame used for image reconstruction. Since the image reconstruction is based on sampling from the adjacent frame, and occluded areas by definition cannot be sampled, reconstructed occluded areas corrupt to the supervisory signal. In previous work [6] occlusion is handled based on reconstruction error; at each pixel location, only the reconstruction with the lowest error is included in the loss. The current study aims to determine whether performance improvements of depth estimation models can be gained by during training only ignoring those regions that are affected by occlusion.</p><p>In this work we introduce occlusion mask, a mask that during training can be used to specifically ignore regions that cannot be reconstructed due to occlusions. Occlusion mask is based entirely on predicted depth information. We introduce two novel loss formulations which incorporate the occlusion mask. The method and implementation of <ref type="bibr" target="#b5">[6]</ref> serves as the foundation for our modifications as well as the baseline in our experiments. We demonstrate that (i) incorporating occlusion mask in the loss function improves the performance of single image depth prediction models on the KITTI benchmark. (ii) loss functions that select from reconstructions based on error are able to ignore some of the reprojection error caused by object motion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Knowing the depth of your surroundings is essential for navigating within an environment. We perceive the majority of our depth information visually without any substantial effort or thought. The importance of depth information becomes more apparent when visual stimuli are diminished or absent. For example visually impaired individuals require a white cane or guide dog to scan their surroundings for obstacles. In the animal kingdom, there are numerous animal species that have evolved to live in low-light conditions or complete darkness and use echolocation to know where they are and what surrounds them. Without explicitly learning how, we are able to perceive the relative distances of objects in our environment.</p><p>Depth information is also important for technologies such as adaptive cruise control and autonomous emergency braking or for use in autonomous systems such as robots and self-driving vehicles. In these systems depth information can be used to decide whether to accelerate, brake or steer. Sonar, radar, and lidar <ref type="bibr" target="#b0">1</ref> are examples of technologies that can be used to measure this information directly. As a complementary source of information or as a costeffective alternative, depth can be predicted from camera data. Ground truth depth data collected using one of the aforementioned techniques can be used to train a depth estimation model in a supervised manner. As an alternative, similar to depth perception in the natural world, it is possible to train a depth estimation model using a self-supervised approach, using no ground truth depth information but only stereo image pairs <ref type="bibr" target="#b2">[3]</ref> or video data from a single camera <ref type="bibr" target="#b10">[11]</ref>.</p><p>A single image depth prediction model can be trained using image reconstruction as the supervision signal. This reconstruction is done using image pairs, where the images are of the same scene, but taken from different positions. If a depth prediction is made for one of the images from such a pair, using information about the change of camera position, the first image can be reconstructed from the second image. By minimizing the difference between the original image and the reconstruction the task of depth prediction is learned.</p><p>The idea of learning depth by image reconstruction was used by <ref type="bibr" target="#b2">[3]</ref> on stereoscopic images with a known fixed camera transformation. <ref type="bibr" target="#b10">[11]</ref> showed that it is possible to train depth prediction models on video data using image reconstruction as a supervision signal, by adding a parallel network that predicts the image-pair camera transformation that is required for image reconstruction. The reconstruc-tion computation will be discussed in further detail in the methods section.</p><p>Compared to using stereoscopic images, learning this task from video has some unique challenges. Stereoscopic images can be taken simultaneously capturing a scene at a single point in time, whereas video frames are inherently captured at different points in time. If a video contains a dynamic scene (e.g. objects are moving) and this is not taken into consideration during training, moving objects will appear incorrectly in the reconstructed image and thereby corrupt the learning signal.</p><p>The authors of <ref type="bibr" target="#b10">[11]</ref> tried to tackle the dynamic scene problem by predicting a 'motion explanation mask' that can be used to ignore such regions. In later implementations of their work that are available online 2 , this mask was disabled, producing better results. In <ref type="bibr" target="#b0">[1]</ref> , instance segmentation masks are used to handle object motion. These instance segmentation masks are only created for known object categories, using a pretrained mask R-CNN model <ref type="bibr" target="#b6">[7]</ref>.</p><p>Another challenge is that the camera transformations between video frames vary, in contrast to the fixed cameradistance of stereo image pairs. When there is little to no camera movement, adjacent frames are nearly identical, and hardly provide any learnable information. <ref type="bibr" target="#b10">[11]</ref> attempts to reduce this problem by filtering out the nearly identical frames from the training data.</p><p>When not taking object motion into account during training, a model may learn to make incorrect depth predictions to compensate for the reconstruction mistakes caused by object motion. For example, objects that during training were often observed while they were moving at the same velocity as the camera, will incorrectly be predicted as being far away. This happens because the observed behavior of same-velocity-objects and distant stationary objects is similar; Their appearance does not change (much) from frame to frame. To counter this phenomenon <ref type="bibr" target="#b5">[6]</ref> applies a mask (called automask) to ignore pixels that do not change appearance from one frame to the adjacent frame. In addition of ignoring objects which appear static because they move with the same velocity as the camera, this mask will also ignore entire frames when the camera does not move.</p><p>One of the phenomena encountered when training a depth prediction model using image reconstruction is called occlusion. Occlusion relates to parts of a scene that can only be observed from one of two camera positions. For example, regions at the image boundaries may move in or out of view when the camera position changes. Another cause of occlusion is parallax, where for two different camera positions the apparent position of closer objects changes more, hiding or revealing what is behind them. Both types of occlusion effects can be seen in figure 1. Incorporating these occlusion effects in the loss function during training 2 https://github.com/tinghuiz/SfMLearner can negatively impact the predictive performance of a depth prediction model since they do not provide meaningful information about the correctness of the depth predictions. The lane markings at the bottom of the image are incorrectly reconstructed because they are not visible in the next frame. The tree on the right-hand side of the road appears twice in this reconstruction. This happens when areas with different predicted depths are projected to the same area. In other words, the sampling would have been correct if the tree was not blocking the view.</p><p>Various solutions have been proposed to ignore these occlusion effects during training. In <ref type="bibr" target="#b8">[9]</ref> a mask is applied in the loss function to account for occlusion effects at the image boundaries. <ref type="bibr" target="#b5">[6]</ref> tackles both types of occlusion at the same time using a loss function called "Per-pixel minimum reprojection loss", which at each pixel location selects the best reconstruction and ignores the other ones. The idea behind this loss function is that when a pixel is occluded in the adjacent video frame, a correct reconstruction of the pixel is unlikely, and the reconstruction of that pixel will not be used to optimize the depth prediction model.</p><p>Since the per-pixel minimum reprojection loss selects which reconstruction will be used per pixel location, it effectively creates a binary mask for each of the reconstructions. This is illustrated in <ref type="figure" target="#fig_1">figure 2</ref>. The frame which is the target for image reconstruction is shown at the top of the figure. On the second row of the figure, two reconstructed images are shown, made from the next and previous video frames respectively. A binary map 3 is located below each of these reconstructions. These maps display for each pixel of the reconstruction above it whether it is used or ignored by the per-pixel minimum reprojection loss. From these binary maps, it can be observed that regions which are mostly black coincide with occlusion effects. This indicates that per-pixel minimum reprojection loss is able to ignore occlusion effects successfully. What also can be observed is that the pixels of the reconstruction target image for which the contents are visible in both adjacent frames, still only one of the reconstructions is selected to be used in the loss, and the other one is ignored.</p><p>The hypothesis that I will test in this work is that by limiting the amount of information which is ignored during training to only those regions that suffer from occlusion, more useful information will be available for training the model. The approach I propose is occlusion mask, a mask that during training can be used to specifically discard regions that cannot be reconstructed due to occlusions. The images at the bottom row of figure 2 illustrate the concept. Occlusion mask is based entirely on depth information. We will test the hypothesis by incorporating the proposed occlusion mask method into two novel loss functions, use these loss functions to train depth prediction models, and compare their performance with that of a model trained with per-pixel minimum reprojection loss. Additionally we will compare the behavioral differences of the loss functions, i.e. those that incorporate the proposed method and the per-pixel minimum reprojection loss, by visualizing the computed loss on training examples. With the occlusion mask we hope to improve the performance of future single image depth prediction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We start by reviewing the framework introduced by <ref type="bibr" target="#b10">[11]</ref> for training a single-view depth network from unlabeled video data, including its core concept: image reconstruction. This will be followed by a review of the loss function computation of the method and implementation 4 by Godard et al <ref type="bibr" target="#b5">[6]</ref>, which serves as the foundation for our modifications as well as the baseline in our experiments. We will finish the method section by describing our proposed occlusion mask and introducing two novel loss formulations which incorporate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self-supervised training 2.1.1 Framework</head><p>All of the models in our experiments are trained using the same framework introduced by <ref type="bibr" target="#b10">[11]</ref> consisting of one network predicting a depth map from a single image (i.e. in this case a frame from a video), and a pose network predicting camera transformation from two consecutive images ( <ref type="figure" target="#fig_2">figure  3</ref>). The two networks are trained together using the same loss but can be used separately after training. The loss used for training is based on warping nearby frames, to create a new image that conforms (as best as possible) to the image for which the depth map is being predicted. This 'warping' means that each pixel is sampled at their new location in the nearby frame, which can be computed using the predictions of the depth map and camera transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Image reconstruction</head><p>A frame is reconstructed by sampling from one of the adjacent video frames. Equation 1 shows how the sample location in the adjacent frame is computed for each pixel in the reconstruction. In this equation the subscript is used to indicate the point in time, t is "now", i.e. the time of the frame for which when the depth is predicted, t is the time point of the next or previous frame, and t → t is the transition from one to the other.</p><p>The used formulation of the image reconstruction has the following implicit assumptions <ref type="bibr" target="#b10">[11]</ref>: 1) the scene is static without moving objects; 2) there is no occlusion/disocclusion between the target view and the source views; 3) the surfaces appear uniformly bright from all directions of view so that the photo-consistency error is meaningful</p><formula xml:id="formula_0">    x t→t z t→t y t→t z t→t z t→t 1     = KT t→t K −1     x t z t y t z t z t 1    <label>(1)</label></formula><p>We will first describe equation 1 and follow with an example that illustrates the computational steps. The purpose of this equation is to compute the sampling location (x t→t , y t→t ) in frame I t for each pixel at position (x t , y t ) in frame I t . In this equation the rightmost column vector consists of the homogeneous coordinates of a pixel in frame I t where z t is the predicted depth of that pixel. The leftmost column vector consists of the homogeneous coordinates of the pixel after projection, i.e. the location at which the pixel is to be expected in frame I t given the predicted depth and camera transformation. The 4x4 camera transformation matrix T t→t contains the rotation and translation of the camera as predicted by the pose network. The matrices K and K −1 denote the camera intrinsics matrix and its inverse, which transforms camera coordinates to image coordinates using camera properties such as focal length and principal point offset. <ref type="bibr" target="#b4">5</ref> In figure 4a &amp; 4b you can see a picture and its predicted depth map. <ref type="figure" target="#fig_3">Figure 4c</ref> shows the outline of an image (the black rectangle) and its pixels neatly arranged in a grid (the blue dots). The depth information of 4b can be added to each pixel in 4c to create the point cloud shown in 4d, where the dots are bigger when the point is closer to the camera. 4e shows the frame that comes next after frame 4a. The camera transformation (from 4a to 4e) predicted by the pose network can be used to calculate where the points of 4d will end up, this is shown in 4f. <ref type="figure" target="#fig_3">Figure 4g</ref> shows the points from 4f but with the depth information removed. Frame 4a can <ref type="bibr" target="#b4">5</ref> To get an idea of how the camera transformation matrix and the camera intrinsics matrix affect what is seen in an image, you can have a look at this interactive tool: https://ksimek.github.io/ perspective_camera_toy.html be reconstructed by sampling at the new pixel positions (4g) in 4e. <ref type="figure" target="#fig_3">Figure 4h</ref> shows this reconstruction. Effects of the sample locations can be seen in the reconstruction, for example pixels where the sample location is outside of the image (the black rectangle) the closest border pixel is used instead. Another example is the small area near the tail light of the (red) car from which there is no sampling as can be seen in 4g. This area in 4e shows the headlamp of the car in the rear, which is not visible (occluded) in the target image 4a.</p><p>The sample coordinates (x t→t , y t→t ) are continuous values. The reconstructed frame I t →t can be made by sampling at these projected pixel coordinates in frame I t using the (sub-)differentiable bilinear sampling mechanism proposed in <ref type="bibr" target="#b7">[8]</ref>. This sampling method linearly interpolates the values of the four pixels that surround the sample location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Baseline loss function</head><p>In this section we will discuss the components of the loss function used in the baseline <ref type="bibr" target="#b5">[6]</ref>. The loss used for training (eq. 2) consists of two components, which will be discussed in this section. One component is the smoothness loss (L s ) over the predicted depth map. This loss remains unchanged in the experiments, and is scaled with smoothness term λ set to 0.001. The other component of the loss function is the photometric loss multiplied with a binary mask in order to ignore certain areas of a reconstruction. This mask (µ in the equation), called automask is the same in all of the experiments. The photometric loss (L p ) is computed differently in each of the experiments.</p><formula xml:id="formula_1">L = λL s + µL p (2)</formula><p>The final loss is averaged over each pixel location, the various scales, and the images in a batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Multiple scales</head><p>Due to the sampling mechanism described in 2.1.2, the gradient is derived from the difference between the target pixel value and the pixel value in the reconstruction which is an interpolation of the four pixels surrounding the sample location. Using this gradient will mean that the depth prediction is changed in the direction that moves the sample location closer to one of the four pixels that surround the current sample location and that matches the target pixel most closely.</p><p>This change in depth prediction is not necessarily in the right direction, for example when there is no gradual color transition of the pixels that lie between the current sample location and the correct sample location. This can happen if the current sample location is far from the correct sample location, or when the scene is complex e.g. many surfaces with different color gradients, or multiple color gradients in a single surface.</p><p>To prevent getting stuck in these local optima, the loss (and therefore also the gradient) is usually computed on multiple scales which allows the gradient to be derived from larger spatial regions directly. <ref type="bibr" target="#b10">[11]</ref> This is done by making reprojections using the intermediate depth map predictions of the network, which have a lower resolution.</p><p>The authors of <ref type="bibr" target="#b5">[6]</ref> observed that projecting the input images with the resolution of the depth map, has the tendency to create artefacts when the depth map resolution is low and there are large low-texture regions in the image. They overcome this problem by upsampling the predicted depth maps to the resolution of the input image instead of downsampling the input image to the resolution of the depth map. <ref type="bibr" target="#b5">[6]</ref> This preserves the details in low-texture regions which reduces ambiguity. Using this method values in the lower resolution depth map will influence a larger spatial region in the higher resolution reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Smoothness loss</head><p>There are many incorrect depth map predictions which could also provide accurate image reconstructions, for example in homogeneous regions of an image. To steer the network to learn more realistic predictions a loss is used that enforces smoothness of the predicted depth map. In this implementation an edge-aware smoothness loss is used (eq. 3) <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_2">L s = |∂ x d * t |e −|∂xIt| + |∂ y d * t |e −|∂yIt|<label>(3)</label></formula><p>In this formulation depth discontinuities, i.e. high disparity gradients (|∂d * t |), contribute less to the loss when the image gradients (|∂I t |) are relatively high. The used (inverse) depth map is normalized (d * t ) by its mean value. This normalization is done to prevent that the loss is minimized by lowering the depth gradients through scaling down the entire depth map. This would be possible since the other loss component (i.e. the photometric error) is unaffected by scale, the depth and pose network would together scale down their predictions which negatively affects training.</p><p>[10]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Automask</head><p>A mask, called automask, is used to ignore the stationary pixels, which come either from objects that move with the same velocity as the observer or when the camera is not moving. It is supposed to prevent the pixels which remain stationary in the image from contaminating the loss. It does this by ignoring the loss of pixels where the photometric error of the original, unwarped frame I t is lower than the warped frame I t →t . <ref type="figure" target="#fig_3">Equation 4</ref> shows the mask definition where pe stands for photometric error, which will be described in the next section.</p><formula xml:id="formula_3">µ = 1 if min t pe(I t , I t ) &gt; min t pe(I t , I t →t ) 0 otherwise<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Photometric error</head><p>The photometric error (eq. 5) used in the photometric loss component is a combination of the L1 loss of the pixel value differences and the structural similarity (SSIM) index <ref type="bibr" target="#b11">[12]</ref> of the image for which the depth is predicted and its reconstruction made from the adjacent frame . SSIM is used because it is a measure of structural information change and the human visual system is adapted to extract structural information. In the experiments α = 0.85 is used. <ref type="figure" target="#fig_4">Figure 5</ref> shows an example of the photometric error (5c) computed on the target "reference" image (5a) and the reprojected image (5b).</p><formula xml:id="formula_4">pe(I a , I b ) = α 1 − SSIM (I a , I b ) 2 +(1−α) I a −I b 1 (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Per-Pixel Minimum Reprojection Loss</head><p>In the baseline per-pixel minimum reprojection loss (equation 6) is used as the photometric loss . Which means that at each pixel location, the reconstruction with the lowest error is used in the loss. This is different to previous selfsupervised depth estimation methods which instead use the average of the reconstructions. Selecting the reconstruction with lowest error at that pixel location is done to account for regions in the reconstruction target (image), that are not visible in some of the images that are used to make the reconstructions. Even when the depth of these regions is predicted correctly, a correct reconstruction is not likely, which would give a high photometric error for such a region. <ref type="bibr" target="#b5">[6]</ref> L p = min t pe(I t , I t →t )</p><p>2.3. Occlusion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">The occlusion mask</head><p>The hypothesis in this thesis is that by selectively ignoring only the occluded regions of a reconstructed image, compared with the per-pixel minimum reprojection loss, more relevant information is available to learn the task of depth prediction. To this end we design an occlusion mask for discarding regions that cannot be reconstructed due to occlusions. Our solution for the occlusion mask utilizes the previously unused depth information of the point/pixel cloud after it has been projected in order to reconstruct the target image (figure 4f in section 2.1.2). <ref type="figure" target="#fig_5">Figure 6</ref> illustrates the computation steps of the occlusion mask. Consider 6a as the frame for which the depth map is predicted and the target of the reconstruction, and 6b the previous video frame. Similar to 4f from the example in section 2.1.2 , <ref type="figure" target="#fig_5">figure 6c</ref> shows the projected points, i.e. where the pixels of the reconstruction target image will end up after the camera transformation to the previous video frame. In contrast with that earlier example, we will use the depth information that is removed from the point cloud visible in 6c to get the sample locations 6d. This information gives the depths that are expected at the sample locations. These depth expectations are visualized in 6e, important to note is that it is not an actual depth map of a point in time, but the depths that you would expect to see if you would sample at the locations of 6d in the depth map of the previous video frame. <ref type="figure" target="#fig_5">Figure 6f</ref> show the depth map prediction of the previous frame. <ref type="figure" target="#fig_5">Figure 6g</ref> shows the depths that are sampled from the prediction 6f using the sample locations from 6d. By comparing the expected depths at the sample locations 6e with the observed depths, i.e. those sampled from the adjacent frame depth map prediction 6g, the occluded regions can be determined 6h. When the sampled observed depth is closer than what is expected from reprojection, it means something is blocking the view (i.e. occlusion). This occlusion mask can be used to ignore these regions in the reconstruction 6i. <ref type="bibr">Equation 7</ref> shows the definition of the occlusion mask, which indicates which of the pixels of the target image are visible in the image from which is sampled and thus can be used to ignore the occluded pixels in the reconstruction. In this formula z t * is the value of the depth map prediction of frame I t sampled at location (x t→t , y t→t ), i.e. the location after the projection.</p><p>If due to variation in the depth map predictions, an object is predicted closer in the adjacent frame, this could introduce regions that are incorrectly recognized as occlusion.</p><p>To avoid this we propose to add the parameter tolerance so that only regions with a sufficiently high ratio of predicted and expected (i.e. projected) depth are considered occluded. In our experiments we used tolerance = 0.3 since this value produced occlusion mask that were good enough for performing our experiments. See section A of the appendix for further considerations on choosing the parameter value.</p><p>The second case of equation 7 ignores the occlusion that is due to projected coordinates being outside of the image boundaries, just like the principled mask in <ref type="bibr" target="#b8">[9]</ref>.</p><formula xml:id="formula_6">ω t→t =      0 if z t * &gt; z t→t * (1 − tolerance) 0 if x t→t or y t→t outside of image 1 otherwise<label>(7)</label></formula><p>This computation of the occlusion mask does not introduce any new learnable parameters. Furthermore the adjacent frames depth map predictions are only used for determining the occlusion mask and not for backpropagation, therefore no gradients have to be computed which limits the computational overhead and thus the additional training time. This comparison gives the occlusion mask (h), which shows an occlusion when the observed depth at the sample location is closer than the depth that is expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Non-occluded average loss</head><p>Equation <ref type="formula" target="#formula_7">8</ref> shows the non-occluded average loss, a photometric loss function that uses the occlusion mask to average the reconstruction errors. The resulting loss map consists of the pixel wise average of the non-occluded regions of the reconstructions. For example, if a region is visible in both of the adjacent images, the loss for that region will be the average of both their reconstruction errors. If a region is only visible in one of the adjacent images, only the reconstruction error of that image is used. When a region is somehow occluded in both the adjacent images, none of the reconstruction errors will be used for training the model.</p><formula xml:id="formula_7">L p = t ω t→t pe(I t , I t →t ) max( t ω t→t , 1)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Non-occluded minimum reprojection loss</head><p>A photometric loss function that more resembles the one used in the baseline can be seen in equation 9. This loss function has the same properties as the per-pixel minimum reprojection loss, however it also incorporates the occlusion mask as a penalty. Per-pixel minimum reprojection loss uses only one reconstruction per pixel location in the loss. It is not known whether selecting only one reconstruction in a non-occluded region has an effect on the trained model. It is possible that this effect is beneficial to the model performance. If this were the case, averaging the non-occluded regions as done in equation 8 might harm per-formance. We will show experimentally that this is indeed the case: it helps to consider the per-pixel minimum also for non-occluded regions. In equation 9 you can see the photometric loss function that has the same behavior as equation 6 for non-occluded regions while further preventing occluded pixels from being used in the loss function, by using the mask as an additional error term. The implementation of the photometric error function has a convenient range of [0, 1], which makes scaling the occlusion mask unnecessary. For the model variant that uses this photometric loss, the occlusion mask is used in the same manner in the automask computation, i.e. as an additional error term on the right hand side of equation 4. This is done in order to prevent that areas that are occluded in both adjacent images, will be used to update the model.</p><formula xml:id="formula_8">L p = min t (pe(I t , I t →t ) + (1 − ω t→t ))<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We will evaluate the performance of models trained using the introduced photometric losses and compare them with the baseline. Additionally we will visualize the effect of the photometric loss function on the learning signal during training using examples from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">KITTI dataset</head><p>In our experiment we will use the KITTI dataset since it is a widely used dataset in single-view depth estimation literature. This dataset contains short videos that are captured while driving through and around Karlsruhe in a car equipped with multiple cameras, a laser scanner and a GPS  <ref type="table">Table 1</ref>: Results of the models trained with different photometric losses on the KITTI dataset <ref type="bibr" target="#b3">[4]</ref>. Best results are in bold. Training is done using the subset of the Eigen split <ref type="bibr" target="#b1">[2]</ref> introduced for monocular training by <ref type="bibr" target="#b10">[11]</ref>. *the numbers shown for this model are from table 2 in <ref type="bibr" target="#b5">[6]</ref>. δ = max( system. <ref type="bibr" target="#b3">[4]</ref> We will use the split of this dataset introduced in <ref type="bibr" target="#b1">[2]</ref> with the static frames filtered out as described in <ref type="bibr" target="#b10">[11]</ref>. This split uses 33 drives (the short videos) to produce 39,810 triplets for training and 4,424 for validation, and for testing 697 images sampled balanced from 28 other drives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Results</head><p>The results in table 1 show that incorporating the occlusion mask into the used loss function improves the accuracy of depth prediction models. The model trained with nonoccluded minimum reprojection loss outperforms the baseline, i.e. the per-pixel minimum reprojection loss model, on all metrics. Incorporating the occlusion mask gives a larger accuracy improvement for the average reprojection loss function (table 1a) than for the per-pixel minimum reprojection loss function <ref type="table">(table 1b)</ref>. This can be explained by the fact that per-pixel minimum reprojection loss already ignores occlusion effects based on reconstruction error, and that further improvement can only come from ignoring occlusion effects that are missed by per-pixel minimum reprojection loss but are recognized by the occlusion mask.</p><p>Although the observed performance improvements validate the benefit of incorporating occlusion mask into the loss function, the results do not validate the hypothesis, which states that more useful information is available for model training if only those regions that suffer from occlusion are ignored. Given this hypothesis, it would be expected that the non-occluded average reprojection model outperforms the per-pixel minimum reprojection model.</p><p>We can think of various explanations for this result: the occlusion mask fails to mask occluded regions, the occlusion mask incorrectly masks non-occluded regions, or perpixel minimum reprojection loss ignores regions which are not occluded but that do have a negative effect on training the model. We will try to answer this question in the next section by analyzing the loss images created by the discussed photometric losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Visualizing Photometric Losses</head><p>In this section we will compare loss images of the two proposed photometric losses (i.e. the non-occluded average reprojection loss from section 2. We intend to identify the differences between the loss functions. For example if one of the methods fails to ignore occluded regions or unexpectedly ignores non-occluded regions. Additionally this analysis can demonstrate whether the occlusion mask is able to mask occlusion. Since the loss function is used to guide the learning behavior during training, we will apply our analysis on the data from the training set. The loss images are all created using depth predictions made by the same model, i.e. one trained with per-pixel minimum reprojection loss. We assume that model choice has limited influence on the results of this analysis. Training examples for visualization will be selected randomly as well as based on the computed loss values. The criterion for selection is a large absolute difference between the computed per-pixel minimum reprojection loss and the non-occluded average reprojection loss. To avoid having many similar examples, we filter the data by only looking at images from one camera and at every tenth frame recorded. <ref type="figure" target="#fig_8">Figure 7</ref> shows the example with the highest difference in the computed loss. The first five images of this figure show the target frame, both reconstructions, and the photometric errors of the reconstructions. In the photometric error images, a brighter color means a higher value. The images of the third row show the calculated binary occlusion masks, where white areas mean there is no occlusion and black means there is occlusion. The last three rows show for each photometric loss, the calculated loss maps, the absolute difference to the baseline photometric loss and if applicable a mask displaying for each pixel location which of the photometric errors is used in the loss (black means from This example has a high absolute difference for the two loss methods, because there is an area that is occluded in both of the adjacent frames and per-pixel minimum reprojection loss is only able to ignore one of the reconstructions. This example shows that the occlusion mask (7f+g) is indeed able to mask the occluded areas, i.e. the black regions of the occlusion masks match with regions that are visible in the target image but not in the reconstructions. The piece of road that is visible in the target frame is occluded by the first car in the previous frame (7b) and the tailgating car in the later frame (7c). Together these occlusions make it impossible to reconstruct that area of the target image. What the example also shows, is that the occlusion mask is able to handle thin objects, which can be seen by the traffic sign in 7b,d and f.</p><p>The areas that are occluded in both of the adjacent images are visible as the black regions in the non-occluded average loss image (7m), because they are directly ignored using the occlusion mask. In the loss image of the nonoccluded minimum reprojection loss (7j) these areas are clearly visible as the yellow areas because the occlusion <ref type="figure">Figure 8</ref>: Visualizing photometric losses: Example that was randomly selected. In (n) we can see that the per-pixel minimum reprojection loss is able to ignore some of the reprojection error caused by object motion. mask is added to the loss, and consequently will be ignored by the automask. <ref type="figure">Figure 8</ref> shows one of the examples that was selected randomly. We can observe that the per-pixel minimum reprojection loss is able to ignore some of the reprojection error that is caused by object motion instead of occlusion. In the image which shows the loss difference between the nonoccluded average loss and the minimum reprojection loss (fig8n), we can see that there is a loss difference for moving objects, in this case the cars, which are not occluded. Although this behavior is not the originally intended effects of the per-pixel minimum reprojection loss it does help training, since the current reprojection computation (eq. 1) does not account for object movement, and consequently any gradient coming from a moving object can interfere with learning correct depth predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>The goal of this thesis was to design an occlusion mask that can be used to specifically discard regions that cannot be reconstructed due to occlusions and to test whether this method can help improve the performance of depth prediction models. We hypothesized that by limiting the amount of information which is ignored during training to only those regions that suffer from occlusion, more useful information will be available for training the model.</p><p>Here, we have introduced an occlusion mask that is based entirely on depth predictions and can be used to specifically ignore regions where occlusion is expected. We have shown that incorporating the occlusion mask in the photometric loss function improves model performance.</p><p>Contrary to our hypothesis, the results of our experi-ments show that ignoring only regions where occlusion is expected (non-occluded average loss) did not give better results compared with per-pixel minimum reprojection loss. We further investigated this outcome by performing a visual analysis of the difference between the photometric losses obtained by both methods.</p><p>In the visual analysis of the photometric losses, we observed (in <ref type="figure" target="#fig_8">fig. 7</ref>) that with the occlusion mask it is possible to ignore areas of reconstructions that are occluded in both of the adjacent frames. This is an improvement compared to the per-pixel minimum reprojection loss which is only able to ignore one of the reconstructions.</p><p>During this analysis we have also discovered (in <ref type="figure">fig. 8n</ref>) that the per-pixel minimum reprojection loss unexpectedly reduces the loss being contaminated with photometric error caused by object motion. This effect takes place because the per-pixel minimum reprojection loss always ignores one of the reconstructions at each pixel location based entirely on a high photometric error, which itself is not caused exclusively by occlusion. The ability to partially ignore moving objects is useful for models that do not take object motion into account.</p><p>It makes sense that a model which does not take object motion into account does not benefit from incorporating errors caused by object motion into the loss function. This however does not give a complete answer about the reason for the observed performance gap between the nonoccluded average photometric loss and the per-pixel minimum reprojection loss. It is possible that for the per-pixel minimum reprojection loss the improvement in model performance which is gained by ignoring motion artefacts is at the same time reduced by ignoring regions which are not affected by occlusion or motion and do have a valid gradient.</p><p>The question of whether ignoring only the areas of reconstructions that are affected by occlusion during training gives a better performing model, remains for models that do take object motion into account.</p><p>It is possible that (partially) ignoring object-motion related photometric error, as is done by the per-pixel minimum reprojection loss, could be disadvantageous for models that do take object motion into consideration since it allows the model to achieve a lower loss while making incorrect predictions about object movement. Occlusion mask does not have this problem because it can specifically ignore occlusion effects, while preserving the photometric error caused by object movement which is valuable information when optimizing the model.</p><p>Looking back to the implicit assumptions underlying self-supervised training using image reconstruction 6 that <ref type="bibr" target="#b5">6</ref> 1. the scene is static without moving objects; 2. there is no occlusion/disocclusion between the target view and the source views;</p><p>were mentioned in section 2.1.2, and combining them with our earlier observations we can see that the per-pixel minimum reprojection loss is able to ignore some or most of the photometric error related to the first two assumptions, i.e. the scene is static and there is no occlusion. We can imagine that per-pixel minimum reprojection loss is also able to ignore some of the photometric error related to the third assumption, i.e. surface appearance changes with direction of view. Or changes in surface appearance caused by a change in illumination due to moving objects, e.g. shade or reflection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we have introduced occlusion mask, a mask that during training can be used to specifically ignore regions that cannot be reconstructed due to occlusions. Occlusion mask is based entirely on predicted depth information. We have demonstrated that (i) incorporating occlusion mask in the used photometric loss function can improve the performance of single image depth prediction models. (ii) per-pixel minimum reprojection loss also ignores some of the reprojection error caused by object motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Occlusion mask -parameter value</head><p>In this work a "tolerance" parameter is introduced in the implementation of the occlusion mask to prevent incorrectly recognizing regions as occluded due to variation in the predicted depth maps of adjacent frames. Due to time constraints limited consideration has been put in choosing the value that was used.</p><p>We think that the "optimal" value depends on the performance of the model for which it is used. For example if a model gives almost perfect depth map predictions, the value could be (very close) to zero. Another example would be a model that gives more accurate depth predictions for closer objects than for objects further away. In this situation the "tolerance" value can be based on the distance to the object. One of the reasons for not further investigating these options is that considering that the choice of parameter value is dependent on the network performance, it means that when future models keep improving, the value choice becomes less important and just taking a constant value close to zero will work just fine.</p><p>During the design of the occlusion mask when determining what value to use for the"tolerance" parameter, we forgot to put the network into evaluation mode, which resulted in the model producing less accurate depth maps. This could have resulted in overestimating the depth map prediction variability and thus choosing an unnecessarily high tolerance value. A consequence of a high value for this parameter are that some occlusions are not recognized when "the occluded" and "the occluding" are close, and the ratio of the distances to them is small. An example of this can be seen in <ref type="figure">figure 8</ref> where "the occluding" parts of the moving car that are close to the road, are not marked as occluding in the occlusion mask and are visible in the non-occluded average loss image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Effects of occlusion on the reconstructed image. Top:The image for which the depth prediction is made. Middle: The next video frame, captured 100ms later. Bottom: Reconstruction of the first image made by sampling from the next video frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Top: Image for which the depth prediction is made and target for image reconstruction. Second row: Reconstructions of the above image, made from the next and previous video frames respectively. Third row: Map of each reconstructions pixels that are used/ignored (white/black) by minimum reprojection loss. Bottom row: Examples of occlusion mask for each reconstruction, black areas are affected by occlusion and should not be used in the loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Depth network A standard, fully convolutional, U-Net is used to predict depth. (b) Pose network Pose between a pair of frames is predicted with a separate pose network. Image from<ref type="bibr" target="#b5">[6]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) Target image (b) Predicted depth map (c) Grid of pixel locations (d) Point cloud = grid + depth (e) Next video frame (f) Point cloud projected to next frame (g) Sample locations (h) Reconstructed image Example of image reconstruction steps.For an image (a), a depth map prediction (b&amp;d) together with a camera transformation prediction is used to compute the sample locations (g) in the adjacent image (e) that can be used to reconstruct (h) the target image .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example of the photometric error Top: The reconstruction target image. Middle: Reconstruction made from the next video frame. Bottom: The calculated photometric error between the target and the reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Example of occlusion mask computation steps. To make a reconstruction (i) of frame (a) from an adjacent frame (b) , sample locations (d) are computed. This calculation also provides each sample locations expected depth (e). These expected depths (e) can be compared with the depths observed at the sample locations (g), i.e. the depths sampled from the adjacent frame depth map prediction(f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>d</head><label></label><figDesc>pred ) (a) photometric losses that are based on averaging the photometric errors of both reconstructions. (b) photometric losses that are based on selecting the reconstruction with the lowest reconstruction error. Both types of loss functions (a, b) get improved result when occlusion mask is incorporated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>3.2 and the non-occluded minimum reprojection loss from section 2.3.3) with loss images created by the baselines photometric loss (i.e. per pixel minimum reprojection loss, section 2.2.5). These loss images are computed on training examples and provide an idea of how important the regions of a reconstructed image are for optimization during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Visualizing photometric losses: Example with the largest difference between between the per-pixel minimum reprojection loss and the non-occluded average reprojection loss. (a) The reconstruction target. (b&amp;c) Reconstructions made from video frames adjacent to the target. (d&amp;e) Photometric errors from (b&amp;c) to (a). (f&amp;g) Occlusion masks depicting which areas are visible of (b&amp;d) and (c&amp;e) respectively. h, j&amp; m: Visualisations of the computed per-pixel minimum reprojection loss, non-occluded minimum reprojection loss, and non-occluded average reprojection loss, respectively. (k&amp;n) Absolute differences of (j&amp;m) with (h) (i&amp;l) Masks displaying for each pixel location which reconstruction's photometric error is used in the loss (black =1, white=2) by (h) and (j), respectively.reconstruction 1, white from reconstruction 2). An important remark is that the shown loss maps are the photometric loss where the automask has not yet been applied.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Using the word "map", instead of "mask" since it is not actively used to mask.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/nianticlabs/monodepth2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. the surfaces appear uniformly bright from all directions of view so that the photo-consistency error is meaningful</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06152</idno>
		<idno>arXiv: 1811.06152. 2</idno>
		<title level="m">Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos</title>
		<meeting><address><addrLine>Nov</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04992</idno>
		<idno>arXiv: 1603.04992. 1</idno>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised Monocular Depth Estimation with Left-Right Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03677</idno>
		<idno>arXiv: 1609.03677. 5</idno>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Brostow. Digging Into Self-Supervised Monocular Depth Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01260</idno>
		<idno>arXiv: 1806.01260</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<idno>arXiv: 1703.06870. 2</idno>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3d Geometric Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05522</idno>
		<idno>arXiv: 1802.05522</idno>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Depth from Monocular Videos Using Direct Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Depth and Ego-Motion from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07813</idno>
		<idno>arXiv: 1704.07813</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
