<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking the Distribution Gap of Person Re-identification with Camera-based Batch Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
							<email>tianyu1949@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking the Distribution Gap of Person Re-identification with Camera-based Batch Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Person Re-identification</term>
					<term>Distribution Gap</term>
					<term>Camera-based Batch Normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The fundamental difficulty in person re-identification (ReID) lies in learning the correspondence among individual cameras. It strongly demands costly inter-camera annotations, yet the trained models are not guaranteed to transfer well to previously unseen cameras. These problems significantly limit the application of ReID. This paper rethinks the working mechanism of conventional ReID approaches and puts forward a new solution. With an effective operator named Camera-based Batch Normalization (CBN), we force the image data of all cameras to fall onto the same subspace, so that the distribution gap between any camera pair is largely shrunk. This alignment brings two benefits. First, the trained model enjoys better abilities to generalize across scenarios with unseen cameras as well as transfer across multiple training sets. Second, we can rely on intra-camera annotations, which have been undervalued before due to the lack of cross-camera information, to achieve competitive ReID performance. Experiments on a wide range of ReID tasks demonstrate the effectiveness of our approach. The code is available at https://github.com/automan000/Camera-based-Person-ReID.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (ReID) aims at matching identities across disjoint cameras. Generally, it is achieved by mapping images from the same and different cameras into a feature space, where features of the same identity are closer than those of different identities. To learn the relations between identities from all cameras, there are two different objectives: learning the relations between identities in the same camera and learning identity relations across cameras.</p><p>However, there is an inconsistency between these two objectives. As shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, due to the large appearance variation caused by illumination conditions, camera views, etc., images from different cameras are subject to distinct distributions. Handling the distribution gap between cameras is crucial for intercamera identity matching, yet learning within a single camera is much easier. As a consequence, the conventional ReID approaches mainly focus on associating different cameras, which demands costly inter-camera annotations. Besides, after learning on a training set, part of the learned knowledge is strongly correlated to the connections among these particular cameras, making the model generalize poorly on scenarios consisting of unseen cameras. As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, the ReID model learned on one dataset often has a limited ability of describing images from other datasets, i.e., its generalization ability across datasets is limited. For simplicity, we denote this formulation neglecting within-dataset inconsistencies as the dataset-based formulation. We emphasize that lacking the ability to bridge the distribution gap between all cameras from all datasets leads to two problems: the unsatisfying generalization ability and the excessive dependence on inter-camera annotations. To tackle these problems simultaneously, we propose to align the distribution of all cameras explicitly. As shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, we eliminate the distribution inconsistency between all cameras, so the ReID knowledge can always be learned, accumulated, and verified in the same input distribution, which facilitates the generalization ability across different ReID scenarios. Moreover, with the aligned distributions among all cameras, intra-and inter-camera annotations can be regarded as the same, i.e., labeling the image relations under the same input distribution. This allows us to approximate the effect of inter-camera annotations with only intra-camera annotations. It may relieve the exhaustive human labor for the costly inter-camera annotations.</p><p>We denote our solution that disassembles ReID datasets and aligns each camera independently as the camera-based formulation. We implement it via an improved version of Batch Normalization (BN) <ref type="bibr" target="#b8">[9]</ref> named Camera-based Batch Normalization (CBN). In training, CBN disassembles each mini-batch and standardizes the corresponding input according to its camera labels. In testing, CBN utilizes few samples to approximate the BN statistics of every testing camera and standardizes the input to the training distribution. In practice, multiple ReID tasks benefit from our work, such as fully-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59]</ref>, direct transfer <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref>, domain adaptation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53]</ref>, and incremental learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12]</ref>. Extensive experiments indicate that our method improves the performance of these tasks simultaneously, such as 0.9%, 5.7%, and 14.2% averaged Rank-1 accuracy improvements on fully-supervised learning, domain adaptation, and direct transfer, respectively, and 9.7% less forgetting on Rank-1 accuracy for incremental learning. Last but not least, even without inter-camera annotations, a weakly-supervised pipeline <ref type="bibr" target="#b60">[61]</ref> with our formulation can achieve competitive performance on multiple ReID datasets, which demonstrates that the value of intra-camera annotations may have been undervalued in the previous literature. To conclude, our contribution is three-fold:</p><p>-In this paper, we emphasize the importance of aligning the distribution of all cameras and propose a camera-based formulation. It can learn discriminative knowledge for ReID tasks while excluding training-set-specific information. -We implement our formulation with Camera-based Batch Normalization. It facilitates the generalization and transfer ability of ReID models across different scenarios and makes better use of intra-camera annotations. It provides a new solution for ReID tasks without costly inter-camera annotations. -Experiments on fully-supervised, weakly-supervised, direct transfer, domain adaptation, and incremental learning tasks validate our method, which confirms the universality and effectiveness of our camera-based formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our formulation aligns the distribution per camera. In training, it eliminates the distribution gap between all cameras. ReID models can treat both intra-camera and inter-camera annotations equally and make better use of them, which benefits both fully-supervised and weakly-supervised ReID tasks. It also guarantees that the distribution of each testing camera is aligned to the same training distribution. Thus, the knowledge can better generalize and transfer across datasets. It helps direct transfer, domain adaptation, and incremental learning. In this section, we briefly categorize and summarize previous works on the above ReID topics. Supervision. The supervision in ReID tasks is usually in the form of identity annotations. Although there are many outstanding unsupervised methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47]</ref> that do not need annotations, it is usually hard for them to achieve competitive performance as the supervised ReID methods. For better performance, lots of previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43]</ref> utilized fullysupervised learning, in which identity labels are annotated manually across all training cameras. Many of them designed spatial alignment <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b34">35]</ref>, visual attention <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>, and semantic segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b31">32]</ref> for extracting accurate and fine-grained features. GAN-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref> were also utilized for data augmentation. However, although these methods achieved remarkable performance on ReID tasks, they required costly inter-camera annotations. To reduce the cost of human labor, ReID researchers began to investigate weakly-supervised learning. SCT <ref type="bibr" target="#b48">[49]</ref> presumes that each identity appears in only one camera. In ICS <ref type="bibr" target="#b60">[61]</ref>, an intra-camera supervision task is studied in which an identity could have different labels under different cameras. In <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, pseudo labels are used to supervised the ReID model. Generalization. The generalization ability in ReID tasks denotes how well a trained model functions on unseen datasets, which is usually examined by direct transfer tasks. Researchers found that many fully-supervised ReID models perform poorly on unseen datasets <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b2">3]</ref>. To improve the generalization ability, various strategies were adopted as additional constraints to avoid over-fitting, such as label smoothing <ref type="bibr" target="#b21">[22]</ref> and sophisticated part alignment approaches <ref type="bibr" target="#b7">[8]</ref>.</p><p>Transfer. The transfer ability in ReID tasks corresponds to the capability of ReID models transferring and preserving the discriminative knowledge across multiple training sets. There are two related tasks. Domain adaptation transfers knowledge from labeled source domains to unlabeled target domains. One solution <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b57">58]</ref> bridged the domain gap by transferring source images to the target image style. Other solutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref> utilized the knowledge learned from the source domain to mine the identity relations in target domains. Incremental learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12</ref>] also values the transfer ability. Its goal is to preserve the previous knowledge and accumulate the common knowledge for all seen datasets. A recent ReID work that relates to incremental learning is MASDF <ref type="bibr" target="#b43">[44]</ref>, which distilled and incorporated the knowledge from multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conventional ReID: Learning Camera-related Knowledge</head><p>ReID is a task of retrieving identities according to their appearance. Given a training set consisting of disjoint cameras, learning a ReID model on it requires two types of annotations: inter-camera annotations and intra-camera annotations. The conventional ReID formulation regards a ReID dataset as a whole and learns the relations between identities as well as the connections between training cameras. Given an image I Dj i from any training set D j , the training goal of this formulation is:</p><formula xml:id="formula_0">arg min E y Dj i − g Dj f Dj I Dj i , I Dj i , y Dj i ∈ D j ,<label>(1)</label></formula><p>where f Dj (·) and g Dj (·) are the corresponding feature extractor and classifier for D j , respectively. y Dj i denotes the identity label of the image I Dj i . In our opinion, this formulation has three drawbacks. First, images from different cameras, even of the same identity, are subject to distinct distributions. To associate images across cameras, conventional approaches strongly demand the costly inter-camera annotations. Meanwhile, the intra-camera annotations are less exploited since they provide little information across cameras. Second, such learned knowledge not only discriminates the identities in the training set but also encodes the connections between training cameras. These connections are associated with the particular training cameras and hard to generalize to other cameras, since the corresponding knowledge may not apply to the distribution of previously unseen cameras. For example, when transferring a ReID model trained on Market-1501 to DukeMTMC-reID, it produces a poor Rank-1 accuracy of 37.0% without fine-tuning. Third, the learned knowledge is hard to preserve when being fine-tuned. For instance, after fine-tuning the aforementioned model on DukeMTMC-reID, the Rank-1 accuracy drops 14.2% on Market-1501, because it turns to fit the relations between the cameras in DukeMTMC-reID. We analyze these three problems and find that the particular relations between training cameras are the primary cause of them. Thus, we believe that the conventional method of handling these camera-related relations may need a re-design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Insight: Towards Camera-independent ReID</head><p>We rethink the relations between cameras. More specifically, we believe that the exclusive knowledge for bridging the distribution gap between the particular training cameras should be suppressed during training. Such knowledge is associated to the cameras in the training set and sacrifices the discriminative and generalization ability on unseen scenarios.</p><p>To this end, we propose to align the distribution of all cameras explicitly, so that the distribution gap between all cameras is eliminated, and much less camera-specific knowledge will be learned during training. We denote this formulation as the camera-based formulation. To align the distribution of each camera, we estimate the raw distribution of each camera and standardize images from each camera with the corresponding distribution statistics. We use η (·) to denote the estimated statistics related to the distribution of a camera. Then, given a related image I (c) i , aligning the camera-wise distribution will transform this image as:Ĩ</p><formula xml:id="formula_1">(c) i = DA I (c) i ; η (c) ,<label>(2)</label></formula><p>where DA (·) represents a distribution alignment mechanism,Ĩ (c) i denotes the aligned I (c) i and η (c) is the estimated alignment parameters for camera c. For any training set D j , we can now learn the ReID knowledge from this aligned distribution by replacing I</p><formula xml:id="formula_2">Dj i in Eq. 1 withĨ (c) i .</formula><p>With the distributions of all cameras aligned by DA (·), images from all these cameras can be regarded as distributing on a "standardized camera". By learning on this "standardized camera", we eliminate the distribution gap between cameras, so the raw learning objectives within the same and across different cameras can be treated equally, making the training procedure more efficient and effective. Besides, without the disturbance caused by the training-camerarelated connections, the learned knowledge can generalize better across various ReID scenarios. Last but not least, now that the additional knowledge for associating diverse distributions is much less required, our formulation can make better use of the intra-camera annotations. It may relieve human labor for the costly inter-camera annotations, and provides a solution for ReID in a large-scale camera network with fewer demands of inter-camera annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Camera-based Batch Normalization</head><p>In practice, a possible solution for aligning camera-related distributions is to conduct batch normalization in a camera-wise manner. We propose the Camerabased Batch Normalization (CBN) for aligning the distribution of all training and testing cameras. It is modified from the conventional Batch Normalization <ref type="bibr" target="#b8">[9]</ref>, and estimates camera-related statistics rather than dataset-related statistics. Batch Normalization Revisited. The Batch Normalization <ref type="bibr" target="#b8">[9]</ref> is designed to reduce the internal covariate shifting. In training, it standardizes the data with the mini-batch statistics and records them for approximating the global statistics. During testing, given an input x i , the output of the BN layer is:</p><formula xml:id="formula_3">x i = γ x i −μ √σ 2 + + β,<label>(3)</label></formula><p>where x i is the input andx i is the corresponding output.μ andσ 2 are the global mean and variance of the training set. γ and β are two parameters learned during training. In ReID tasks, BN has significant limitations. It assumes and requires that all testing images are subject to the same training distribution. However, this assumption is satisfied only when the cameras in the testing set and training set are exactly the same. Otherwise, the standardization fails. </p><formula xml:id="formula_4">µ (c) = 1 M M m=1 x (c) m , σ 2 (c) = 1 M M m=1 x (c) m − µ (c) 2 ,x m =γ x m − µ (c) σ 2 (c) + + β,<label>(4)</label></formula><p>where µ (c) and σ 2 (c) denote the mean and variance related to this camera c. During training, we disassemble each mini-batch and calculate the camera-related mean and variance for each involved camera. The camera with only one sampled images is ignored. During testing, before employing the learned ReID model to extract features, the above statistics have to be renewed for every testing camera. In short, we collect several unlabeled images and calculate the camera-related statistics per testing camera. Then, we employ these statistics and the learned weights to generate the final features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Applying CBN to Multiple ReID Scenarios</head><p>The proposed CBN is generic and nearly cost-free for existing methods on multiple ReID tasks. To demonstrate its superiority, we setup a bare-bones baseline, which only contains a deep neural network, an additional BN layer as the bottleneck, and a fully connected layer as the classifier. As shown in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>, our camera-based formulation can be implemented by simply replacing all BN layers in a usual convolutional network with CBN layers. With a modified network mentioned above, our camera-based formulation can be applied to many popular tasks, such as fully-supervised learning, weaklysupervised learning, direct transfer, and domain adaptation. Apart from them, we also evaluate a rarely discussed ReID task, i.e., incremental learning. It studies the problem of learning knowledge incrementally from a sequence of training sets while preserving and accumulating the previously learned knowledge. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we propose two settings. (1) Data-Free: once we finish the training procedure on a dataset, the training data along with the corresponding classifier are abandoned. When training the model on the subsequent training sets, the old data will never show up again. (2) Replay: unlike Data-Free, we construct an exemplar set from each old training set. The exemplar set and the corresponding classifier are preserved and used during the entire training sequence.</p><formula xml:id="formula_5">Classifier(s) Backbone Training Data Another BN Layer CBN BN Replace BN with CBN (a) Training Sequence Dataset ! Dataset !"# Dataset !"$ Network Abandoned Classifiers Memory !"# Memory !"$ Network Classifier !"# Training Sequence Classifier !"$ Classifier ! Dataset ! Classifier ! (b) Training Sequence Dataset ! Dataset !"# Dataset !"$ Network Abandoned Classifiers Memory !"# Memory !"$ Network Classifier !"# Training Sequence Classifier !"$ Classifier ! Dataset ! Classifier ! (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussions</head><p>Bridging ReID Tasks. We briefly demonstrate our understandings of the relations between ReID tasks and how we bridge these tasks. Different ReID tasks handle different combinations of training and testing sets. Since datasets have distinct cameras, previous methods have to learn exclusive relations between particular training cameras and adapt them to specific testing camera sets. Our formulation aligns the distribution of all cameras for learning and testing ReID knowledge, and suppresses the exclusive training-camera relations. It may reveal the latent connections between ReID tasks. First, by aligning the distribution of seen and unseen cameras, fully-supervised learning and direct transfer are united since training and testing distributions are always aligned in a camerawise manner. Second, since there is no need to learn relations between distinct camera-related distributions, intra-and inter-camera annotations can be treated almost equally. Knowledge is better shared among cameras which helps fully-and weakly-supervised learning. Third, with the aligned training and testing distributions, it is more efficient to learn, accumulate, and preserve knowledge across datasets. It offers an elegant solution to preserve old knowledge (incremental learning) and absorb new knowledge (domain adaptation) in the same model. Relationship to Previous Works. There are two types of previous works that closely relate to ours: camera-related methods and BN variants. Same with our work, camera-related methods such as CamStyle <ref type="bibr" target="#b57">[58]</ref> and CAMEL <ref type="bibr" target="#b45">[46]</ref> noticed the camera view discrepancy inside the dataset. CamStyle augmented the dataset by transferring the image style in a camera-to-camera manner, but still learned ReID models in the dataset-based formulation. Consequently, transferring across datasets is still difficult. CAMEL <ref type="bibr" target="#b45">[46]</ref> is the most similar work with ours, which learned camera-related projections and mapped camera-related distributions into an implicit common distribution. However, these projections are associated with the particular training cameras, limiting its ability to transfer across datasets. BN variants such as AdaBN also inspire us. AdaBN aligned the distribution of the entire dataset. It neither eliminated the camera-related relations in training, nor handled the camera-related distribution gap in testing. Unlike them, CBN is specially designed for our camera-based formulation. It is much more general and precise for ReID tasks. More comparisons and discussions will be provided in Secs. 4.2 and 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets. We utilize three large scale ReID datasets, including Market-1501 <ref type="bibr" target="#b50">[51]</ref>, DukeMTMC-reID <ref type="bibr" target="#b52">[53]</ref>, and MSMT17 <ref type="bibr" target="#b41">[42]</ref>. Market-1501 dataset has 1,501 identities in total. 751 identities are used for training and the rest for testing. The training set contains 12,936 images and the testing set contains 15,913 images. DukeMTMC-reID dataset contains 16,522 images of 702 identities for training, and 1,110 identities with 17,661 images are used for testing. MSMT17 dataset is the current largest ReID dataset with 126,441 images of 4,101 identities from 15 cameras. For short, we denote Market-1501 as Market, DukeMTMC-reID as Duke, and MSMT17 as MSMT in the rest of this paper. It is worth noting that in these datasets, the training and testing subsets contain the same camera combinations. It could be the reason that previous dataset-based methods create remarkable fully-supervised performance but catastrophic direct transfer results. Implementation Details. In this paper, all experiments are conducted with PyTorch. In both training and testing, the image size is 256 × 128 and the batch size is 64. In training, we sample 4 images for each identity. The baseline network presented in Sec. 3.4 uses the ResNet-50 <ref type="bibr" target="#b6">[7]</ref> as the backbone. To train this network, we adopt SGD optimizer with momentum <ref type="bibr" target="#b27">[28]</ref> of 0.9 and weight decay of 5 × 10 −4 . Moreover, the initial learning rate is 0.01, and it decays after the 40th epoch by a factor of 10. For all experiments, the training stage will end up with 60 epochs. For incremental learning, we include a warm-up stage. In this stage, we freeze the backbone and only fine-tune the classifier(s) to avoid damaging the previously learned knowledge. During testing, our framework will first sample a few unlabeled images from each camera and use them to approximate the camera-related statistics. Then, these statistics are fixed and employed to process the corresponding testing images. Following the conventions, mean Average Precision (mAP) and Cumulative Matching Characteristic (CMC) curves are utilized for evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance on Different ReID Tasks</head><p>We evaluate our proposed method on five types of ReID tasks, i.e., fully-supervised learning, weakly-supervised learning, direct transfer, domain adaptation, and incremental learning. The corresponding experiments are organized as follows. First, we demonstrate the importance of aligning the distribution of all cameras from all datasets, and simultaneously conduct fully-supervised learning and direct transfer on multiple ReID datasets. Second, we demonstrate that it is possible to learn discriminative knowledge with only intra-camera annotations. We utilize the network architecture in Sec. 3.4 to compare the fully-supervised learning and weakly-supervised learning. To evaluate the generalization ability, direct transfer is also conducted for these two settings. Third, we evaluate the transfer ability of our method. This part of experiments includes domain adaptation, i.e., transferring the knowledge from the old domain to new domains, and incremental learning, i.e., preserving the old knowledge and accumulating the common knowledge for all training sets. Note that, for simplicity, we denote the results of training and testing the model on the same dataset with fully annotated data as the fully-supervised learning results. For similar experiments that only use the intra-camera annotations, we denote their results as the weakly-supervised learning results. Supervisions and Generalization. In this section, we evaluate and analyze the supervisions and the generalization ability in ReID tasks. For all experiments in this section, the testing results on both the training domain and other unseen testing domains are always obtained by the same learned model. We first conduct experiments on fully-supervised learning and direct transfer. As shown in <ref type="table">Table 2</ref>. Results of the state-of-the-art fully-supervised learning methods. BoT* denotes our results with the official BoT code. In BoT*, Random Erasing is disabled due to its negative effect on direct transfer. Unless otherwise stated, the baseline method in the following sections refers to the network described in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Market Duke Rank-1 Rank-5 Rank-10 mAP Rank-1 Rank-5 Rank-10 mAP CamStyle <ref type="bibr" target="#b57">[58]</ref> 88. Tab. 1, our proposed method shows good advantages, e.g., there is an averaged 1.1% improvement in Rank-1 accuracy for the fully-supervised learning task. Meanwhile, without bells and whistles, there is an average 13.6% improvement in Rank-1 accuracy for the direct transfer task. We recognize that our method has to collect a few unlabeled samples from each testing camera for estimating the camera-related statistics. However, this process is fast and nearly cost-free. Our method can also boost previous methods. Take BoT <ref type="bibr" target="#b21">[22]</ref>, a recent stateof-the-art method, as an example. We integrate our proposed CBN into BoT and conduct experiments with almost the same settings as in the original paper, including the network architecture, objective functions, and training strategies. The only difference is that we disable Random Erasing <ref type="bibr" target="#b54">[55]</ref> due to its constant negative effects on direct transfer. The results of the fully-supervised learning on Market and Duke are shown in Tab. 2. It should be pointed out that in fully-supervised learning, training and testing subsets contain the same cameras. Therefore, there is no significant shift among the BN statistics of the training set and the testing set, which favors the conventional formulation. Even so, our method still improves the performance on both Market and Duke. We believe that both aligning camera-wise distributions and better utilizing all annotations contribute to these improvements. Moreover, we also present results on direct transfer in Tab. 4. It is clear that our method improves BoT significantly, e.g., there is a 15.3% Rank-1 improvement when training on Duke but testing on Market. These improvements on both fully-supervised learning and direct transfer demonstrate the advantages of our camera-based formulation. Weak Supervisions. As we demonstrated in Sec. 3.1, the conventional ReID formulation strongly demands the inter-camera annotations for associating identities under distinct camera-related distributions. Since our method eliminates the distribution gap between cameras, the intra-camera annotations can be better used for learning the appearance features. We compare the performance of using all annotations (fully-supervised learning) and only intra-camera annotations (weakly-supervised learning). The results are in Tab. 3. For weakly-supervised experiments, we follow the same settings in MT <ref type="bibr" target="#b60">[61]</ref>. Since there are no intercamera annotations, the identity labels of different cameras are independent, and we assign each individual camera with a separate classifier. Each of these classifiers is supervised by the corresponding intra-camera identity labels. Surprisingly, even without inter-camera annotations, the weakly-supervised learning achieves competitive performance. According to these results, we believe that the importance of intra-camera annotations is significantly undervalued. Transfer. In this section, we evaluate the ability to transfer ReID knowledge between the old and new datasets. First, we evaluate the ability to transfer previous knowledge to new domains. The related task is domain adaptation, which usually involves a labeled source training set and another unlabeled target training set. We integrate our formulation into a recent state-of-the-art method ECN <ref type="bibr" target="#b56">[57]</ref>. The results are shown in Tab. 4. By aligning the distributions of source labeled images and target unlabeled images, the performance of ECN is largely boosted, e.g., when transferring from Duke to Market, the Rank-1 accuracy and mAP are improved by 6.6% and 9.0%, respectively. Meanwhile, compared to other methods that also utilize camera labels, such as CamStyle <ref type="bibr" target="#b57">[58]</ref> and CASCL <ref type="bibr" target="#b44">[45]</ref>, our method outperforms them significantly. These improvements demonstrate the effectiveness of our camera-based formulation in domain adaptation. Second, we evaluate the ability to preserve old knowledge as well as accumulate common knowledge for all seen datasets when being fine-tuned. Incremental learning, which fine-tunes a model on a sequence of training sets, is used for this evaluation. Experiments are designed as follows. Given three large-scale ReID datasets, there are in total six training sequences of length 2, such as (Market→Duke) and six sequences of length 3, such as (Market→Duke→MSMT). We use the baseline method described in Sec. 3.4 and train it on all sequences separately. After training on each dataset of every sequence, we evaluate the latest model on the first dataset of the corresponding sequence and record the performance decreases. Both the Data-Free and Replay settings are tested. For the Replay settings, the exemplars are selected by randomly sampling one image for each identity. Compared to the original training sets, the size of the exemplar set for Market, Duke, and MSMT is only 5.5%, 4.2%, and 3.4%, respectively. Note that in Replay settings, the old classifiers will also be updated in training. The corresponding results are shown in Tab. 5. To better demonstrate our improvements, we report the averaged results of the sequences that are of the same length and share the same initial dataset, e.g., averaging the results of testing Market on the sequences Market→Duke and Market→MSMT. In short, our formulation outperforms the dataset-based formulation in all experiments. These results further demonstrate the effectiveness of our formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>The experiments above demonstrate that our camera-based formulation boosts all the mentioned tasks. Now, we conduct more ablation studies to validate CBN.</p><p>Comparisons between CBN and other BN variants. We compare CBN with three types of BN variants. (1) BN <ref type="bibr" target="#b8">[9]</ref> and IBN <ref type="bibr" target="#b25">[26]</ref> correspond to the methods that use training-set-specific statistics to normalize all testing data.  V2 <ref type="bibr" target="#b22">[23]</ref>, and evaluate their performance on fully-supervised learning and direct transfer. As shown in Tab. 8, the performance is also boosted significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we advocate for a novel camera-based formulation for person reidentification (ReID) and present a simple yet effective solution named camerabased batch normalization. With only a few additional costs, our approach shrinks the gap between intra-camera learning and inter-camera learning. It significantly boosts the performance on multiple ReID tasks, regardless of the source of supervision, and whether the trained model is tested on the same dataset or transferred to another dataset. Our research delivers two key messages. First, it is crucial to align all camerarelated distributions in ReID tasks, so the ReID models can enjoy better abilities to generalize across different scenarios as well as transfer across multiple datasets. Second, with the aligned distributions, we unleash the potential of intra-camera annotations, which may have been undervalued in the community. With promising performance under the weakly-supervised setting (only intracamera annotations are available), our approach provides a practical solution for deploying ReID models in large-scale, real-world scenarios. The method used in the training stage is introduced in Section 3.3. In the testing stage, before generating the final features for each testing image, we first cluster these images according to their camera labels. For each of these camerarelated clusters, we randomly collect several unlabeled images. Then, we group these images into mini-batches and forward them across the ReID network. In this stage, the standardization procedure in every CBN uses mini-batch statistics, i.e., the same procedure in training. For each mini-batch, we collect the mini-batch mean and variance of every CBN layer. After forwarding all related mini-batches, we approximate the overall mean and variance of each CBN layer with these mini-batch statistics using the same way in the conventional BN. Finally, we inject our estimated results into each CBN layer and generate the final features of all images from this specific camera. The above procedure ends when images from all testing cameras are processed. The detailed algorithm is presented in Algorithm. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The Warm-Up Strategy in Section 4.1</head><p>In this section, we describe the warm-up strategy for initializing fully-connected classifiers in incremental learning tasks. Given a model that has already been trained on one or multiple ReID datasets, when fine-tuning it on a new training set, a new fully-connected classifier for classifying images from this specific dataset is required. Since this classifier is randomly initialized, if we directly fine-tune the entire model in an end-to-end manner, this classifier will introduce lots of noises to the feature extractor and heavily damage the previously learned knowledge. To alleviate the knowledge forgetting in the early stage of training, we warm-up the newest classifier before the formal training. Note that in the Replay incremental learning, there could be classifiers and images that correspond to multiple training sets (the exemplar memory and the current training set). However, in the warm-up stage of all incremental learning tasks, we only consider the latest training set and the corresponding new classifier. The details of this warm-up strategy are presented in Algorithm 2. In short, we freeze all previously learned layers and only iteratively fine-tune the new classifier on the latest training set until the loss becomes stable. After the warm-up stage, we start to train the entire network in a conventional end-to-end manner. Our experiments show that this warm-up stage is essential for preserving the previously-learned knowledge. Without the warm-up, the conventional BN-based</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Build the exemplar memory</head><p>Input: a ReID set D with an identity set K and a camera set C Output: the exemplar memory M in which each identity from D has exactly one image Initialize: create a dict Ω that records the number of already picked images from each camera. for all identity k in K do Collect all images that belong to the identity k Collect the camera ID of the above images as {c} Query Ω with {c} and find the camera c that has the least picked images Randomly pick an image that simultaneously belongs to camera c and identity k, and add it to M Ω [c] = Ω [c] + 1 end for method loses another 5.3% Rank-1 accuracy on average, while our formulation loses 3.7% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Exemplar Memory in Section 4.2</head><p>The exemplar memory is built for the Replay incremental learning task. Its goal is to reinforce the discriminative knowledge of the previous training sets with the least amount of old images. In this paper, we design a straightforward approach to achieve this goal. For each old training set, we propose a greedy algorithm that saves one image for each identity and tries to keep an equal number of images for each old camera. The details are presented in Algorithm 3. With this approach, the size of the exemplar memory for Market <ref type="bibr" target="#b50">[51]</ref>, Duke <ref type="bibr" target="#b52">[53]</ref>, and MSMT17 <ref type="bibr" target="#b41">[42]</ref> is only 5.5%, 4.2%, and 3.4% of their original training set, respectively.</p><p>Another thing worth noting is the way of utilizing these exemplars together with the data from the latest training set. On the one hand, in the exemplar memory, there are only very few samples that describe the previous cameras, and each old identity only has one image. On the other hand, as described in Section 3.3 and Section 4.1, for the latest training set, each identity has multiple images in the mini-batch, so does each camera. To make sure that our method can accurately approximate the CBN statistics of all previous and current cameras, we design a mixed sampling strategy. As shown in <ref type="figure">Fig. 3</ref>, when handling images from the latest training set, we follow the pipeline presented in Section 3.3. When sampling identities from the exemplar memory, we cluster images from the exemplar memory and make sure that each group has four successive old images that correspond to the same old camera. Then, these groups are randomly fused with the images sampled from the latest training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current Training Set</head><p>An identity with four images</p><p>The Exemplar Memory Four images of the same old camera A mini-batch <ref type="figure">Fig. 3</ref>. The demonstration of a mini-batch. (1) A blue rectangle denotes four images of the same identity. <ref type="formula" target="#formula_1">(2)</ref> The rectangles in other colors represent the images from the exemplar memory. Each rectangle corresponds to one image of an old identity. We group these exemplars according to their camera ID, and randomly fuse these groups with the data sampled from the current training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experiments on partially Replacing BN with CBN</head><p>These are supplementary experiments for demonstrating the necessity of replacing all BN layers with CBN layers, rather than only part of them. We go back to our baseline and divide the BN layers into six parts: the BN that appears before all residual blocks, the BN within each of the four residual stages, and the BN that appears after all blocks. The following table summarizes the direct transfer performance when the model trained on Duke is tested on Market. Since the vanilla BN is below satisfaction in the direct transfer experiments, we utilize AdaBN for adapting testing set statistics. These results indicate that replacing all BN layers with CBN layers obtains the best results in the direct transfer. More importantly, we emphasize that only replacing part of BN layers contradicts the fundamental idea of this paper, because we believe that distribution statistics should only be collected within a camera, and all camera-related distributions should be aligned explicitly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:2001.08680v3 [cs.CV] 18 Jul 2020 (a) We visualize the distributions of several cameras in Market-1501. Each curve corresponds to an approximated marginal density function. Curves of different cameras demonstrate the differences between the corresponding distributions. (b) The Barnes-Hut t-SNE [40] visualization of the distribution inconsistency among datasets. (c) Illustration of the proposed camera-based formulation. Note that Cam1, Cam2, and Cam3 could come from any ReID datasets. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Batch Normalization within Cameras. Our Camera-based Batch Normalization (CBN) aligns all training and testing cameras independently. It guarantees an invariant input distribution for learning, accumulating, and verifying the ReID knowledge. Given images or corresponding intermediate features x (c) m from camera c, CBN standardizes them according to the camera-related statistics:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Demonstrations of our bare-bones baseline network and two incremental learning settings involved in this paper. (a) Given an arbitrary backbone with BN layers, we simply replace all BN layers with our CBN layers. (b) Data-Free. (c) Replay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>N</head><label></label><figDesc>Injectμ (c) andσ 2 (c) into the corresponding CBN layer end for for all images I (c) from camera c do Compute final features f I (c) end for end for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2</head><label>2</label><figDesc>Warm-up the latest classifier Input: a trained ReID model with the feature extractor f (·), image Ii and the corresponding ID yi from the latest training set D Initialize: freeze all trainable parameters in f (·), randomly initialize a new classifier g (·) for D, set counter n = 0, set an empty list L = [] repeat Randomly sample a mini-batch {Ii} and the corresponding {yi} from D L = get loss (g (f ({Ii})) , {yi}) Backward L and only update g (·) Append L to L Truncate L and only preserve the latest 50 items if (L has 50 items) &amp; (|L − mean (L) | &lt;= 0.1) then n = n + 1 else n = 0 end if until n = 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of the baseline method with our formulation and the conventional formulation. The fully-supervised learning results are in italics.</figDesc><table><row><cell>Training Set</cell><cell cols="7">Testing Set Formulation Rank-1 mAP Rank-1 mAP Rank-1 mAP Market Duke MSMT</cell></row><row><cell>Market</cell><cell>Conventional Ours</cell><cell>90.2 91.3</cell><cell>74.0 77.3</cell><cell>37.0 58.7</cell><cell>20.7 38.2</cell><cell>17.1 25.3</cell><cell>5.5 9.5</cell></row><row><cell>Duke</cell><cell>Conventional Ours</cell><cell>53.2 72.7</cell><cell>25.1 43.0</cell><cell>81.5 82.5</cell><cell>66.6 67.3</cell><cell>27.2 35.4</cell><cell>9.1 13.0</cell></row><row><cell>MSMT</cell><cell>Conventional Ours</cell><cell>58.1 73.7</cell><cell>30.8 45.0</cell><cell>57.8 66.2</cell><cell>38.4 46.7</cell><cell>71.5 72.8</cell><cell>42.3 42.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The comparisons of fully-and weakly-supervised learning. Results of training and testing on the same domain are in italics. MT<ref type="bibr" target="#b60">[61]</ref> is our baseline. Except for the camera-based formulation, our weakly-supervised model follows all its settings.</figDesc><table><row><cell>Training Set</cell><cell cols="7">Testing Set Supervision Rank-1 mAP Rank-1 mAP Rank-1 mAP Market Duke MSMT</cell></row><row><cell></cell><cell>MT [61]</cell><cell>78.4</cell><cell>52.1</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>Market</cell><cell>Weakly</cell><cell>83.3</cell><cell>60.4</cell><cell>48.9</cell><cell>29.7</cell><cell>26.8</cell><cell>9.6</cell></row><row><cell></cell><cell>Fully</cell><cell>91.3</cell><cell>77.3</cell><cell>58.7</cell><cell>38.2</cell><cell>25.3</cell><cell>9.5</cell></row><row><cell></cell><cell>MT</cell><cell>−</cell><cell>−</cell><cell>65.2</cell><cell>44.7</cell><cell>−</cell><cell>−</cell></row><row><cell>Duke</cell><cell>Weakly</cell><cell>68.4</cell><cell>37.7</cell><cell>73.9</cell><cell>54.4</cell><cell>33.7</cell><cell>11.9</cell></row><row><cell></cell><cell>Fully</cell><cell>72.7</cell><cell>43.0</cell><cell>82.5</cell><cell>67.3</cell><cell>35.4</cell><cell>13.0</cell></row><row><cell></cell><cell>MT</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>39.6</cell><cell>15.9</cell></row><row><cell>MSMT</cell><cell>Weakly</cell><cell>68.3</cell><cell>37.2</cell><cell>59.2</cell><cell>38.2</cell><cell>49.4</cell><cell>21.5</cell></row><row><cell></cell><cell>Fully</cell><cell>73.7</cell><cell>45.0</cell><cell>66.2</cell><cell>46.7</cell><cell>72.8</cell><cell>42.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The results of testing ReID models across datasets. ‡ marks methods that only use the source domain data for training, i.e., direct transfer. Other methods listed in this table utilize both the source and target training data, i.e., domain adaptation.</figDesc><table><row><cell>Method</cell><cell cols="8">Duke to Market Rank-1 Rank-5 Rank-10 mAP Rank-1 Rank-5 Rank-10 mAP Market to Duke</cell></row><row><cell>UMDL [27]</cell><cell>34.5</cell><cell>52.6</cell><cell>59.6</cell><cell>12.4</cell><cell>18.5</cell><cell>31.4</cell><cell>37.6</cell><cell>7.3</cell></row><row><cell>PTGAN [42]</cell><cell>38.6</cell><cell>-</cell><cell>66.1</cell><cell>-</cell><cell>27.4</cell><cell>-</cell><cell>50.7</cell><cell>-</cell></row><row><cell>PUL [4]</cell><cell>45.5</cell><cell>60.7</cell><cell>66.7</cell><cell>20.5</cell><cell>30.0</cell><cell>43.4</cell><cell>48.5</cell><cell>16.4</cell></row><row><cell>SPGAN [3]</cell><cell>51.5</cell><cell>70.1</cell><cell>76.8</cell><cell>22.8</cell><cell>41.1</cell><cell>56.6</cell><cell>63.0</cell><cell>22.3</cell></row><row><cell>BoT*  ‡ [22]</cell><cell>53.3</cell><cell>69.7</cell><cell>76.4</cell><cell>24.9</cell><cell>43.9</cell><cell>58.8</cell><cell>64.9</cell><cell>26.1</cell></row><row><cell>MMFA [17]</cell><cell>56.7</cell><cell>75.0</cell><cell>81.8</cell><cell>27.4</cell><cell>45.3</cell><cell>59.8</cell><cell>66.3</cell><cell>24.7</cell></row><row><cell>TJ-AIDL [41]</cell><cell>58.2</cell><cell>74.8</cell><cell>81.1</cell><cell>26.5</cell><cell>44.3</cell><cell>59.6</cell><cell>65.0</cell><cell>23.0</cell></row><row><cell>CamStyle [58]</cell><cell>58.8</cell><cell>78.2</cell><cell>84.3</cell><cell>27.4</cell><cell>48.4</cell><cell>62.5</cell><cell>68.9</cell><cell>25.1</cell></row><row><cell>HHL [56]</cell><cell>62.2</cell><cell>78.8</cell><cell>84.0</cell><cell>31.4</cell><cell>46.9</cell><cell>61.0</cell><cell>66.7</cell><cell>27.2</cell></row><row><cell>CASCL [45]</cell><cell>64.7</cell><cell>80.2</cell><cell>85.6</cell><cell>35.6</cell><cell>51.5</cell><cell>66.7</cell><cell>71.7</cell><cell>30.5</cell></row><row><cell>ECN [57]</cell><cell>75.1</cell><cell>87.6</cell><cell>91.6</cell><cell>43.0</cell><cell>63.3</cell><cell>75.8</cell><cell>80.4</cell><cell>40.4</cell></row><row><cell>Baseline  ‡</cell><cell>53.2</cell><cell>70.0</cell><cell>76.0</cell><cell>25.1</cell><cell>37.0</cell><cell>52.6</cell><cell>58.9</cell><cell>20.7</cell></row><row><cell>Ours+BoT*  ‡</cell><cell>68.6</cell><cell>82.5</cell><cell>87.7</cell><cell>39.0</cell><cell>60.6</cell><cell>74.0</cell><cell>78.5</cell><cell>39.8</cell></row><row><cell cols="2">Ours+Baseline  ‡ 72.7</cell><cell>85.8</cell><cell>90.7</cell><cell>43.0</cell><cell>58.7</cell><cell>74.1</cell><cell>78.1</cell><cell>38.2</cell></row><row><cell>Ours+ECN</cell><cell>81.7</cell><cell>91.9</cell><cell>94.7</cell><cell cols="2">52.0 68.0</cell><cell>80.0</cell><cell>83.9</cell><cell>44.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Results of ReID models on incremental learning tasks. Each result denotes the percentage of the performance preserved on the first dataset after learning on new datasets. § marks the Data-Free settings. † corresponds to the Replay settings. Results of combining different normalization strategies in fully-supervised learning and direct transfer. In this table, BN and IBN correspond to the training-setspecific normalization methods. AdaBN adapts the dataset-wise normalization statistics. CBN follows our camera-based formulation and aligns each camera independently. The combination of BN and our CBN is to verify the importance of training ReID models with CBN. As shown in Tab. 6, training and testing the ReID model with CBN achieves the best performance in both fully-supervised learning and direct transfer. Samples Required for CBN Approximation. We conduct experiments for approximating the camera-related statistics with different numbers of samples. Note that if a camera contains less than the required number of images, we simply use all available images rather than duplicate them. We repeat all experiments 10 times and list the averaged results in Tab. 7. As demonstrated, the performance is better and more stable when using more samples to estimate the camera-related statistics. Besides, results are already good enough when only utilizing very few samples, e.g., 10 mini-batches. For the balance of simplicity and performance, we adopt 10 mini-batches for approximation in all experiments.</figDesc><table><row><cell></cell><cell>Testing Set</cell><cell cols="2">Market</cell><cell></cell><cell></cell><cell cols="2">Duke</cell><cell>MSMT</cell></row><row><cell cols="9">Seq Length Formulation Rank-1 mAP Rank-1 mAP Rank-1 mAP</cell></row><row><cell>1</cell><cell>−</cell><cell>100%</cell><cell cols="2">100%</cell><cell cols="2">100%</cell><cell cols="2">100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell></cell><cell cols="8">Conventional  § 82.2% 62.5% 80.2% 68.8% 55.5% 38.7%</cell></row><row><cell>2</cell><cell cols="8">Ours  § Conventional  † 92.5% 84.1% 90.9% 84.7% 81.7% 70.1% 88.3% 71.2% 89.3% 83.2% 74.5% 58.9%</cell></row><row><cell></cell><cell>Ours  †</cell><cell cols="7">95.0% 85.7% 94.3% 91.1% 91.6% 84.6%</cell></row><row><cell></cell><cell cols="8">Conventional  § 74.8% 52.2% 75.2% 63.0% 38.9% 24.7%</cell></row><row><cell>3</cell><cell cols="8">Ours  § Conventional  † 86.5% 74.0% 84.1% 76.4% 74.3% 60.9% 85.8% 66.0% 85.8% 77.4% 56.6% 39.4%</cell></row><row><cell></cell><cell>Ours  †</cell><cell cols="7">94.4% 83.1% 91.5% 87.6% 86.4% 76.0%</cell></row><row><cell cols="4">Training Method Testing Method</cell><cell cols="5">Duke to Duke Rank-1 mAP Rank-1 mAP Duke to Market</cell></row><row><cell></cell><cell>BN</cell><cell>BN</cell><cell></cell><cell cols="2">81.5</cell><cell cols="2">66.6</cell><cell>53.2</cell><cell>25.1</cell></row><row><cell></cell><cell>IBN [26]</cell><cell>IBN</cell><cell></cell><cell cols="2">77.6</cell><cell cols="2">57.0</cell><cell>61.7</cell><cell>29.5</cell></row><row><cell></cell><cell>BN</cell><cell>AdaBN [15]</cell><cell></cell><cell cols="2">81.2</cell><cell cols="2">66.2</cell><cell>55.8</cell><cell>28.1</cell></row><row><cell></cell><cell>BN</cell><cell>Our CBN</cell><cell></cell><cell cols="2">80.2</cell><cell cols="2">63.7</cell><cell>69.5</cell><cell>40.6</cell></row><row><cell></cell><cell>Our CBN</cell><cell>Our CBN</cell><cell></cell><cell cols="2">82.5</cell><cell cols="2">67.3</cell><cell>72.7</cell><cell>43.0</cell></row><row><cell cols="9">(2) AdaBN [15] is a dataset-wise adaptation that utilizes the testing-set-wise</cell></row><row><cell cols="9">statistics to align the entire testing set. (3) Compatibility with Different Backbones. Apart from ResNet [7] used in</cell></row><row><cell cols="9">the above experiments, we further evaluate the compatibility of CBN. We embed</cell></row><row><cell cols="9">CBN with other commonly used backbones: MobileNet V2 [30] and ShuffleNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>The mAP of our method on fully-supervised learning and direct transfer. We repeat each experiment 10 times and calculate the mean and variance of all results. Results of combining our camera-based formulation with different convolutional backbones. The fully-supervised learning results are in italics.</figDesc><table><row><cell># Batches</cell><cell cols="2">Market to Market mean variance</cell><cell cols="2">Market to Duke mean variance</cell></row><row><cell>1</cell><cell>76.29</cell><cell>0.032</cell><cell>37.34</cell><cell>0.047</cell></row><row><cell>5</cell><cell>77.21</cell><cell>0.010</cell><cell>38.08</cell><cell>0.017</cell></row><row><cell>10</cell><cell>77.33</cell><cell>0.007</cell><cell>38.19</cell><cell>0.008</cell></row><row><cell>20</cell><cell>77.37</cell><cell>0.005</cell><cell>38.18</cell><cell>0.002</cell></row><row><cell>50</cell><cell>77.39</cell><cell>0.001</cell><cell>38.21</cell><cell>0.001</cell></row><row><cell>Backbone</cell><cell>Training Set</cell><cell cols="3">Testing Set Formulation Rank-1 mAP Rank-1 mAP Market Duke</cell></row><row><cell>MobileNet V2 [30]</cell><cell>Market Duke</cell><cell cols="3">Conventional 87.7 Ours 89.8 73.7 69.2 Conventional 51.4 22.6 Ours 70.7 39.0 79.9 62.4 34.7 18.9 54.4 34.0 79.8 60.2</cell></row><row><cell>ShuffleNet V2 [23]</cell><cell>Market Duke</cell><cell cols="3">Conventional 82.6 Ours 85.9 65.8 58.4 Conventional 48.1 20.3 Ours 70.0 38.9 77.1 58.6 34.6 18.4 53.8 33.8 74.7 52.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>The direct transfer performance from Duke to Market. marks the component in which all its BN layers are replaced with CBN layers.</figDesc><table><row><cell cols="2">First BN Block 1 Block 2 Block 3 Block 4 Last BN Rank-1 mAP</cell></row><row><cell>55.8</cell><cell>28.1</cell></row><row><cell>60.6</cell><cell>31.6</cell></row><row><cell>61.9</cell><cell>32.9</cell></row><row><cell>65.0</cell><cell>35.3</cell></row><row><cell>65.7</cell><cell>35.7</cell></row><row><cell>67.3</cell><cell>37.0</cell></row><row><cell>72.7</cell><cell>43.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by National Science Foundation of China under grant No. 61521002.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A Camera-based Testing Scheme in <ref type="bibr">Section 3.3</ref> In this section, we introduce the testing scheme of our camera-based formulation. Unlike the conventional BN <ref type="bibr" target="#b8">[9]</ref>, which only calculates the statistics in the training stage and directly uses the recorded value for testing, our camerabased formulation with CBN utilizes a symmetrical approach, i.e., estimating the camera-related statistics in both training and testing stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Inference with CBN layers</head><p>Input: a trained feature extractor f (·), images from the testing camera set C. Initialize: grouping testing images according to their camera ID and randomly samples N mini-batches from each group, denoted as {Ii} (c) for all c ← 1 to |C| do Forward all images from {Ii} <ref type="bibr">(c)</ref> in N mini-batches for all CBN layers in f (·) do Collect the corresponding mini-batch mean µn and variance σ 2 n µ (c) = accumulate {µ1, µ2, ..., µN } σ 2 (c) = accumulate σ 2 1 , σ 2 2 , ..., σ 2</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Re-id done right: towards good practices for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05339</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scpnet: Spatial-channel parallelism network for joint holistic and partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Eanet: Enhancing alignment for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11369</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep low-resolution person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Harmonious attention network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Harmonious attention network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04779</idno>
		<title level="m">Revisiting batch normalization for practical domain adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification via softened similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pose transferrable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Resolution-invariant person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Encoder based lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<publisher>CVPR. IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generalizable person re-identification by domain-invariant mapping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mvp matching: A maximum-value perfect matching for mining hard samples, with application to person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Eliminating background-bias for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Glad: Global-local-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACMMM. ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Distilled person re-identification: Towards a more scalable system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification by cameraaware similarity consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification by deep asymmetric metric embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Single camera training for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Generalizing a person retrieval model heteroand homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Efficient online local metric adaptation via negative samples for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Intra-camera supervised person re-identification: A new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Viewpointaware loss with angular regularization for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
