<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-Stream Video Classification with Cross-Modality Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chi</surname></persName>
							<email>chilu@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiyu</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Two-Stream Video Classification with Cross-Modality Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fusing multi-modality information is known to be able to effectively bring significant improvement in video classification. However, the most popular method up to now is still simply fusing each stream's prediction scores at the last stage. A valid question is whether there exists a more effective method to fuse information cross modality. With the development of attention mechanism in natural language processing, there emerge many successful applications of attention in the field of computer vision. In this paper, we propose a cross-modality attention operation, which can obtain information from other modality in a more effective way than two-stream. Correspondingly we implement a compatible block named CMA block, which is a wrapper of our proposed attention operation. CMA can be plugged into many existing architectures. In the experiments, we comprehensively compare our method with two-stream and non-local models widely used in video classification. All experiments clearly demonstrate strong performance superiority by our proposed method. We also analyze the advantages of the CMA block by visualizing the attention map, which intuitively shows how the block helps the final prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, thanks to the emergence of massive video datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>, applications of deep learning in video classification have witnessed a rapid development. However, there is still a considerable improvement space towards human-level video understanding. The state-of-theart video classification methods are mainly based on convolutional neural networks. Despite tremendous progress has been recently made in designing highly discriminative network architectures, there still remain many open research problems. This research essentially concerns the following two problems:</p><p>Firstly, an insufficiently explore problem in video understanding is a more powerful way to capture the dynamic * is the corresponding author. information or motions in videos. As one of the main differentiators between videos and images, dynamic information is regarded to be indispensable for effective video classification. For example, it is a difficult task even for a human being to discriminate from different kinds of dances (e.g., salsa dancing, tango dancing and dancing macarena) by only having a glimpse at a single frame. A large number of widely-known video semantic categories, such as dancing and other sports, can be faithfully classified when we can extract sufficient motion-related information like moving trajectories.</p><p>Secondly, subtle details are key for recognizing some video categories or actions. The literature still lacks some in-depth analysis on effectively attending to those discriminative video details. Attention plays an important role in the field of natural language processing and image recognition. But it is still a nascent research topic in video action recognition. By grasping subtle details, humans can easily distinguish many classes. Considering the action sword fighting or playing cricket, only a single frame is enough as long as you can find a sword or a cricket. Generally, video motions attract more human attention and are likely to be related to key clues. For example, for the two actions making a sandwich and making pizza, their key objects sandwich or pizza are both around the moving hands. In this situation, motion can help attention.</p><p>Motivated by these observations, we propose a crossmodality attention operation, which can make full use of both static and motion information. Unlike the classic twostream framework <ref type="bibr" target="#b27">[28]</ref> that fuses information from two modalities in a late stage, we fuse the information in a more hierarchical and effective way.</p><p>Our proposed cross-modality attention operation devises such an attention mechanism that it encourages one modality absorbs most complementary information from other modalities. In contrast to the recently-proposed non-local operation <ref type="bibr" target="#b37">[38]</ref>, the proposed cross-modality attention can pay attention to other modalities rather than being constrained in the same modality. When two modalities under investigation are identical, our proposed method boils down to the non-local operation. Another key trait is that atten-tions are computed in a global manner. Specifically, spiritually alike the non-local operation, our proposed method computes the response as a weighted sum of the features of the other modality at all positions.</p><p>There are three main advantages of using the proposed cross-modality attention operation, sketched briefly as below: 1) It can effectively fuse the information between two or more modalities; 2) It can capture long-range dependencies by globally investigating the feature maps; 3) It can be wrapped as a highly compatible block that can be inserted into almost all existing neural networks and frameworks.</p><p>The rest of this paper is organized as: we first review related work in Section 2 and detail the novel cross-modality attention operation / network design in Sections 3 and 4. Section 5 shows the experiments and detailed analysis of our module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>With the significant development of deep learning in image recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, a large number of active researches have emerged in the field of video classification. Karpathy et al. <ref type="bibr" target="#b12">[13]</ref> contributed significant breakthrough in the video classification task. Their major contribution is 3-D convolutional neural networks trained on the Sports-1M data, which far exceeds traditional methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21]</ref> in terms of top-1 or top-5 classification accuracies. This seminal work demonstrates the power of temporal information in video-related tasks.</p><p>Optical flow fields are known as conceptually simple and empirically effective when attempting to capture the temporal information. A variety of approaches have been developed to utilize optical flow in video classification. A large body of existing works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref> has re-iteratively found that feeding the optical flow fields into a deep model can bring comparable performance with the RGB stream in video classification. After properly fused via late-stage fusion, one can accomplish a performance better than either stream. Recent endeavor along this research thrust includes direct mapping two adjacent frames to the optical flow field <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11]</ref>. Researchers have also investigated using deep neural networks for computing optical flows, which can be expedited by modern GPU hardware. However, the major obstacle stems from the lack of high-quality training data. To mitigate the data scarcity, some train an optical flow model from synthesized datasets <ref type="bibr" target="#b17">[18]</ref>, or predict the label of videos in an end-to-end way for improving the accuracy <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19]</ref>. In addition, the optimization ideas in the traditional methods are integrated into the design of the neural networks. Fan et al. <ref type="bibr" target="#b6">[7]</ref> unfold the optimization iterations in TV-L1 <ref type="bibr" target="#b40">[41]</ref> as neural layers, and Sun et al. <ref type="bibr" target="#b30">[31]</ref> propose neural networks to learn the representations orthogonal to the optical flow. We would point out that, though tremendous efforts have been noticed in computing optical flows, litter has been done to explore how to effectively using optical flow in video classification.</p><p>Optical flow can be regarded as an explicit way to utilize motion information to video classification. More recent research is pursuing other alternatives that rely on deep neural networks is automatically distill spatio-temporal video information. Typical examples include inflating 2D convolution into 3D convolution <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. One of key weaknesses of these models are the gigantic parameters used for defining high-dimensional convolutions etc. Using pre-trained models is a popularly-verified effective strategy for easing the model training in many tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, such as transferring deep models pre-trained on ImageNet to 3D CNN. A naive solution is to duplicate the parameters of the 2D filters T times along the time dimension, and rescale all parameters by dividing by T <ref type="bibr" target="#b2">[3]</ref>. This ensures a same response from the convolution filters. To reduce parameter number in 3D CNNs, some works factorize 3D convolutional filters into separate spatial and temporal components and strike a compromise in accuracy and efficacy <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>. Other relevant works mix 3D convolution with 2D convolution in a neural network <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. Despite the empirical success on indicative video benchmarks, 3D CNNs are far from reaching the point of fully acquiring the motion information and replacing the optical flow. Fusing with the optical flow stream is still an effective practice <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>. In fact, we can regard 3D CNNs as a general tool that acquires relation among adjacent frames. It can be fed with either RGB frames or other modalities (e.g., optical flow).</p><p>For complex video objects, other information also provide complementary information, including audio <ref type="bibr" target="#b16">[17]</ref>, human pose <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>, and semantic body part <ref type="bibr" target="#b42">[43]</ref> etc. Learning how to efficiently integrate multi-modality information is an emerging research direction. Existing researches, such as pooling at different stages <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> or modeling long-range temporal structure using LSTM <ref type="bibr" target="#b38">[39]</ref>, mainly concern fusing in the temporal dimension. There are rarely relevant studies about the fusion of different modalities <ref type="bibr" target="#b38">[39]</ref>. To date, the mainstream method is still the two-stream method <ref type="bibr" target="#b27">[28]</ref>. Our primary goal is to design a network structure that is more effective than two-stream and meanwhile achieves higher precision.</p><p>Attention networks have been originally popularized in natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>, used for comprehension and abstractive summarization etc. Recent years have observed a quick spread in computer vision <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42]</ref>. Xie et al <ref type="bibr" target="#b39">[40]</ref> place a feature gating module after some convolutional layers to weight the features in each channel in an adaptive, data-dependent way. Long et al <ref type="bibr" target="#b16">[17]</ref> propose attention clusters, which aggregates local features to generate a valid global representations for video classification. Nonlocal networks <ref type="bibr" target="#b37">[38]</ref> can weight all information (including space and time) by adopting a mechanism similar to selfattention. Our motivating observation is the lack of cross modality attention block, which works globally as non-local block but can make full use of cross-modality information. Importantly, this block shall be compatibly inserted into most existing network structures including the classic twostream inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cross Modality Attention</head><p>In this section, we give detailed description of the proposed Cross Modality Attention(CMA) operation and its implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Our proposed Cross Modality Attention(CMA) operation can be precisely described in the Q-K-V language, namely matching a query from one modality with a set of key-value pairs from the other modality and thereby extracting most critical cross-modality information. Following the notations in <ref type="bibr" target="#b33">[34]</ref>, we define the generic CMA operation as:</p><formula xml:id="formula_0">CMA(Q 1 , K 2 , V 2 ) = sof tmax Q 1 K T 2 √ d k V 2 ,<label>(1)</label></formula><p>where the index 1 or 2 represents different modality. Q is the set of queries, K is a matrix of the memory keys and V contains memory values. All Q, K and V are of feature dimension d k .</p><p>Here we give a concrete instance of the CMA operation in neural networks. Given a typical two-stream data (RGB + flow), a CMA operation can be written as:</p><formula xml:id="formula_1">z i = 1 C(x, y) ∀j f (x i , y j )v(y j ) (2) f (x i , y j ) = e q(xi)k(yj ) T / d k (3) C(x, y) = ∀j f (x i , y j ),<label>(4)</label></formula><p>where x is from the feature maps of specific stage of the RGB branch, such as the output of res 4 in ResNet <ref type="bibr" target="#b8">[9]</ref>. y is from the feature maps in the flow branch. z i denotes the output of the CMA operation. i and j are both indices of feature maps (can be in space, time, or spacetime). q, k and v are linear embeddings which map x or y to queries, keys and values of d k dimensions respectively. The function f can be flexibly defined, with many instantiations discussed in <ref type="bibr" target="#b37">[38]</ref>. For simplicity, we choose the embedded Gaussian version in this paper.</p><p>The non-local operation <ref type="bibr" target="#b37">[38]</ref> is essentially self-attention and only pays attention to intra-modality. In comparison, our proposed CMA is cross-modal. Moreover, the non-local operation can be regarded as a special case of CMA when K, Q and V are all from the same modality.  <ref type="figure">Figure 1</ref>: An example of CMA block. We show the shape of feature maps at each stage, such as H × W × 1024, where 1024 is the number of channels. Let X be the feature maps of the RGB branch and Y be the feature maps of flow branch. The number of channels is halved via 1 × 1 convolutions. Reshaping or transposing is performed whenever needed. " " denotes matrix multiplication, and " " denotes element-wise sum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CMA Block</head><p>A CMA block is a wrapper of the CMA operation that can be inserted into many existing neural networks, which is defined as:</p><formula xml:id="formula_2">out i = W out z i + x i ,<label>(5)</label></formula><p>where x i and z i are given in Eqn. <ref type="bibr" target="#b1">(2)</ref>. W out defines a linear embedding that can be implemented by convolution operation. <ref type="figure">Figure 1</ref> presents an example of the CMA block, where Q comes from the RGB branch and V , K come from the flow branch. This allows the RGB branch to attend over all positions in the flow branch at a specific stage. As a result, it can get more valuable information selectively from the flow branch which may be weak or even missing in itself. A CMA block can be added into any location of deep neural networks, since it can be fed with input of any shape and ensure a same-shaped output. This flexibility allows us to fuse richer hierarchical features between different modalities. To make the CMA block more compatible, we add a residual connection "+x i " <ref type="bibr" target="#b8">[9]</ref>. This guarantees a non-worse accuracy with the CMA block by some simple means (e.g., zeroing W out ).</p><p>Implementation of CMA Blocks: We implement functions q, k, and v as 1 × 1 convolutions in space or 1 × 1 × 1  convolution in space-time, denoted as conv q , conv k and conv v respectively. To reduce the computational complexity and GPU memory consumption, we let conv q reduce the number of channels to be half of that in x, and conv k and conv v have same number of channels as conv q . Note that we also insert a spatial max-pooling layer with stride 2 before k and v, further simplifying the computation. Inspired by <ref type="bibr" target="#b37">[38]</ref>, a batch normalization layer is added after W out and has the scale parameters initialized to be zeros, which sets the entire CMA block as an identity mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network Architecture</head><p>This section elaborates on our proposed video classification model. We first introduce two branches in our model and how the CMA blocks are inserted. Then, we depict the whole network architecture and finally describe the details of training strategy and implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Two Branches for video classification</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our model contains two branches, including the RGB branch and Flow branch. As mentioned in <ref type="bibr" target="#b27">[28]</ref>, the RGB branch carries visual information about scenes and objects in the video, while the Flow branch conveys the motion. All the information from both branches are crucial for video classification. In two-stream <ref type="bibr" target="#b27">[28]</ref>, they simply average the scores of the two branches to make the final prediction.</p><p>We add several CMA blocks at some intermediate stages of each branch, obtaining information from the other branch. Compared with the two-stream method, this fuses two modalities much earlier and hierarchically. There are three classification scores in our model. The first two scores are from the RGB branch and the Flow branch respectively, and the last one is a weighted summation of RGB / Flow branches. Empirical investigation deferred to Section 5 shows that any of these three scores can make an excellent prediction. In fact, in the scenario of highly-budgeted parameters, one can just use the scores of the RGB branch without much loss of performance.</p><p>Implementation: Both branches adopt ResNet-50 <ref type="bibr" target="#b8">[9]</ref> as base network. Considering the limited GPU memory and precise spatial information, we add 5 CMA blocks in res <ref type="bibr" target="#b2">3</ref> and res 4 to every other residual block, which is also similar to the setting in non-local neural networks <ref type="bibr" target="#b37">[38]</ref>. The RGB branch takes only one RGB frame as input, while the Flow branch stacks five consecutive optical flow fields as input. The RGB branch can be directly initialized from the ResNet weights pre-trained on ImageNet <ref type="bibr" target="#b24">[25]</ref>. Since the number of input channels of the Flow branch is different from that of the models pre-trained on ImageNet, we initialize the weights of the first convolution by replicating the means of the pre-trained weights across channels. The CMA blocks are initialized via the same scheme in <ref type="bibr" target="#b8">[9]</ref>. We zero the scale parameters of the last BN layer as previously mentioned in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">TSN Framework</head><p>Temporal Segment Networks (TSN) has been proved to be powerful in modeling long-range temporal structure <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b30">31]</ref>. We also incorporate this effective albeit simple framework. Given a video, we divide it into K segments, ensuring the duration of each segment equal. For each segment, a snippet (1 RGB frame for the RGB branch and 5 consecutive optical flow fields for the Flow branch) is randomly sampled. We average the scores produced by each segment to get the final video-level score, namely</p><formula xml:id="formula_3">G = 1 K K i G i ,<label>(6)</label></formula><p>where K is the number of segments and G i is the score of one specific snippet. The overall loss function can be defined as:</p><formula xml:id="formula_4">L(y, G) = − C c=1 y c (G c − log C j=1 e Gj ),<label>(7)</label></formula><p>where C is the number of video classes and y c is the groundtruth label concerning class c. G c are the scores of the same class on all snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Strategy</head><p>Since the Flow models converge much slowly than RGB models <ref type="bibr" target="#b36">[37]</ref>, we firstly train the flow branch on Kinetics data <ref type="bibr" target="#b13">[14]</ref>. After that, considering the limited GPU memory, we train the CMA Model in an iterative way between two branches. Thinking that the CMA blocks is initially an identity mapping and the Flow branch has been trained on the kinetics, the Flow branch can provide more reliable information to the RGB branch before the iterative training stage. Therefore, we train the RGB branch in iter 1 , iter 3 , iter 5 ... and train the Flow branch in iter 2 , iter 4 , iter 6 , .... When training the RGB branch, its parameters are optimized according to the loss of the current branch and we freeze all the layers in the Flow branch, including CMA blocks of the Flow branch. Similar treatment for training the Flow branch. The total number of epochs at each iteration is set to 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Details</head><p>Input: The video frames are scaled to size 256 × 256. We choose the TVL1 optical flow algorithm <ref type="bibr" target="#b40">[41]</ref> to extract optical flow for the Flow branch, based on the GPU version from the OpenCV toolbox. The pixel values of optical flow are truncated to the range [−20, 20], and then re-scaled between -1 and 1. The input size of two branches are both 224 × 224, cropped from the video frames or optical flow fields. The RGB branch takes only one frame (f rame t ) as the input and the Flow branch reads a stack of consecutive optical flow fields ([of t ,of t+1 ,of t+2 ,of t+3 ,of t+4 ]). In other words, the input shapes of two branches are N ×224× 224 × 3 and N × 224 × 224 × 10 respectively, where N is the batch size and the last dimension represents the number of channels. It's important to note that the RGB frame is corresponding to the first optical flow field in the temporal dimension, and all RGB frame / optical flows are spatially aligned. For data augmentation, we use random horizontal flipping, random cropping and scale jittering <ref type="bibr" target="#b36">[37]</ref>. And the number of segments is set to 3.</p><p>Training: We use a standard cross-entropy loss and mini-batch stochastic gradient descent algorithm to optimize the network parameters, where the batch size is 128. We train the model with BN enabled, which is the same to <ref type="bibr" target="#b37">[38]</ref>. To make the statistics of each BN layer more accurate, we use the synchronized batch normalization <ref type="bibr" target="#b21">[22]</ref>. The learning rate is initialized as 0.01 and get reduced by a factor of 10 when the accuracy is stuck in some plateau. At the beginning of each iteration, we reset the learning rate to the initial value. The dropout ratio is 0.7 and the weight decay is 5 −4 , which are introduced to reduce over-fitting.</p><p>Testing: During test time we use ten-croppings and flip four corners and the center of the frame or optical flow filed as <ref type="bibr" target="#b14">[15]</ref>. The number of segments is set to 25 and the temporal spacing between each segment is equal. We average the scores across all the samples and crops of them to get the final video-level score. For the fusion score, we firstly get the frame-level scores via weighted sum and then average all the scores to get the video-level score. We will provide an empirical study on the fusion weights in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>We evaluate the proposed methods and perform ablation studies on two popular datasets, UCF101 <ref type="bibr" target="#b29">[30]</ref> and Kinetics <ref type="bibr" target="#b13">[14]</ref>. For clarity, let CMA iter i be the model trained after ith iterations. We add the suffix "-R", "-F", "-S" for the RGB / Flow streams or two-stream respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>UCF-101 <ref type="bibr" target="#b29">[30]</ref> consists of 101 action classes and over 13-K clips. All the videos are downloaded from YouTube, and all of them are recorded in unconstrained environments, including various lighting conditions, partial occlusion, low quality frames etc.</p><p>Kinetics <ref type="bibr" target="#b13">[14]</ref> is a large-scale trimmed video dataset which contains more than 300-K video clips in total, and each clip has a duration of around 10 seconds. The dataset covers 400 human-centric classes and each class has at least 400 video clips. For unknown reasons, there are some invalid urls and we are unable to crawl some of the videos. We get 232,679 videos for training and 19,169 for validation. We skip processing the testing set since their labels are not provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Investigation of Fusion Weights</head><p>To get the fusion score, two-stream <ref type="bibr" target="#b27">[28]</ref> averages the scores from two modalities and <ref type="bibr" target="#b36">[37]</ref> gives more credits to the RGB modality by setting its weight as 1 and that of Flow modality as 1.5. But our proposed model contains two branches which are interdependent, consequently, training one branch inevitably have an effect on the other one. In this situation, exploring suitable fusion weights is necessary. <ref type="figure" target="#fig_2">Figure 3</ref> shows the top-1 accuracy with different fusion weights. We use the two-stream as a baseline whose base model is the same as ours (ResNet50). The two-stream model can achieve a higher accuracy when the weight of the two branches is almost the same. But for the CMA  The CMA models perform better than the two-stream when we give higher weight to the more reliable branch. models, at the first training iteration, we just train the RGB branch with the Flow branch fixed, so the RGB branch performs better than the Flow branch. In other words, the RGB branch is more reliable. Giving the RGB branch more weight will make the final accuracy higher, but too much weight will make the other branch almost completely ignored. At the second iteration, we should similarly give more weight to the Flow branch. From <ref type="figure" target="#fig_2">Figure 3</ref> one can see that the fusion accuracy of CMA models is always higher than the baseline, as long as we give more weight to the more reliable branch.</p><p>Based on the above analysis, we give the equal weights to the two branches in two-stream, identical to <ref type="bibr" target="#b27">[28]</ref>, and set the weights of the RGB / Flow branches as 5 : 1 at iter 1 , iter 3 , ... and 1 : 5 at iter 2 , iter 4 , .... For all the following experiments we adopt such setting. In Section 4.3, we introduce the iterative training strategy. Here let us study how many iterations we need for convergence. <ref type="table" target="#tab_2">Table 1</ref> lists the accuracy at different iterations. iter 0 represents the baseline that has the two branches trained independently. The fusion accuracy is equal to twostream 1 . After iter 1 , the RGB branch has exceeded the two-stream, and the Flow branch keeps the same as the baseline because we have not trained on it at this iteration and the CMA blocks in this branch are now just an identity mapping. Additionally, the accuracy of fusion is much higher than others. In order to achieve higher accuracy for the Flow branch, we train the Flow branch with the RGB branch freezed at the second iteration. As expected, the accuracy of the Flow branch is improved and can be comparable to the two-stream. But the performance of the RGB branch drops due to that the distribution of the feature maps of the Flow branch has changed, which can affect the RGB branch through the CMA blocks. After iter 3 , the accuracy of the RGB branch returns to the relative high level while the Flow branch degrades slightly. The fusion score doesn't be improved any more. It is thus observed that the first iteration is almost sufficient for our models.     <ref type="figure" target="#fig_3">Figure 4</ref> showed the top-20 most improved categories and compare between our CMA model / two-stream. We also list the top-10 confusing categories in <ref type="table" target="#tab_6">Table 3</ref>. Compared with two-stream, the proposed CMA model is more sensitive about the motion trajectories, such as water sliding and jumping into pool, although the background is similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance at Each Iteration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis and Visualization</head><p>Due to the fact that almost all of the samples of yawning are about babies, it's very easy to confuse with baby waking up. But our model can improve the performance according to the different motion between two categories. Additionally, the CMA model can pay more attention to the moving objects, such as jaw or mouth, or the tools (cup or razor) held on the hand, while discriminating between gargling and trimming or shaving beard. We also visualize some attention maps in <ref type="figure">Figure 6</ref>.  Non-local block <ref type="bibr" target="#b37">[38]</ref> is also a kind of attention-based model which pays attention to the intra-modality features. It also shows good performance in video classification. In order to compare the performance and mutual influence between self-attention blocks and our proposed crossmodality attention blocks, we carry out some experiments, and the results are shown in <ref type="table" target="#tab_8">Table 4</ref>. Following <ref type="bibr" target="#b37">[38]</ref>, we add five blocks to ResNet50 in res 3 and res 4 , the same numbers and locations as that of the CMA blocks. To explore the influence between these two kinds of blocks, we conduct the experiments that adding CMA blocks just behind the nonlocal blocks. To ensure the comparisons more tractable, we only add the nonlocal blocks in the RGB branch, which implies that the Flow branch is the same to the Flow modality of the ResNet50 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with Non-local Blocks</head><p>From the results in <ref type="table" target="#tab_8">Table 4</ref>, we find that non-local blocks can roughly improve top-1 accuracy by 1% in both RGB and Flow modalities of ResNet50 model. For our proposed model, even only the results of the RGB branch outperform the fusion results of ResNet50 with nonlocal blocks. More importantly, the non-local blocks seem unnecessary while using our CMA blocks, which shows that the CMA blocks can also play a role of non-local blocks while fusing different modalities. In other words, the CMA blocks can also capture global information. We also visualize the attention maps of non-local blocks in Appendix A, which can intuitively show the improvement of CMA blocks over nonlocal blocks.  To illustrate that the proposed CMA blocks can also be compatible with 3D convolutional neural networks and fur- <ref type="figure">Figure 5</ref>: Examples of the attention maps. We train CMA iter1-R on Kinetics and visualize the attention maps of the last CMA block in res4 since the last block is the most related to the final classification. These samples are taken from Kinetics randomly. Each set contains three images, including (from left to right) RGB frame, optical flow fields, and the attention map. In the attention map, we draw some arrows that start from the query location and point to the more interesting parts in the CMA block. We observe that the block can easily focus on moving objects, such as the moving hand in the top-left set and the swimming person in the bottom-left set. And as a result, the RGB branch can take important information from the Flow branch as much as possible within limited capacity. We also find that the CMA operation is global. Looking at the example in the top-right, the pixel on the right person can not only focus on the nearby region but also pay attention to the other boxer, which shows our CMA block can capture long-range dependencies. Moreover, not all the moving objects can attract the attention, only key information does. In the last example tying bow tie, the block pays more attention to the region around the hand although the whole upper body is moving, because that the object held in hands often has more impact on the prediction. And more attention maps can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">3D-CMA Blocks</head><p>ther improve its performance, we insert this operation into P3D ResNet <ref type="bibr" target="#b22">[23]</ref>. We initialize the P3D network with the weights duplicated from the official caffemodel 2 and finetune it using data argumentation. For the CMA model, considering the limited GPU memory, we only add one CMA block after the last layer in res 4 , and train the CMA block and all layers behind it. We train our CMA model with different numbers of input frames. <ref type="table" target="#tab_10">Table 5</ref> summarizes the experimental results. As seen, the CMA block can also bring an improvement for P3D compared with two-stream. Fusion with two branches can further improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Transfer learning</head><p>We also conduct transfer learning experiments from Kinetics to UCF-101. We only fine-tune the last fc layer of our 2-D CMA model. <ref type="table">Table 6</ref> shows the results. We find that our model is somewhat easier to over-fit on the small dataset. Nonetheless, the proposed CMA iter 1 -S can outperform most of the state-of-the-art 2D models. It even approximates the performance of 3D models (e.g., I3D with 64 RGB frames and 64 flows as its input) although only 2D convolutional network as base model is used.  <ref type="table">Table 6</ref>: Comparison with state-of-the-art on the UCF-101. The first set is the results reported by other papers, and the second set is our results of transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future work</head><p>We have shown that the cross-modality operation can significantly improve the performance in video classification. The proposed CMA block can be compatibly inserted to most existing neural networks. It proves very effective to fuse information between different modalities. Our future works include extending the evaluations on other more sophisticated deep models, and evaluating the CMA operation among more modalities beyond both the RGB and Flow branches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the video classification model. This model contains both RGB branch and Flow branch. In each branch, we insert n CMA blocks, which play an important role in transmitting information between different branches. There are three outputs in this model. Operations in dotted box are not essential since practically we can only use the output of the RGB branch for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The top-1 accuracies with different fusion weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparing top-20 most improved categories between the proposed CMA model and two-stream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Accuracies at each iteration on the Kinetics dataset.</figDesc><table><row><cell>Iteration</cell><cell>RGB</cell><cell>Flow</cell><cell>two-stream</cell></row><row><cell></cell><cell cols="3">top-1 top-5 top-1 top-5 top-1 top-5</cell></row><row><cell>0</cell><cell cols="3">67.73 87.94 55.73 79.04 71.21 89.92</cell></row><row><cell>1</cell><cell cols="3">72.17 90.70 55.73 79.04 72.62 91.04</cell></row><row><cell>2</cell><cell cols="3">68.45 88.54 71.17 90.12 71.55 90.24</cell></row><row><cell>3</cell><cell cols="3">72.19 90.63 69.81 89.41 72.55 90.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>CMA model vs two-stream in terms of parameter number and accuracy. The number of parameters are relative to the ResNet50 baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>compares our method with two-stream in terms of a few key factors, including number of parameters and final accuracy. CMA iter 1 -R is more accurate than twostream, though fewer parameters are used. That validates that our CMA model is more effective than two-stream for fusing.</figDesc><table><row><cell>groundtruth</cell><cell>confusing category</cell></row><row><cell>gargling</cell><cell>trimming or shaving beard</cell></row><row><cell>tying tie</cell><cell>tying bow tie</cell></row><row><cell>yawning</cell><cell>baby waking up</cell></row><row><cell>cracking neck</cell><cell>massaging back</cell></row><row><cell>kissing</cell><cell>hugging</cell></row><row><cell>rock scissors paper</cell><cell>shaking hands</cell></row><row><cell>running on treadmill</cell><cell>waxing leg</cell></row><row><cell>water sliding</cell><cell>jumping into pool</cell></row><row><cell>sneezing</cell><cell>crying</cell></row><row><cell>breading or breadcrumbing</cell><cell>cooking chicken</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The top-10 confusing categories on which the CMA model achieves the largest gain compared with two-stream in Kinetics. The gain is the improved accuracy (%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparisons with non-local networks on Kinetics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performance of P3D and 3D-CMA models on Kinetics when varying the count of input frames. All models adopt ResNet-152 as the backbone, and the input of CMA blocks are all 3D.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although we name the baseline as iter 0 , we don't initialize the CMA model with the parameters in iter 0 . The train strategy keep the same as described in Section 4.3</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A. More attention maps</head><p>To more intuitively illustrate the effect of CMA blocks, we show more attention maps of the last CMA block of CMA iter 1 -R in <ref type="figure">Figure 6</ref>. Consistent with the conclusions in the main text, our CMA blocks tend to focus on moving objects (which are strong evidence for video classification), and are able to capture long-range dependencies. In experiments we also notice some failure cases. <ref type="figure">Figure 6</ref> highlights two examples using red bounding boxes. As seen, when the query position is located at the background, the CMA block may focus on itself or other positions of background, which supposedly does not benefit the performance.</p><p>We also compare the attention maps generated by nonlocal block <ref type="bibr" target="#b37">[38]</ref> and CMA block in <ref type="figure">Figure 7</ref>. The non-local blocks are specified exactly the same to the description in Section 5.5 in the main text, adopting ResNet50 as backbone and using RGB frames as input. From <ref type="figure">Figure 7</ref>, we find that these two kinds of blocks have similar behavior in some examples where both focus on critical objects in the videos (like the one from action trapezing). Nonetheless, our proposed CMA block can pay more attention to the moving objects which are at least equally crucial to the final prediction. For example, in the last two cases in <ref type="figure">Figure 7</ref>, representing actions of cartwheelin and high kick respectively, CMA block only focuses on the moving person while the non-local block disperses its attention to the stationary object or the audiences that are less relevant to video classification.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno>abs/1609.08675</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1711.07319</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rpan: An end-to-end recurrent pose-attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/1809.02983</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Zisserman. The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7834" to="7843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1616" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="392" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1711.07240</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>abs/1712.08416</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1511.04119</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Appearanceand-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1712.04851</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-L 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno>abs/1805.08318</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Single image action recognition using semantic body part actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3411" to="3419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ECO: efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
