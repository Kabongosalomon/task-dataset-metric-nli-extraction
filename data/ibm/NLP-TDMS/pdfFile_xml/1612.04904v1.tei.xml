<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuãn</forename><surname>Trãn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Open University of Israel</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 3D shapes of faces are well known to be discriminative. Yet despite this, they are rarely used for face recognition and always under controlled viewing conditions. We claim that this is a symptom of a serious but often overlooked problem with existing methods for single view 3D face reconstruction: when applied "in the wild", their 3D estimates are either unstable and change for different photos of the same subject or they are over-regularized and generic. In response, we describe a robust method for regressing discriminative 3D morphable face models (3DMM). We use a convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from an input photo. We overcome the shortage of training data required for this purpose by offering a method for generating huge numbers of labeled examples. The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set. Coupled with a 3D-3D face matching pipeline, we show the first competitive face recognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes as representations, rather than the opaque deep feature vectors used by other modern systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single view 3D face shape estimation methods originally proposed using their 3D shapes for recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">26]</ref>. This makes sense because 3D shapes are discriminativedifferent people have different face shapes -yet invariant to lighting, texture changes and more. Indeed, previous work showed that when available, high resolution 3D face scans are excellent face representations which can even be used to distinguish between the faces of identical twins <ref type="bibr" target="#b8">[9]</ref>.</p><p>Curiously, however, despite their widespread use, single view face reconstruction methods are rarely employed by modern face recognition systems. The highly successful 3D Morphable Models (3DMM), for example, were only ever used for recognition in limited, controlled viewing conditions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b26">26]</ref>. To our knowledge, there are no  <ref type="bibr" target="#b31">[31]</ref>, (c) flow based method <ref type="bibr" target="#b11">[12]</ref> (d) 3DDFA <ref type="bibr" target="#b45">[45]</ref>, (e) Our proposed approach. (b-c) Present different 3D shapes for the same subject and (d) appears generic, whereas our method (e) is robust, producing similar discriminative 3D shapes for different views.</p><p>reports of successfully using single view face shape estimation -3DMM or any other method -to recognize faces in challenging unconstrained, in the wild settings.</p><p>An important reason why this maybe so, is that these methods can be unstable in unconstrained viewing conditions. We later verify this quantitatively but it can also be seen in <ref type="figure" target="#fig_0">Fig. 1</ref> which presents 3D shapes estimated from three unconstrained photos by three different methods ( <ref type="figure" target="#fig_0">Fig. 1 (b-d)</ref>). Clearly, though the same subject appears in all photos, shapes produced by the same method are either very different (b,c) or highly regularized and generic <ref type="bibr">(d)</ref>. It is therefore unsurprising that these shapes are poor representations for recognition. It also explains why some recently proposed using coarse, simple 3D shape approximations only as proxies when rendering faces to new views rather than as face representations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b37">37]</ref>.</p><p>Contrary to previous work, we show that robust and dis-criminative 3D face shapes can, in fact, be estimated from single, unconstrained images ( <ref type="figure" target="#fig_0">Fig. 1 (e)</ref>). We propose estimating 3D facial shapes using a very deep convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from single face photos. We identify shortage of labeled training data as an obstacle to using data-hungry CNNs for this purpose. We address this problem with a novel means for generating a huge labeled training set of unconstrained faces and their 3DMM representations. Coupled with additional technical novelties, we obtain a method which is fast, robust and accurate. The accuracy of our estimated shapes is verified on the MICC data set <ref type="bibr" target="#b0">[1]</ref> and quantitatively shown to surpass the accuracy of other 3D reconstruction methods. We further show that our estimated shapes are robust and discriminative by presenting face recognition results on the Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b17">[17]</ref>, YouTube Faces (YTF) <ref type="bibr" target="#b40">[40]</ref> and IJB-A <ref type="bibr" target="#b22">[22]</ref> benchmarks. To our knowledge, this is the first time single image 3D face shapes are successfully used to represent faces from modern, unconstrained face recognition benchmarks. Finally, to promote reproduction of our results, we publicly release our code and models. 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Over the years, many attempts were made to estimate the 3D surface of a face appearing in a single view. Before listing them, it is important to mention recent multi image methods which use image sets for reconstruction (e.g., <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36]</ref>). Although these methods produce accurate 3D reconstructions, they require many images from multiple sources to produce a single 3D face shape whereas we reconstruct faces from single images.</p><p>Methods for single view 3D face reconstructions can broadly be categorized into the following types. Statistical shape representations, such as the widely popular 3DMM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b43">43]</ref>, use many aligned 3D face shapes to learn a distribution of 3D faces, represented as a high dimensional subspace. Each point on this subspace is a parameter vector representing facial geometry and sometimes expression and texture. Reconstruction is performed by searching for a point on this subspace that represents a face similar to the one in the input image. These methods do not attempt to produce discriminative facial geometries and indeed, as mentioned earlier, were only used for face recognition under controlled settings.</p><p>The very recent method of <ref type="bibr" target="#b29">[29]</ref> also uses a CNN to regress 3DMM parameters for face photos. They too recognize absence of training data as a major concern. Contrary to us, they propose synthesizing training faces with known geometry by sampling from the 3DMM distribution. <ref type="bibr" target="#b0">1</ref> Please see www.openu.ac.il/home/hassner/projects/ CNN3DMM for updates. This approach produces synthetic looking photos which can easily cause overfitting problems when training large networks <ref type="bibr" target="#b24">[24]</ref>. They were therefore able to train only a shallow residual network (seven layers compared to our 101) and their estimated shapes were not shown to be more robust or discriminative than other methods. Scene assumption methods. In order to obtain correct reconstructions, some make strong assumptions on the scene and the viewing conditions in the input image. Shape from shading methods <ref type="bibr" target="#b20">[20]</ref>, for example, make assumptions on the light sources, facial reflectance and more. Others instead use facial symmetry <ref type="bibr" target="#b10">[11]</ref>. The assumptions they and others make often do not hold in practice, limiting the application of these methods to controlled settings. Example based methods, beginning from the work of <ref type="bibr" target="#b13">[13]</ref> and more recently <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">37]</ref>, modify the 3D surface of example face shapes, fitting them to the face appearing in input photo. These methods favor robustness to challenging viewing conditions over detailed reconstructions. They were thus only used for face recognition to synthesize new views from unseen poses. Landmark fitting methods. Finally, some reconstruction techniques fit a 3D surface to detected facial landmarks rather than to face intensities directly. These include methods designed for videos (e.g., <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b34">34]</ref>) and the CNN based approaches of <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b45">45]</ref>. These focus more on landmark detection than 3D shape estimation and so do not attempt to produce detailed and discriminative facial geometries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Regressing 3DMM parameters with a CNN</head><p>We propose to regress 3DMM face shape parameters directly from an input photo using a very deep CNN. Ostensibly, CNNs are ideal for this task: After all, they are being successfully applied to many related computer vision tasks. But despite their success, apart from <ref type="bibr" target="#b29">[29]</ref>, we are unaware of published reports of using CNNs for 3DMM parameter regression.</p><p>We believe CNNs were not used here because this is a regression problem where both the input photo and the output 3DMM shape parameters are high dimensional. Solving such problems requires deep networks and these need massive amounts of training data. Unfortunately, existing unconstrained face sets with ground truth 3D shapes are far too small for this purpose and obtaining large quantities of 3D face scans is labor intensive and impractical.</p><p>We therefore instead leverage three key observations.</p><p>1. As discussed in Sec. 2, accurate 3D estimates can be obtained by using multiple images of the same face.</p><p>2. Unlike the limited availability of ground truth 3D face shapes, there is certainly no shortage of challenging face sets containing multiple photos per subject.</p><p>3. Highly effective deep networks are available for the related task of extracting robust and discriminative face representations for face recognition.</p><p>From (1), we have a reasonable way of producing 3D face shape estimates for training, as surrogates for ground truth shapes: by using a robust method for multi-view 3DMM estimation. Getting multiple photos for enough subjects is very easy <ref type="bibr" target="#b1">(2)</ref>. This abundance of examples further allows balancing any reconstruction errors with potentially limitless subjects to train on. Finally, (3), a state of the art CNN for face recognition may be fine-tuned to this problem. It should already be tuned for unconstrained facial appearance variations and trained to produce similar, discriminative outputs for different images of the same face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generating training data</head><p>To generate training data, we use a simple yet effective multi image 3DMM estimation method, loosely based on the one recently proposed by <ref type="bibr" target="#b28">[28]</ref>. We run it on the unconstrained faces in the CASIA WebFace dataset <ref type="bibr" target="#b44">[44]</ref>. These multi image 3DMM estimates are then used as ground truth 3D face shapes when training our CNN 3DMM regressor.</p><p>Multi image 3DMM reconstruction is performed by first estimating 3DMM parameters from the 500k single images in CASIA. 3DMM estimates for images of the same subject are then aggregated into a single 3DMM per subject (∼10k subjects). This process is described next (see also, <ref type="figure" target="#fig_1">Fig. 2</ref>). The 3DMM representation. Our system uses the popular Basel Face Model (BFM) <ref type="bibr" target="#b26">[26]</ref>. It is a publicly available 3DMM representation and one of the state of the art methods for single view 3D face modeling.</p><p>A face is modeled by decoupling its shape and texture giving the following two independent generative models.</p><formula xml:id="formula_0">S = s + W S α , T = t + W T β.<label>(1)</label></formula><p>Here, the vectors s and t are the mean face shape and texture, computed over the aligned facial 3D scans in the Basel Faces collection and represented by the concatenated 3D coordinates of the 3D point clouds and the concatenated RGB values of their textures. Matrices W S and W T are the principle components, computed from the same aligned facial scans. Finally, α and β are each 99D parameter vectors, representing shape and texture respectively. Single image 3DMM fitting. Fitting a 3DMM to each training image is performed with a slightly modified version of the two standard methods of <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b31">[31]</ref>. Given an image I, we estimate parameter vectors α and β which represent a face similar to the one in I (Eq. (1)). Unlike previous work, we begin processing by applying the CLNF <ref type="bibr" target="#b21">[21]</ref> state of the art facial landmark detector. It provides K = 68 facial landmarks p k ∈ R 2 , k ∈ 1..K, and a confidence score value w (which we use later on).</p><p>Landmarks are used to obtain an initial estimate for the pose of the input face, in the reference 3DMM coordinate system. Pose is represented by six degrees of freedom for rotation, r = [r α , r β , r γ ], and translation, t = [t X , t Y , t Z ], and estimated similar to <ref type="bibr" target="#b11">[12]</ref>. 3DMM fitting then proceeds by optimizing over the shape, texture, pose, illumination, and color model following <ref type="bibr" target="#b7">[8]</ref>. We found that CLNF makes occasional localization errors. To introduce more stability, our optimization also uses the edge-based cost of <ref type="bibr" target="#b31">[31]</ref>. For more details on this optimization, we refer to <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b31">[31]</ref>.</p><p>Once the optimization converges, we take the shape and texture parameters, α and β , from the last iteration as our single image 3DMM estimate for the input image I. Importantly, though this process is known to be computationally expensive, it is applied in our pipeline only in preprocessing and once for every training image. We later show our CNN regressor to be much faster. Multi image 3DMM fitting. Although a number of multi image 3D face shape estimation methods were proposed in the past, we found the following simple approach, inspired by the very recent work of <ref type="bibr" target="#b28">[28]</ref>, to be particularly effective.</p><p>Specifically, we pool the shape and texture 3DMM pa-</p><formula xml:id="formula_1">rameters γ i = [α i , β i ], i ∈ 1.</formula><p>.N across all the N single view estimates belonging to the same subject. Pooling is performed by element wise weighted averaging of the N 3DMM vectors, resulting in a single 3DMM estimate for that subject, γ. That is,</p><formula xml:id="formula_2">γ = N i=1 w i · γ i and N i=1 w i = 1,<label>(2)</label></formula><p>where w i are normalized per-image confidences provided by the CLNF facial landmark detector. Note that unlike <ref type="bibr" target="#b28">[28]</ref>, we do not use a rank-list based on distances of normals as a quality measure to pool 3DMM parameters, instead taking the landmark detection confidence measure for these weights. Following this process, each CASIA subject is associated with a single, pooled 3DMM parameter vector γ. For ease of notation, henceforth we will drop the hat when denoting pooled features, assuming all training set 3DMM parameters were pooled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning to regress pooled 3DMM</head><p>Following the process described in Sec. 3.1, each subject in our data set is associated with a number of images and a single, pooled 3DMM. We now use this data to learn a function which, ideally, regresses the same pooled 3DMM feature vector for different photos of the same subject.</p><p>To this end, we use a state of the art CNN, trained for face recognition. We use the very deep ResNet architecture <ref type="bibr" target="#b15">[15]</ref> with 101 layers, recently trained for face recognition by <ref type="bibr" target="#b24">[24]</ref>. We modify its last fully-connected layer to output the 198D 3DMM feature vector γ. The network is then fine-tuned on CASIA images using the pooled 3DMM estimates as target values; different images of the same subject presented to the CNN using the same target 3DMM shape. We note that we also tried using the VGG-Face CNN of <ref type="bibr" target="#b25">[25]</ref> with 16 layers. Its results were similar to those obtained by the ResNet architecture, though somewhat lower. The asymmetric Euclidean loss. Training our network requires some care when defining its loss function. 3DMM vectors, by construction, belong to a multivariate Gaussian distribution with its mean on the origin, representing the mean face (Sec. 3.1). Consequently, during training, using the standard Euclidean loss to minimize distances between estimated and target 3DMM vectors will favor estimates closer to the origin: these will have a higher probability of being closer to their target values than those further away. In practice, we found that a network trained with the Euclidean loss tends to output less detailed faces <ref type="figure" target="#fig_2">(Fig. 3)</ref>.</p><p>To counter this bias towards a mean face shape, we introduce an asymmetric Euclidean loss. It is designed to encourage the network to favor estimates further away from the origin by decoupling under-estimation errors (errors on the side of the 3DMM target closer to the origin) from overestimation errors (where the estimate is further out from the origin than the target). It is defined by:</p><formula xml:id="formula_3">L(γ p , γ) = λ 1 · ||γ + − γ max || 2 2 over-estimate +λ 2 · ||γ + p − γ max || 2 2 under-estimate ,<label>(3)</label></formula><p>using the element-wise operators:</p><formula xml:id="formula_4">γ + . = abs(γ) . = sign(γ) · γ; γ + p . = sign(γ) · γ p ,<label>(4)</label></formula><p>γ max . = max(γ + , γ + p ). (5) Here, γ is the target pooled 3DMM value, γ p is the output, regressed 3DMM and λ 1,2 control the trade-off between the over and under estimation errors. When both equal 1, this reduces to the traditional Euclidean loss. In practice, we set λ 1 = 1, λ 2 = 3, thus changing the behavior of the training process, allowing it to escape under-fitting faster and encouraging the network to produce more detailed, realistic 3D face models <ref type="figure" target="#fig_2">(Fig. 3)</ref>. Network hyperparameters. Eq. (3) is solved using Stochastic Gradient Descent (SGD) with a mini-batch of size 144, momentum set to 0.9 and with regularization over the weights provided by 2 with a weight decay of 0.0005. When performing back-propagation, we learn the inner product layer (fc) after pool5 faster, setting the learning rate to 0.01, since it is trained from scratch for the regression problem. Other network weights are updated with a learning rate an order of magnitude lower. When the validation loss saturates, we decrease learning rates by an order of magnitude, until the validation loss stops decreasing. Discussion: Render-free 3DMM estimator. It is important to note that by choosing to use a CNN to regress 3DMM parameters, we obtain a function that is render-free. That is, 3DMM parameters are regressed directly from the input image, without an optimization process which renders the face and compares it to the photo, as do existing methods for 3DMM estimation (including our method for generating training data in Sec. 3.1). By using a CNN, we therefore hope to gain not only improved accuracy, but also much faster 3DMM estimation speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Parameter based 3D-3D recognition</head><p>The CNN we train in Sec. 3.2 represents a function f : I → γ p , giving us 3DMM parameters γ p for an input image I. We later use our 3DMM estimates in face recognition benchmarks, to test how robust and discriminative they are. We next describe the method used for that purpose to evaluate the similarity of two face shapes and textures to determine if they represent the same subject. 3D-3D recognition with a single image. We perform face recognition using the 3DMM parameters regressed by our network: By using the 3DMM parameters γ p as face descriptors. Because different benchmarks often exhibit specific appearance biases, we apply Principal Component Analysis (PCA), learned from the training splits of the test benchmark, to adapt our estimated parameter vectors to the benchmark. Signed, element wise square rooting of these vectors is then used to further improve representation power <ref type="bibr" target="#b27">[27]</ref>. Finally, the similarity of two faces, s(γ p1 , γ p2 ), is evaluated by computing their cosine score:</p><formula xml:id="formula_5">s(γ 1 , γ 2 ) = γ p1 · γ T p2 ||γ p1 || · ||γ p2 || .<label>(6)</label></formula><p>3D-3D recognition with multiple-images. In some scenarios, a subject is represented by a set of images, rather than just one. This is the case in the YTF benchmark <ref type="bibr" target="#b40">[40]</ref> where videos are used, each containing multiple frames, and in the recent IJB-A <ref type="bibr" target="#b22">[22]</ref>, which uses templates containing heterogeneous visual data (images, videos and possibly more). We use the same pipeline for single images also for image sets. Here, however, 3DMM parameters for different images or frames are first pooled using Eq. (2). Unlike the process applied in Sec. 3.1, all images here have equal weights, as we do not run landmark detection prior to 3DMM fitting with our CNN (see below). When using templates with both videos and images, following <ref type="bibr" target="#b24">[24]</ref>, we first pool the 3DMM estimates for frames in each video separately, obtaining one 3DMM per video. We then pool these 3DMMs with those of other images in the same template. Face alignment. Facial landmark detection and face alignment are known to improve recognition accuracy (e.g., <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b14">14]</ref>). In fact, the recent, related work of <ref type="bibr" target="#b16">[16]</ref> manually assigned landmarks before using their 3DMM fitting method for recognition on controlled images. We, however, did not align faces beyond using the bounding boxes provided in their data sets. We found our method robust to misalignments and so spared the runtime this required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>We test our proposed method, comparing the accuracy of its estimated 3D shapes, its speed and its ability to represent faces for recognition with existing methods. Importantly, we are unaware of any previous work on single view 3D face shape estimation which reported as many quantitative tests as we do, in terms of the number of benchmarks used, the number of baseline methods compared with and the level of difficulty of the photos used in these tests. Specifically, we evaluate the accuracy of our estimated 3D shapes using videos and photos and their corresponding scanned, ground truth 3D shapes from the MICC Florence Faces dataset <ref type="bibr" target="#b0">[1]</ref> (Sec. 4.1). To test how discriminative and robust our shapes are when estimated from unconstrained images, we perform single image and multi image face recognition using the LFW <ref type="bibr" target="#b17">[17]</ref>, YTF <ref type="bibr" target="#b40">[40]</ref> and the new IARPA JANUS Benchmark-A (IJB-A) <ref type="bibr" target="#b22">[22]</ref> (Sec. 4.3). Finally we also provide qualitative results in Sec. 4.4.</p><p>As baseline 3D reconstruction methods we used standard 3DMM fitting <ref type="bibr" target="#b31">[31]</ref>, which we implemented ourselves, the flow-based method of <ref type="bibr" target="#b11">[12]</ref>, the edge based method of <ref type="bibr" target="#b1">[2]</ref>, the multi resolution, multi-view approach of <ref type="bibr" target="#b18">[18]</ref> and the recent 3DDFA <ref type="bibr" target="#b45">[45]</ref>, were all tested with their authors' implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">3D shape reconstruction accuracy</head><p>The MICC dataset <ref type="bibr" target="#b0">[1]</ref> contains challenging face videos of 53 subjects. The videos span the range of controlled to challenging unconstrained outdoor settings. For each of the subjects in these videos, the data set contains also a groundtruth 3D model acquired using a structured-light scanning system with high precision. This allows comparing our 3D face shape estimates with the ground truth shapes.</p><p>These videos were used for single image and multi frame 3D reconstructions, comparing our method to existing alternatives. In these tests, estimated and ground truth shape parameters were converted to 3D using Eq. (1), cropped at a radius of 95mm around the tip of the nose and globally aligned using the standard, rigid iterative closest point (ICP) method <ref type="bibr" target="#b2">[3]</ref>, obtaining X, X * ⊆ R 3 , respectively. They were additionally projected to a frontal view, obtaining depth maps D Q and D * Q . Estimation accuracy was then   computed with standard error measures <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">35]</ref>:</p><formula xml:id="formula_6">• 3D Root Mean Square Error (3DRMSE): i (X − X * ) 2 /N v • Root Mean Square Error (RMSE): i (D Qi − D * Qi ) 2 /N p • log 10 : | log 10 (D Q ) − log 10 (D * Q )| • Relative error (Rel): |D Q − D * Q |/|D * Q |</formula><p>Here, N v is the number of 3D vertices and N p the number of pixels in these representations. Single view estimation was performed on the most frontal frame. Multi frame reconstructions were given the entire videos. Our multi frame results were produced by pooling 3DMM estimates from different frames, using Eq. (2), with equal weights used for all frames. For all 3DMM fitting baselines <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b45">45]</ref>, we found that estimating shape, texture and expression parameters but using only shape and texture for comparisons, gave the best results. This approach was therefore used in all our tests.  Results are reported in Tab. 1. Error rates are averaged across all videos and provided ± standard deviation. Our method is clearly the most accurate. Remarkably, both its single view and multiple frame versions outperform the method used to produce the training set target 3DMM labels (3DMM+pool). This may be due to our use of such a large dataset to train the CNN and their known robustness to training label errors and noise <ref type="bibr" target="#b42">[42]</ref>.</p><p>Our estimates are more accurate than the very recent state-of-the-art. This includes 3DDFA <ref type="bibr" target="#b45">[45]</ref> which fits 3DMM parameters by using a CNN to deal with large pose variations as well as <ref type="bibr" target="#b18">[18]</ref> and <ref type="bibr" target="#b1">[2]</ref>. To better appreciate these numbers, note that our improvement over standard 3DMM fitting is comparable to their improvement over using a unmodified, generic Basel face shape <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3DMM regression speed</head><p>Tab. 1 (rightmost column) also reports the average, per image runtime in seconds, required by the various methods to predict 3D face shapes. We compared our approach with iterative methods such as classic 3DMM implementations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b31">31]</ref>, the flow-based method of <ref type="bibr" target="#b11">[12]</ref> and also with a recent CNN based method <ref type="bibr" target="#b45">[45]</ref>.</p><p>As mentioned earlier, our method is render-free, without optimization loops which render the estimated parameters and compare them to the input photo. Unsurprisingly, at 0.088s (∼11Hz), our CNN is several orders of magnitude faster predicting 3DMM parameters than most of the methods we tested. The second fastest method, by a wide gap, is the 3DDFA of <ref type="bibr" target="#b45">[45]</ref>, requiring 0.146s (∼7Hz) for prediction. Runtime was measured on two different systems. All our baselines required MS-Windows to run and were tested on an Intel Core i7-4820K CPU @ 3.7GHz with 16GB RAM and a NVIDIA GeForce GTX 770. Our method requires Linux and so was tested on an Intel Xeon CPU @ 3.60GHz, with 12 GB of RAM and GeForce GTX 590. Importantly, the system used to measure our runtime is the slower of the two. Our runtimes may therefore be exaggerated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face recognition in the wild</head><p>We next consider the robustness of our 3DMM estimates and how discriminative they are. We aim to see if our 3DMM estimates for different unconstrained photos of the same person are more similar to each other than to those of other subjects. An effective way of doing this is by testing our 3DMM estimates on face recognition benchmarks. We emphasize that our goal is not to set new face recognition records. Doing so would require competing with state of the art systems designed exclusively for that problem. We provide performances of relevant (though not necessarily state of the art) recognition systems only as a reference. Nevertheless, our results below are the highest we know of that were obtained with meaningful features (here, shape and texture parameters) rather than opaque representations.</p><p>Our tests use the pipeline described in Sec. 3.3 and report multiple recognition metrics for verification (in LFW and YTF) and identification metrics (in IJB-A). These metrics are verification accuracy, 100%-EER (Equal Error Rate), Area Under the Curve (AUC), and recall (True Acceptance Rate) at two cut-off points of the False Alarm Rate (TAR-{10%,1%}). For identification we report the recognition rates at various ranks from the CMC (Cumulative Matching Characteristic). For each tested method we also indicate its use of estimated 3D shape and/or texture. Finally, bold values indicate best scoring 3D reconstruction methods.</p><p>Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b17">[17]</ref> results are provided in Tab. 2 (top) and <ref type="figure" target="#fig_3">Fig. 4 (left)</ref>. Evidently, the shapes estimated by 3DDFA <ref type="bibr" target="#b45">[45]</ref> are only slightly more robust and discriminative than the classical eigenfaces <ref type="bibr" target="#b39">[39]</ref>. Fitting 3DMMs using <ref type="bibr" target="#b31">[31]</ref> does better, but falls behind the Hybrid method of <ref type="bibr" target="#b41">[41]</ref>, one of the first results on LFW, now nearly a decade old. Both results suggest that the shapes estimated by these methods are unstable in unconstrained settings and/or are too generic. By comparison, recognition performances with our estimated 3DMM parameters is not far behind those recently reported by Facebook, using their multi-CNN approach trained on four million images <ref type="bibr" target="#b37">[37]</ref>.</p><p>YouTube Faces (YTF) <ref type="bibr" target="#b40">[40]</ref> Accuracy on YTF videos is reported in Tab. 2 (bottom) and <ref type="figure" target="#fig_3">Fig. 4 (mid-left)</ref>. Though video frames in this set are often low in quality and resolution, our method performs well. It is outperformed by the Facebook CNN ensemble system <ref type="bibr" target="#b37">[37]</ref>, explicitly designed for face recognition, by an AUC gap of only ∼1%. The 3DMM shapes and textures estimated by other methods perform far worst, with <ref type="bibr" target="#b31">[31]</ref> doing only slightly better than the MBGS face recognition system <ref type="bibr" target="#b40">[40]</ref>, which is the oldest result on that benchmark and <ref type="bibr" target="#b45">[45]</ref> falling far behind.</p><p>IARPA Janus Benchmark A. (IJB-A) <ref type="bibr" target="#b22">[22]</ref> Released recently, IJB-A was designed to offer elevated challenges compared to other face recognition benchmarks. In particular, it presents faces in near profile poses, almost nonex- <ref type="figure">Figure 5</ref>: Qualitative comparison of surface errors, visualized as heat maps with real world mm errors on faces from MICC videos and their ground truth 3D shapes. Left to right, top to bottom: frame from input and 3D groundtruth shape; the generic face; estimates for flow-based method <ref type="bibr" target="#b11">[12]</ref>, Huber et al. <ref type="bibr" target="#b18">[18]</ref>, 3DDFA <ref type="bibr" target="#b45">[45]</ref>, Bas et al. <ref type="bibr" target="#b1">[2]</ref>, 3DMM +pool <ref type="bibr" target="#b31">[31]</ref>, our method +pool. istent in previous face sets. It further contains faces in extremely low resolution and often strongly affected by noise.</p><p>We evaluated both the face verification (1:1) and recognition (1:N) protocols and report results in Tab. 3 and <ref type="figure" target="#fig_3">Fig. 4</ref> (mid-right, right). Here too, performances adopt the same pattern as in the previous two benchmarks, with 3D shapes estimated by 3DDFA <ref type="bibr" target="#b45">[45]</ref> performing far worst than other methods. Our own method performs quite well, though it is outperformed by a wide margin by the very recent face recognition system of <ref type="bibr" target="#b24">[24]</ref>, which was designed for this set. <ref type="figure">Fig. 5</ref> provides a qualitative comparison of the surface errors in mm for different methods for a subject in the MICC dataset. Our method produces visually smaller errors compared to ground-truth. The areas around the nose and mouth in particular have very low errors, while other methods are more sensitive in these regions (e.g 3DDFA <ref type="bibr" target="#b45">[45]</ref>). We provide also qualitative 3D reconstructions of faces in the wild, using images from LFW and single frames from YTF videos. <ref type="figure">Fig. 6</ref> presents these results showing both rendered 3D shapes and (when available) also its estimated texture. These results show that our method generates more visually plausible 3D and texture estimates compared with those produced by other methods. <ref type="figure">Fig. 5</ref> also shows a few failure cases, here due to facial hair which was missing from the original 3DMM representation and extreme out-ofplane rotation which produced a thin, unrealistic 3D shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We show that existing methods for estimating 3D face shapes may either be sensitive to changing viewing conditions, particularly in unconstrained settings, or too generic. Their estimated shapes therefore do not capture identity very well, despite the fact that true 3D face shapes are known to be highly discriminative.</p><p>We propose instead to use a very deep CNN architecture <ref type="figure">Figure 6</ref>: Qualitative results, produced by 3DMM <ref type="bibr" target="#b31">[31]</ref>, 3DDFA <ref type="bibr" target="#b45">[45]</ref> and our method on still-images from LFW and single frames from YTF. Bottom: Two failure examples.</p><p>to regress 3DMM parameters directly from input images. We provide a solution to the problem of obtaining sufficient labeled data to train this network. We show our regressed 3D shapes to be more accurate than those of alternative methods. We further run extensive face recognition tests showing these shapes to be robust to unconstrained viewing conditions and discriminative. Our results are furthermore the highest recognition results we know of, obtained with interpretable representations rather than opaque features. We leave it to future work to regress more 3DMM parameters (e.g., expressions) and design state of the art recognition systems using these shapes instead of the abstract features used by others.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Unconstrained, single view, 3D face shape reconstruction. (a) Input images of the same subject with disruptive poses and occlusions. (b-e) 3D reconstructions using (b) single-view 3DMM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our process. (a) Large quantities of unconstrained photos are used to fit a single 3DMM for each subject. (b) This is done by first fitting single image 3DMM shape and texture parameters to each image separately. Then, all 3DMM estimates for the same subject are pooled together for a single estimate per subject. (c) These pooled estimates are used in place of expensive ground truth face scans to train a very deep CNN to regress 3DMM parameters directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Effect of our loss function: (left) Input image, (a) generic model, (b) regressed shape and texture with a regular 2 loss and (c) our proposed asymmetric 2 loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Face verification and recognition results. From left to right: Verification ROC curves for LFW, YTF, and IJB-A, and the recognition CMC for IJB-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Method 3DRMSE RMSE log 10 ×10 4 Rel×10 4 Sec. 3D estimation accuracy and per-image speed on the MICC dataset. Top are single view methods, bottom are multi frame. See text for details on measures. 3DRMSE in real-world mm; log 10 and Rel were both scaled to preserve space.</figDesc><table><row><cell>Generic</cell><cell>1.88±.52 3.48±.76 28±7</cell><cell>65±16 -</cell></row><row><cell>3DMM [31]</cell><cell>1.75±.42 3.64±.94 29±8</cell><cell>68±18 120</cell></row><row><cell>Flow-based [12]</cell><cell>1.83±.39 3.29±.70 27±6</cell><cell>62±14 13.3</cell></row><row><cell>Us</cell><cell>1.57±.33 3.18±.77 26±6</cell><cell>59±14 .088</cell></row><row><cell>Generic+pool</cell><cell>1.88±.52 3.48±.76 28±7</cell><cell>65±16 -</cell></row><row><cell cols="2">3DMM [31]+pool  *  1.60±.46 3.31±.98 27±9</cell><cell>62±20 120</cell></row><row><cell cols="2">3DDFA [45]+pool 1.83±.58 3.45±.85 28±7</cell><cell>65±17 .146</cell></row><row><cell>[18]</cell><cell>1.84±.32 3.73±.62 30±5</cell><cell>68±11 .372</cell></row><row><cell>[2]+pool</cell><cell>1.84±.58 3.45±.85 28±6</cell><cell>65±13 52.3</cell></row><row><cell>Us +pool</cell><cell>1.53±.29 3.14±.70 25±6</cell><cell>58±13 .088</cell></row></table><note>* Denotes the method used to produce the training data in Sec. 3.1. Lower values are better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>13±2.79 65.70±2.81 72.24±2.75 35.90±3.74 12.37±4.81 74.93±1.14 74.50±1.21 82.94±1.14 60.40±3.15 28.73±7.17 75.25±2.12 74.73±2.56 83.21±1.93 59.4±4.64 29.67±4.73 3DDFA [45] 66.98±2.56 67.13±1.90 73.30±2.49 36.76±6.27 10.00±3.22 Us 90.53±1.34 90.63±1.61 96.6±0.79 91.13±2.62 58.20±12.14 90.6±1.07 90.70±1.17 96.75±0.59 91.23±2.42 52.60±8.14 92.35±1.29 92.33±1.33 97.71±0.64 94.28±1.84 88.32±2.16 95.95±1.38 86.60±3.95 51.12±8.86 87.56±2.56 87.68±2.25 94.44±1.38 84.80±4.89 40.92±8.26 88.80±2.21 88.84±2.40 95.37±1.43 87.92±4.18 46.56±6.20</figDesc><table><row><cell>Method</cell><cell>3D Texture</cell><cell>Accuracy</cell><cell>100%-EER</cell><cell>AUC</cell><cell>TAR-10%</cell><cell>TAR-1%</cell></row><row><cell></cell><cell cols="3">Labeled Faces in the Wild</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EigenFaces [39]</cell><cell>-</cell><cell>60.02±0.79</cell><cell>-</cell><cell>-</cell><cell>25</cell><cell>6.2</cell></row><row><cell>Hybrid Descriptor [41]</cell><cell>-</cell><cell>78.47±0.51</cell><cell>-</cell><cell>-</cell><cell>66.60</cell><cell>42.4</cell></row><row><cell>DeepFace-ensemble [37]</cell><cell>-</cell><cell>97.35±0.25</cell><cell>-</cell><cell>-</cell><cell>99.6</cell><cell>93.7</cell></row><row><cell>AugNet [24]</cell><cell>-</cell><cell cols="2">98.06±0.60 98.00±0.73</cell><cell>-</cell><cell>99.5</cell><cell>94.2</cell></row><row><cell>3DMM [31]</cell><cell></cell><cell cols="4">66.2±2.00</cell><cell>65.57±6.93</cell></row><row><cell></cell><cell></cell><cell>YouTube Faces</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MBGS LBP [40]</cell><cell>-</cell><cell>76.4±1.8</cell><cell>74.7</cell><cell>82.6</cell><cell>60.5</cell><cell>35.8</cell></row><row><cell>DeepFace-ensemble [37]</cell><cell>-</cell><cell>91.4±1.1</cell><cell>91.4</cell><cell>96.3</cell><cell>92</cell><cell>54</cell></row><row><cell></cell><cell></cell><cell cols="5">73.26±2.51 73.08±2.65 80.41±2.60 51.36±5.11 24.04±4.56</cell></row><row><cell>3DMM [31]+pool  *</cell><cell></cell><cell cols="5">77.34±2.54 76.96±2.64 85.32±2.63 63.16±5.07 31.36±5.21</cell></row><row><cell></cell><cell></cell><cell cols="5">79.56±2.08 79.20±2.07 87.35±1.92 69.08±5.00 34.56±6.89</cell></row><row><cell>3DDFA [45]+pool</cell><cell></cell><cell cols="4">68.10±2.93 67.96±3.12 74.95±3.04 40.52±3.65</cell><cell>12.2±2.67</cell></row><row><cell></cell><cell></cell><cell>88.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Us +pool</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>LFW and YTF face verification. Comparing our 3DMM regression with others, including baseline face recognition methods.</figDesc><table><row><cell></cell><cell></cell><cell>LFW</cell><cell></cell><cell></cell><cell>YTF</cell><cell></cell><cell></cell><cell>IJB−A Verification</cell><cell></cell><cell></cell><cell></cell><cell cols="2">IJB−A Identification</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Acceptance Rate</cell><cell>0 0 0.2 0.4 0.6 0.8</cell><cell>0.5 False Acceptance Rate EigenFaces Hybrid Descriptor DeepFace−ensemble AugNet 3DMM (Shape) 3DMM (Texture) 3DMM (Shape+Texture) 1 3DDFA (Shape) Us (Shape) Us (Texture) Us (Shape+Texture)</cell><cell>True Acceptance Rate</cell><cell>0 0 0.2 0.4 0.6 0.8</cell><cell>0.5 False Acceptance Rate MBGS LBP DeepFace−ensemble 3DMM (Shape) 3DMM (Texture) 3DMM (Shape+Texture) 1 3DDFA Us (Shape) Us (Texture) Us (Shape+Texture)</cell><cell>True Acceptance Rate</cell><cell>0 0 0.2 0.4 0.6 0.8</cell><cell>0.5 False Acceptance Rate AugNet 3DMM (Shape) 3DMM (Texture) 3DMM (Shape+Texture) 1 3DDFA Us (Shape) Us (Texture) Us (Shape+Texture)</cell><cell>Recognition Rate</cell><cell>40 60 80 20 0</cell><cell>2</cell><cell>4</cell><cell>Rank AugNet 3DMM (Shape) 3DMM (Texture) 3DMM (Shape+Texture) 3DDFA Us (Shape) Us (Texture) 6 8 10 Us (Shape+Texture)</cell></row></table><note>* Denotes the same method used to produce 3DMM target values for our CNN training (Sec. 3.1).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>6±1.6 90.6±1.2 96.2±0.6 97.7±0.4 3DMM * +p. 60.7±2.0 30.6±3.2 34.3±2.2 55.1±2.1 65.1±2.0 71.1±1.8 39.5±4.8 49.8±2.5 69.5±1.4 76.8±1.0 75.4±1.6 46.6±5.1 57.2±1.9 74.4±1.3 80.5±1.1 3DDFA+p. 43.3±2.5 12.5±1.9 16.7±1.9 38.3±2.7 51.3±3.0</figDesc><table><row><cell>Method</cell><cell>3DText.TAR-10%TAR-1% Rank-1 Rank-5 Rank-10</cell></row><row><cell cols="2">AugNet 88.Us +pool --86.0±1.7 55.9±5.5 72.3±1.4 88.0±1.4 91.8±1.1 83.5±2.2 50.3±5.8 70.9±1.5 87.3±1.1 91.5±1.0</cell></row><row><cell></cell><cell>87.0±1.5 60.0±5.6 76.2±1.8 89.7±1.0 92.9±1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>IJB-A face verification and recognition. Comparing our 3DMM regression with others, including baseline face recognition methods. * Denotes the same method used to produce 3DMM target values for our CNN training.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA 2014-14071600011. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The florence 2D/3D hybrid face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<ptr target="Available:www.micc.unifi.it/masi/research/ffd.2,5" />
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conf. Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fitting a 3D morphable model to edges: A comparison between hard and soft correspondences. arxiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
		<idno>abs/1602.01125</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A method for registration of 3-D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face identification across different poses and illuminations with a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="192" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exchanging faces in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH Conf. Comput. Graphics</title>
		<meeting>ACM SIGGRAPH Conf. Comput. Graphics</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Threedimensional face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="30" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D-aided face recognition robust to expression and pose variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Statistical symmetric shape from shading for 3D structure recovery of faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dovgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>European Conf. Comput. Vision</publisher>
			<biblScope unit="page" from="99" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Viewing real-world faces in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<biblScope unit="page" from="3607" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<ptr target="Available:www.openu.ac.il/home/hassner/projects/poses.1" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Example based 3D reconstruction from single 2D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face recognition using a unified 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
			<pubPlace>UMass, Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multiresolution 3D morphable face model and fitting framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mortazavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rtsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via cnnbased dense 3D model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D face reconstruction from a single image using a single reference face shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="405" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Holistically constrained local model: Going beyond frontal poses for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltruaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf</title>
		<meeting>British Mach. Vision Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus Benchmark-A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Head reconstruction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="360" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do We Really Need to Collect Millions of Faces for Effective Face Recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Available www.openu.ac.il/home/hassner/ projects/augmented_faces. 1, 2, 3, 5, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf</title>
		<meeting>British Mach. Vision Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vetter. A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Advanced Video and Signal based Surveillance</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated 3D face reconstruction from multiple images using quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piotraschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient, robust and accurate fitting of a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating 3D shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unconstrained 3D face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive 3D face reconstruction from unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-time facial segmentation and performance capture from RGB input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Make3D: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">824840</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Total moving face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="796" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time conversion from a single 2d face image to a 3D text-driven emotive audio-visual avatar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1205" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Descriptor based methods in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Life Images workshop at the European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distur-bLabel: Regularizing CNN on the loss layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Expression flow for 3D-aware face component transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<ptr target="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html.3" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
