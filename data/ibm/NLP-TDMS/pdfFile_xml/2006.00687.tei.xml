<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phase-aware Single-stage Speech Denoising and Dereverberation with U-Net</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeong-Seok</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music and Audio Research Group (MARG)</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Supertone Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoon</forename><surname>Heo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Supertone Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><forename type="middle">Hwan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music and Audio Research Group (MARG)</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Supertone Inc</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyogu</forename><surname>Lee</surname></persName>
							<email>kglee@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Music and Audio Research Group (MARG)</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Supertone Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Phase-aware Single-stage Speech Denoising and Dereverberation with U-Net</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech enhancement</term>
					<term>phase</term>
					<term>denoising</term>
					<term>derever- beration</term>
					<term>U-Net</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we tackle a denoising and dereverberation problem with a single-stage framework. Although denoising and dereverberation may be considered two separate challenging tasks, and thus, two modules are typically required for each task, we show that a single deep network can be shared to solve the two problems. To this end, we propose a new masking method called phase-aware β-sigmoid mask (PHM), which reuses the estimated magnitude values to estimate the clean phase by respecting the triangle inequality in the complex domain between three signal components such as mixture, source and the rest. Two PHMs are used to deal with direct and reverberant source, which allows controlling the proportion of reverberation in the enhanced speech at inference time. In addition, to improve the speech enhancement performance, we propose a new time-domain loss function and show a reasonable performance gain compared to MSE loss in the complex domain. Finally, to achieve a real-time inference, an optimization strategy for U-Net is proposed which significantly reduces the computational overhead up to 88.9% compared to the naïve version 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Corrupted speech signal by noise and reverberation is one of the most common signals we hear in our everyday life. The desire to listen to clean speech signal, therefore, is strong let alone its usage for machine such as speech recognition system. While there has been numerous studies to address the problem of single channel denoising and dereverberation, only few have tried to solve this problem with a single deep learning model <ref type="bibr" target="#b0">[1]</ref>. This motivates us to tackle this real-world problem using a single-stage deep learning model. We address this problem by dissecting the elements that compose the noisy-reverberant mixture, that is, noise, direct source, and reverberation. By dissecting each part of the mixture, one could handle each element at hand and even mix them with a desired proportion. Note that this is a desirable property for users as the effective amount of reverberation is important to achieve better speech intelligibility for both impaired and nonimpaired listeners <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>.</p><p>The key contributions of our proposed approach are three folds. First, to suppress the noise and reverberation, we propose a new type of complex-valued mask called phase-aware β-sigmoid mask (PHM). While the complex-valued mask suggested by <ref type="bibr" target="#b4">[4]</ref> separately estimates the real and imaginary part of a complex spectrogram, we believe the phase part of it can be effectively estimated by reusing an estimated magnitude value of it in a trigonometric perspective as suggested in <ref type="bibr" target="#b5">[5]</ref>. The major difference between PHM and the suggested approach in <ref type="bibr" target="#b5">[5]</ref> 1 Audio samples link: https://tinyurl.com/ycndlmfm is that PHM is designed to respect the triangular relationship between mixture, source and the rest, and hence the sum of the estimated source and the rest is always equal to the mixture. By exploiting this property, we train the deep network to output two different PHMs simultaneously to effectively deal with both denoising and derverbration problem. Second, we propose a new time-domain loss function, an emphasized multi-scale cos similarity loss function. A time-domain loss function has recently been used as a popular loss function <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10]</ref>. To better design the time-domain cos similarity loss function proposed in <ref type="bibr" target="#b6">[6]</ref>, we change it into a multi-scale version of it with proper emphasis functions and show the effectiveness of it. Finally, we suggest an optimization strategy for two-dimensional U-Net to significantly reduce the computational inefficiency in runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Recently, there has been an increasing interests in phase-aware speech enhancement because of the sub-optimality of reusing the phase of mixture signal. The first work that tried to address this problem was by using phase-sensitive mask (PSM) <ref type="bibr" target="#b11">[11]</ref>. PSM estimates the real-part of the signal which is still suboptimal. As a more direct remedy for this, a complex masking <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b5">5]</ref> or complex spectral mapping <ref type="bibr" target="#b13">[12]</ref> has also been proposed to estimate a clean phase part. Another line of research is to sequentially estimate the clean phase part using an additional sub-module <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b16">15]</ref>. This, however, is limited in that it requires an additional module resulting in inefficient computation. While most of these works tried to estimate the clean phase by using phase mask or an additional network, the absolute phase difference between mixture and source can be actually computed using the law of cosines using the estimated magnitude values as the three sides of a triangle <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b18">17]</ref>. Inspired by this, <ref type="bibr" target="#b19">[18]</ref> proposed to estimate a rotational direction of the absolute phase difference using a sign-prediction network.</p><p>The efforts to deal with denoising or dereverberation using deep networks have been tried in many works. Recently, <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b21">20]</ref> tried to address this problem with two modules for each task. We believe, however, a two-stage framework is not necessary and can be achieved using a single deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Single-stage Denoising and Dereverberation</head><p>A noisy-reverberant mixture signal x is commonly modeled as the sum of additive noise y (n) and reverberant sourceỹ, whereỹ is a result of convolution between room impulse response (RIR) h and dry source y as follows, x =ỹ + y (n) = h y + y (n) . More concretely, we can break down h into two parts, that is, direct path part h (d) which does not includes the reflection path and the rest of the part h (r) that includes all the reflection paths as follows, x = (h (d) + h (r) ) y + y (n) = h (d) y + h (r) y + y (n) = y (d) + y (r) + y (n) , where y (d) and y (r) denotes direct path source and reverberation, respectively. In this setting, our goal is to separate x into three elements y (d) , y (r) , and y (n) . Each of the corresponding timefrequency (t, f ) representations computed by STFT is denoted</p><formula xml:id="formula_0">as X t,f ∈ C, Y (d) t,f ∈ C, Y (r) t,f ∈ C, Y (n)</formula><p>t,f ∈ C, and the estimated values will be denoted by the hat operator· .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Phase-aware β-sigmoid mask</head><p>Designing a mask that is not limited to output the optimal value of ideal mask requires two conditions to satisfy. First, the range of magnitude mask should not be limited. Second, the mask has to be complex-valued so that it can correct both the magnitude part and phase part of the mixture signal. The proposed phase-aware β-sigmoid mask (PHM) is designed to handle both conditions while systemically restricting the sum of estimated complex values to be exactly the value of mixture,</p><formula xml:id="formula_1">X t,f = Y (k) t,f + Y (¬k)</formula><p>t,f . The PHM separates the mixture X t,f in STFT domain into two parts as one-vs-rest approach, that is, the signal Y </p><formula xml:id="formula_2">(¬k) t,f = X t,f − Y (k)</formula><p>t,f , where index k could be one of direct path source (d), reverberation (r), and noise (n) in our setting, k ∈ {d, r, n}. The complex-valued mask M (k) t,f ∈ C estimates the magnitude and phase value of the source of interest k. The mask is composed of two parts, (1) magnitude mask estimation, (2) phase estimation by reusing the magnitude estimation from (1) and two-class sign prediction.</p><p>First, the network outputs the magnitude part of two masks M</p><formula xml:id="formula_3">(k) t,f and M (¬k) t,f with sigmoid function σ (k) (z t,f ) multi- plied by coefficient β t,f as follows, M (k) t,f = β t,f · σ (k) (z t,f ) = β t,f · 1 1 + e −(z (k) t,f −z (¬k) t,f ) (1) where z (k)</formula><p>t,f is the output located at (t, f ) from the last layer of neural-network function ψ (k) (φ), and φ is a function composed of network layers before the last layer. M (k) t,f serves as a magnitude mask to estimate source k and the value of it ranges from 0 to β t,f . The role of β t,f is to design a mask that is close to the optimal mask with a flexible magnitude range so that the value is not bounded between 0 and 1 unlike the typically used sigmoid mask. In addition, because the sum of the complex valued masks M (k) t,f and M (¬k) t,f must compose a triangle, it is reasonable to design a mask that satisfies the triangle inequalities, that</p><formula xml:id="formula_4">is, M (k) t,f + M (¬k) t,f ≥ 1 and M (k) t,f − M (¬k) t,f ≤ 1.</formula><p>To address the first inequality we designed the network to output β t,f from the last layer with a softplus activation function as follows, β t,f = 1 + softplus((ψ β (φ)) t,f ), where ψ β denotes an additional network layer to output β t,f . The second inequality can be satisfied by clipping the upper bound of the β t,f by</p><formula xml:id="formula_5">1 / σ (k) (z t,f ) − σ (¬k) (z t,f ) .</formula><p>Once the magnitude masks are decided, we can construct a phase mask e jθ (k) t,f . Given the magnitudes as three sides of a triangle, we can compute the cosine of absolute phase difference ∆θ (k) t,f between the mixture and source k as follows, cos(∆θ</p><formula xml:id="formula_6">(k) t,f ) = (1+ M (k) t,f 2 − M (¬k) t,f 2 ) /(2 M (k)</formula><p>t,f ). Next, the rotational direction (clockwise or counterclockwise) for phase correction can be decided by estimating sign value ξ t,f ∈ {−1, 1} as follows,</p><formula xml:id="formula_7">e jθ (k) t,f = cos(∆θ (k) t,f ) + jξ t,f sin(∆θ (k) t,f ).<label>(2)</label></formula><p>Two-class straight-through Gumbel-softmax estimator was used to estimate ξ t,f <ref type="bibr" target="#b22">[21]</ref>. It allows us to discretize the output of the Gumbel-softmax function γ (i) with arg max and still be able to train the network in an end-to-end manner using a continuous approximation in the backward pass. ξ t,f is defined as follows,</p><formula xml:id="formula_8">ξ t,f = −1, γ (0) (q t,f ) &gt; γ (1) (q t,f ) 1, otherwise<label>(3)</label></formula><p>where γ (i) (q t,f ) is defined as follows,</p><formula xml:id="formula_9">γ (i) (q t,f ) = e q (i) t,f i e q (i) t,f = e ((ψ i (φ)) t,f +g i )/τ i e ((ψ i (φ)) t,f +g i )/τ ,<label>(4)</label></formula><p>and g0 and g1 are samples from Gumbel(0, 1), ψi is an additional network layer to output logit value q </p><formula xml:id="formula_10">(i) t,f , and τ is a temper- ature parameter for Gumbel-softmax. Finally, M (k) t,f is defined as follows, M (k) t,f = M (k) t,f e jθ (k) t,f .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Masking from the perspective of quadrangle</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Emphasized multi-scale cosine similarity loss</head><p>Learning to maximize cosine similarity can be regarded as maximizing the signal-to-distortion ratio (SDR) <ref type="bibr" target="#b6">[6]</ref>. Cosine similarity loss C between estimated signalŷ (k) ∈ R N and ground truth signal y (k) ∈ R N is defined as follows,</p><formula xml:id="formula_11">C(y (k) ,ŷ (k) ) = − &lt; y (k) ,ŷ (k) &gt; y (k) ŷ (k) ,<label>(6)</label></formula><p>where N denotes the temporal dimensionality of a signal and k denotes the type of signal (k ∈ {d, r, n}). Consider a sliced signal y</p><formula xml:id="formula_12">(k) [ N M (i−1): N M i]</formula><p>, where i denotes the segment index and M denotes the number of segment. By slicing the signal and normalize it by its norm, each sliced segment is considered as an unit for computing C. Therefore, we hypothesize that it is important to choose a proper segment length unit N M when computing C. In our case, we used multiple settings of segment lengths gj = N M j as follows,</p><formula xml:id="formula_13">L(y (k) ,ŷ (k) ) = j 1 Mj M j i=1 C(y (k) [g j (i−1):g j i] ,ŷ (k) [g j (i−1):g j i] ),<label>(7)</label></formula><p>where Mj denotes the number of sliced segments. In our case the set of gj's was chosen as follows, gj ∈ {4064, 2032, 1016, 508}, assuming they moderately cover the range of duration of phonemes in speech.</p><p>To further improve the design of the loss function, we applied two simple techniques -1. pre-emphasis (π) and 2. µlaw encoding (µ) -on signals. As most of the speech signal components are concentrated in the lower frequency bands, we found that applying pre-emphasis on loss function can help penalize high frequency components. In addition, since the samples of speech signals are usually centered around zero, we found that it is helpful to use 16-bit µ-law encoding as it distributes samples more uniformly by the nature of continuous logarithmic transform. The proposed loss function L + is defined as follows,</p><formula xml:id="formula_14">L + (y (k) ,ŷ (k) ) = L(y (k) ,ŷ (k) ) + L(π(y (k) ), π(ŷ (k) )) + L(µ(π(y (k) )), µ(π(ŷ (k) )))<label>(8)</label></formula><p>Finally, we used the proposed loss function for every k and ¬k combinations as follows, Lfinal = k (L + (y (k) ,ŷ (k) ) + L + (y (¬k) ,ŷ (¬k) )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimization for Real-Time U-Net</head><p>To connect each encoder layer with its corresponding decoder, U-Net is often composed of convolutional layers with zero padding for dynamic input sizes. Without zero padding, the valid size of input and output is uniquely determined by the kernel sizes and strides. This obviously takes less computation and allows to keep only the essential part. In our real-time setting, the input spectrogram has 253 frequency bins and 65 frames by discarding the four lowest bins from the original spectrogram with a 512-point FFT, assuming that 16 kHz speech signals have no significant spectral component below 93.75 Hz. We followed the network architecture of the real-valued U-Net proposed in <ref type="bibr" target="#b6">[6]</ref> (model10 and model20 specifically) with the modification of the last layer to output PHM. All the batch normalizations were fused into convolution filters. In the encoder, the naïve implementation of U-Net repeatedly performs the same computation that has already been computed previously. This redundancy can be efficiently reduced by caching the pre-computed values using queues. Likewise, we utilized a similar concept with 2D convolution, but more than one queues are needed for the strided convolution in each layer. The number of required queues for depth d is derived by d l=1 s l , where s l denotes the temporal stride of the l-th encoder layer.   Most computation of the naïve U-Net is concentrated on a few decoder layers before the output. Fortunately, only a single frame of the output mask is needed for real-time inference. Although using the latest frame can achieve the shortest latency, it is better to preview a few milliseconds for performance. It is computationally less efficient to use a longer lookahead because more frames should be calculated in the previous decoder layer. Our real-time implementation previews 32ms which is shorter than allowed in the DNS challenge <ref type="bibr" target="#b23">[22]</ref>. The schematic details are shown in <ref type="figure" target="#fig_3">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>We used the DNS challenge dataset <ref type="bibr" target="#b23">[22]</ref> and internally collected dataset for training. The former is a large scale dataset where the speech samples were collected from Librivox <ref type="bibr" target="#b24">[23]</ref>, and noise samples from Audioset <ref type="bibr" target="#b25">[24]</ref> and Freesound <ref type="bibr" target="#b26">[25]</ref>. Note that we did not use the provided noisy speech data from the DNS dataset but used on-the-fly augmentation with the clean speech and noise in the two datasets during the training phase. Since our goal is to perform both denoising and dereverberation, we used pyroomacoustics <ref type="bibr" target="#b28">[26]</ref> to simulate an artificial reverberation with randomly sampled absorption, room size, location of source and microphone distance. We also trimmed random segments of 2 seconds from speech and noise data, and mixed them with uniformly distributed source-to-noise ratio (SNR) ranging from -10 dB to 30 dB.</p><p>For test, we used two datasets such as the synthesized testset in the DNS challenge (DNS) and WHAMR <ref type="bibr" target="#b21">[20]</ref>. The DNS synthesized testset provides noisy-reverberant mixtures and noisy mixtures without reverb. DNS was used only to test the denoising performance since it does not provide the direct source signal of synthesized mixture samples. Therefore, a reverberant source was given as ground truth when the model is tested on noisy-reverberant mixtures. Both the denoising and dereverberation performance were tested on the min subset of WHAMR dataset which contains 3,000 audio files. To test the denoising and dereverberation performances both simultaneously and separately, we tested our models on four scenarios: 1) nr2d: noisy-reverberant mixture to direct source 2) nr2r: noisy-reverberant mixture to reverberant source 3) n2d: noisy mixture to direct source 4) r2d: reverberant source to direct source. The corresponding four pair of test subsets, denoted in a following way (mixture, ground truth), were used as follows, 1. nr2d: (mix single reverb, s1 anechoic), 2. nr2r: (mix single reverb, s1 reverb), 3. n2d: (mix single anechoic, s1 anechoic), 4. r2d: (s1 reverb, s1 anechoic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation</head><p>Input features were used as a channel-wise concatenation of log-magnitude spectrogram, real and imaginary part of demodulated phase <ref type="bibr" target="#b14">[13]</ref>, group delay, and delta-phase <ref type="bibr" target="#b29">[27]</ref>. The window size of model20 was 1024 with 256 hop size and the window size of model10 was 512 with 128 hop size. All models were trained for 125k iterations with AdamW optimizer <ref type="bibr" target="#b30">[28]</ref>. The learning rate was set to 0.0004 and halved at 62.5k iteration. Every test was done with a non-causal inference using model20 except the experiments in subsection 5.5. To show the effect of loss functions we observed SI-SDR <ref type="bibr" target="#b10">[10]</ref> and PESQ <ref type="bibr" target="#b31">[29]</ref> while changing four different loss functions. Complex MSE (CMSE) and three different cosine similarity based loss functions -SingleScale, MultiScale, and Mul-tiScale+ -were compared each of which corresponds to Eq. 6, Eq. 7, and Eq. 8, respectively. The quantitative results in <ref type="table" target="#tab_1">Table 1</ref> show that the proposed multi-scale and emphasis functions are beneficial for both denoising and dereverberation tasks in most of the cases.  Here, we used the phase distance defined in <ref type="bibr" target="#b6">[6]</ref> to quantitatively measure the phase enhancement performance. Phase distance (P D) between spectrogram A and B is formulated as follows,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis on phase enhancement</head><formula xml:id="formula_16">P D(A, B) = t,f |A t,f | t ,f A t ,f ∠(A t,f , B t,f ),<label>(10)</label></formula><p>where ∠(A t,f , B t,f ) is the angle between A t,f and B t,f , ranging from 0°to 180°. We measured P D between ground truth Y and mixture X, and P D between ground truth Y and es-timationŶ , and checked how much P haseGain(%) was obtained. This was tested on all four scenarios of WHAMR testset and shown in <ref type="table" target="#tab_2">Table 2</ref>. We found that the network is able to give a reasonable P haseGain in tasks including dereverberation (nr2d, nr2r). However, P haseGain was marginal for only-denoising-tasks (nr2r, n2d). We conjecture that this is because the network is not able to estimate a precise magnitude value for noisy mixture and this issue is left for futurework. A visualization of enhanced phase group delay tested on a reverberant source is shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. <ref type="figure" target="#fig_4">Fig. 3 (b)</ref> shows the enhanced harmonic structure of phase group delay. Following the constraint for real-time model suggested by the DNS challenge, we measured the elapsed time to compute a single frame. model20 that took 40 ms to compute a frame will be denoted as non-real-time (NRT) model, and model10 that took 4.32 ms to compute a frame will be denoted as real-time (RT) model. To compare the two models and how causal inference affects the model performance, we compare four combinations in nr2d task. <ref type="table" target="#tab_3">Table 3</ref> shows that both non-causal inference and model size are significant factors for performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Computation of real-time U-Net</head><p>Finally, we report the Mean Opinion Score (MOS) results from the DNS challenge based on the online subjective evaluation framework ITU-T P.808 <ref type="bibr" target="#b32">[30]</ref>. For better perceptual quality, we linearly added the estimated direct source and reverberant source with a 15 dB ratio, and implemented a simple and zerodelay dynamic range compression to apply on it. Our causal-NRT and causal-RT model achieved a mean opinion score of 3.36 and 3.24, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We proposed a new mask and loss function to improve the performance of single-stage denoising and dereverberation. As the proposed PHM and loss function are orthogonal to the network structure, we believe that a better performance can be achieved using the variant of U-Net architectures such as <ref type="bibr" target="#b33">[31,</ref><ref type="bibr" target="#b35">32]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and the sum of the rest of the signals Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The illustration of masks on quadrangleAs we desire to extract both direct source and reverberant source, two pairs of PHMs are used for each of them. The first pair of masks separates direct source and the rest of the component, denoted as M(d) t,f and M (¬d) t,f . The second pair of masks separates noise and reverberant source component, denoted as M (n) t,f and M (¬n) t,f . Since PHM guarantees the mixture and separated components to construct a triangle in the complex STFT domain, the outcome of the separation can be seen from the perspective of quadrangle as in Fig 1. In this setting, as the three sides and two side angles are already determined by the two pairs of PHMs, the last fourth side of quadrangle, the reverberation componentŶ (r) t,f , is uniquely decided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>A graphical illustration of U-Net optimization for real-time inference. As a schematic view of 2D feature map, the number in the box indicates the relative index to the latest frame. LA and T denote the lookahead and the frame length respectively. The number of multiplications reduced from the naïve one is shown at the bottom of the box (in millions). The overall reduction reached 88.9%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Group delay of enhanced phase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The effect of proposed loss function. The denoising performance was tested on the DNS challenge synthesized testset (w/o and w/ reverb) and both denoising and dereverberation performance was tested on WHAMR dataset (nr2d: noisyreverberant mixture to direct source, r2d: reverberant source to direct source). Si-SDR 15.63 14.21 17.47 15.79 17.57 15.93 17.91 16.22 PESQ 2.22 2.59 2.57 2.90 2.63 2.97 2.71 3.01 Si-SDR 4.21 8.87 5.08 9.88 5.24 10.13 5.33 10.40 PESQ 1.38 2.58 1.45 2.96 1.54 3.09 1.52 3.16</figDesc><table><row><cell></cell><cell>DNS-challenge</cell></row><row><cell>Loss</cell><cell>CMSE SingleScale MultiScale MultiScale+</cell></row><row><cell cols="2">Reverb w/o w/ w/o w/ w/o w/ w/o w/</cell></row><row><cell></cell><cell>WHAMR</cell></row><row><cell>Loss</cell><cell>CMSE SingleScale MultiScale MultiScale+</cell></row><row><cell cols="2">Task nr2d r2d nr2d r2d nr2d r2d nr2d r2d</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Phase distance and gain under four different tasks.</figDesc><table><row><cell>Task</cell><cell>nr2r</cell><cell>n2d</cell><cell>nr2d</cell><cell>r2d</cell></row><row><cell>P D(Y , X)</cell><cell cols="2">21.1°23.3°36.3°24.7°P</cell><cell></cell><cell></cell></row><row><cell>D(Y ,Ŷ )</cell><cell cols="2">20.2°21.9°29.5°15.0°P</cell><cell></cell><cell></cell></row><row><cell cols="2">haseGain 4.5%</cell><cell>6%</cell><cell cols="2">17.6% 64%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The effect of causality and the model size.</figDesc><table><row><cell cols="5">Causal/Model /NRT /NRT /RT /RT</cell></row><row><cell>SI-SDR</cell><cell>5.33</cell><cell>4.60</cell><cell>3.42</cell><cell>2.33</cell></row><row><cell>PESQ</cell><cell>1.52</cell><cell>1.43</cell><cell>1.39</cell><cell>1.34</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Enhanced time-frequency masking by using neural networks for monaural source separation in reverberant room environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Naqvi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>26th European Signal Processing Conference (EUSIPCO</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1647" to="1651" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the importance of early reflections for speech in rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3233" to="3244" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effects of early and late reflections on intelligibility of reverberated speech by cochlear implant listeners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kokkinakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="28" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Masking estimation with phase restoration of clean speech for monaural speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019</title>
		<meeting>Interspeech 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3188" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Phase-aware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03107</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Coarse-to-fine optimization for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08044</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10204</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech enhancement using self-adaptation and multi-head self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yaiabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Maxuxama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="181" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sdrhalf-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Phasesensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="708" to="712" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Complex spectral mapping with a convolutional recurrent network for monaural speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6865" to="6869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phasenet: Discretized phase modeling with deep neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc</title>
		<imprint>
			<biblScope unit="page" from="2713" to="2717" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc</title>
		<imprint>
			<biblScope unit="page" from="3244" to="3248" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Phasen: A phase-andharmonics-aware speech enhancement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04697</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Phase estimation for signal reconstruction in single-channel source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mowlaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time-frequency constraints for phase estimation in single-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mowlaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saeidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 14th International Workshop on Acoustic Signal Enhancement (IWAENC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="337" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-stage deep learning for noisy-reverberant speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Whamr!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10279</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The interspeech 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Librivox: Free public domain audiobooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mcguire</surname></persName>
		</author>
		<ptr target="https://librivox.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Freesound datasets: a platform for the creation of open audio datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Corbera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ISMIR Conference</title>
		<editor>Hu X, Cunningham SJ, Turnbull D, Duan Z</editor>
		<meeting>the 18th ISMIR Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename><surname>Suzhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="486" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyroomacoustics: A python package for audio room simulation and array processing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scheibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bezzam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dokmanić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="351" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The delta-phase spectrum with application to voice activity detection and speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2026" to="2038" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Itu-t recommendation p.808, subjective evaluation of speech quality with a crowdsourcing approach</title>
	</analytic>
	<monogr>
		<title level="j">Geneva: International Telecommunication Union</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale multi-band densenets for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="21" to="25" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Channel-attention dense u-net for multichannel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tolooshams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11542</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
