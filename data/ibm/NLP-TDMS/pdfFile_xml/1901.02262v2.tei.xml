<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Style Generative Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-05-27">27 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
							<email>kyosuke.nishida@acm.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Media Intelligence Laboratory</orgName>
								<orgName type="institution">NTT Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Media Intelligence Laboratory</orgName>
								<orgName type="institution">NTT Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazutoshi</forename><surname>Shinoda</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Media Intelligence Laboratory</orgName>
								<orgName type="institution">NTT Corporation</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Media Intelligence Laboratory</orgName>
								<orgName type="institution">NTT Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisako</forename><surname>Asano</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Media Intelligence Laboratory</orgName>
								<orgName type="institution">NTT Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junji</forename><surname>Tomita</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NTT Media Intelligence Laboratory</orgName>
								<orgName type="institution">NTT Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Style Generative Reading Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-27">27 May 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque. The proposed model has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the NLG capability for all styles involved. This also enables our model to give an answer in the target style. Experiments show that our model achieves state-of-the-art performance on the Q&amp;A task and the Q&amp;A + NLG task of MS MARCO 2.1 and the summary task of Nar-rativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering has been a long-standing research problem. Recently, reading comprehension (RC), a challenge to answer a question given textual evidence provided in a document set, has received much attention. Current mainstream studies have treated RC as a process of extracting an answer span from one passage <ref type="bibr" target="#b41">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b40">(Rajpurkar et al., , 2018</ref> or multiple passages <ref type="bibr" target="#b23">(Joshi et al., 2017;</ref>, which is usually done by predicting the start and end positions of the answer <ref type="bibr" target="#b7">Devlin et al., 2018)</ref>. * Work done during an internship at NTT. The demand for answering questions in natural language is increasing rapidly, and this has led to the development of smart devices such as Alexa.</p><p>In comparison with answer span extraction, however, the natural language generation (NLG) capability for RC has been less studied. While datasets such as MS MARCO <ref type="bibr" target="#b1">(Bajaj et al., 2018)</ref> and Nar-rativeQA <ref type="bibr" target="#b26">(Kociský et al., 2018)</ref> have been proposed for providing abstractive answers, the stateof-the-art methods for these datasets are based on answer span extraction <ref type="bibr" target="#b17">Hu et al., 2018)</ref>. Generative models suffer from a dearth of training data to cover open-domain questions.</p><p>Moreover, to satisfy various information needs, intelligent agents should be capable of answering one question in multiple styles, such as wellformed sentences, which make sense even without the context of the question and passages, and concise phrases. These capabilities complement each other, but previous studies cannot use and control different styles within a model.</p><p>In this study, we propose Masque, a generative model for multi-passage RC. It achieves stateof-the-art performance on the Q&amp;A task and the Q&amp;A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. The main contri-butions of this study are as follows.</p><p>Multi-source abstractive summarization. We introduce the pointer-generator mechanism <ref type="bibr" target="#b44">(See et al., 2017)</ref> for generating an abstractive answer from the question and multiple passages, which covers various answer styles. We extend the mechanism to a Transformer <ref type="bibr" target="#b55">(Vaswani et al., 2017)</ref> based one that allows words to be generated from a vocabulary and to be copied from the question and passages.</p><p>Multi-style learning for style control and transfer. We introduce multi-style learning that enables our model to control answer styles and improves RC for all styles involved. We also extend the pointer-generator to a conditional decoder by introducing an artificial token corresponding to each style, as in <ref type="bibr" target="#b22">(Johnson et al., 2017)</ref>. For each decoding step, it controls the mixture weights over three distributions with the given style ( <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>This paper considers the following task:</p><formula xml:id="formula_0">PROBLEM 1. Given a question with J words x q = {x q 1 , . . . , x q J }, a set of K passages, where the k-th passage is composed of L words x p k = {x p k 1 , . . . , x p k L }</formula><p>, and an answer style label s, an RC model outputs an answer y = {y 1 , . . . , y T } conditioned on the style.</p><p>In short, given a 3-tuple (x q , {x p k }, s), the system predicts P (y). The training data is a set of 6-tuples: (x q , {x p k }, s, y, a, {r p k }), where a and {r p k } are optional. Here, a is 1 if the question is answerable with the provided passages and 0 otherwise, and r p k is 1 if the k-th passage is required to formulate the answer and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>We <ref type="formula">propose</ref>   4. The answer sentence decoder ( §3.4) outputs an answer sentence conditioned on the target style.</p><p>Our model is based on multi-source abstractive summarization: the answer that it generates can be viewed as a summary from the question and passages. The model also learns multi-style answers together. With these two characteristics, we aim to acquire the style-independent NLG ability and transfer it to the target style. In addition, to improve natural language understanding in the reader module, our model considers RC, passage ranking, and answer possibility classification together as multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question-Passages Reader</head><p>The reader module is shared among multiple answer styles and the three task-specific modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Word Embedding Layer</head><p>Let x q and x p k represent one-hot vectors (of size V ) for words in the question and the k-th passage. First, this layer projects each of the vectors to a d word -dimensional vector with a pretrained weight matrix W e ∈ R d word ×V such as GloVe <ref type="bibr" target="#b36">(Pennington et al., 2014)</ref>. Next, it uses contextualized word representations via ELMo <ref type="bibr" target="#b37">(Peters et al., 2018)</ref>, which allows our model to use morphological clues to form robust representations for out-of-vocabulary words unseen in training. Then, the concatenation of the word and con-textualized vectors is passed to a two-layer highway network <ref type="bibr" target="#b48">(Srivastava et al., 2015)</ref> to fuse the two types of embeddings, as in . The highway network is shared by the question and passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Shared Encoder Layer</head><p>This layer uses a stack of Transformer blocks, which are shared by the question and passages, on top of the embeddings provided by the word embedding layer. The input of the first block is immediately mapped to a d-dimensional vector by a linear transformation. The outputs of this layer are E p k ∈ R d×L for each k-th passage, and E q ∈ R d×J for the question.</p><p>Transformer encoder block. The block consists of two sub-layers: a self-attention layer and a position-wise feed-forward network. For the self-attention layer, we adopt the multi-head attention mechanism <ref type="bibr" target="#b55">(Vaswani et al., 2017)</ref>. Following GPT <ref type="bibr" target="#b38">(Radford et al., 2018)</ref>, the feed-forward network consists of two linear transformations with a GELU <ref type="bibr" target="#b15">(Hendrycks and Gimpel, 2016)</ref> activation function in between. Each sub-layer is placed inside a residual block <ref type="bibr" target="#b13">(He et al., 2016)</ref>. For an input x and a given sub-layer function f , the output is LN(f (x) + x), where LN indicates the layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>. To facilitate these residual connections, all sub-layers produce a sequence of d-dimensional vectors. Note that our model does not use any position embeddings in this block because ELMo gives the positional information of the words in each sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Dual Attention Layer</head><p>This layer uses a dual attention mechanism to fuse information from the question to the passages as well as from the passages to the question.</p><p>It first computes a similarity matrix U p k ∈ R L×J between the question and the k-th passage, as done in , where</p><formula xml:id="formula_1">U p k lj = w a⊤ [E p k l ; E q j ; E p k l ⊙ E q j ]</formula><p>indicates the similarity between the l-th word of the k-th passage and the j-th question word. The w a ∈ R 3d are learnable parameters. The ⊙ operator denotes the Hadamard product, and the [; ] operator denotes vector concatenation across the rows. Next, the layer obtains the row and column normalized similarity matrices A p k = softmax j (U p k ⊤ ) and B p k = softmax l (U p k ). It then uses DCN <ref type="bibr" target="#b59">(Xiong et al., 2017)</ref> to obtain dual attention representations, G q→p k ∈ R 5d×L and G p→q ∈ R 5d×J :</p><formula xml:id="formula_2">G q→p k = [E p k ;Ā p k ;Ā p k ; E p k ⊙Ā p k ; E p k ⊙Ā p k ] G p→q = [E q ;B;B; E q ⊙B; E q ⊙B].</formula><p>Here,</p><formula xml:id="formula_3">Ā p k = E q A p k ,B p k = E p k B p k ,Ā p k = B p k A p k ,B p k =Ā p k B p k ,B = max k (B p k ), and B = max k (B p k ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Modeling Encoder Layer</head><p>This layer uses a stack of the Transformer encoder blocks for question representations and obtains M q ∈ R d×J from G p→q . It also uses another stack for passage representations and obtains M p k ∈ R d×L from G q→p k for each k-th passage. The outputs of this layer, M q and {M p k }, are passed on to the answer sentence decoder; the {M p k } are also passed on to the passage ranker and the answer possibility classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage Ranker</head><p>The ranker maps the output of the modeling layer, {M p k }, to the relevance score of each passage. It takes the output for the first word, M p k 1 , which corresponds to the beginning-of-sentence token, to obtain the aggregate representation of each passage sequence. Given w r ∈ R d as learnable parameters, it calculates the relevance of each k-th passage to the question as</p><formula xml:id="formula_4">β p k = sigmoid(w r ⊤ M p k 1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Possibility Classifier</head><p>The classifier maps the output of the modeling layer to a probability for the answer possibility. It also takes the output for the first word, M p k 1 , for all passages and concatenates them. Given w c ∈ R Kd as learnable parameters, it calculates the answer possibility for the question as</p><formula xml:id="formula_5">P (a) = sigmoid(w c⊤ [M p 1 1 ; . . . ; M p K 1 ]).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Sentence Decoder</head><p>Given the outputs provided by the reader module, the decoder generates a sequence of answer words one element at a time. It is autoregressive <ref type="bibr" target="#b11">(Graves, 2013)</ref>, consuming the previously generated words as additional input at each decoding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Word Embedding Layer</head><p>Let y represent one-hot vectors of the words in the answer. This layer has the same components as the word embedding layer of the reader module, except that it uses a unidirectional ELMo to ensure that the predictions for position t depend only on the known outputs at positions previous to t.</p><p>Artificial tokens. To be able to use multiple answer styles within a single system, our model introduces an artificial token corresponding to the style at the beginning of the answer (y 1 ), as done in <ref type="bibr" target="#b22">(Johnson et al., 2017;</ref><ref type="bibr" target="#b51">Takeno et al., 2017)</ref>. At test time, the user can specify the first token to control the style. This modification does not require any changes to the model architecture. Note that introducing the token at the decoder prevents the reader module from depending on the answer style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Attentional Decoder Layer</head><p>This layer uses a stack of Transformer decoder blocks on top of the embeddings provided by the word embedding layer. The input is immediately mapped to a d-dimensional vector by a linear transformation, and the output is a sequence of d-dimensional vectors: {s 1 , . . . , s T }.</p><p>Transformer decoder block. In addition to the encoder block, this block consists of the second and third sub-layers after the self-attention block and before the feed-forward network, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. As in <ref type="bibr" target="#b55">(Vaswani et al., 2017)</ref>, the selfattention sub-layer uses a sub-sequent mask to prevent positions from attending to subsequent positions. The second and third sub-layers perform the multi-head attention over M q and M p all , respectively. The M p all is the concatenated outputs of the encoder stack for the passages,</p><formula xml:id="formula_6">M p all = [M p 1 , . . . , M p K ] ∈ R d×KL .</formula><p>Here, the [, ] operator denotes vector concatenation across the columns. This attention for the concatenated passages produces attention weights that are comparable between passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Multi-source Pointer-Generator</head><p>Our extended mechanism allows both words to be generated from a vocabulary and words to be copied from both the question and multiple passages ( <ref type="figure" target="#fig_2">Figure 3</ref>). We expect that the capability of copying words will be shared among answer styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additive Attention</head><p>Additive Attention  For each decoding step t, mixture weights λ v , λ q , λ p for the probability of generating words from the vocabulary and copying words from the question and the passages are calculated. The three distributions are weighted and summed to obtain the final distribution.</p><p>Extended vocabulary distribution. Let the extended vocabulary, V ext , be the union of the common words (a small subset of the full vocabulary, V , defined by the input-side word embedding matrix) and all words appearing in the input question and passages. P v then denotes the probability distribution of the t-th answer word, y t , over the extended vocabulary. It is defined as:</p><formula xml:id="formula_7">P v (y t ) = softmax(W 2 ⊤ (W 1 s t + b 1 )),</formula><p>where the output embedding W 2 ∈ R d word ×Vext is tied with the corresponding part of the input embedding <ref type="bibr" target="#b20">(Inan et al., 2017)</ref>, and</p><formula xml:id="formula_8">W 1 ∈ R d word ×d and b 1 ∈ R d word are learnable parameters. P v (y t ) is zero if y t is an out-of-vocabulary word for V .</formula><p>Copy distributions. A recent Transformerbased pointer-generator randomly chooses one of the attention-heads to form a copy distribution; that approach gave no significant improvements in text summarization <ref type="bibr" target="#b10">(Gehrmann et al., 2018)</ref>.</p><p>In contrast, our model uses an additional attention layer for each copy distribution on top of the decoder stack. For the passages, the layer takes s t as the query and outputs α p t ∈ R KL as the attention weights and c p t ∈ R d as the context vectors:</p><formula xml:id="formula_9">e p k l = w p⊤ tanh(W pm M p k l + W ps s t + b p ), α p t = softmax([e p 1 ; . . . ; e p K ]),<label>(1)</label></formula><formula xml:id="formula_10">c p t = l α p tl M p all l ,</formula><p>where w p , b p ∈ R d and W pm , W ps ∈ R d×d are learnable parameters. For the question, our model uses another identical layer and obtains α q t ∈ R J and c q t ∈ R d . As a result, P q and P p are the copy distributions over the extended vocabulary:</p><formula xml:id="formula_11">P q (y t ) = j:x q j =yt α q tj , P p (y t ) = l:x p k(l) l =yt α p tl ,</formula><p>where k(l) means the passage index corresponding to the l-th word in the concatenated passages.</p><p>Final distribution. The final distribution of y t is defined as a mixture of the three distributions:</p><formula xml:id="formula_12">P (y t ) = λ v P v (y t ) + λ q P q (y t ) + λ p P p (y t ), λ v , λ q , λ p = softmax(W m [s t ; c q t ; c p t ] + b m ),</formula><p>where W m ∈ R 3×3d and b m ∈ R 3 are learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Combined Attention</head><p>In order not to attend words in irrelevant passages, our model introduces a combined attention. While the original technique combined word and sentence level attentions <ref type="bibr" target="#b16">(Hsu et al., 2018)</ref>, our model combines the word and passage level attentions. The word attention, Eq. 1, is re-defined as</p><formula xml:id="formula_13">α p tl = α p tl β p k(l) l ′ α p tl ′ β p k(l ′ ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>We define the training loss as the sum of losses via</p><formula xml:id="formula_14">L(θ) = L dec + γ rank L rank + γ cls L cls</formula><p>where θ is the set of all learnable parameters, and γ rank and γ cls are balancing parameters.</p><p>The loss of the decoder, L dec , is the negative log likelihood of the whole target answer sentence averaged over N able answerable examples:</p><formula xml:id="formula_15">L dec = − 1 N able (a,y)∈D a T t log P (y t ),</formula><p>where D is the training dataset. The losses of the passage ranker, L rank , and the answer possibility classifier, L cls , are the binary cross entropy between the true and predicted values averaged over all N examples:</p><formula xml:id="formula_16">L rank = − 1 N K k r p k ∈D r p k log β p k + (1 − r p k ) log(1 − β p k ) , L cls = − 1 N a∈D a log P (a)+ (1 − a) log(1 − P (a))</formula><p>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Datasets. MS MARCO 2.1 provides two tasks for generative open-domain QA: the Q&amp;A task and the Q&amp;A + Natural Language Generation (NLG) task. Both tasks consist of questions submitted to Bing by real users, and each question refers to ten passages. The dataset also includes annotations on the relevant passages, which were selected by humans to form the final answers, and on whether there was no answer in the passages.</p><p>Answer styles. We associated the two tasks with two answer styles. The NLG task requires a wellformed answer that is an abstractive summary of the question and passages, averaging 16.6 words. The Q&amp;A task also requires an abstractive answer but prefers it to be more concise than in the NLG task, averaging 13.1 words, and many of the answers do not contain the context of the question. For the question "tablespoon in cup", a reference answer in the Q&amp;A task is "16," while that in the NLG task is "There are 16 tablespoons in a cup."</p><p>Subsets. In addition to the ALL dataset, we prepared two subsets for ablation tests as listed in Table 1. The ANS set consisted of answerable questions, and the NLG set consisted of the answerable questions and well-formed answers, so that NLG ⊂ ANS ⊂ ALL. We note that multi-style learning enables our model to learn from different answer styles of data (i.e., the ANS set), and multi-task learning with the answer possibility classifier enables our model to learn from both answerable and unanswerable data (i.e., the ALL set).</p><p>Training and Inference. We trained our model with mini-batches consisting of multi-style an-  swers that were randomly sampled. We used a greedy decoding algorithm and did not use any beam search or random sampling, because they did not provide any improvements.</p><p>Evaluation metrics and baselines. ROUGE-L and BLEU-1 were used to evaluate the models' RC performance, where ROUGE-L is the main metric on the official leaderboard. We used the reported scores of extractive <ref type="bibr" target="#b60">Yan et al., 2019;</ref>, generative , and unpublished RC models at the submission time.</p><p>In addition, to evaluate the individual contributions of our modules, we used MAP and MRR for the ranker and F 1 for the classifier, where the positive class was the answerable questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Does our model achieve state-of-the-art on the two tasks with different styles? <ref type="table" target="#tab_5">Table 2</ref> shows the performance of our model and competing models on the leaderboard. Our ensemble model of six training runs, where each model was trained with the two answer styles, achieved state-of-theart performance on both tasks in terms of ROUGE-L. In particular, for the NLG task, our single model outperformed competing models in terms of both ROUGE-L and BLEU-1.</p><p>Does multi-style learning improve the NLG performance? <ref type="table" target="#tab_7">Table 3</ref>    Does the Transformer-based pointer-generator improve the NLG performance? <ref type="table" target="#tab_7">Table 3</ref> shows that our model also outperformed the model that used RNNs and self-attentions instead of Transformer blocks as in <ref type="bibr">MCAN (McCann et al., 2018)</ref>. Our deep decoder captured the multi-hop interaction among the question, the passages, and the answer better than a single-layer LSTM decoder could.</p><p>Does joint learning with the ranker and classifier improve NLG performance? Furthermore, <ref type="table" target="#tab_7">Table 3</ref> shows that our model (jointly trained with the passage ranker and answer possibility classifier) outperformed the model that did not use the ranker and classifier. Joint learning thus had a regularization effect on the question-passages reader. We also confirmed that the gold passage ranker, which can perfectly predict the relevance of passages, significantly improved the RC performance. Passage ranking will be a key to developing a system that can outperform humans.</p><p>Does joint learning improve the passage ranking performance? <ref type="table" target="#tab_8">Table 4</ref> lists the passage ranking performance on the ANS dev. set 2 . The ranker shares the question-passages reader with the answer decoder, and this sharing contributed to improvements over the ranker trained without the answer decoder. Also, our ranker outperformed the initial ranking provided by Bing by a significant margin.</p><p>Does our model accurately identify answerable questions? <ref type="figure">Figure 4</ref> shows the precision-recall curve for answer possibility classification on the ALL dev. set. Our model identified the answerable questions well. The maximum F 1 score was 0.7893, where the threshold of answer possibility was 0.4411. This is the first report on answer possibility classification with MS MARCO 2.1.</p><p>Does our model control answer lengths with different styles? <ref type="figure">Figure 5</ref> shows the lengths of the answers generated by our model broken down by the answer style and query type. The generated answers were relatively shorter than the reference answers, especially for the Q&amp;A task, but well controlled with the target style for every query type. The short answers degraded our model's BLEU scores in the Q&amp;A task <ref type="table" target="#tab_5">(Table 2)</ref> because of BLEU's brevity penalty <ref type="bibr" target="#b34">(Papineni et al., 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on NarrativeQA</head><p>Next, we evaluated our model on Narra-tiveQA <ref type="bibr" target="#b26">(Kociský et al., 2018)</ref>. It requires understanding the underlying narrative rather than relying on shallow pattern matching. Our detailed setup and output examples are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We only describe the settings specific to this experiment.</p><p>Datasets. Following previous studies, we used the summary setting for the comparisons with the reported baselines, where each question refers to one summary (averaging 659 words), and there is no unanswerable questions. Our model therefore did not use the passage ranker and answer possibility classifier.</p><p>Answer styles. The NarrativeQA dataset does not explicitly provide multiple answer styles. In order to evaluate the effectiveness of multi-style learning, we used the NLG subset of MS MARCO as additional training data. We associated the NarrativeQA and NLG datasets with two answer styles. The answer style of NarrativeQA (NQA) is different from that of MS MARCO (NLG) in that the answers are short (averaging 4.73 words) and contained frequently pronouns. For instance, for the question "Who is Mark Hunter?", a reference is "He is a high school student in Phoenix."</p><p>Evaluation metrics and baselines. BLEU-1 and 4, METEOR, and ROUGE-L were used in accordance with the evaluation in the dataset paper <ref type="bibr" target="#b26">(Kociský et al., 2018)</ref>. We used the reports of top-performing extractive <ref type="bibr" target="#b53">Tay et al., 2018;</ref><ref type="bibr" target="#b17">Hu et al., 2018)</ref> and generative <ref type="bibr" target="#b2">(Bauer et al., 2018;</ref><ref type="bibr" target="#b21">Indurthi et al., 2018)</ref> models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Does our model achieve state-of-the-art performance? <ref type="table" target="#tab_10">Table 5</ref> shows that our single model, trained with two styles and controlled with the NQA style, pushed forward the state-of-the-art by a significant margin. The evaluation scores of the model controlled with the NLG style were low because the two styles are different. Also, our model without multi-style learning (trained with only the NQA style) outperformed the baselines in terms of ROUGE-L. This indicates that our model architec-  ture itself is powerful for natural language understanding in RC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work and Discussion</head><p>Transfer and multi-task learning in RC. Recent breakthroughs in transfer learning demonstrate that pre-trained language models perform well on RC with minimal modifications <ref type="bibr" target="#b37">(Peters et al., 2018;</ref><ref type="bibr" target="#b7">Devlin et al., 2018;</ref><ref type="bibr" target="#b38">Radford et al., 2018</ref><ref type="bibr" target="#b39">Radford et al., , 2019</ref>. In addition, our model also uses ELMo <ref type="bibr" target="#b37">(Peters et al., 2018)</ref> for contextualized embeddings.</p><p>Multi-task learning is a transfer mechanism to improve generalization performance <ref type="bibr" target="#b3">(Caruana, 1997)</ref>, and it is generally applied by sharing the hidden layers between all tasks, while keeping task-specific layers.  and <ref type="bibr" target="#b33">Nishida et al. (2018)</ref> reported that the sharing of the hidden layers between the multi-passage RC and passage ranking tasks was effective. Our results also showed the effectiveness of the sharing of the question-passages reader module among the RC, passage ranking, and answer possibility classification tasks.</p><p>In multi-task learning without task-specific layers, <ref type="bibr" target="#b7">Devlin et al. (2018)</ref> and  improved RC performance by learning multiple datasets from the same extractive RC setting. <ref type="bibr">Mc-Cann et al. (2018)</ref> and <ref type="bibr" target="#b62">Yogatama et al. (2019)</ref> investigated multi-task and curriculum learning on many different NLP tasks; their results were below task-specific RC models. Our multi-style learning does not use style-specific layers; instead uses a style-conditional decoder.</p><p>Generative RC. S-Net  used an extraction-then-synthesis mechanism for multi-passage RC. The models proposed by <ref type="bibr" target="#b30">McCann et al. (2018)</ref>, <ref type="bibr" target="#b2">Bauer et al. (2018), and</ref><ref type="bibr" target="#b21">Indurthi et al. (2018)</ref> used an RNN-based pointer-generator mechanism for single-passage RC. Although these mechanisms can alleviate the lack of training data, large amounts of data are still required. Our multistyle learning will be a key technique enabling learning from many RC datasets with different styles.</p><p>In addition to MS MARCO and NarrativeQA, there are other datasets that provide abstractive answers. DuReader , a Chinese multi-document RC dataset, provides longer documents and answers than those of MS MARCO. DuoRC <ref type="bibr" target="#b43">(Saha et al., 2018)</ref> and CoQA  contain abstractive answers; most of the answers are short phrases.</p><p>Controllable text generation. Many studies have been carried out in the framework of style transfer, which is the task of rephrasing a text so that it contains specific styles such as sentiment. Recent studies have used artificial tokens <ref type="bibr" target="#b45">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b22">Johnson et al., 2017)</ref>, variational auto-encoders <ref type="bibr" target="#b19">(Hu et al., 2017)</ref>, or adversarial training <ref type="bibr" target="#b54">Tsvetkov et al., 2018)</ref> to separate the content and style on the encoder side. On the decoder side, conditional language modeling has been used to generate output sentences with the target style. In addition, output length control with conditional language modeling has been well studied <ref type="bibr" target="#b24">(Kikuchi et al., 2016;</ref><ref type="bibr" target="#b51">Takeno et al., 2017;</ref><ref type="bibr" target="#b8">Fan et al., 2018)</ref>. Our style-controllable RC relies on conditional language modeling in the decoder.</p><p>Multi-passage RC. The simplest approach is to concatenate the passages and find the answer from the concatenation, as in <ref type="bibr" target="#b57">(Wang et al., 2017)</ref>. Earlier pipelined models found a small number of relevant passages with a TF-IDF based ranker and passed them to a neural reader , while more recent models have used a neural re-ranker to more accurately select the relevant passages <ref type="bibr" target="#b33">Nishida et al., 2018)</ref>. Also, non-pipelined models (including ours) consider all the provided passages and find the answer by comparing scores between passages . The most recent models make a proper trade-off between efficiency and accuracy <ref type="bibr" target="#b60">(Yan et al., 2019;</ref>.</p><p>RC with unanswerable question identification. The previous work of ( <ref type="bibr" target="#b27">Levy et al., 2017;</ref>) outputted a no-answer score depending on the probability of all answer spans. <ref type="bibr" target="#b18">Hu et al. (2019)</ref> proposed an answer verifier to compare an answer with the question. <ref type="bibr" target="#b49">Sun et al. (2018)</ref> jointly learned an RC model and an answer verifier. Our model introduces a classifier on top of the question-passages reader, which is not dependent on the generated answer.</p><p>Abstractive summarization. Current state-ofthe-art models use the pointer-generator mechanism <ref type="bibr" target="#b44">(See et al., 2017)</ref>. In particular, content selection approaches, which decide what to summarize, have recently been used with abstractive models. Most methods select content at the sentence level <ref type="bibr" target="#b16">(Hsu et al., 2018;</ref><ref type="bibr" target="#b5">Chen and Bansal, 2018)</ref> or the word level <ref type="bibr" target="#b35">(Pasunuru and Bansal, 2018;</ref><ref type="bibr" target="#b10">Gehrmann et al., 2018)</ref>. Our model incorporates content selection at the passage level in the combined attention.</p><p>Query-based summarization has rarely been studied because of a lack of datasets. <ref type="bibr" target="#b32">Nema et al. (2017)</ref> proposed an attentional encoder-decoder model; however, <ref type="bibr" target="#b43">Saha et al. (2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This study sheds light on multi-style generative RC. Our proposed model, Masque, is based on multi-source abstractive summarization and learns multi-style answers together. It achieved stateof-the-art performance on the Q&amp;A task and the Q&amp;A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. The key to its success is transferring the style-independent NLG capability to the target style by use of the question-passages reader and the conditional pointer-generator decoder. In particular, the capability of copying words from the question and passages can be shared among the styles, while the capability of controlling the mixture weights for the generative and copy distributions can be acquired for each style. Our future work will involve exploring the potential of our multi-style learning towards natural language understanding.</p><p>A.2 Experimental Setup for NarrativeQA Model configurations. Our best model was jointly trained with the NarrativeQA and MS MARCO NLG datasets for a total of seven epochs with a batch size of 64, where each batch consisted of multi-style answers that were randomly sampled. For efficient multi-style learning, each summary in the NarrativeQA dataset was divided into ten passages (size of 130 words) with sentencelevel overlaps such that each sentence in the summary was entirely contained in a passage. Each passage from MS MARCO was also truncated to 130 words. The rest of the configuration was the same as in the MS MARCO experiments.</p><p>Evaluation settings. An official evaluation script is not provided, so we used the evaluation script created by <ref type="bibr" target="#b2">Bauer et al. (2018)</ref>  <ref type="bibr">5</ref> . The answers were normalized by making words lowercase and removing punctuation marks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Output Examples Generated by Masque</head><p>Tables 6 and 7 list the generated examples for questions from MS MARCO 2.1 and Narra-tiveQA, respectively. We can see from the examples that our model could control answer styles appropriately for various question and reasoning types. We did find some important errors: style errors, yes/no classification errors, copy errors with respect to numerical values, grammatical errors, and multi-hop reasoning errors. <ref type="table">Table 6</ref>: Output examples generated by Masque from MS MARCO. The model was trained with the Q&amp;A and NLG styles. The relevant passage is one that an annotator selected to compose the reference answer. The model could control answer styles appropriately for (a) natural language, (b) cloze-style, and (c) keywords questions. (d) The answer style was incorrect. (e) The answers were not consistent between the styles. (f) Copying from numerical words worked poorly. There were some grammatical errors in the generative answers, which are underlined. <ref type="table">Table 7</ref>: Output examples generated by Masque from NarrativeQA. The model was trained with the NarrativeQA (NQA) and MS MARCO (NLG) styles. It could control answer styles appropriately for questions that required <ref type="bibr">(a,b)</ref> single-sentence reasoning and (c) multi-sentence reasoning. (d) Example of an error in multi-sentence reasoning. There were some grammatical errors in the generative answers, which are underlined.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of how our model generates an answer on MS MARCO. Given an answer style (top: NLG, bottom: Q&amp;A), the model controls the mixture of three distributions for generating words from a vocabulary and copying words from the question and multiple passages at each decoding step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Masque model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Multi-source pointer-generator mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Precision-recall curve for answer possibility classification on the ALL dev. set. Lengths of answers generated by Masque broken down by the answer style and query type on the NLG dev. set. The error bars indicate standard errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>a Multi-style Abstractive Summarization model for QUEstion answering, called Masque. Masque directly models the conditional probability p(y|x q , {x p k }, s). As shown in Figure 2, it consists of the following modules.</figDesc><table><row><cell>Answer Possibility</cell><cell></cell><cell cols="2">Answer Word Sequence</cell></row><row><cell></cell><cell></cell><cell cols="2">Multi-source Pointer-Generator</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8x</cell></row><row><cell></cell><cell>Combined</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Attention</cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell></cell><cell>Additive</cell><cell>Additive</cell><cell>Feed</cell></row><row><cell></cell><cell>Attention</cell><cell>Attention</cell><cell>Forward</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Multi-Head</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Attention</cell></row><row><cell></cell><cell>Concat</cell><cell></cell><cell></cell></row><row><cell>5x</cell><cell></cell><cell>2x</cell><cell>Add &amp; Norm</cell></row><row><cell>Modeling Encoder Block</cell><cell>Modeling Encoder Block</cell><cell>Modeling Encoder Block</cell><cell>Multi-Head Attention</cell></row><row><cell></cell><cell>Dual Attention</cell><cell></cell><cell></cell></row><row><cell>3x</cell><cell></cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell>Shared</cell><cell>Shared</cell><cell>Shared</cell><cell>Masked</cell></row><row><cell>Encoder</cell><cell>Encoder</cell><cell>Encoder</cell><cell>Multi-Head</cell></row><row><cell>Block</cell><cell>Block</cell><cell>Block</cell><cell>Attention</cell></row><row><cell>Highway</cell><cell>Highway</cell><cell>Highway</cell><cell>Highway</cell></row><row><cell>Glove&amp;ELMo</cell><cell>Glove&amp;ELMo</cell><cell>Glove&amp;ELMo</cell><cell>Glove&amp;ELMo</cell></row><row><cell>Passage 1</cell><cell>Passage K</cell><cell>Question</cell><cell>Style + Answer (Shiftted)</cell></row><row><cell>1. The question-passages reader ( §3.1) models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>interactions between the question and passages.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2. The passage ranker ( §3.2) finds passages rele-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>vant to the question.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3. The answer possibility classifier ( §3.3) identi-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>fies answerable questions.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Numbers of questions used in the experiments.We evaluated our model on MS MARCO 2.1<ref type="bibr" target="#b1">(Bajaj et al., 2018)</ref>. It is the sole dataset providing abstractive answers with multiple styles and serves as a great test bed for building open-domain QA agents with the NLG capability that can be used in smart devices. The details of our setup and output examples are in the supplementary material.</figDesc><table><row><cell>4 Experiments on MS MARCO 2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Performance of our and competing models on</cell></row><row><cell>the MS MARCO V2 leaderboard (4 March 2019). a Seo</cell></row><row><cell>et al. (2017); b Yan et al. (2019); c Shao (unpublished), a</cell></row><row><cell>variant of Tan et al. (2018); d Li (unpublished), a model</cell></row><row><cell>using Devlin et al. (2018) and See et al. (2017); e Qian</cell></row><row><cell>(unpublished);</cell></row></table><note>f Wu et al. (2018). Whether the compet- ing models are ensemble models or not is unreported.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>lists the results of an ablation test for our single model (controlled with Masque (NLG style; single) ALL 69.77 65.56 w/o multi-style learning ( §3.4.2) NLG 68.20 63.95 ֒→ w/o Transformer ( §3.1.2, §3.4.2) NLG 67.13 62.96 w/o passage ranker ( §3.2) NLG 68.05 63.82 w/o possibility classifier ( §3.3) ANS 69.64 65.41 Masque w/ gold passage ranker ALL 78.70 78.14</figDesc><table><row><cell>Model</cell><cell>Train R-L B-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation test results on the NLG dev. set. The models were trained with the subset listed in "Train".</figDesc><table><row><cell>Model</cell><cell cols="2">Train MAP MRR</cell></row><row><cell>Bing (initial ranking)</cell><cell>-</cell><cell>34.62 35.00</cell></row><row><cell>Masque (single)</cell><cell cols="2">ALL 69.51 69.96</cell></row><row><cell>w/o answer decoder ( §3.4)</cell><cell cols="2">ALL 67.03 67.49</cell></row><row><cell cols="3">w/o multi-style learning ( §3.4.2) NLG 65.51 65.59</cell></row><row><cell>w/o possibility classifier ( §3.3)</cell><cell cols="2">ANS 69.08 69.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Passage ranking results on the ANS dev. set.</figDesc><table><row><cell>the NLG style) on the NLG dev. set 1 . Our model</cell></row><row><cell>trained with both styles outperformed the model</cell></row><row><cell>trained with the single NLG style. Multi-style</cell></row><row><cell>learning enabled our model to improve its NLG</cell></row><row><cell>performance by also using non-sentence answers.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performance of our and competing models on the NarrativeQA test set. a Seo et al. (2017); b Tay et al. (2018); c Bauer et al. (2018); d Indurthi et al. (2018); e Hu et al. (2018). f Results on the NarrativeQA validation set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We confirmed with the organizer that the dev. results were much better than the test results, but there was no problem.2 This evaluation requires our ranker to re-rank 10 passages. It is not the same as the Passage Re-ranking task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://nlp.stanford.edu/projects/ glove/ 4 https://allennlp.org/elmo</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/yicheng-w/ CommonSenseMultiHopQA/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>A.1 Experimental Setup for MS MARCO Model configurations. We trained our model on a machine with eight NVIDIA P100 GPUs. Our best model was jointly trained with the two answer styles in the ALL set for a total of eight epochs with a batch size of 80, where each batch consisted of multi-style answers that were randomly sampled. The training took roughly six days. The hidden size d was 304, and the number of attention heads was 8. The inner state size of the feedforward networks was 256. The numbers of shared encoding blocks, modeling blocks for a question, modeling blocks for passages, and decoder blocks were 3, 2, 5, and 8, respectively. We used the pretrained uncased 300-dimensional GloVe (Pennington et al., 2014) 3 and the original 512-dimensional ELMo <ref type="bibr">(Peters et al., 2018) 4</ref> . We used the spaCy tokenizer, and all input words were lowercased except the input for ELMo. The output words were also lowercase. The number of common words in V ext in the extended vocabulary was 5,000. Each passage and each answer were truncated to 100 words for training.</p><p>Optimizer. We used Adam (Kingma and Ba, 2015) with β 1 = 0.9, β 2 = 0.999, and ǫ = 10 −8 . The weights were initialized using N (0, 0.02), except that the biases of all the linear transformations were initialized with zero vectors. The learning rate was increased linearly from zero to 2.5×10 −4 in the first 2,000 steps and then annealed to 0 by using a cosine schedule. All parameter gradients were clipped to a maximum norm of 1. An exponential moving average was applied to all trainable variables with a decay rate of 0.9995. The balancing factors for joint learning, λ rank and λ cls , were set to 0.5 and 0.1, respectively.</p><p>Regularization. We used a modified version of the L 2 regularization proposed in <ref type="bibr" target="#b29">(Loshchilov and Hutter, 2017)</ref>, with w = 0.01 on all non-bias. We additionally used a dropout <ref type="bibr" target="#b47">(Srivastava et al., 2014)</ref> rate of 0.3 for all highway networks and residual and scaled dot-product attention operations in the multi-head attention mechanism. We also used one-sided label smoothing <ref type="bibr" target="#b50">(Szegedy et al., 2016)</ref> for the passage relevance and answer possibility labels. We smoothed only the positive labels to 0.9.</p><p>Ensemble model. The ensemble model consisted of six training runs with identical architectures and hyperparameters but with different weight initializations. The final answer was decided with a weighted majority, where we used the ROUGE-L score for the dev. set as the weight of each model. Evaluation settings. We used the official evaluation script. The answers were normalized by making words lowercase.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization. Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Version 3</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Commonsense for generative multi-hop question answering tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4220" to="4230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast abstractive summarization with reinforce-selected sentence rewriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="845" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Version 1</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Controllable abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Neural Machine Translation and Generation (NMT@ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Style transfer in text: Exploration and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bottom-up abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4098" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Version 5</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hasselqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Helmertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kågebäck</surname></persName>
		</author>
		<idno>arXiv, 1712.06100</idno>
		<title level="m">Query-based abstractive summarization using neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DuReader: a chinese machine reading comprehension dataset from real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Reading for Question Answering (MRQA@ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Version 2</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified model for extractive and abstractive summarization using inconsistency loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Ting</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Kai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerui</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
	<note>Jing Tang, and Min Sun</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention-guided answer distillation for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2077" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Read + Verify: Machine reading comprehension with unanswerable questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cut to the chase: A context zoom-in network for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Sathish Reddy Indurthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuayáhuitl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="570" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistic (TACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The NarrativeQA reading comprehension challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistic (TACL)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Guiding generation for abstractive text summarization based on key information guide network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Version 1</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient and robust question answering from minimal context over documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Diversity driven attention model for query-based abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preksha</forename><surname>Nema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1063" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Retrieve-andread: Multi-task learning of information retrieval and reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="647" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi--reward reinforced summarization with saliency and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="646" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training. Technical report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CoQA: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07042</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DuoRC: Towards complex language understanding with paraphrased reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1683" to="1693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Controlling politeness in neural machine translation via side constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Version 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
	</analytic>
	<monogr>
		<title level="m">Highway networks. Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">U-Net: Machine reading comprehension with unanswerable questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06638</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Version 1</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Controlling target features in neural machine translation via prefix constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Takeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhide</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Asian Translation (WAT@IJCNLP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">S-Net: From answer extraction to answer synthesis for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5940" to="5947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Densely connected attention propagation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4911" to="4922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Style transfer through back-translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="866" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">R 3 : Reinforced reader-ranker for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5981" to="5988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-passage machine reading comprehension with cross-passage answer verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1918" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A deep cascade model for multi-document reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning and evaluating general linguistic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyprien</forename><surname>De Masson D&amp;apos;autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11373</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Version 1</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">QANet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Shaking is a symptom in which a person has tremors (shakiness or small back and forth movements) in part or all of his body. Shaking can be due to cold body temperatures, rising fever (such as with infections), neurological problems, medicine effects, drug abuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relevant</forename><surname>Passage</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Read more</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Shaking can be due to cold body temperatures, rising fever (such as with infections), neurological problems, medicine effects, drug abuse, etc. Prediction (Q&amp;A): because of cold body temperatures , rising fever , neurological problems , medicine effects , drug abuse . ✓ Reference Answers (NLG): Shaking can be due to cold body temperatures, rising fever, neurological problems, medicine effects and drug abuse. / Body would feel like it is shaking due to cold body temperatures, rising fever</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reference</forename><surname>Answer</surname></persName>
			<affiliation>
				<orgName type="collaboration">Q&amp;A</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
	<note>neurological problems, medicine effects, drug abuse</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">✓ (b) Question: is the name used to refer to the era of legalized segregation in the united states Relevant Passage: Jim Crow law, in U.S. history, any of the laws that enforced racial segregation in the South between the end of Reconstruction in 1877 and the beginning of the civil rights movement in the 1950s. Reference Answer (Q&amp;A): Jim Crow Prediction (Q&amp;A): jim crow ✓ Reference Answer (NLG): Jim Crow is the name used to refer to the era of legalized segregation in the United States. Prediction (NLG): jim crow is the name used to refer to the era of legalized segregation in the united states . ✓ (c) Question: average height nba player Relevant Passage: The average height of an NBA player is around 6 feet 7 inches. The tallest NBA player ever was Gheorghe Muresan, who was 7 feet 7 inches tall. In contrast, the shortest NBA player ever was Tyrone Muggsy Bogues, who was 5 feet 3 inches tall. Reference Answer (Q&amp;A): Around 6 feet 7 inches Prediction (Q&amp;A): 6 feet 7 inches ✓ Reference Answers (NLG): The average height of NBA players is around 6 feet, 7 inches. / The height of NBA player is around 6 feet 7 inches. Prediction (NLG): the average height of an national basketball association player is 6 feet 7 inches . ✓ (d) Question: population of henryetta ok Relevant Passage: With it&apos;s population of 5,927 people</title>
	</analytic>
	<monogr>
		<title level="m">Prediction (NLG): your body would feel like it is shaking because of cold body temperatures , rising fever , neurological problems , medicine effects , drug abuse</title>
		<meeting><address><addrLine>Henryetta; Okmulgee County</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>represented 14.79% of the county&apos;s total. In addition, where the city of Henryetta has a population density of 891 people per square mile [344.2 people/km2</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Reference Answer</title>
		<imprint>
			<biblScope unit="page">5927</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">✓ (e) Question: does gameplay programmer need math skill Relevant Passage: A good computer programmer is more of a problem solver and logical thinker than a math buff. And besides, the industry is peppered with many successful computer programmers who do not really know much about mathematics. Reference Answer (Q&amp;A): No Prediction (Q&amp;A): yes ✗ Reference Answers (NLG): No, a gameplay programmer doesn&apos;t need math skill. / No, gameplay programmer do not need an math skill. Prediction (NLG): no , gameplay programmer does not need math skill . ✓ (f) Question: how long does a freezer take to cool down Relevant Passage: Quick Answer. It takes anywhere from three to 24 hours for a refrigerator to reach safe temperatures for storing food, depending on the size and type of unit. When the refrigerator compartment reaches 40 degrees Fahrenheit and the freezer reaches 5 degrees Fahrenheit, it is safe to transfer food items</title>
	</analytic>
	<monogr>
		<title level="m">Prediction (Q&amp;A): the population of henryetta , oklahoma is 5,927 . ✓ (content is OK, but style is NG ✗) Reference Answer (NLG): The population of Henryetta, Oklahoma is 5</title>
		<imprint>
			<biblScope unit="volume">927</biblScope>
		</imprint>
	</monogr>
	<note>Q&amp;A): 24 hours Prediction (Q&amp;A): 4 to 5 hours ✗ Reference Answers (NLG): A freezer takes 24 hours to cool down. / A freezer take to cool down is 24 hours. Prediction (NLG): a freezer takes 4 to 12 hours to cool down . ✗ (a) Question: Where does Mark broadcast his radio station? Summary: Mark Hunter (Slater), a high school student in a sleepy suburb of Phoenix, Arizona, starts an FM pirate radio station that broadcasts from the basement of his parents&apos; house. Mark is a loner, an outsider, whose only outlet for his teenage angst and aggression is his unauthorized radio station. His pirate station&apos;s theme song is &quot;Everybody Knows&quot; by</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">) Reference Answers: Approximately 150 days / 150 days Prediction (NQA): 150 days ✓ Prediction (NLG): the voyage from london to thailand was supposed to take 150 days . ✓ (d) Question: Why does Jamie start avoiding Landon? Summmary: (...) During these functions, Landon notices Jamie Sullivan, a girl he has known since kindergarten and who has attended many of the same classes as him, and is also the local minister&apos;s daughter. Since he&apos;s one of the in-crowd, he has seldom paid any attention to Jamie, who wears modest dresses and owns only one sweater. Jamie is labeled an outsider and a geek. She makes no attempt to wear make-up or otherwise improve her looks or attract attention to herself. Landon has trouble learning his lines for the play. Jamie, who is also in the play, agrees to help him on one condition: Jamie warns Landon not to fall in love with her; he laughs it off and dismisses it as a foolish idea. Landon and Jamie begin practicing together at her house after school. They get to know each other and a spark of affection arises between them. On the opening night of the play, Jamie astounds Landon and the entire audience with her beauty and her voice. Onstage at the peak of the ending to the play, Jamie sings. When Jamie finishes, Landon improvises and kisses her which is not a part of the play. Afterwards, Jamie avoids Landon, and it is not until Landon</title>
	</analytic>
	<monogr>
		<title level="m">Reference Answers: In his parent&apos;s basement. / His parents&apos; basement. Prediction (NQA): the basement of his parents &apos; house ✓ Prediction (NLG): mark broadcast his radio station in the basement of his parents &apos; house . ✓ (b) Question: Fletch is a reporter for what newspaper? Summary: Los Angeles Times reporter Irwin &quot;Fletch&quot; Fletcher (Chase) is writing an article exposing drug trafficking on the beaches of Los Angeles</title>
		<imprint/>
		<respStmt>
			<orgName>Camper Van Beethoven, Primal Scream, Soundgarden, Ice-T, Bad Brains, Concrete Blonde, Henry Rollins, and The Pixies. By day</orgName>
		</respStmt>
	</monogr>
	<note>Posing as an addict during his investigation, he is approached by Boyd Aviation executive vice president Alan Stanwyk (Matheson) who mistakenly assumes Fletch is a junkie. Stanwyk claims to have bone cancer, with only months left to live, and wishes to avoid the pain and suffering. Stanwyk offers $50,000 for Fletch to come to his mansion in a few days time, kill him, and then escape to Rio de Janeiro, staging the murder to look like a burglary. Fletch, while not completely convinced on the truth of Stanwyk&apos;s story, reluctantly agrees to the plan. Along with his colleague Larry (Davis), he begins investigating Stanwyk instead of completing his drug trafficking expos, much to the disapproval of his overbearing editor Frank Walker (Libertini). s friends play a cruel prank on Jamie and he protects her in opposition to his friends that she warms up to him again. Landon asks Jamie on a date soon after. but Jamie says her father doesn&apos;t allow her to date. (...) Reference Answers: Because he kissed her in the play. / He kisses her Prediction (NQA): he is not a part of the play ✗ Prediction (NLG): he is not a part of the play ✗</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
