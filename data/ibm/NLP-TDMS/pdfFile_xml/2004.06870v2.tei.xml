<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coreferential Reasoning Learning for Language Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Coreferential Reasoning Learning for Language Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github. com/thunlp/CorefBERT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, language representation models such as <ref type="bibr">BERT (Devlin et al., 2019)</ref> have attracted considerable attention. These models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference , sentiment classification <ref type="bibr" target="#b34">(Sun et al., 2019b)</ref>, question answering <ref type="bibr" target="#b36">(Talmor and Berant, 2019)</ref>, relation extraction <ref type="bibr" target="#b22">(Peters et al., 2019)</ref>, fact extraction and verification <ref type="bibr" target="#b53">(Zhou et al., 2019)</ref>, and coreference resolution .</p><p>However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning <ref type="bibr" target="#b20">(Paperno et al., 2016;</ref><ref type="bibr">Dasigi et al., 2019)</ref>, and they can be further improved on long-text tasks with external coreference information <ref type="bibr" target="#b4">(Cheng and Erk, 2020;</ref><ref type="bibr" target="#b50">Xu et al., 2020;</ref>. Coreference occurs when two or more expressions in a text refer to the same entity, which is an important element for a coherent understanding of the whole discourse. For example, for comprehending the whole context of "Antoine published The Little Prince in 1943. The book follows a young prince who visits various planets in space.", we must realize that The book refers to The Little Prince. Therefore, resolving coreference is an essential step for abundant higherlevel NLP tasks requiring full-text understanding.</p><p>To improve the capability of coreferential reasoning for language representation models, a straightforward solution is to fine-tune these models on supervised coreference resolution data. Nevertheless, on the one hand, we find fine-tuning on existing small coreference datasets cannot improve the model performance on downstream tasks in our preliminary experiments. On the other hand, it is impractical to obtain a large-scale supervised coreference dataset.</p><p>To address this issue, we present CorefBERT, a language representation model designed to better capture and represent the coreference information. To learn coreferential reasoning ability from largescale unlabeled corpus, CorefBERT introduces a novel pre-training task called Mention Reference Prediction (MRP). MRP leverages those repeated mentions (e.g., noun or noun phrase) that appear multiple times in the passage to acquire abundant co-referring relations. Among the repeated mentions in a passage, MRP applies mention reference masking strategy to mask one or several mentions and requires model to predict the masked mention's corresponding referents. <ref type="figure">Figure 1</ref> shows an example of the MRP task, we substitute one of the repeated mentions, Claire, with [MASK] and ask the model to find the proper contextual candidate for filling it. To explicitly model the coreference information, we further introduce a copybased training objective to encourage the model to select words from context instead of the whole vocabulary. The internal logic of our method is essentially similar to that of coreference resolution, which aims to find out all the mentions that refer to the masked mentions in a text. Besides, rather than using a context-free word embedding matrix when predicting words from the vocabulary, copying from context encourages the model to generate more context-sensitive representations, which is more feasible to model coreferential reasoning.</p><p>We conduct experiments on a suite of downstream tasks which require coreferential reasoning in language understanding, including extractive question answering, relation extraction, fact extraction and verification, and coreference resolution. The results show that CorefBERT outperforms the vanilla BERT on almost all benchmarks and even strengthens the performance of the strong RoBERTa model. To verify the model's robustness, we also evaluate CorefBERT on other common NLP tasks where CorefBERT still achieves comparable results to BERT. It demonstrates that the introduction of the new pre-training task about coreferential reasoning would not impair BERT's ability in common language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pre-training language representation models aim to capture language information from the text, which facilitate various downstream NLP applications <ref type="bibr">(Kim, 2014;</ref><ref type="bibr" target="#b12">Lin et al., 2016;</ref><ref type="bibr" target="#b28">Seo et al., 2017)</ref>. Early works <ref type="bibr" target="#b18">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b21">Pennington et al., 2014)</ref> focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SA-LSTM (Dai and <ref type="bibr" target="#b8">Le, 2015)</ref> and <ref type="bibr">ULMFiT (Howard and Ruder, 2018)</ref> pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo <ref type="bibr" target="#b23">(Peters et al., 2018)</ref> further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, Ope-nAI GPT <ref type="bibr" target="#b24">(Radford et al., 2018)</ref> and <ref type="bibr">BERT (Devlin et al., 2019)</ref> learn pre-trained language representation with Transformer architecture <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref>, achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as Span- <ref type="bibr">BERT (Joshi et al., 2020)</ref> with span-based learning, XLNet <ref type="bibr" target="#b51">(Yang et al., 2019)</ref> considering masked positions dependency with auto-regressive loss, MASS <ref type="bibr" target="#b30">(Song et al., 2019)</ref> and BART <ref type="bibr" target="#b46">(Wang et al., 2019b)</ref> with sequence-to-sequence pre-training, ELECTRA <ref type="bibr" target="#b6">(Clark et al., 2020)</ref> learning from replaced token detection with generative adversarial networks and InfoWord <ref type="bibr">(Kong et al., 2020)</ref> with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs <ref type="bibr" target="#b57">(Zhang et al., 2019;</ref><ref type="bibr" target="#b22">Peters et al., 2019;</ref><ref type="bibr">Liu et al., 2020a)</ref>; and (3) exploring multilingual learning <ref type="bibr" target="#b7">(Conneau and Lample, 2019;</ref><ref type="bibr" target="#b37">Tan and Bansal, 2019;</ref><ref type="bibr">Kondratyuk and Straka, 2019)</ref> or multimodal learning <ref type="bibr" target="#b33">Sun et al., 2019a;</ref><ref type="bibr" target="#b32">Su et al., 2020)</ref>. Though existing language representation models have achieved a great success, their coreferential reasoning capability are still far less than that of human beings <ref type="bibr" target="#b20">(Paperno et al., 2016;</ref><ref type="bibr">Dasigi et al., 2019)</ref>. In this paper, we design a mention reference prediction task to enhance language representation models in terms of coreferential reasoning.</p><p>Our work, which acquires coreference resolution ability from an unlabeled corpus, can also be viewed as a special form of unsupervised coreference resolution. Formerly, researchers have made efforts to explore feature-based unsupervised coreference resolution methods <ref type="bibr" target="#b0">(Bejan et al., 2009;</ref><ref type="bibr" target="#b17">Ma et al., 2016)</ref>. After that, Word-LM (Trinh and <ref type="bibr" target="#b55">Le, 2018)</ref> uncovers that it is natural to resolve pronouns in the sentence according to the probability of language models. Moreover, WikiCREM (Kocijan et al., 2019) builds sentence-level unsupervised coreference resolution dataset for learning coreference discriminator. However, these methods cannot be directly transferred to language representation models since their task-specific design could weaken the model's performance on other NLP tasks. To address this issue, we introduce a mention reference prediction objective, complementary to masked language modeling, which could make the obtained coreferential reasoning ability compatible with more downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we present CorefBERT, a language representation model, which aims to better capture the coreference information of the text. As illustrated in <ref type="figure">Figure 1</ref>, CorefBERT adopts the deep bidirectional Transformer architecture <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> and utilizes two training tasks:</p><p>(1) Mention Reference Prediction (MRP) is a novel training task which is proposed to enhance coreferential reasoning ability. MRP utilizes the mention reference masking strategy to mask one of the repeated mentions and then employs a copybased training objective to predict the masked tokens by copying from other tokens in the sequence.</p><p>(2) Masked Language Modeling (MLM) 1 is proposed from vanilla <ref type="bibr">BERT (Devlin et al., 2019)</ref>, aiming to learn the general language understanding. MLM is regarded as a kind of cloze tasks and aims to predict the missing tokens according to its final contextual representation. Except for MLM, Next Sentence Prediction (NSP) is also commonly used in BERT, but we train our model without the NSP objective since some previous works <ref type="bibr">Joshi et al., 2020)</ref> have revealed that NSP is not as helpful as expected.</p><p>Formally, given a sequence of tokens 2 X = (x 1 , x 2 , . . . , x n ), we first represent each token by aggregating the corresponding token and position embeddings, and then feeds the input representations into deep bidirectional Transformer to obtain the contextual representations, which is used to compute the loss for pre-training tasks. The overall loss of CorefBERT is composed of two training losses: the mention reference prediction loss L MRP and the masked language modeling loss L MLM , which can be formulated as:</p><formula xml:id="formula_0">L = L MRP + L MLM .</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mention Reference Masking</head><p>To better capture the coreference information in the text, we propose a novel masking strategy: mention reference masking, which masks tokens of the repeated mentions in the sequence instead of masking random tokens. We follow a distant supervision assumption: the repeated mentions in a sequence would refer to each other. Therefore, if we mask one of them, the masked tokens would be inferred through its context and unmasked references. Based on the above strategy and assumption, the CorefBERT model is expected to capture the coreference information in the text for filling the masked token.</p><p>In practice, we regard nouns in the text as mentions. We first use a part-of-speech tagging tool to extract all nouns in the given sequence. Then, we cluster the nouns into several groups where each group contains all mentions of the same noun. After that, we select the masked nouns from different groups uniformly. For example, when Jane occurs three times and Claire occurs two time in the text, all the mentions of Jane or Claire will be grouped. Then, we choose one of the groups, and then sample one mention of the selected group.</p><p>To maintain the universal language representation ability in CorefBERT, we utilize both the MLM (masking random word) and MRP (masking mention reference) in the training process. Empirically, the masked words for MLM and MRP are sampled on a ratio of 4:1. Similar to BERT, 15% of the tokens are sampled for both masking strategies mentioned above, where 80% of them are replaced with a special token [MASK], 10% of them are replaced with random tokens, and 10% of them are unchanged. We also adopt whole word masking (WWM) <ref type="bibr">(Joshi et al., 2020)</ref>, which masks all the subwords belong to the masked words or mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Copy-based Training Objective</head><p>In order to capture the coreference information of the text, CorefBERT models the correlation among words in the sequence. Inspired by copy mechanism <ref type="bibr">(Gu et al., 2016;</ref><ref type="bibr" target="#b2">Cao et al., 2017)</ref> in sequence-to-sequence tasks, we introduce a copybased training objective to require the model to predict missing tokens of the masked mention by copying the unmasked tokens in the context. Since the masked tokens would be copied from context, lowfrequency tokens, such as proper nouns, could be well processed to some extent. Moreover, through copying mechanism, the CorefBERT model could explicitly capture the relations between the masked mention and its referring mentions, therefore, to obtain the coreference information in the context.</p><p>Formally, we first encode the given input sequence X = (x 1 , . . . , x n ) into hidden states H = (h 1 , . . . , h n ) via multi-layer Transformer <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref>. The probability of recovering the masked token x i by copying from x j is defined as:</p><formula xml:id="formula_1">Pr(x j |x i ) = exp((V h j ) T h i ) x k ∈X exp((V h k ) T h i ) ,<label>(2)</label></formula><p>where denotes element-wise product function and V is a trainable parameter to measure the importance of each dimension for token's similarity. Moreover, since we split a word into several word pieces as BERT does and we adopt whole word masking strategy for MRP, we need to extend our copy-based objective into word-level. To this end, we apply the token-level copy-based training objective on both start and end tokens of the masked word, because the representations of these two tokens could typically cover the major information of the whole word <ref type="bibr" target="#b10">(Lee et al., 2017;</ref><ref type="bibr">He et al., 2018)</ref>. For a masked noun w i consisting of a sequence of tokens (x (i) s , . . . , x (i) t ), we recover w i by copying its referring context word, and define the probability of choosing word w j as:</p><formula xml:id="formula_2">Pr(w j |w i ) = Pr(x (j) s |x (i) s ) × Pr(x (j) t |x (i) t ). (3)</formula><p>A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering <ref type="bibr">(Kadlec et al., 2016;</ref><ref type="bibr" target="#b35">Swayamdipta et al., 2018;</ref> designed to handle multiple answers. Finally, we define the loss of Mention Reference Prediction (MRP) as:</p><formula xml:id="formula_3">L MRP = − w i ∈M log w j ∈Cw i Pr(w j |w i ), (4)</formula><p>where M is the set of all masked mentions for mention reference masking, and C w i is the set of all corresponding words of word w i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we first introduce the training details of CorefBERT. After that, we present the finetuning results on a comprehensive suite of tasks, including extractive question answering, documentlevel relation extraction, fact extraction and verification, coreference resolution, and eight tasks in the GLUE benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>Since training CorefBERT from scratch would be time-consuming, we initialize the parameters of CorefBERT with BERT released by Google 3 , which is also used as our baselines on downstream tasks. Similar to previous language representation models <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr">Joshi et al., 2020)</ref>, we adopt English Wikipeida 4 as our training corpus, which contains about 3,000M tokens. We employ spaCy 5 for part-of-speech-tagging on the corpus. We train CorefBERT with contiguous sequences of up to 512 tokens, and randomly shorten the input sequences with 10% probability in training. To verify the effectiveness of our method for the language representation model trained with tremendous corpus, we also train CorefBERT initialized with RoBERTa 6 , referred as CorefRoBERTa. Additionally, we follow the pre-training hyper-parameters used in BERT, and adopt Adam optimizer <ref type="bibr">(Kingma and Ba, 2015)</ref> with batch size of 256. Learning rate of 5×10 −5 is used for the base model and 1×10 −5 is used for the large model. The optimization runs 33k steps, where the learning rate is warmed-up over the first 20% steps and then linearly decayed. The pre-training process consumes 1.5 days for base model and 11 days for large model with 8 RTX 2080 Ti GPUs in mixed precision. We search the ratio of MRP loss and MLM loss in 1:1, 1:2 and 2:1, and find the ratio of 1:1 achieves the best result. Beyond this, training details for downstream tasks are shown in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extractive Question Answering</head><p>Given a question and passage, the extractive question answering task aims to select spans in passage to answer the question. We first evaluate models on Questions Requiring Coreferential Reasoning dataset (QUOREF) <ref type="bibr">(Dasigi et al., 2019)</ref>. Compared to previous reading comprehension benchmarks, QUOREF is more challenging as 78% of the questions in QUOREF cannot be answered without coreference resolution. In this case, it can be an effective tool to examine the coreferential reasoning capability of question answering models.</p><p>We also adopt the MRQA, a dataset not specially designed for examining coreferential reasoning capability, which involves paragraphs from different sources and questions with manifold styles. Through MRQA, we hope to evaluate the performance of our model in various domains.</p><p>We use six benchmarks of MRQA, including SQuAD <ref type="bibr" target="#b26">(Rajpurkar et al., 2016)</ref>, NewsQA <ref type="bibr">(Trischler et al., 2017)</ref> achieves the best performance to date without pretraining; (2) QANet+BERT adopts BERT representation as an additional input feature into QANet;</p><p>(3) BERT (Devlin et al., 2019), simply fine-tunes BERT for extractive question answering. We further design two components accounting for coreferential reasoning and multiple answers, by which we obtain stronger BERT baselines; (4) RoBERTa-MT trains RoBERTa on CoLA, SST2, SQuAD datasets before on QUOREF. For MRQA, we compare CorefBERT to vanilla BERT with the same question answering framework.</p><p>Implementation Details Following BERT's setting (Devlin et al., 2019), given the question Q = (q 1 , q 2 , . . . , q m ) and the passage P = (p 1 , p 2 , . . . , p n ), we represent them as a sequence X = ([CLS], q 1 , q 2 , . . . , q m , [SEP], p 1 , p 2 , . . . , p n , [SEP]), feed the sequence X into the pre-trained encoder and train two classifiers on the top of it to seek answer's start and end positions simultaneously. For MRQA, CorefBERT maintains the same framework as BERT. For QUOREF, we further employ two extra components to process multiple mentions of the answers: (1) Spurred by the idea from <ref type="bibr">MTMSN (Hu et al., 2019)</ref> in handling the problem of multiple answer spans, we utilize the representation of [CLS] to predict the number of answers. After that, we first selects the answer span of the current highest scores, then continues to choose that of the second-highest score with no overlap to previous spans, until reaching the predicted answer number. (2) When answering a question from QUOREF, the relevant mention could possibly be a pronoun, so we attach a reasoning Transformer layer for pronoun resolution before the span boundary classifier.   <ref type="table" target="#tab_2">Table 2</ref> further shows that the effectiveness of CorefBERT is consistent in six datasets of the MRQA shared task besides QUOREF. Though the MRQA shared task is not designed for coreferential reasoning, CorefBERT still achieves averagely over 1% F1 improvement on all of the six datasets, especially on NewsQA and HotpotQA. In NewsQA, 20.7% of the answers can only be inferred by synthesizing information distributed across multiple sentences. In HotpotQA, 63% of the answers need to be inferred through either bridge entities or checking multiple properties in different positions. It demonstrates that coreferential reasoning is an essential ability in question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relation Extraction</head><p>Relation extraction (RE) aims to extract the relationship between two entities in a given text. We evaluate our model on DocRED <ref type="bibr" target="#b53">(Yao et al., 2019)</ref>, a challenging document-level RE dataset which requires the model to extract relations between entities by synthesizing information from all the mentions of them after reading the whole document. DocRED requires a variety of reasoning types, where 17.6% of the relational facts need to be uncovered through coreferential reasoning.  Baselines We compare our model with the following baselines for document-level relation extraction: (1) CNN / LSTM / BiLSTM / BERT. CNN <ref type="bibr" target="#b56">(Zeng et al., 2014)</ref>, LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiL-STM) <ref type="bibr" target="#b1">(Cai et al., 2016)</ref>, <ref type="bibr">BERT (Devlin et al., 2019)</ref> are widely adopted as text encoders in relation extraction tasks. With these encoders, <ref type="bibr" target="#b53">Yao et al. (2019)</ref> generates representations of entities for further predicting of the relationships between entities.</p><p>(2) ContextAware (Sorokin and Gurevych, 2017) takes relations' interaction into account, which demonstrates that other relations in the context are beneficial for target relation prediction. (3) BERT-TS <ref type="bibr" target="#b45">(Wang et al., 2019a</ref>) applies a two-step prediction to deal with the large number of irrelevant entities, which first predicts whether two entities have a relationship and then predicts the specific relation. (4) HinBERT <ref type="bibr" target="#b38">(Tang et al., 2020)</ref> proposes a hierarchical inference network to aggregate the inference information with different granularity.</p><p>Results <ref type="table" target="#tab_4">Table 3</ref> shows the performance on Do-cRED. The BERT BASE model we implemented with mean-pooling entity representation and hyperpa-rameter tuning 7 performed better than previous RE models with BERT BASE size, which provides a stronger baseline. CorefBERT BASE outperforms BERT BASE model by 0.7% F1. CorefBERT LARGE beats BERT LARGE by 0.5% F1. We also show a case study in the appendix, which further proves that considering coreference information of text is helpful for exacting relational facts from documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Fact Extraction and Verification</head><p>Fact extraction and verification aim to verify deliberately fabricated claims with trust-worthy corpora. We evaluate our model on a large-scale public fact verification dataset FEVER <ref type="bibr" target="#b39">(Thorne et al., 2018)</ref>. FEVER consists of 185, 455 annotated claims with all Wikipedia documents.</p><p>Baselines We compare our model with four BERT-based fact verification models: <ref type="formula">(1)</ref>  Results <ref type="table" target="#tab_6">Table 4</ref> shows the performance on FEVER. KGAT with CorefBERT BASE outperforms KGAT with BERT BASE by 0.4% FEVER score. KGAT with CorefRoBERTa LARGE gains 1.9% FEVER score improvement compared to the model with RoBERTa LARGE , and arrives at a new state-ofthe-art on FEVER benchmark. It again demonstrates the effectiveness of our model. Coref-BERT, which incorporates coreference information in distant-supervised pre-training, contributes to verify if the claim and evidence discuss about the same mentions, such as a person or an object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Coreference Resolution</head><p>Coreference resolution aims to link referring expressions that evoke the same discourse entity. We examine models' coreference resolution ability under the setting that all mentions have been detected. We evaluate models on several widely-used datasets, including GAP <ref type="bibr" target="#b48">(Webster et al., 2018)</ref>, DPR <ref type="bibr" target="#b25">(Rahman and Ng, 2012)</ref>, WSC <ref type="bibr" target="#b11">(Levesque, 2011)</ref>, Winogender <ref type="bibr" target="#b27">(Rudinger et al., 2018)</ref> and 7 Details are in the appendix due to space limit.   PDP <ref type="bibr">(Davis et al., 2017)</ref>. These datasets provide two sentences where the former has two or more mentions and the latter contains an ambiguous pronoun. It is required that the ambiguous pronoun should be connected to the right mention.</p><p>Baselines We compare our model with two coreference resolution models: (1) BERT-LM (Trinh and <ref type="bibr" target="#b55">Le, 2018)</ref>    Results <ref type="table" target="#tab_7">Table 5</ref> shows the performance on the test set of the above coreference resolution dataset. Our CorefBERT model significantly outperforms BERT-LM, which demonstrates that the intrinsic coreference resolution ability of CorefBERT has been enhanced by involving the mention reference prediction training task. Moreover, it achieves comparable performance with state-of-the-art baseline WikiCREM. Note that, WikiCREM is specially designed for sentence-level coreference resolution and is not suitable for other NLP tasks. On the contrary, the coreferential reasoning capability of CorefBERT can be transferred to other NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">GLUE</head><p>The Generalized Language Understanding Evaluation dataset (GLUE) <ref type="bibr" target="#b44">(Wang et al., 2018)</ref> is designed to evaluate and analyze the performance of models across a diverse range of existing natural language understanding tasks. We evaluate CorefBERT on the main GLUE benchmark used in BERT.</p><p>Implementation Details Following BERT's setting, we add [CLS] token in front of the input sentences, and extract its representation on the top layer as the whole sentence or sentence pair's representation for classification or regression.</p><p>Results <ref type="table" target="#tab_9">Table 6</ref> shows the performance on GLUE. We notice that CorefBERT achieves comparable results to BERT. Though GLUE does not require much coreference resolution ability due to its attributes, the results prove that our masking strategy and auxiliary training objective would not weaken the performance on generalized language understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head><p>In this section, we explore the effects of the Whole Word Masking (WWM), Mention Reference Masking (MRM), Next Sentence Prediction (NSP) and copy-based training objective using several benchmark datasets. We continue to train Google's released BERT BASE on the same Wikipedia corpus with different strategies. As shown in <ref type="table" target="#tab_10">Table 7</ref>, we have the following observations: (1) Deleting NSP training task triggers a better performance on almost all tasks. The conclusion is consistent with that of RoBERTa ; (2) MRM scheme usually achieves parity with WWM scheme except on SearchQA, and both of them outperform the original subword masking scheme especially on NewsQA (averagely +1.7% F1) and TriviaQA (averagely +1.5% F1); (3) On the basis of MRM scheme, our copy-based training objective explicitly requires model to look for mention's referents in the context, which could adequately consider the coreference information of the sequence. Coref-BERT takes advantage of the objective and further improves the performance, with a substantial gain (+2.3% F1) on QUOREF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we present a language representation model named CorefBERT, which is trained on a novel task, Mention Reference Prediction (MRP), for strengthening the coreferential reasoning ability of BERT. Experimental results on several downstream NLP tasks show that our CorefBERT significantly outperforms BERT by considering the coreference information within the text and even improve the performance of the strong RoBERTa model. In the future, there are several prospective research directions: (1) We introduce a distant supervision (DS) assumption in our MRP training task. However, the automatic labeling mechanism inevitably accompanies with the wrong labeling problem and it is still an open problem to mitigate the noise.</p><p>(2) The DS assumption does not consider pronouns in the text, while pronouns play an important role in coreferential reasoning. Hence, it is worth developing a novel strategy such as selfsupervised learning to further consider the pronoun. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Masked Language Modeling (MLM)</head><p>MLM is regarded as a kind of cloze tasks and aims to predict the missing tokens according to its contextual representation. In our work, 15% of the tokens in input sequence are sampled as the missing tokens. Among them, 80% are replaced with a special token [MASK], 10% are replaced with random tokens and 10% are unchanged. The task aims to predict original tokens from corrupted input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Leaderboard Results on QUOREF</head><p>TASE (Efrat et al., 2020) converts the multi-span prediction problem as a sequence tagging problem, which substantially improves the model's ability in terms of handling multi-span answer. Though the study of TASE and our CorefBERT are conducted in the same period, we still run TASE with CorefRoBERTa encoder. As      <ref type="bibr" target="#b53">(Yao et al., 2019)</ref>. We show some relational facts detected by CorefBERT BASE but missed by BERT BASE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Case Study on QUOREF</head><p>Noreen. Reasoning over the above information, we could know that Noreen's uncle trains the asthmatic boy. For the second example, it needs to infer that Tippett is a composer from the second sentence for obtaining the final answer from the first sentence. After training on the mention reference prediction task, CorefBERT has become capable of reasoning over these mentions, summarizing messages from mentions in different positions, and finally figuring out the correct answer. For the third and fourth examples, it is necessary to know she refers to Elena, and he refers to Ector by respective coreference resolution. Benefiting from a large amount of distant-supervised coreference resolution training data, CorefBERT successfully finds out the reference relationship and provides accurate answers. <ref type="table" target="#tab_0">Table 10</ref> shows an example from DocRED <ref type="bibr" target="#b53">(Yao et al., 2019)</ref>. We show some relational facts detected by CorefBERT BASE but missed by BERT BASE . For the first relational fact, it is necessary to connect the first and the third sentences through the co-Claim: Bob Ross created ABC drama The Joy of Painting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Case Study on DocRED</head><p>[1] [Bob Ross] Robert Norman Ross was an American painter and television host.</p><p>[2] [Bob Ross] He was the creator and host of The Joy of Painting, an instructional television program that aired from 1983 to 1994 on PBS in the United States, and also aired in Canada, ...</p><p>[3] [Bob Ross] The Joy of Painting is an American half hour instructional television show hosted by painter Bob Ross which ran from <ref type="bibr">January 11, 1983</ref><ref type="bibr">, until May 17, 1994</ref> [The Joy of Painting] In each episode, Ross taught techniques for landscape oil painting, completing a painting in each session.</p><p>[5] [The Joy of Painting] The program followed the same format as its predecessor, The Magic of Oil Painting , hosted by Ross's mentor. reference of Eclipse for acquiring the fact that New Moon and Breaking Dawn are also the novel in the Twilight Saga. For the second and the third relational fact, the referring expressions it, the novel, and the book should be linked to Eclipse correctly to increase model's confidence to find out all the characters and the publication date of the novel from the context. CorefBERT considers coreference information of text, which helps to discover relation facts beyond sentence boundary. <ref type="table" target="#tab_0">Table 11</ref> shows an example from FEVER <ref type="bibr" target="#b39">(Thorne et al., 2018)</ref>. The given claim is fabricated since the drama "The Joy of Painting" was aired on PBS instead of ABC. With the CorefBERT encoder, KGAT <ref type="bibr" target="#b15">(Liu et al., 2020b)</ref> could propagate and aggregate the entity information from evidence for refuting the wrong claim more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label: REFUTES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Case Study on FEVER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Task-Specific Model Details</head><p>All the models are implemented based on Huggingface transformers 8 . We train models on down-stream tasks with Adam optimizer (Kingma and Ba, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Question Answering (QA)</head><p>For QA models, we uses a batch size of 32 instances with a maximum sequence length of 512.</p><p>We adopt the official data split for QUOREF <ref type="bibr">(Dasigi et al., 2019)</ref>, where train / development / test set contains 19399 / 2418 / 2537 instances respectively. And we submit our model to the test sever 9 for online evaluation. We conduct a grid search on the learning rate (lr) in [1 × 10 −5 , 2 × 10 −5 , 3 × 10 −5 ] and epoch number in <ref type="bibr">[2,</ref><ref type="bibr">4,</ref><ref type="bibr">6]</ref>. The best BERT BASE configuration on development set used lr = 2 × 10 −5 , 6 epochs. We adopt this configuration for the BERT LARGE and RoBERTa LARGE models. We regard MRQA <ref type="bibr">(Fisch et al., 2019)</ref> as a testbed to examine whether models can answer questions well across various data distributions. For fair comparison, we keep lr = 3 × 10 −5 , 2 epochs for all of the MRQA experiments.</p><p>For TASE <ref type="bibr">(Efrat et al., 2020)</ref> with Core-fRoBERTa encoder, we keep the same configuration 10 as that of the original paper, which used a batch size of 12, learning rate of 5 × 10 −6 , 35 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Document-level Relation Extraction</head><p>We modify the official code 11 to implement BERTbased models for DocRED <ref type="bibr" target="#b53">(Yao et al., 2019)</ref>. In our implementation, the representation of a mention, which consists of several words, is the average of representations of those words. Furthermore, the representation of an entity is defined as the mean of all mentions referring to it. Finally, two entities' representations are fed to a bi-linear layer to predict relations between them.</p><p>We use the official data split for DocRED, where train / development / test set consists of 3053 / 1000 / 1000 documents respectively. We adopt batch size of 32 instances with maximum sequence length of 512 and conduct a grid search on the learning rate in [2 × 10 −5 , 3 × 10 −5 , 4 × 10 −5 , 5 × 10 −5 ] and number epochs in <ref type="bibr">[100,</ref><ref type="bibr">150,</ref><ref type="bibr">200]</ref>. We find the configuration used learning rate of 4 × 10 −5 , 200 epochs is best for both the base and the large model. We evaluate models on development set every 5 epochs and save the checkpoint with the highest F1 score. After that, the test results of the best model are submitted to the evaluation server 12 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Fact Extraction and Verification</head><p>We apply the released code 13 of KGAT <ref type="bibr" target="#b15">(Liu et al., 2020b)</ref> for evaluating CorefBERT. We use the official data split for FEVER <ref type="bibr" target="#b39">(Thorne et al., 2018)</ref>, where train / development / test set contains 145449 / 19998 / 19998 claims respectively. We adopt a batch size of 32, maximum length of 512 tokens and search the learning rate in [2 × 10 −5 , 3 × 10 −5 , 5×10 −5 ]. We achieved the best performance with learning rate of 5 × 10 −5 for the base model and 2 × 10 −5 for the large model. All models are trained with a batch size of 32 instances for 3 epochs and evaluated on development set every 1000 steps. After that, we submit test results of our best model to evaluation server 14 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Coreference Resolution</head><p>We use the released code 15 of WikiCREM (Kocijan et al., 2019) for fine-tuning BERT-LM (Trinh and <ref type="bibr" target="#b55">Le, 2018)</ref> and CorefBERT on supervised datasets. For a sentence S, which possesses a correct candidate a and an incorrect candidate b, the loss consists of two parts: (1) the negative log-likelihood of the correct candidate; (2) a max-margin between the log-likelihood of the correct candidate and the incorrect candidate: L = − log Pr(a|S) + α max (0, log Pr(b|S) − log Pr(a|S) + β) ,</p><p>where α, β are hyperparameters. We follow the data split and fine-tuning setting of WikiCREM, which adopts a batch size of 64, a maximum sequence length of 128 and 10 epochs training. We search the learning rate lr ∈ [3 × 10 −5 , 1 × 10 −5 , 5 × 10 −6 , 3 × 10 −6 ], hyperparameters α ∈ [5, 10, 20], β ∈ [0.1, 0.2, 0.4]. The best performance of models with base size and CorefBERT LARGE on validation set were achieved with lr = 3 × 10 −5 , α = 10, β = 0.2. We keep this configuration for the RoBERTa-based models.</p><p>Model MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE CorefBERTBASE 2 × 10 −5 4 × 10 −5 3 × 10 −5 3 × 10 −5 5 × 10 −5 4 × 10 −5 5 × 10 −5 4 × 10 −5 CorefBERTLARGE 2 × 10 −5 2 × 10 −5 2 × 10 −5 2 × 10 −5 3 × 10 −5 5 × 10 −5 5 × 10 −5 3 × 10 −5   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Generalized Language Understanding (GLUE)</head><p>We evaluate CorefBERT on the main GLUE benchmark <ref type="bibr" target="#b44">(Wang et al., 2018)</ref>  We use a batch size of 32, maximum sequence length of 128, fine-tune models for 3 epochs for all GLUE tasks and select the learning rate of Adam among [2×10 −5 , 3×10 −5 , 4×10 −5 , 5×10 −5 ] for the best performance on the development set. After that, we submit the result of our best model to the official evaluation server 17 . <ref type="table" target="#tab_0">Table 12</ref> shows the best learning rate for CorefBERT BASE and CorefBERT LARGE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.6 Number of Parameters and Average Runtime</head><p>CorefBERT's architecture is a multi-layer bidirectional Transformer <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref>. Tables 13 shows the parameter number of Coref-BERTs with different model size. Compared to <ref type="bibr">BERT (Devlin et al., 2019)</ref>, CorefBERT add a few parameters for computing the copy-based objective. Hence, CorefBERT keeps similar number of 16 https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs 17 https://gluebenchmark.com parameters as BERT with the same size. <ref type="table" target="#tab_0">Table 14</ref> shows the task-specific average inference runtime per example for CorefBERT. The inferenece is done on a RTX 2080ti GPU with a batch of 32 instances. The inference time includes time on CPU and time on GPU. CorefRoBERTa LARGE consumes a similar time as CorefBERT LARGE since they both use a 24-layer Transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.7 Resolving the Coreference in the Corpus</head><p>In our preliminary experiment, we resolve the coreference of training corpus via the StanfordNLP tool 18 and apply our copy-based objective on this training corpus. We find the obtained model performs better than the BERT model without NSP but worse than the current CorefBERT. We think that considering coreference such as pronoun in pre-training could also enhance model's coreferential reasoning ability, while how to deal with the noise from coreference tools remains a problem to be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>BERT Concat (Zhou et al., 2019) concatenates all of the evidence pieces and the claim to predict the claim label; (2) SR-MRS (Nie et al., 2019) employs hierarchical BERT retrieval to improve the performance; (3) GEAR (Zhou et al., 2019) constructs an evidence graph and conducts a graph attention network for jointly reasoning over several evidence pieces; (4) KGAT (Liu et al., 2020b) conducts a fine-grained graph attention network with kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>used in BERT, including MNLI (Williams et al., 2018), QQP 16 , QNLI (Rajpurkar et al., 2016), SST-2 (Socher et al., 2013), CoLA (Warstadt et al., 2019) , STS-B (Cer et al., 2017), MRPC (Dolan and Brockett, 2005) and RTE (Giampiccolo et al., 2007).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>, SearchQA(Dunn  et al., 2017),TriviaQA (Joshi et al., 2017), Hot-potQA<ref type="bibr" target="#b52">(Yang et al., 2018)</ref>, and Natural Questions (NaturalQA)(Kwiatkowski et al., 2019). Since MRQA does not provide a public test set, we randomly split the development set into two halves to generate new validation and test sets. 34.41 38.26 34.17 38.90 QANet+BERTBASE * 43.09 47.38 42.41 47.20 BERTBASE * 58.44 64.95 59.28 66.39 BERTBASE 61.29 67.25 61.37 68.56 CorefBERTBase 66.87 72.27 66.22 72.96 Results on QUOREF measured by exact match (EM) and F1. Results with * , + are from Dasigi et al. (2019) and official leaderboard respectively.</figDesc><table><row><cell>Baselines For QUOREF, we compare our Coref-</cell></row><row><cell>BERT with four baseline models: (1) QANet (Yu</cell></row><row><cell>et al., 2018) combines self-attention mechanism</cell></row><row><cell>with the convolutional neural network, which</cell></row><row><cell>6 https://github.com/pytorch/fairseq</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance (F1) on six MRQA extractive question answering benchmarks. Results Table 1 shows the performance on QUO-ERF. Our adapted BERT BASE surpasses original BERT by about 2% in EM and F1 score, indicating the effectiveness of the added reasoning layer and multi-span prediction module. CorefBERT BASE and CorefBERT LARGE exceeds our adapted BERT BASE and BERT LARGE by 4.4% and 2.9% F1 respectively. Leaderboard results are shown in the appendix. Based on the TASE framework (Efrat et al., 2020), the model with CorefRoBERTa achieves a new state-of-the-art with about 1% EM improvement compared to the model with RoBERTa. We also show four case studies in the appendix, which indicate that through reasoning over mentions, Coref-BERT could aggregate information to answer the question requiring coreferential reasoning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on DocRED measured by micro ignore F1 (IgnF1) and micro F1. IgnF1 metrics ignores the relational facts shared by the training and dev/test sets. Results with * , + , # are from Yao et al. (2019), Wang et al. (2019a), and Tang et al. (2020) respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: Results on FEVER test set measured by label</cell></row><row><cell cols="5">accuracy (LA) and FEVER. The FEVER score evalu-</cell></row><row><cell cols="5">ates the model performance and considers whether the</cell></row><row><cell cols="5">golden evidence is provided. Results with  *  , + , # are</cell></row><row><cell cols="5">from Zhou et al. (2019), Nie et al. (2019) and Liu et al.</cell></row><row><cell>(2020b) respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">GAP DPR WSC WG PDP</cell></row><row><cell>BERT-LMBASE</cell><cell>75.3</cell><cell>75.4</cell><cell>61.2</cell><cell>68.3 76.7</cell></row><row><cell>CorefBERTBASE</cell><cell>75.7</cell><cell>76.4</cell><cell>64.1</cell><cell>70.8 80.0</cell></row><row><cell>BERT-LMLARGE  *</cell><cell>76.0</cell><cell>80.1</cell><cell>70.0</cell><cell>78.8 81.7</cell></row><row><cell>WikiCREMLARGE  *</cell><cell>78.0</cell><cell>84.8</cell><cell>70.0</cell><cell>76.7 86.7</cell></row><row><cell>CorefBERTLARGE</cell><cell>76.8</cell><cell>85.1</cell><cell>71.4</cell><cell>80.8 90.0</cell></row><row><cell>RoBERTa-LMLARGE</cell><cell>77.8</cell><cell>90.6</cell><cell>83.2</cell><cell>77.1 93.3</cell></row><row><cell>CorefRoBERTaLARGE</cell><cell>77.8</cell><cell>92.2</cell><cell>83.2</cell><cell>77.9 95.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Results on coreference resolution test sets. Per- formance on GAP is measured by F1, while scores on the others are given in accuracy. WG: Winogender. Re- sults with* are from Kocijan et al. (2019).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Test set performance metrics on GLUE benchmarks. Matched/mistached accuracies are reported for MNLI; F1 scores are reported for QQP and MRPC, Spearmanr correlation is reported for STS-B; Accuracy scores are reported for the other tasks.</figDesc><table><row><cell>Model</cell><cell cols="8">QUOREF SQuAD NewsQA TriviaQA SearchQA HotpotQA NaturalQA DocRED</cell></row><row><cell>BERTBASE</cell><cell>67.3</cell><cell>88.4</cell><cell>66.9</cell><cell>68.8</cell><cell>78.5</cell><cell>74.2</cell><cell>75.6</cell><cell>56.8</cell></row><row><cell>-NSP</cell><cell>70.6</cell><cell>88.7</cell><cell>67.5</cell><cell>68.9</cell><cell>79.4</cell><cell>75.2</cell><cell>75.4</cell><cell>56.7</cell></row><row><cell>-NSP, +WWM</cell><cell>70.1</cell><cell>88.3</cell><cell>69.2</cell><cell>70.5</cell><cell>79.7</cell><cell>75.5</cell><cell>75.2</cell><cell>57.1</cell></row><row><cell>-NSP, +MRM</cell><cell>70.0</cell><cell>88.5</cell><cell>69.2</cell><cell>70.2</cell><cell>78.6</cell><cell>75.8</cell><cell>74.8</cell><cell>57.1</cell></row><row><cell>CorefBERTBASE</cell><cell>72.3</cell><cell>89.0</cell><cell>69.5</cell><cell>70.7</cell><cell>79.6</cell><cell>76.3</cell><cell>77.7</cell><cell>57.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Ablation study. Results are F1 scores on development set for QUOREF and DocRED, and on test set for</figDesc><table /><note>others. CorefBERT BASE combines "-NSP, +MRM" scheme and copy-based training objective.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">shows, the per-</cell></row><row><cell cols="3">formance of TASE with CorefRoBERTa encoder</cell></row><row><cell cols="3">gains about 1% EM improvement compared to that</cell></row><row><cell cols="3">with RoBERTa encoder, which demonstrates the</cell></row><row><cell cols="3">effectiveness of CorefBERT for different question</cell></row><row><cell>answering frameworks.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>EM</cell><cell>F1</cell></row><row><cell>XLNet (Dasigi et al., 2019)</cell><cell cols="2">61.88 71.51</cell></row><row><cell>RoBERTa-MT</cell><cell cols="2">72.61 80.68</cell></row><row><cell>CorefRoBERTaLARGE</cell><cell cols="2">75.80 82.81</cell></row><row><cell cols="3">TASE (RoBERTa) (Efrat et al., 2020) 79.66 86.13</cell></row><row><cell>TASE (CorefRoBERTa)</cell><cell cols="2">80.61 86.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Leaderboard results on QUOREF test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9</head><label>9</label><figDesc>Barry Gabrewski is an asthmatic boy ... [2] Barry wants to learn the martial arts, but is rejected by the arrogant dojo owner Kelly Stone for being too weak. [3] Instead, he is taken on as a student by an old Chinese man called Mr. Lee, Noreen's sly uncle. [4] Mr. William wears Ector's armour to impersonate him, winning the tournament and taking the prize.</figDesc><table><row><cell>shows examples from QUOREF (Dasigi</cell></row><row><cell>et al., 2019). For the first example, it is essential to</cell></row><row><cell>obtain the fact that the asthmatic boy in question</cell></row><row><cell>refers to Barry. After that, we should synthesize</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Examples from QUOREEF (Dasigi et al., 2019) that were correctly predicted by CorefBERT BASE , but wrongly predicted by BERT BASE . Answers from BERT BASE , Answers from CorefBERT BASE , and Clue are colored respectively. Eclipse is the third novel in the Twilight Saga by Stephenie Meyer. It continues the story of Bella Swan and her vampire love, Edward Cullen. [2] The novel explores Bella's compromise between her love for Edward and her friendship with shape-shifter Jacob Black, ... [3]Eclipse is preceded by New Moon and followed by Breaking Dawn.[4] The book was released on August 7, 2007, with an initial print run of one million copies, and sold more than 150,000 copies in the first 24 hours alone.</figDesc><table><row><cell>information from two Mr. Lee's mentions: (1)</cell></row><row><cell>Mr. Lee trains Barray; (2) Mr. Lee is the uncle of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>An example from DocRED</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>An example from FEVER (Thorne et al., 2018). Five pieces of evidence from article [Bob Ross] and [The Joy of Painting] are retrieved by the retriever.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Learning rate for CorefBERT on GLUE benchmarks.</figDesc><table><row><cell>Model</cell><cell cols="5">Parameters Layers Hidden Embedding Vocabulary</cell></row><row><cell>CorefBERTBASE</cell><cell>110M</cell><cell>12</cell><cell>768</cell><cell>768</cell><cell>28,996</cell></row><row><cell>CorefBERTLARGE</cell><cell>340M</cell><cell>24</cell><cell>1,024</cell><cell>1,024</cell><cell>28,996</cell></row><row><cell>CorefRoBERTaLARGE</cell><cell>355M</cell><cell>24</cell><cell>1,024</cell><cell>1,024</cell><cell>50,265</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Parameter number and the configuration of CorefBERT.</figDesc><table><row><cell>Model</cell><cell cols="6">QUOREF MRQA DocRED FEVER GLUE Coref.</cell></row><row><cell>CorefBERTBASE</cell><cell>13.23</cell><cell>13.15</cell><cell>117.37</cell><cell>18.88</cell><cell>2.95</cell><cell>4.27</cell></row><row><cell>CorefBERTLARGE</cell><cell>43.40</cell><cell>43.37</cell><cell>180.65</cell><cell>54.03</cell><cell>9.22</cell><cell>10.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Average inference runtime per example for CorefBERTs on different benchmarks. Inference is done on a RTX 2080ti GPU with a batch of 32 instances and inference time is measured in milliseconds. The input sequence length is 512 for QUOREF, MRQA, DocRED, FEVER, and 128 for others. Coref.: Coreference resolution.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Details of MLM are in the appendix due to space limit. 2 In this paper, tokens are at the subword level.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/google-research/bert 4 https://en.wikipedia.org 5 https://spacy.io</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/huggingface/transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://leaderboard.allenai.org/quoref/submissions/public 10 https://github.com/eladsegal/tag-based-multi-spanextraction 11 https://github.com/thunlp/DocRED</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://competitions.codalab.org/competitions/20717 13 https://github.com/thunlp/KernelGAT 14 https://competitions.codalab.org/competitions/18814 15 https://github.com/vid-koci/bert-commonsense</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">https://stanfordnlp.github.io/CoreNLP</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonparametric bayesian models for unsupervised event coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Cosmin Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Titsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-12" />
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint copying and restricted generation for paraphrase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="3152" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attending to entities for better text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7554" to="7561" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1078</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="845" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="7057" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research. TACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<idno>SS-11-06</idno>
	</analytic>
	<monogr>
		<title level="m">Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium</title>
		<meeting><address><addrLine>Stanford, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-03-21" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Haotang Deng, and Ping Wang. 2020a. K-BERT: enabling language representation with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>RoBERTa: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fine-grained fact verification with kernel graph attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7342" to="7351" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised ranking model for entity coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="1012" to="1018" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting held</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revealing the importance of semantic retrieval for machine reading at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1258</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="2553" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernández</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1144</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Long Papers. The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>NAACL-. Long Papers</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Resolving complex cases of definite pronouns: The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-12" />
			<biblScope unit="page" from="777" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SQuAD: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA; Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MASS: masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contextaware representations for knowledge base relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="1784" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">VL-BERT: pretraining of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00756</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="7463" to="7472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-mention learning for reading comprehension with neural cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MultiQA: An empirical investigation of generalization and transfer in reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4911" to="4921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LXMERT: learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="5099" to="5110" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">HIN: hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-47426-3_16</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining -24th Pacific-Asia Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-05-11" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<pubPlace>Justin Harris, Alessandro Sordoni, Philip Bachman, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the Workshop: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Fine-tune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Denoising based sequenceto-sequence pre-training for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1412</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="4001" to="4013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bowman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural network acceptability judgments. TACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mind the GAP: A balanced corpus of gendered ambiguous pronouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="605" to="617" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Bowman. Long Papers</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discourse-aware neural extractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5021" to="5031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy; Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">ERNIE: enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuailiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9628" to="9635" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
