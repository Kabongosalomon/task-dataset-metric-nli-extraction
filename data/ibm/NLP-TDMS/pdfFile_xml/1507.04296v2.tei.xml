<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Massively Parallel Methods for Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Nair</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Blackwell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagdas</forename><surname>Alcicek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rory</forename><surname>Fearon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>De Maria</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedavyas</forename><surname>Panneershelvam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">{ARUNSNAIR, PRAV</orgName>
								<orgName type="department" key="dep2">DAVIDSILVER @GOOGLE.COM } Google DeepMind</orgName>
								<orgName type="institution" key="instit1">BLACKWELLS</orgName>
								<orgName type="institution" key="instit2">CAGDASALCICEK, RORYF</orgName>
								<orgName type="institution" key="instit3">ADEMARIA</orgName>
								<orgName type="institution" key="instit4">MUSTAFASUL</orgName>
								<orgName type="institution" key="instit5">CBEATTIE</orgName>
								<orgName type="institution" key="instit6">SVP, LEGG</orgName>
								<orgName type="institution" key="instit7">VMNIH</orgName>
								<orgName type="institution" key="instit8">KORAYK</orgName>
								<address>
									<settlement>London</settlement>
									<region>DARTHVEDA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Massively Parallel Methods for Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN) <ref type="bibr" target="#b11">(Mnih et al., 2013)</ref>. Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning methods have recently achieved state-ofthe-art results in vision and speech domains <ref type="bibr" target="#b7">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b14">Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b16">Szegedy et al., 2014;</ref><ref type="bibr" target="#b5">Graves et al., 2013;</ref><ref type="bibr" target="#b2">Dahl et al., 2012)</ref>, mainly due to their ability to automatically learn high-level features from a supervised signal. Recent advances in reinforcement learning (RL) have successfully combined deep learning with value function approximation, by using a deep convolutional neural network to represent the action-value (Q) function <ref type="bibr" target="#b11">(Mnih et al., 2013)</ref>. Specifically, a new method for training such deep Q-networks, known as DQN, has enabled RL to learn control policies in complex environments with high dimensional images as inputs <ref type="bibr" target="#b12">(Mnih et al., 2015)</ref>. This method outperformed a human professional in many Presented at the Deep Learning Workshop, International Conference on Machine Learning, <ref type="bibr">Lille, France, 2015.</ref> games on the Atari 2600 platform, using the same network architecture and hyper-parameters. However, DQN has only previously been applied to single-machine architectures, in practice leading to long training times. For example, it took 12-14 days on a GPU to train the DQN algorithm on a single Atari game <ref type="bibr" target="#b12">(Mnih et al., 2015)</ref>. In this work, our goal is to build a distributed architecture that enables us to scale up deep reinforcement learning algorithms such as DQN by exploiting massive computational resources.</p><p>One of the main advantages of deep learning is that computation can be easily parallelized. In order to exploit this scalability, deep learning algorithms have made extensive use of hardware advances such as GPUs. However, recent approaches have focused on massively distributed architectures that can learn from more data in parallel and therefore outperform training on a single machine <ref type="bibr" target="#b1">(Coates et al., 2013;</ref><ref type="bibr" target="#b3">Dean et al., 2012)</ref>. For example, the DistBelief framework <ref type="bibr" target="#b3">(Dean et al., 2012)</ref> distributes the neural network parameters across many machines, and parallelizes the training by using asynchronous stochastic gradient <ref type="bibr">descent (ASGD)</ref>. DistBelief has been used to achieve stateof-the-art results in several domains <ref type="bibr" target="#b16">(Szegedy et al., 2014)</ref> and has been shown to be much faster than single GPU training <ref type="bibr" target="#b3">(Dean et al., 2012)</ref>.</p><p>Existing work on distributed deep learning has focused exclusively on supervised and unsupervised learning. In this paper we develop a new architecture for the reinforcement learning paradigm. This architecture consists of four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed experience replay memory.</p><p>A unique property of RL is that an agent influences the training data distribution by interacting with its environment. In order to generate more data, we deploy multiple agents running in parallel that interact with multiple arXiv:1507.04296v2 <ref type="bibr">[cs.</ref>LG] 16 Jul 2015 instances of the same environment. Each such actor can store its own record of past experience, effectively providing a distributed experience replay memory with vastly increased capacity compared to a single machine implementation. Alternatively this experience can be explicitly aggregated into a distributed database. In addition to generating more data, distributed actors can explore the state space more effectively, as each actor behaves according to a slightly different policy.</p><p>A conceptually distinct set of distributed learners reads samples of stored experience from the experience replay memory, and updates the value function or policy according to a given RL algorithm. Specifically, we focus in this paper on a variant of the DQN algorithm, which applies ASGD updates to the parameters of the Q-network. As in DistBelief, the parameters of the Q-network may also be distributed over many machines.</p><p>We applied our distributed framework for RL, known as Gorila (General Reinforcement Learning Architecture), to create a massively distributed version of the DQN algorithm. We applied Gorila DQN to 49 games on the Atari 2600 platform. We outperformed single GPU DQN on 41 games and outperformed human professional on 25 games. Gorila DQN also trained much faster than the nondistributed version in terms of wall-time, reaching the performance of single GPU DQN roughly ten times faster for most games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There have been several previous approaches to parallel or distributed RL. A significant part of this work has focused on distributed multi-agent systems <ref type="bibr" target="#b18">(Weiss, 1995;</ref><ref type="bibr" target="#b8">Lauer &amp; Riedmiller, 2000)</ref>. In this approach, there are many agents taking actions within a single shared environment, working cooperatively to achieve a common objective. While computation is distributed in the sense of decentralized control, these algorithms focus on effective teamwork and emergent group behaviors. Another paradigm which has been explored is concurrent reinforcement learning , in which an agent can interact in parallel with an inherently distributed environment, e.g. to optimize interactions with multiple users on the internet. Our goal is quite different to both these distributed and concurrent RL paradigms: we simply seek to solve a single-agent problem more efficiently by exploiting parallel computation.</p><p>The MapReduce framework has been applied to standard MDP solution methods such as policy evaluation, policy iteration and value iteration, by distributing the computation involved in large matrix multiplications <ref type="bibr" target="#b9">(Li &amp; Schuurmans, 2011)</ref>. However, this work is narrowly focused on batch methods for linear function approximation, and is not immediately applicable to non-linear representations using online reinforcement learning in environments with unknown dynamics.</p><p>Perhaps the closest prior work to our own is a parallelization of the canonical Sarsa algorithm over multiple machines. Each machine has its own instance of the agent and environment <ref type="bibr" target="#b6">(Grounds &amp; Kudenko, 2008)</ref>, running a simple reinforcement learning algorithm (linear Sarsa, in this case). The changes to the parameters of the linear function approximator are periodically communicated using a peerto-peer mechanism, focusing especially on those parameters that have changed most. In contrast, our architecture allows for client-server communication and a separation between acting, learning and parameter updates; furthermore we exploit much richer function approximators using a distributed framework for deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DistBelief</head><p>DistBelief <ref type="bibr" target="#b3">(Dean et al., 2012</ref>) is a distributed system for training large neural networks on massive amounts of data efficiently by using two types of parallelism. Model parallelism, where different machines are responsible for storing and training different parts of the model, is used to allow efficient training of models much larger than what is feasible on a single machine or GPU. Data parallelism, where multiple copies or replicas of each model are trained on different parts of the data in parallel, allows for more efficient training on massive datasets than a single process. We briefly discuss the two main components of the DistBelief architecture -the central parameter server and the model replicas.</p><p>The central parameter server holds the master copy of the model. The job of the parameter server is to apply the incoming gradients from the replicas to the model and, when requested, to send its latest copy of the model to the replicas. The parameter server can be sharded across many machines and different shards apply gradients independently of other shards.</p><p>Each replica maintains a copy of the model being trained. This copy could be sharded across multiple machines if, for example, the model is too big to fit on a single machine. The job of the replicas is to calculate the gradients given a mini-batch, send them to the parameter server, and to periodically query the parameter server for an updated version of the model. The replicas send gradients and request updated parameters independently of each other and hence may not be synced to the same parameters at any given time.  <ref type="figure">Figure 1</ref>. The DQN algorithm is composed of three main components, the Q-network (Q(s, a; θ)) that defines the behavior policy, the target Q-network (Q(s, a; θ − )) that is used to generate target Q values for the DQN loss term and the replay memory that the agent uses to sample random transitions for training the Q-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Reinforcement Learning</head><p>In the reinforcement learning (RL) paradigm, the agent interacts sequentially with an environment, with the goal of maximising cumulative rewards. At each step t the agent observes state s t , selects an action a t , and receives a reward r t . The agent's policy π(a|s) maps states to actions and defines its behavior. The goal of an RL agent is to maximize its expected total reward, where the rewards are discounted by a factor γ ∈ [0, 1] per time-step. Specifically, the return at time t is R t = T t =t γ t −t r t where T is the step when the episode terminates. The action-value function Q π (s, a) is the expected return after observing state s t and taking an action under a policy π, Q π (s, a) = E [R t |s t = s, a t = a, π], and the optimal action-value function is the maximum possible value that can be achieved by any policy, Q * (s, a) = argmax π Q π (s, a). The action-value function obeys a fundamental recursion known as the Bellman equation,</p><formula xml:id="formula_0">Q * (s, a) = E r + γ max a Q * (s , a ) .</formula><p>One of the core ideas behind reinforcement learning is to represent the action-value function using a function approximator such as a neural network, Q(s, a) = Q(s, a; θ). The parameters θ of the so-called Q-network are optimized so as to approximately solve the Bellman equation. For example, the Q-learning algorithm iteratively updates the action-value function Q(s, a; θ) towards a sample of the Bellman target, r + γ max a Q(s , a ; θ). However, it is well-known that the Q-learning algorithm is highly unstable when combined with non-linear function approximators such as deep neural networks <ref type="bibr" target="#b17">(Tsitsiklis &amp; Roy, 1997)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep Q-Networks</head><p>Recently, a new RL algorithm has been developed which is in practice much more stable when combined with deep Qnetworks <ref type="bibr" target="#b11">(Mnih et al., 2013;</ref><ref type="bibr" target="#b12">2015)</ref>. Like Q-learning, it iteratively solves the Bellman equation by adjusting the parameters of the Q-network towards the Bellman target. However, DQN, as shown in <ref type="figure">Figure 1</ref> differs from Q-learning in two ways. First, DQN uses experience replay <ref type="bibr" target="#b10">(Lin, 1993)</ref>. At each time-step t during an agent's interaction with the environment it stores the experience tuple e t = (s t , a t , r t , s t+1 ) into a replay memory D t = {e 1 , ..., e t }.</p><p>Second, DQN maintains two separate Q-networks Q(s, a; θ) and Q(s, a; θ − ) with current parameters θ and old parameters θ − respectively. The current parameters θ may be updated many times per time-step, and are copied into the old parameters θ − after N iterations. At every update iteration i the current parameters θ are updated so as to minimise the mean-squared Bellman error with respect to old parameters θ − , by optimizing the following loss function (DQN Loss),</p><formula xml:id="formula_1">Li(θi) = E r + γ max a Q(s , a ; θ − i ) − Q(s, a; θi) 2<label>(1)</label></formula><p>For each update i, a tuple of experience (s, a, r, s ) ∼ U (D) (or a minibatch of such samples) is sampled uniformly from the replay memory D. For each sample (or minibatch), the current parameters θ are updated by a stochastic gradient descent algorithm. Specifically, θ is adjusted in the direction of the sample gradient g i of the loss with respect to θ,</p><formula xml:id="formula_2">gi = r + γ max a Q(s , a ; θ − i ) − Q(s, a; θi) ∇ θ i Q(s, a; θ)<label>(2)</label></formula><p>Finally, actions are selected at each time-step t by an -greedy behavior with respect to the current Q-network Q(s, a; θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Distributed Architecture</head><p>We now introduce Gorila (General Reinforcement Learning Architecture), a framework for massively distributed reinforcement learning. The Gorila architecture, shown in <ref type="figure">Figure 2</ref> contains the following components:</p><p>Actors. Any reinforcement learning agent must ultimately select actions a t to apply in its environment. We refer to this process as acting. The Gorila architecture contains N act different actor processes, applied to N act corresponding instantiations of the same environment. Each actor i generates its own trajectories of experience s i 1 , a i 1 , r i 1 , ..., s i T , a i T , r i T within the environment, and as a result each actor may visit different parts of the state space. The quantity of experience that is generated by the actors after T time-steps is approximately T N act . <ref type="figure">Figure 2</ref>. The Gorila agent parallelises the training procedure by separating out learners, actors and parameter server. In a single experiment, several learner processes exist and they continuously send the gradients to parameter server and receive updated parameters. At the same time, independent actors can also in parallel accumulate experience and update their Q-networks from the parameter server.</p><p>Each actor contains a replica of the Q-network, which is used to determine behavior, for example using an -greedy policy. The parameters of the Q-network are synchronized periodically from the parameter server.</p><p>Experience replay memory. The experience tuples e i t = (s i t , a i t , r i t , s i t+1 ) generated by the actors are stored in a replay memory D. We consider two forms of experience replay memory. First, a local replay memory stores each actor's experience D i t = {e i 1 , ..., e i t } locally on that actor's machine. If a single machine has sufficient memory to store M experience tuples, then the overall memory capacity becomes M N act . Second, a global replay memory aggregates the experience into a distributed database. In this approach the overall memory capacity is independent of N act and may be scaled as desired, at the cost of additional communication overhead.</p><p>Learners. Gorila contains N learn learner processes. Each learner contains a replica of the Q-network and its job is to compute desired changes to the parameters of the Qnetwork. For each learner update k, a minibatch of experience tuples e = (s, a, r, s ) is sampled from either a local or global experience replay memory D (see above). The learner applies an off-policy RL algorithm such as DQN <ref type="bibr" target="#b11">(Mnih et al., 2013)</ref> to this minibatch of experience, in order to generate a gradient vector g i . <ref type="bibr">1</ref> The gradients g i are communicated to the parameter server; and the parameters of the Q-network are updated periodically from the parameter server.</p><p>Parameter server. Like DistBelief, the Gorila architecture uses a central parameter server to maintain a distributed representation of the Q-network Q(s, a; θ + ). The parameter vector θ + is split disjointly across N param different machines. Each machine is responsible for applying gradient updates to a subset of the parameters. The parameter server receives gradients from the learners, and applies these gradients to modify the parameter vector θ + , using an asynchronous stochastic gradient descent algorithm.</p><p>The Gorila architecture provides considerable flexibility in the number of ways an RL agent may be parallelized. It is possible to have parallel acting to generate large quantities of data into a global replay database, and then process that data with a single serial learner. In contrast, it is possible to have a single actor generating data into a local replay memory, and then have multiple learners process this data in parallel to learn as effectively as possible from this experience. However, to avoid any individual component from becoming a bottleneck, the Gorila architecture in general allows for arbitrary numbers of actors, learners, and parameter servers to both generate data, learn from that data, and update the model in a scalable and fully distributed fashion.</p><p>The simplest overall instantiation of Gorila, which we consider in our subsequent experiments, is the bundled mode in which there is a one-to-one correspondence between actors, replay memory, and learners (N act = N learn ). Each bundle has an actor generating experience, a local replay</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Distributed DQN Algorithm</head><p>Initialise replay memory D to size P . Initialise the training network for the action-value function Q(s, a; θ) with weights θ and target network Q(s, a; θ − ) with weights θ − = θ. for episode = 1 to M do Initialise the start state to s 1 . Update θ from parameters θ + of the parameter server. for t = 1 to T do With probability take a random action a t or else a t = argmax a Q(s, a; θ).</p><p>Execute the action in the environment and observe the reward r t and the next state s t+1 . Store (s t , a t , r t , s t+1 ) in D.</p><p>Update θ from parameters θ + of the parameter server. Sample random mini-batch from D. And for each</p><formula xml:id="formula_3">tuple (s i , a i , r i , s i+1 ) set target y t as if s i+1 is terminal then y t = r i else y t = r i + γmax a Q(s i+1 , a ; θ − ) end if Calculate the loss L t = (y t − Q(s i , a i ; θ) 2 ).</formula><p>Compute gradients with respect to the network parameters θ using equation 2. Send gradients to the parameter server. Every global N steps sync θ − with parameters θ + from the parameter server. end for end for memory to store that experience, and a learner that updates parameters based on samples of experience from the local replay memory. The only communication between bundles is via parameters: the learners communicate their gradients to the parameter server; and the Q-networks in the actors and learners are periodically synchronized to the parameter server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Gorila DQN</head><p>We now consider a specific instantiation of the Gorila architecture implementing the DQN algorithm. As described in the previous section, the DQN algorithm utilizes two copies of the Q-network: a current Q-network with parameters θ and a target Q-network with parameters θ − . The DQN algorithm is extended to the distributed implementation in Gorila as follows. The parameter server maintains the current parameters θ + and the actors and learners contain replicas of the current Q-network Q(s, a; θ) that are synchronized from the parameter server before every acting step.</p><p>The learner additionally maintains the target Q-network Q(s, a; θ − ). The learner's target network is updated from the parameter server θ + after every N gradient updates in the central parameter server.</p><p>Note that N is a global parameter that counts the total number of updates to the central parameter server rather than counting the updates from the local learner.</p><p>The learners generate gradients using the DQN gradient given in Equation 2. However, the gradients are not applied directly, but instead communicated to the parameter server. The parameter server then applies the updates that are accumulated from many learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Stability</head><p>While the DQN training algorithm was designed to ensure stability of training neural networks with reinforcement learning, training using a large cluster of machines running multiple other tasks poses additional challenges. The Gorila DQN implementation uses additional safeguards to ensure stability in the presence of disappearing nodes, slowdowns in network traffic, and slowdowns of individual machines. One such safeguard is a parameter that determines the maximum time delay between the local parameters θ (the gradients g i are computed using θ) and the parameters θ + in the parameter server.</p><p>All gradients older than the threshold are discarded by the parameter server. Additionally, each actor/learner keeps a running average and standard deviation of the absolute DQN loss for the data it sees and discards gradients with absolute loss higher than the mean plus several standard deviations. Finally, we used the AdaGrad update rule <ref type="bibr" target="#b4">(Duchi et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Set Up</head><p>We evaluated Gorila by conducting experiments on 49 Atari 2600 games using the Arcade Learning Environment <ref type="bibr" target="#b0">(Bellemare et al., 2012)</ref>. Atari games provide a challenging and diverse set of reinforcement learning problems where an agent must learn to play the games directly from 210 × 160 RGB video input with only the changes in the score provided as rewards. We closely followed the experimental setup of DQN <ref type="bibr" target="#b12">(Mnih et al., 2015)</ref> using the same preprocessing and network architecture. We preprocessed the 210 × 160 RGB images by downsampling them to 84 × 84 and extracting the luminance channel.</p><p>The Q-network Q(s, a; θ) had 3 convolutional layers followed by a fully-connected hidden layer. The 84 × 84 × 4 input to the network is obtained by concatenating the images from four previous preprocessed frames. The first convolutional layer had 32 filters of size 4 × 8 × 8 and stride 4. The second convolutional layer had 64 filters of size 32 × 4 × 4 with stride 2, while the third had 64 filters with size 64 × 3 × 3 and stride 1. The next layer had 512 fully-connected output units, which is followed by a linear fully-connected output layer with a single output unit for each valid action. Each hidden layer was followed by a rectifier nonlinearity.</p><p>We have used the same frame skipping step implemented in <ref type="bibr" target="#b12">(Mnih et al., 2015)</ref> by repeating every action a t over the next 4 frames.</p><p>In all experiments, Gorila DQN used: N param = 31 and N learn = N act = 100. We use the bundled mode. Replay memory size D = 1 million frames and used -greedy as the behaviour policy with annealed from 1 to 0.1 over the first one million global updates. Each learner syncs the parameters θ − of its target network after every 60K parameter updates performed in the parameter server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation</head><p>We used two types of evaluations. The first follows the protocol established by DQN. Each trained agent was evaluated on 30 episodes of the game it was trained on. A random number of frames were skipped by repeatedly taking the null or do nothing action before giving control to the agent in order to ensure variation in the initial conditions. The agents were allowed to play until the end of the game or up to 18000 frames (5 minutes), whichever came first, and the scores were averaged over all 30 episodes. We refer to this evaluation procedure as null op starts.</p><p>Testing how well an agent generalizes is especially important in the Atari domain because the emulator is completely deterministic.</p><p>Our second evaluation method, which we call human starts, aims to measure how well the agent generalizes to states it may not have trained on. To that end, we have introduced 100 random starting points that were sampled from a human professional's gameplay for each game. To evaluate an agent, we ran it from each of the 100 starting points until the end of the game or until a total of 108000 frames (equivalent to 30 minutes) were played counting the frames the human played to reach the starting point. The total score accumulated only by the agent (not considering any points won by the human player) were averaged to obtain the evaluation score.</p><p>In order to make it easier to compare results on 49 games with a greatly varying range of scores we present the results on a scale where 0 is the score obtained by a random agent and 100 is the score obtained by a professional human game player. The random agent selected actions uniformly at random at 10Hz and it was evaluated using the same starting states as the agents for both kinds of evaluations (null op starts and human starts).</p><p>We selected hyperparameter values by performing an informal search on the games of Breakout, Pong and Seaquest which were then fixed for all the games. We have trained Gorila DQN 5 times on each game using the same fixed hyperparameter settings and random network initializations. Following DQN, we periodically evaluated each model during training and kept the best performing network parameters for the final evaluation. We average these final evaluations over the 5 runs, and compare the mean evaluations with DQN and human expert scores.  <ref type="figure">Figure 5</ref>. The time required by Gorila DQN to surpass single DQN performance (red curve) and to reach its peak performance (blue curve).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>We first compared Gorila DQN agents trained for up to 6 days to single GPU DQN agents trained for 12-14 days. <ref type="figure" target="#fig_2">Figure 3</ref> shows the normalized scores under the human starts evaluation.  scores from null op starts (gray bars). In fact, Gorila DQN performs at a level similar or superior to a human professional (75% of the human score or above) in 25 games despite starting from states sampled from human play. One possible reason for the improved generalization is the significant increase in the number of states Gorila DQN sees by using 100 parallel actors.</p><p>We next look at how the performance of Gorila DQN improved during training. <ref type="figure">Figure 5</ref> shows how quickly Gorila DQN reached the performance of single GPU DQN and how quickly Gorila DQN reached its own best score under the human starts evaluation. Gorila DQN surpassed the best single GPU DQN scores on 19 games in 6 hours, 23 games in 12 hours, 30 in 24 hours and 38 games in 36 hours (red curve). This is a roughly an order of magnitude reduc- tion in training time required to reach the single process DQN score. On some games Gorila DQN achieved its best score in under two days but for most of the games the performance keeps improving with longer training time (blue curve).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we have introduced the first massively distributed architecture for deep reinforcement learning. The Gorila architecture acts and learns in parallel, using a distributed replay memory and distributed neural network. We applied Gorila to an asynchronous variant of the state-of-the-art DQN algorithm. A single machine had previously achieved state-of-the-art results in the challenging suite of Atari 2600 games, but it was not previously known whether the good performance of DQN would continue to scale with additional computation. By leveraging massive parallelism, Gorila DQN significantly outperformed single-GPU DQN on 41 out of 49 games; achieving by far the best results in this domain to date. Gorila takes a further step towards fulfilling the promise of deep learning in RL: a scalable architecture that performs better and better with increased computation and memory.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Performance of the Gorila agent on 49 Atari games with human starts evaluation compared with DQN<ref type="bibr" target="#b12">(Mnih et al., 2015)</ref> performance with scores normalized to expert human performance. Font color indicates which method has the higher score. *Not showing DQN scores for Asterix, Asteroids, Double Dunk, Private Eye, Wizard Of Wor and Gravitar because the DQN human starts scores are less than the random agent baselines. Also not showing Video Pinball because the human expert scores are less than the random agent scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Performance of the Gorila agent on 49 Atari games with human starts and null op evaluations normalized with respect to DQN human start and null op scores respectively. This figure shows the generalization improvements of Gorila compared to DQN. *Using a score of 0 for the human starts random agent score for Asterix, Asteroids, Double Dunk, Private Eye, Wizard Of Wor and Gravitar because the human starts DQN scores are less than the random agent scores. Not showing Double Dunk because both the DQN scores and the random agent scores are negative. **Not showing null op scores for Montezuma Revenge because both the human start scores and random agent scores are 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Using human starts Gorila DQN outperformed single GPU DQN on 41 out of 49 games given roughly one half of the training time of single GPU DQN. On 22 of the games Gorila DQN obtained double the score of single GPU DQN, and on 11 games Gorila DQN's score was 5 times higher. Similarly, using the original null op starts evaluation Gorila DQN outperformed the single GPU DQN on 31 out of 49 games. These results show that parallel training significantly improved performance in less training time. Also, better results on human starts compared to null op starts suggest that Gorila DQN is es-</figDesc><table><row><cell>0%</cell><cell>200%</cell><cell>400%</cell><cell>600%</cell><cell>800%</cell><cell>1,000%</cell><cell>5,000%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">pecially good at generalizing to potentially unseen states</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">compared to single GPU DQN. Figure 4 further illustrates</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">these improvements in generalization by showing Gorila</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">DQN scores with human starts normalized with respect to</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">GPU DQN scores with human starts (blue bars) and Gorila</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">DQN scores from null op starts normalized by GPU DQN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>NULL OP NORMALIZED</figDesc><table><row><cell>Games</cell><cell>DQN</cell><cell>Gorila</cell><cell>Gorila</cell></row><row><cell></cell><cell cols="3">(human normalized) (human normalized) (DQN normalized)</cell></row><row><cell>Alien</cell><cell>42.74</cell><cell>35.99</cell><cell>84.20</cell></row><row><cell>Amidar</cell><cell>43.93</cell><cell>70.89</cell><cell>161.36</cell></row><row><cell>Assault</cell><cell>246.16</cell><cell>96.39</cell><cell>39.15</cell></row><row><cell>Asterix</cell><cell>69.95</cell><cell>75.04</cell><cell>107.26</cell></row><row><cell>Asteroids</cell><cell>7.31</cell><cell>2.64</cell><cell>36.09</cell></row><row><cell>Atlantis</cell><cell>451.84</cell><cell>539.11</cell><cell>119.31</cell></row><row><cell>Bank Heist</cell><cell>57.69</cell><cell>82.58</cell><cell>143.15</cell></row><row><cell>Battle Zone</cell><cell>67.55</cell><cell>64.63</cell><cell>95.68</cell></row><row><cell>Beam Rider</cell><cell>119.79</cell><cell>54.31</cell><cell>45.34</cell></row><row><cell>Bowling</cell><cell>14.65</cell><cell>23.47</cell><cell>160.18</cell></row><row><cell>Boxing</cell><cell>1707.14</cell><cell>2256.66</cell><cell>132.18</cell></row><row><cell>Breakout</cell><cell>1327.24</cell><cell>1330.56</cell><cell>100.25</cell></row><row><cell>Centipede</cell><cell>62.98</cell><cell>64.23</cell><cell>101.97</cell></row><row><cell>Chopper Command</cell><cell>64.77</cell><cell>37.00</cell><cell>57.12</cell></row><row><cell>Crazy Climber</cell><cell>419.49</cell><cell>305.06</cell><cell>72.72</cell></row><row><cell>Demon Attack</cell><cell>294.19</cell><cell>416.74</cell><cell>141.65</cell></row><row><cell>Double Dunk</cell><cell>16.12</cell><cell>257.34</cell><cell>1595.55</cell></row><row><cell>Enduro</cell><cell>97.48</cell><cell>37.11</cell><cell>38.07</cell></row><row><cell>Fishing Derby</cell><cell>93.51</cell><cell>115.11</cell><cell>123.09</cell></row><row><cell>Freeway</cell><cell>102.36</cell><cell>39.49</cell><cell>38.58</cell></row><row><cell>Frostbite</cell><cell>6.16</cell><cell>12.64</cell><cell>205.23</cell></row><row><cell>Gopher</cell><cell>400.42</cell><cell>243.35</cell><cell>60.77</cell></row><row><cell>Gravitar</cell><cell>5.35</cell><cell>35.27</cell><cell>659.37</cell></row><row><cell>Hero</cell><cell>76.50</cell><cell>56.14</cell><cell>73.38</cell></row><row><cell>Ice Hockey</cell><cell>79.33</cell><cell>87.49</cell><cell>110.27</cell></row><row><cell>JamesBond</cell><cell>145.00</cell><cell>152.50</cell><cell>105.16</cell></row><row><cell>Kangaroo</cell><cell>224.20</cell><cell>83.71</cell><cell>37.33</cell></row><row><cell>Krull</cell><cell>277.01</cell><cell>788.85</cell><cell>284.76</cell></row><row><cell>Kung Fu Master</cell><cell>102.37</cell><cell>121.38</cell><cell>118.57</cell></row><row><cell>Montezuma Revenge</cell><cell>0.00</cell><cell>0.09</cell><cell>0.00</cell></row><row><cell>Ms Pacman</cell><cell>13.02</cell><cell>19.01</cell><cell>146.03</cell></row><row><cell>Name This Game</cell><cell>278.28</cell><cell>218.05</cell><cell>78.35</cell></row><row><cell>Pong</cell><cell>132</cell><cell>130</cell><cell>98.48</cell></row><row><cell>Private Eye</cell><cell>2.53</cell><cell>1.04</cell><cell>41.05</cell></row><row><cell>QBert</cell><cell>78.48</cell><cell>80.14</cell><cell>102.10</cell></row><row><cell>RiverRaid</cell><cell>57.30</cell><cell>57.54</cell><cell>100.41</cell></row><row><cell>Road Runner</cell><cell>232.91</cell><cell>651.00</cell><cell>279.50</cell></row><row><cell>Robotank</cell><cell>509.27</cell><cell>352.92</cell><cell>69.29</cell></row><row><cell>Seaquest</cell><cell>25.94</cell><cell>65.13</cell><cell>251.08</cell></row><row><cell>Space Invaders</cell><cell>121.48</cell><cell>115.36</cell><cell>94.96</cell></row><row><cell>Star Gunner</cell><cell>598.08</cell><cell>192.79</cell><cell>32.23</cell></row><row><cell>Tennis</cell><cell>148.99</cell><cell>232.70</cell><cell>156.18</cell></row><row><cell>Time Pilot</cell><cell>100.92</cell><cell>300.86</cell><cell>298.11</cell></row><row><cell>Tutankham</cell><cell>112.22</cell><cell>149.53</cell><cell>133.24</cell></row><row><cell>Up n Down</cell><cell>92.68</cell><cell>140.70</cell><cell>151.81</cell></row><row><cell>Venture</cell><cell>32.00</cell><cell>104.87</cell><cell>327.71</cell></row><row><cell>Video Pinball</cell><cell>2539.36</cell><cell>13576.75</cell><cell>534.65</cell></row><row><cell>Wizard of Wor</cell><cell>67.48</cell><cell>314.04</cell><cell>465.32</cell></row><row><cell>Zaxxon</cell><cell>54.08</cell><cell>77.63</cell><cell>143.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>RAW DATA -HUMAN STARTS</figDesc><table><row><cell>Games</cell><cell>Random</cell><cell>Human</cell><cell cols="2">DQN Gorila Avg</cell></row><row><cell>Alien</cell><cell>128.30</cell><cell>6371.30</cell><cell>570.20</cell><cell>813.54</cell></row><row><cell>Amidar</cell><cell>11.80</cell><cell>1540.40</cell><cell>133.40</cell><cell>189.15</cell></row><row><cell>Assault</cell><cell>166.90</cell><cell>628.90</cell><cell>3332.30</cell><cell>1195.85</cell></row><row><cell>Asterix</cell><cell>164.50</cell><cell>7536.00</cell><cell>124.50</cell><cell>3324.70</cell></row><row><cell>Asteroids</cell><cell cols="2">877.10 36517.30</cell><cell>697.10</cell><cell>933.63</cell></row><row><cell>Atlantis</cell><cell cols="4">13463.00 26575.00 76108.00 629166.50</cell></row><row><cell>Bank Heist</cell><cell>21.70</cell><cell>644.50</cell><cell>176.30</cell><cell>399.42</cell></row><row><cell>Battle Zone</cell><cell cols="3">3560.00 33030.00 17560.00</cell><cell>19938.00</cell></row><row><cell>Beam Rider</cell><cell cols="2">254.60 14961.00</cell><cell>8672.40</cell><cell>3822.07</cell></row><row><cell>Bowling</cell><cell>35.20</cell><cell>146.50</cell><cell>41.20</cell><cell>53.95</cell></row><row><cell>Boxing</cell><cell>-1.50</cell><cell>9.60</cell><cell>25.80</cell><cell>74.20</cell></row><row><cell>Breakout</cell><cell>1.60</cell><cell>27.90</cell><cell>303.90</cell><cell>313.03</cell></row><row><cell>Centipede</cell><cell cols="2">1925.50 10321.90</cell><cell>3773.10</cell><cell>6296.87</cell></row><row><cell>Chopper Command</cell><cell>644.00</cell><cell>8930.00</cell><cell>3046.00</cell><cell>3191.75</cell></row><row><cell>Crazy Climber</cell><cell cols="3">9337.00 32667.00 50992.00</cell><cell>65451.00</cell></row><row><cell>Demon Attack</cell><cell>208.30</cell><cell cols="2">3442.80 12835.20</cell><cell>14880.13</cell></row><row><cell>Double Dunk</cell><cell>-16.00</cell><cell>-14.40</cell><cell>-21.60</cell><cell>-11.35</cell></row><row><cell>Enduro</cell><cell>-81.80</cell><cell>740.20</cell><cell>475.60</cell><cell>71.04</cell></row><row><cell>Fishing Derby</cell><cell>-77.10</cell><cell>5.10</cell><cell>-2.30</cell><cell>4.64</cell></row><row><cell>Freeway</cell><cell>0.20</cell><cell>25.60</cell><cell>25.80</cell><cell>10.16</cell></row><row><cell>Frostbite</cell><cell>66.40</cell><cell>4202.80</cell><cell>157.40</cell><cell>426.60</cell></row><row><cell>Gopher</cell><cell>250.00</cell><cell>2311.00</cell><cell>2731.80</cell><cell>4373.04</cell></row><row><cell>Gravitar</cell><cell>245.50</cell><cell>3116.00</cell><cell>216.50</cell><cell>538.37</cell></row><row><cell>Hero</cell><cell cols="3">1580.30 25839.40 12952.50</cell><cell>8963.36</cell></row><row><cell>Ice Hockey</cell><cell>-9.70</cell><cell>0.50</cell><cell>-3.80</cell><cell>-1.72</cell></row><row><cell>JamesBond</cell><cell>33.50</cell><cell>368.50</cell><cell>348.50</cell><cell>444.00</cell></row><row><cell>Kangaroo</cell><cell>100.00</cell><cell>2739.00</cell><cell>2696.00</cell><cell>1431.00</cell></row><row><cell>Krull</cell><cell>1151.90</cell><cell>2109.10</cell><cell>3864.00</cell><cell>6363.09</cell></row><row><cell>Kung Fu Master</cell><cell cols="3">304.00 20786.80 11875.00</cell><cell>20620.00</cell></row><row><cell>Montezuma Revenge</cell><cell>25.00</cell><cell>4182.00</cell><cell>50.00</cell><cell>84.00</cell></row><row><cell>Ms Pacman</cell><cell cols="2">197.80 15375.00</cell><cell>763.50</cell><cell>1263.05</cell></row><row><cell>Name This Game</cell><cell>1747.80</cell><cell>6796.00</cell><cell>5439.90</cell><cell>9238.50</cell></row><row><cell>Pong</cell><cell>-18.00</cell><cell>15.50</cell><cell>16.20</cell><cell>16.71</cell></row><row><cell>Private Eye</cell><cell cols="2">662.80 64169.10</cell><cell>298.20</cell><cell>2598.55</cell></row><row><cell>QBert</cell><cell cols="2">271.80 12085.00</cell><cell>4589.80</cell><cell>7089.83</cell></row><row><cell>RiverRaid</cell><cell cols="2">588.30 14382.20</cell><cell>4065.30</cell><cell>5310.27</cell></row><row><cell>Road Runner</cell><cell>200.00</cell><cell>6878.00</cell><cell>9264.00</cell><cell>43079.80</cell></row><row><cell>Robotank</cell><cell>2.40</cell><cell>8.90</cell><cell>58.50</cell><cell>61.78</cell></row><row><cell>Seaquest</cell><cell cols="2">215.50 40425.80</cell><cell>2793.90</cell><cell>10145.85</cell></row><row><cell>Space Invaders</cell><cell>182.60</cell><cell>1464.90</cell><cell>1449.70</cell><cell>1183.29</cell></row><row><cell>Star Gunner</cell><cell>697.00</cell><cell cols="2">9528.00 34081.00</cell><cell>14919.25</cell></row><row><cell>Tennis</cell><cell>-21.40</cell><cell>-6.70</cell><cell>-2.30</cell><cell>-0.69</cell></row><row><cell>Time Pilot</cell><cell>3273.00</cell><cell>5650.00</cell><cell>5640.00</cell><cell>8267.80</cell></row><row><cell>Tutankham</cell><cell>12.70</cell><cell>138.30</cell><cell>32.40</cell><cell>118.45</cell></row><row><cell>Up n Down</cell><cell>707.20</cell><cell>9896.10</cell><cell>3311.30</cell><cell>8747.67</cell></row><row><cell>Venture</cell><cell>18.00</cell><cell>1039.00</cell><cell>54.00</cell><cell>523.40</cell></row><row><cell>Video Pinball</cell><cell cols="4">20452.00 15641.10 20228.10 112093.37</cell></row><row><cell>Wizard of Wor</cell><cell>804.00</cell><cell>4556.00</cell><cell>246.00</cell><cell>10431.00</cell></row><row><cell>Zaxxon</cell><cell>475.00</cell><cell>8443.00</cell><cell>831.00</cell><cell>6159.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>RAW DATA -NULL OP</figDesc><table><row><cell>Games</cell><cell>Random</cell><cell>Human</cell><cell cols="2">DQN Gorila Avg</cell></row><row><cell>Alien</cell><cell>227.80</cell><cell>6875.40</cell><cell>3069.30</cell><cell>2620.53</cell></row><row><cell>Amidar</cell><cell>5.80</cell><cell>1675.80</cell><cell>739.50</cell><cell>1189.70</cell></row><row><cell>Assault</cell><cell>222.40</cell><cell>1496.40</cell><cell>3358.60</cell><cell>1450.41</cell></row><row><cell>Asterix</cell><cell>210.00</cell><cell>8503.30</cell><cell>6011.70</cell><cell>6433.33</cell></row><row><cell>Asteroids</cell><cell cols="2">719.10 13156.70</cell><cell>1629.30</cell><cell>1047.66</cell></row><row><cell>Atlantis</cell><cell cols="2">12850.00 29028.10</cell><cell cols="2">85950.00 100069.16</cell></row><row><cell>Bank Heist</cell><cell>14.20</cell><cell>734.40</cell><cell>429.70</cell><cell>609.00</cell></row><row><cell>Battle Zone</cell><cell cols="2">2360.00 37800.00</cell><cell>26300.00</cell><cell>25266.66</cell></row><row><cell>Beam Rider</cell><cell>363.90</cell><cell>5774.70</cell><cell>6845.90</cell><cell>3302.91</cell></row><row><cell>Bowling</cell><cell>23.10</cell><cell>154.80</cell><cell>42.40</cell><cell>54.01</cell></row><row><cell>Boxing</cell><cell>0.10</cell><cell>4.30</cell><cell>71.80</cell><cell>94.88</cell></row><row><cell>Breakout</cell><cell>1.70</cell><cell>31.80</cell><cell>401.20</cell><cell>402.20</cell></row><row><cell>Centipede</cell><cell cols="2">2090.90 11963.20</cell><cell>8309.40</cell><cell>8432.30</cell></row><row><cell>Chopper Command</cell><cell>811.00</cell><cell>9881.80</cell><cell>6686.70</cell><cell>4167.50</cell></row><row><cell>Crazy Climber</cell><cell cols="3">10780.50 35410.50 114103.30</cell><cell>85919.16</cell></row><row><cell>Demon Attack</cell><cell>152.10</cell><cell>3401.30</cell><cell>9711.20</cell><cell>13693.12</cell></row><row><cell>Double Dunk</cell><cell>-18.60</cell><cell>-15.50</cell><cell>-18.10</cell><cell>-10.62</cell></row><row><cell>Enduro</cell><cell>0.00</cell><cell>309.60</cell><cell>301.80</cell><cell>114.90</cell></row><row><cell>Fishing Derby</cell><cell>-91.70</cell><cell>5.50</cell><cell>-0.80</cell><cell>20.19</cell></row><row><cell>Freeway</cell><cell>0.00</cell><cell>29.60</cell><cell>30.30</cell><cell>11.69</cell></row><row><cell>Frostbite</cell><cell>65.20</cell><cell>4334.70</cell><cell>328.30</cell><cell>605.16</cell></row><row><cell>Gopher</cell><cell>257.60</cell><cell>2321.00</cell><cell>8520.00</cell><cell>5279.00</cell></row><row><cell>Gravitar</cell><cell>173.00</cell><cell>2672.00</cell><cell>306.70</cell><cell>1054.58</cell></row><row><cell>Hero</cell><cell cols="2">1027.00 25762.50</cell><cell>19950.30</cell><cell>14913.87</cell></row><row><cell>Ice Hockey</cell><cell>-11.20</cell><cell>0.90</cell><cell>-1.60</cell><cell>-0.61</cell></row><row><cell>JamesBond</cell><cell>29.00</cell><cell>406.70</cell><cell>576.70</cell><cell>605.00</cell></row><row><cell>Kangaroo</cell><cell>52.00</cell><cell>3035.00</cell><cell>6740.00</cell><cell>2549.16</cell></row><row><cell>Krull</cell><cell>1598.00</cell><cell>2394.60</cell><cell>3804.70</cell><cell>7882.00</cell></row><row><cell>Kung Fu Master</cell><cell cols="2">258.50 22736.20</cell><cell>23270.00</cell><cell>27543.33</cell></row><row><cell>Montezuma Revenge</cell><cell>0.00</cell><cell>4366.70</cell><cell>0.00</cell><cell>4.16</cell></row><row><cell>Ms Pacman</cell><cell cols="2">307.30 15693.40</cell><cell>2311.00</cell><cell>3233.50</cell></row><row><cell>Name This Game</cell><cell>2292.30</cell><cell>4076.20</cell><cell>7256.70</cell><cell>6182.16</cell></row><row><cell>Pong</cell><cell>-20.70</cell><cell>9.30</cell><cell>18.90</cell><cell>18.30</cell></row><row><cell>Private Eye</cell><cell cols="2">24.90 69571.30</cell><cell>1787.60</cell><cell>748.60</cell></row><row><cell>QBert</cell><cell cols="2">163.90 13455.00</cell><cell>10595.80</cell><cell>10815.55</cell></row><row><cell>RiverRaid</cell><cell cols="2">1338.50 13513.30</cell><cell>8315.70</cell><cell>8344.83</cell></row><row><cell>Road Runner</cell><cell>11.50</cell><cell>7845.00</cell><cell>18256.70</cell><cell>51007.99</cell></row><row><cell>Robotank</cell><cell>2.20</cell><cell>11.90</cell><cell>51.60</cell><cell>36.43</cell></row><row><cell>Seaquest</cell><cell cols="2">68.40 20181.80</cell><cell>5286.00</cell><cell>13169.06</cell></row><row><cell>Space Invaders</cell><cell>148.00</cell><cell>1652.30</cell><cell>1975.50</cell><cell>1883.41</cell></row><row><cell>Star Gunner</cell><cell cols="2">664.00 10250.00</cell><cell>57996.70</cell><cell>19144.99</cell></row><row><cell>Tennis</cell><cell>-23.80</cell><cell>-8.90</cell><cell>-1.60</cell><cell>10.87</cell></row><row><cell>Time Pilot</cell><cell>3568.00</cell><cell>5925.00</cell><cell>5946.70</cell><cell>10659.33</cell></row><row><cell>Tutankham</cell><cell>11.40</cell><cell>167.60</cell><cell>186.70</cell><cell>244.97</cell></row><row><cell>Up n Down</cell><cell>533.40</cell><cell>9082.00</cell><cell>8456.30</cell><cell>12561.58</cell></row><row><cell>Venture</cell><cell>0.00</cell><cell>1187.50</cell><cell>380.00</cell><cell>1245.33</cell></row><row><cell>Video Pinball</cell><cell cols="2">16256.90 17297.60</cell><cell cols="2">42684.10 157550.21</cell></row><row><cell>Wizard of Wor</cell><cell>563.50</cell><cell>4756.50</cell><cell>3393.30</cell><cell>13731.33</cell></row><row><cell>Zaxxon</cell><cell>32.50</cell><cell>9173.30</cell><cell>4976.70</cell><cell>7129.33</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The experience in the replay memory is generated by old behavior policies which are most likely different to the current behavior of the agent; therefore all updates must be performed offpolicy<ref type="bibr" target="#b15">(Sutton &amp; Barto, 1998)</ref>.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>July 17, 2015 8. Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Data</head><p>We present all the data that has been used in the paper.</p><p>• <ref type="table">Table 1</ref> shows the various normalized scores for null op evaluation.</p><p>• <ref type="table">Table 2</ref> shows the various normalized scores for human start evaluation.</p><p>• <ref type="table">Table 3</ref> shows the various raw scores for human start evaluation.</p><p>• <ref type="table">Table 4</ref> shows the various raw scores for null op evaluation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yavar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.4708</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning with cots hpc systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1337" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A-R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel reinforcement learning with linear function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Grounds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kudenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th, 6th and 7th European Conference on Adaptive and Learning Agents and Multi-agent Systems: Adaptation and Multi-agent Learning</title>
		<meeting>the 5th, 6th and 7th European Conference on Adaptive and Learning Agents and Multi-agent Systems: Adaptation and Multi-agent Learning</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="60" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An algorithm for distributed reinforcement learning in cooperative multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mapreduce for parallel reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Reinforcement Learning -9th European Workshop</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09-09" />
			<biblScope unit="page" from="309" to="320" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long-Ji</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DTIC Document</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Helen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dharshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
		<ptr target="http://dx.doi.org/10.1038/nature14236" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Concurrent reinforcement learning from customer interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newnham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Mcfall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="924" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: an Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An analysis of temporaldifference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="674" to="690" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distributed reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
