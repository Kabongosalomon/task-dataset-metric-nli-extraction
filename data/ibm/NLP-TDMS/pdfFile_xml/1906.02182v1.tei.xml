<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-Stream Region Convolutional 3D Network for Temporal Activity Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
						</author>
						<title level="a" type="main">Two-Stream Region Convolutional 3D Network for Temporal Activity Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. 6, NO. 1, APRIL 2019 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Temporal Activity Detection</term>
					<term>Two-stream Architec- ture</term>
					<term>Hard Mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of temporal activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream. The twostream network is jointly optimized by fusing the flow and RGB feature maps at different levels. Additionally, the training stage incorporates an online hard example mining strategy to address the extreme foreground-background imbalance typically observed in any detection pipeline. Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model. This improves the model without heavy hyper-parameter tuning. Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods. Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets. We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-Stream Region Convolutional 3D Network for</head><p>Temporal Activity Detection Huijuan Xu * , Abir Das and Kate Saenko Abstract-We address the problem of temporal activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream. The twostream network is jointly optimized by fusing the flow and RGB feature maps at different levels. Additionally, the training stage incorporates an online hard example mining strategy to address the extreme foreground-background imbalance typically observed in any detection pipeline. Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model. This improves the model without heavy hyper-parameter tuning. Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods. Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets. We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.</p><p>Keywords-Temporal Activity Detection, Two-stream Architecture, Hard Mining</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO scene understanding is an important computer vision problem with many practical applications, including smart surveillance, monitoring of patients or elderly, online video retrieval etc. Over the past few years, it has quickly evolved from classifying a short, trimmed video to detecting multiple activities in long, untrimmed videos. This is a more challenging problem compared to trimmed video classification as it requires not only recognizing, but also precisely localizing activities in time. Most of the existing works rely on a large set of features and separate classifiers exhaustively applied to a set of video segments extracted from the input video using sliding windows <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. These approaches suffer from one <ref type="figure">Fig. 1</ref>. We propose a fast two-stream Region Convolutional 3D Network (R-C3D) for temporal activity detection in continuous videos. It encodes both the RGB frames and the optical flow maps in two separate streams with fullyconvolutional 3D filters, proposes activity segments, then classifies and refines them based on pooled features within the segment boundaries. Our model improves both speed and accuracy compared to existing methods. or more of the following major drawbacks: they do not learn deep representations in a jointly optimized fashion, but rather use hand-crafted features <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, or employ deep features like VGG <ref type="bibr" target="#b6">[7]</ref>, ResNet <ref type="bibr" target="#b7">[8]</ref>, C3D <ref type="bibr" target="#b8">[9]</ref> etc., learned separately on image/video classification tasks. Such off-the-shelf representations may not be optimal for localizing activities in diverse video domains, resulting in inferior performance. Furthermore, current methods' dependence on external proposal generation or exhaustive sliding windows leads to poor computational efficiency. Finally, the sliding-window models cannot easily predict flexible activity boundaries due to the fixed temporal granularity of the sliding windows.</p><p>In this paper, we propose an activity detection model that addresses the above issues. Our Region Convolutional 3D Network (R-C3D) is jointly trainable and learns taskdependent convolutional features by jointly optimizing proposal generation and activity classification. Inspired by the Faster R-CNN <ref type="bibr" target="#b9">[10]</ref> object detection approach, we compute fully-convolutional 3D ConvNet features and propose temporal arXiv:1906.02182v1 [cs.CV] 5 Jun 2019 regions likely to contain activities, then pool features within the proposals to predict activity classes ( <ref type="figure">Figure 1</ref>). The proposal generation stage filters out many background segments with superior computational efficiency compared to sliding window models. Furthermore, proposals are predicted with respect to predefined anchor segments and can be of variable length, allowing detection of activities with flexible boundaries.</p><p>Convolutional Neural Network (CNN) features learned endto-end have been successfully used for activity recognition <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, particularly in 3D ConvNets (C3D <ref type="bibr" target="#b8">[9]</ref>), which learns to capture spatio-temporal features. However, unlike the traditional usage of 3D ConvNets <ref type="bibr" target="#b8">[9]</ref> where the input is short 16-frame video chunks, our method applies full convolution along the temporal dimension to encode as many frames as the GPU memory allows. Thus, rich spatio-temporal features are automatically learned from longer videos. These feature maps are shared between the activity proposal and classification subnets to save computation time and jointly optimize features for both tasks.</p><p>Alternative activity detection approaches <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> use a recurrent neural network (RNN) to encode sequence of frames or video chunk features (VGG <ref type="bibr" target="#b6">[7]</ref>, C3D <ref type="bibr" target="#b8">[9]</ref>) and predict the activity label at each time step. However, these RNN based methods can only model temporal features at a fixed granularity (e.g. per-frame CNN features or 16-frame C3D features). In order to use the same classification network to classify variable length proposals into specific activities, we extend 2D region of interest (RoI) pooling to 3D which extracts a fixed-length feature representation for these proposals. Thus, our model can utilize video features at any temporal granularity. Furthermore, some RNN-based detectors rely on direct regression to predict the temporal boundaries. As shown in object detection <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and semantic segmentation <ref type="bibr" target="#b18">[19]</ref>, object boundaries obtained using a regression-only framework are inferior compared to 'proposal based detection'.</p><p>Motion information plays a pivotal role in video scene understanding. While deep CNN features have worked remarkably well in combating a multitude of spatial variations due to changes in lighting, pose, scale etc., the performance of 3-D CNNs alone for large-scale video understanding is, still, limited <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Optical flow encodes the motion field in a scene and represents the pattern of apparent object motion. It is a useful motion representation and often acts in a complementary way to spatial features <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>In this paper, we explore the use of optical flow along with spatio-temporal features in a two stream 3-D convolutional architecture to better capture long-range temporal structure in videos. This framework takes in stacked video frames as well as dense optical flow fields as inputs in two separate streams, and a subsequent convolution operation learns a hierarchical representation of the features. The two streams are fused at different levels in both proposal generation and activity classification stages with a view to efficiently reuse the computed features for multiple tasks in the detection pipeline.</p><p>One of the major problems of 'proposal based detection' is that it can suffer from an extreme foreground-background (positive-negative) class imbalance during training. The foreground action proposals (positives) typically account for only a tiny fraction of all possible segments which includes a large number of background segments (negatives). Though using a separate proposal stage reduces the number of candidate segments by filtering out most background samples, still the classification stage typically has to evaluate hundreds of such segments where only a few are actual foreground activities <ref type="bibr" target="#b24">[25]</ref>. Such an imbalance causes two types of problems for a detection architecture. First, training is inefficient as loss is computed and back-propagated for a large number of relatively unimportant candidate proposals. Second, a large portion of negative examples is easy and can adversely affect the training as these examples contribute no useful learning. The problem is mainly addressed by maintaining a manageable balance between positive and negative training examples via sampling heuristics (fixed positive-negative ratio of 1:2) <ref type="bibr" target="#b9">[10]</ref>, hard negative mining <ref type="bibr" target="#b25">[26]</ref>, bootstrapping <ref type="bibr" target="#b26">[27]</ref> etc. A recent work <ref type="bibr" target="#b24">[25]</ref> uses dynamically scaled focal loss where the scaling factor down-weighs the contribution of the easy examples during training and rapidly focuses on hard examples.</p><p>Inspired by the success of the Online Hard Example Mining (OHEM) <ref type="bibr" target="#b25">[26]</ref> for fast R-CNN <ref type="bibr" target="#b27">[28]</ref> object detection, we employ a similar strategy where training proposals are subsampled according to their chances of being misclassified. OHEM not only boosts the detection performance significantly but also is computationally efficient as the hard examples are mined online. OHEM involves only forward pass operation through the R-C3D network for all the generated candidate proposals. Then, instead of heuristically sampling the proposals, they are ranked according to the classification and localization loss values and only the top few (i.e., the worst performers) are selected. Training is performed by back-propagating errors only for these chosen few hard examples. This improves the performance of the model since good proposals (hard examples) are selected for updating the model instead of randomly sampled ones.</p><p>We perform extensive comparisons of R-C3D to state-ofthe-art activity detection methods using three publicly available benchmark datasets -THUMOS'14 <ref type="bibr" target="#b28">[29]</ref>, ActivityNet <ref type="bibr" target="#b29">[30]</ref> and Charades <ref type="bibr" target="#b30">[31]</ref>. New state-of-the-art results are achieved on THUMOS'14 and Charades, while the detection performance on ActivityNet is competitive. A preliminary version of this work was published in <ref type="bibr" target="#b31">[32]</ref>. This paper additionally explores an optical flow based two-stream architecture, which better captures the motion of objects present in the scene, resulting in better detection of the activities. We also make training more robust by addressing the class imbalance problem with an online hard mining strategy. We perform a detailed ablation analysis of the optical flow and OHEM with the basic R-C3D architecture <ref type="bibr" target="#b31">[32]</ref> and show their effectiveness for efficient temporal activity detection in untrimmed videos.</p><p>To summarize, the main contributions of our paper are:</p><p>• a jointly optimized activity detection model with combined activity proposal and classification stages that can detect variable length activities; • efficient integration of flow based motion stream and online hard example mining to spatio-temporal features from stacked video frames to boost activity detection performance in untrimmed videos;</p><p>• fast detection speeds achieved by sharing fullyconvolutional C3D features between the proposal generation and classification parts of the network. The rest of the paper is organized as follows. Section II gives a description of the state-of-the-art approaches in activity detection, optical flow computation and hard example mining for robust training. The activity detection approach, including the convolutional feature extraction, proposal generation and classification, optical flow as a second stream and OHEM are described in Section III. Experimental results and comparisons with state-of-the-art methods are presented in Section IV. Finally, conclusions are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Activity Detection</head><p>There is a long history of activity recognition, or classifying trimmed video clips into fixed set of categories <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Activity detection in untrimmed videos, on the other hand, has emerged as a new challenging problem over the past few years and is more practical as most real-life videos are unsegmented and contain multiple activities.</p><p>Activity detection can be broadly categorized into two types: spatio-temporal activity detection and temporal activity detection. Spatio-temporal activity detection <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> aims to localize spatiotemporal action tubes over consecutive video frames while temporal activity detection <ref type="bibr" target="#b37">[38]</ref> predicts the start and end times of the activities within untrimmed long videos. Spatio-temporal activity detection tasks tend to be computationally heavy and require more effort in annotating the training data than temporal detection. In this work, we only focus on supervised temporal activity detection.</p><p>Sliding-window methods. Prior to this work, existing temporal activity detection approaches were dominated by models that use sliding temporal windows to generate segments and subsequently classify them with activity classifiers trained on multiple features <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Sliding windows can be thought of as a primitive method to generate temporal proposals for actions. Most of these methods have stage-wise pipelines which are not trained jointly, and therefore have limited ability to recover from errors accumulated in each stage. Moreover, the use of exhaustive sliding windows is computationally inefficient and constrains the boundary of the detected activities by the sliding windows' duration and strides.</p><p>Frame/Snippet-level methods. Recently, several approaches have leveraged recurrent networks to avoid exhaustive sliding window search in detecting activities with variable lengths. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b38">[39]</ref> model the temporal evolution of activities using recurrent neural nets (RNNs) or longshort term memory (LSTM) based networks and predict an activity (or background) label for each frame. These framelevel labels are then merged into variable-length segments.</p><p>The deep action proposal model <ref type="bibr" target="#b12">[13]</ref> uses LSTM to encode C3D features of every 16-frame video chunk, and directly regresses and classifies activity segments without an extra proposal generation stage. <ref type="bibr" target="#b39">[40]</ref> extends the action proposal model in <ref type="bibr" target="#b12">[13]</ref> to design a single-pass network for end-to-end temporal action detection that directly outputs the temporal bounds and corresponding action classes for the detections. More recently, CDC <ref type="bibr" target="#b40">[41]</ref> and SSN <ref type="bibr" target="#b41">[42]</ref> propose bottom-up activity detection by first predicting at the frame-level/snippetlevel and then fusing these predictions.</p><p>Proposal-classifier methods. In this work, we introduce a two stage approach to activity detection: the first, "proposal" stage generates temporal proposals containing non-background activity, and the subsequent "classifier" stage applies a classifier to each proposal to obtain the detected activity. We avoid recurrent layers, instead encode a large video buffer with a fully-convolutional 3D ConvNet. We use 3D RoI pooling, inspired by RoI pooling in Faster R-CNN <ref type="bibr" target="#b9">[10]</ref> to allow feature extraction from variable length proposals. 3D RoI pooling divides the 3D video encoding into 3D bins and samples values from there. A contemporaneous method for spatiotemporal activity detection, Tube-CNN <ref type="bibr" target="#b42">[43]</ref>, also employs a proposal classification appraoch. They first divide the video into equal-length clips and and generate a set of tube proposals for each clip. Then, the tube proposals are linked together to perform spatio-temporal action detection. Tube-CNN <ref type="bibr" target="#b42">[43]</ref> proposes tube of interest (ToI) pooling to extract features from tube proposals, which consist of a fixed number of bounding boxes of variable sizes. The ToI pooling performs RoI pooling in each of the bounding boxes, followed by a temporal pooling over the feature encoding of all the bounding boxes. Contrary to the 3D RoI pooling proposed in this paper, which extracts features of the temporal proposals from the whole video feature encoding, the tube of interest pooling divides the 3-D pooling into a sequence of 2-D spatial pooling and then 1-D temporal pooling.</p><p>Weakly-supervised methods. Aside from supervised activity detection, a recent work <ref type="bibr" target="#b43">[44]</ref> has addressed weakly supervised activity localization from data labeled only with video level class labels by learning attention weights on shot based or uniformly sampled proposals. Another type of weakly supervised temporal activity localization is provided with an action sequence and paired video without temporal annotation that requires to localize each action to the input video under the action sequential constraint. The model in <ref type="bibr" target="#b44">[45]</ref> addresses this problem by a dynamic programming based strategy. We only focus on supervised temporal activity localization in this paper where the ground truth temporal annotation for each activity is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Detection</head><p>Activity detection in untrimmed videos is closely related to object detection in images. The inspiration for our work, Faster R-CNN <ref type="bibr" target="#b9">[10]</ref>, extends R-CNN <ref type="bibr" target="#b16">[17]</ref> and Fast R-CNN <ref type="bibr" target="#b27">[28]</ref> object detection approaches, incorporating RoI pooling and a region proposal network. Compared to recent object detection models e.g., SSD <ref type="bibr" target="#b45">[46]</ref> and R-FCN <ref type="bibr" target="#b46">[47]</ref>, Faster R-CNN is a general and robust object detection framework that has been deployed on different datasets with little data augmentation effort. Like Faster R-CNN, our R-C3D model is also designed with the goal of easy deployment on varied activity detection datasets. It avoids making certain assumptions based on unique characteristics of a dataset, such as the UPC model for Activ-ityNet <ref type="bibr" target="#b13">[14]</ref> which assumes that each video contains a single activity class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optical Flow</head><p>Over the last few years, use of optical flow to represent motion has improved many video related tasks such as activity classification <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, video description <ref type="bibr" target="#b47">[48]</ref>, visual odometry <ref type="bibr" target="#b48">[49]</ref> and pose estimation in videos <ref type="bibr" target="#b49">[50]</ref> among others. Flow has been used as additional input in detecting activities too. Histogram of Optical Flow was used in <ref type="bibr" target="#b50">[51]</ref> to characterize the temporal signature of the actions and to generate activity proposals by learning a sparse dictionary of the features. We, on the other hand, use optical flow as an additional input in a 'classification by proposal' framework for temporal activity detection. Concurrent to our work, Structured Segment Network <ref type="bibr" target="#b41">[42]</ref> models each activity instance as a composition of three major stages namely 'starting', 'course' and 'ending', and uses both RGB values and optical flow field features for each stage to detect activities. In a similar framework as above, Dai et. al. <ref type="bibr" target="#b51">[52]</ref>, also uses optical flow and RGB features from the candidate proposals as well as the surrounding contexts and has shown good activity detection performance in benchmark datasets. In the temporal activity detection models <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b51">[52]</ref>, the two-stream action recognition model <ref type="bibr" target="#b11">[12]</ref> with optical flow branch is used as feature extractor where 2D convolutional operations are applied on the optical flow input stream. However, 3D convolutional operations are applied on the optical flow input stream in our case. We also employ online hard mining of training examples to further boost the activity detection performance. Next, we will discuss, in brief, about some of the relevant works in hard mining of training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hard Mining</head><p>Class imbalance between positive and negative proposals is one of the main obstacles for training a jointly optimized object or activity detection system. One of the earliest attempts <ref type="bibr" target="#b26">[27]</ref> tried to address this problem by freezing the model during training and letting it choose the examples which are hard depending on the misclassification errors. At the next iteration the model was trained with these hard examples and it became more and more robust until convergence. The deformable part based object detection model <ref type="bibr" target="#b52">[53]</ref> uses a slight variation of the above, where easily classified examples are removed along with the addition of hard examples. Some of the recent works <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> select hard examples while training deep networks based on the current loss for each example. Lin et. al. <ref type="bibr" target="#b24">[25]</ref> proposes a new loss function for dealing with class imbalance where the loss for hard examples is dynamically set to high values until confidence in the correct class increases. Region-Based Object Detectors with OHEM <ref type="bibr" target="#b25">[26]</ref>, on the other hand, proposes an online hard example mining strategy where the detection network is used in the inference mode to get a list of proposals ranked in descending order of their loss values. The training set is formed of the top few hard examples from this list and these are used for back-propagating the loss through the network. The boost in performance for object detection and the online nature of the algorithm inspire us to adopt OHEM for the R-C3D network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>In this section, first, the single stream Region Convolutional 3D Network (R-C3D), a convolutional neural network for activity detection in continuous video streams will be described. Next, the two stream architecture with OHEM will be discussed. The single stream network, illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, consists of three components: a shared 3D ConvNet feature extractor <ref type="bibr" target="#b8">[9]</ref>, a temporal proposal stage, and an activity classification and refinement stage. To enable efficient computation and joint training, the proposal and classification subnetworks share the same C3D feature maps. The proposal subnet predicts variable length temporal segments that potentially contain activities, while the classification subnet classifies these proposals into specific activity categories or background, and further refines the proposal segment boundaries. A key innovation is to extend the 2D RoI pooling in Faster R-CNN to 3D RoI pooling which allows our model to extract features at various resolutions for variable length proposals. Next, we describe different parts of the architecture starting with the shared video feature hierarchies in Sec. III-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D Convolutional Feature Hierarchies</head><p>We use a 3D ConvNet to extract rich spatio-temporal feature hierarchies from a given input video buffer. It has been shown that both spatial and temporal features are important for representing videos, and a 3D ConvNet encodes rich spatial and temporal features in a hierarchical manner. The input video frames have dimension 3 × L × H × W , where 3 is the number of color channels, L is the number of frames, H is the height and W is the width. The architecture of the 3D ConvNet is taken from the C3D architecture proposed in <ref type="bibr" target="#b8">[9]</ref>. However, unlike <ref type="bibr" target="#b8">[9]</ref>, the input to our model is of variable length. We adopt the convolutional layers (conv1a to conv5b) of C3D, so that a feature map</p><formula xml:id="formula_0">C conv5b ∈ R 512× L 8 × H 16 × W 16</formula><p>(512 is the channel dimension of the layer conv5b) is produced as the output of this subnetwork. We use C conv5b activations as the shared input to the proposal and classification subnets. The height (H) and width (W ) of the frames are taken as 112 each following <ref type="bibr" target="#b8">[9]</ref>. The number of frames L can be variable and is only limited by memory. Note that the same frames are used for computing optical flow fields in the second stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Temporal Proposal Subnet</head><p>To allow the model to predict variable length proposals, we incorporate anchor segments into the temporal proposal sub-network. The subnet predicts potential proposal segments with respect to anchor segments and a binary label indicating whether the predicted proposal contains an activity or not. The anchor segments are pre-defined multiscale windows centered at L/8 uniformly distributed temporal locations. Each temporal location specifies K anchor segments, each at different scales. Thus, the total number of anchor segments is (L/8) * K. The same set of K anchor segments exists in different temporal locations, which ensures that the proposal prediction is temporally invariant. The anchors serve as reference activity segments for proposals at each temporal location, where the maximum number of scales K is dataset dependent.</p><p>To obtain features at each temporal location for predicting proposals, we first add a 3D convolutional filter with kernel size 3×3×3 on top of C conv5b to extend the temporal receptive field for the temporal proposal subnet. Then, we downsample the spatial dimensions (from H 16 × W 16 to 1 × 1) to produce a temporal only feature map C tpn ∈ R 512× L 8 ×1×1 by applying a 3D max-pooling filter with kernel size 1× H 16 × W 16 . The 512dimensional feature vector at each temporal location in C tpn is used to predict a relative offset {δc i , δl i } to the center location and the length of each anchor segment {c i , l i }, i ∈ {1, · · · , K}. It also predicts the binary scores for each proposal being an activity or background. The proposal offsets and scores are predicted by adding two 1×1×1 convolutional layers on top of C tpn . Training: For training, we need to assign positive/negative labels to the anchor segments. Following the standard practice in object detection <ref type="bibr" target="#b9">[10]</ref>, we choose a positive label if the anchor segment 1) overlaps with some ground-truth activity with temporal Intersection-over-Union (tIoU) higher than 0.7, or 2) has the highest tIoU overlap with some ground-truth activity. If the anchor segment has tIoU overlap lower than 0.3 with all ground-truth activities, then it is given a negative label. All others are held out from training. For proposal regression, ground truth activity segments are transformed with respect to nearby positive anchor segments using the coordinate transformations described in Sec. III-D. Generally, the number of negative proposals is much more than the positive proposals due to smaller number of ground truth activity segments. To avoid producing a degenerate proposal generation module by the presence of overwhelming number of negative candidates, we fixed the foreground-to-background ratio to 1 : 1 per training batch, and the batch size in the proposal subnet is set to be 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Activity Classification Subnet</head><p>The activity classification stage has three main functions: 1) selecting proposal segments from the previous stage, 2) threedimensional region of interest (3D RoI) pooling to extract fixed-size features for selected proposals, and 3) activity classification and boundary regression for the selected proposals based on the pooled features. Some activity proposals generated by the proposal subnet highly overlap with each other and some have low proposal scores indicating low confidence. Following the standard practice in object detection <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b52">[53]</ref> and activity detection <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b15">[16]</ref>, we employ a greedy Non-Maximum Suppression (NMS) strategy to eliminate highly overlapping and low confidence proposals. The NMS threshold is set to 0.7.</p><p>The selected proposals can be of variable length. However we need to extract fixed-size features for each of them in order to use fully connected layers for further activity classification and regression. We design a 3D RoI pooling layer to extract the fixed-size volume features for each variable-length proposal from the shared convolutional features C conv5b ∈ R 512×(L/8)×7×7 (shared with the temporal proposal subnet). Specifically, in 3D RoI pooling, an input feature volume of size, say, l × h × w is divided into l s × h s × w s sub-volumes each with approximate size l ls × h hs × w ws , and then max pooling is performed inside each sub-volume. In our case, suppose a proposal has the feature volume of size l p ×7×7 in C conv5b , then this feature volume will be divided into 1 × 4 × 4 grids and max pooled inside each grid. Thus, proposals of variable lengths give rise to output volume features of the same size 512×1×4×4.</p><p>The output of the 3D RoI pooling is fed to a series of two fully connected layers. Here, the proposals are classified to activity categories by a classification layer and the refined startend times for these proposals are given by a regression layer. The classification and regression layers are also two separate fully connected layers and for both of them the input comes from the aforementioned fully connected layers (after the 3D RoI pooling layer). Training: We need to assign activity label to each proposal for training. An activity label is assigned if the proposal has the highest tIoU overlap with a ground-truth activity, and at the same time, the tIoU overlap is greater than 0.5. A background label is assigned to proposals with tIoU overlap lower than 0.5 with all ground-truth activities. The batch size in the classification subnet is set as 128, and training batches are chosen with positive/negative ratio of 1:3. Later we will show that substituting this heuristic with OHEM increases the efficiency of the detection further in Sec. III-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization</head><p>We train the network by optimizing both the classification and regression tasks jointly for the two subnets. The softmax loss function is used for classification, and smooth L1 loss function <ref type="bibr" target="#b27">[28]</ref> is used for regression. Specifically, the objective function is given by:</p><formula xml:id="formula_1">Loss = 1 N cls i L cls (ai, a * i ) + λ 1 Nreg i a * i Lreg(ti, t * i )<label>(1)</label></formula><p>where N cls and N reg stand for batch size and the number of positive anchor/proposal segments, λ is the loss trade-off parameter and is set to a value 1. i is the anchor/proposal segments index in a batch, a i is the predicted probability of the proposal or activities, a * i is the ground truth, t i = {δĉ i , δl i } represents predicted relative offset to transformed ground truth segments. t * i = {δc i , δl i } represents the coordinate transformation of ground truth segments to anchor segments or proposals. The coordinate transformations are computed as follows:</p><formula xml:id="formula_2">δc i = (c * i − c i )/l i δl i = log(l * i /l i )<label>(2)</label></formula><p>where c i and l i are the center location and the length of anchor segments or proposals while c * i and l * i denote the same for the ground truth activity segments.</p><p>In our R-C3D model, the above loss function is applied for both the temporal proposal subnet and the activity classification subnet. In the proposal subnet, the binary classification loss L cls predicts whether the proposal contains an activity or not, and the regression loss L reg optimizes the relative offset between proposals and ground truths. Here the losses are activity class agnostic. For the activity classification subnet, the multiclass classification loss L cls predicts the specific activity class for the proposal, and the number of classes are the number of activities plus one for the background. The regression loss L reg optimizes the relative displacement between activities and ground truths. All four losses for the two subnets are optimized jointly. The network is optimized using SGD solver with momentum 0.9 and weight decay 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Two Stream Model</head><p>Action representation is crucial for good performance. Inspired by the recent success of two-stream CNNs for action classification <ref type="bibr" target="#b11">[12]</ref>, we posit that a natural approach for better action representation is to use optical flow features in a second stream. The input is a stack of optical flow fields computed for each frame of the first stream (denoted as the RGB stream). The two-stream R-C3D architecture is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>A dense optical flow field for two consecutive frames of a video gives a two-dimensional vector at each pixel. The horizontal and vertical components of the flow are taken as two separate channels. TVL1 optical flow algorithm <ref type="bibr" target="#b55">[56]</ref> is used to compute the optical flow fields. The optical flow input has dimension 2×(L−1)×H ×W . A 3-D convolutional operation is performed separately on this stream. The output conv feature maps from both the streams are fused before the proposal generation stage. The fusion of the two features allows us to use the same proposal subnet (Sec. III-B) without having to learn too many extra parameters. We tried two different fusion strategies. In the first one, the output of the two streams after the 3D convolution stage is element-wise summed, while in the second, the feature maps from the optical flow stream are added as additional channels to the RGB stream feature maps.</p><p>The fused features from the two streams are used to generate the proposals using the Proposal Subnet (Sec. III-B).</p><p>After the proposals are obtained, NMS is performed to suppress the highly overlapping and low confidence proposals as described in Sec. III-C. The proposals that remain after the NMS step are projected separately on two conv5b feature maps obtained in two different streams. 3-D RoI pooling is performed on the projected features in both streams separately. The RoI pooled proposal features are then passed through a series of two fully-connected layers. Before performing the final classification and boundary regression of the candidate proposals, the outputs from the two separate streams are fused. Similar to the Proposal Subnet, here also we explore two different fusion strategies. Firstly, we sum the response of the two streams element-wise and secondly the two responses are concatenated to be fed to the final activity classification and boundary regression layers. We show results for both fusion strategies unless otherwise mentioned.</p><p>Since the final classification or regression are performed on the fused feature vector, the loss formulation for the joint training remains the same as the single stream architecture (Sec. III-D). This is also true for the proposal classification and time regression losses in the Proposal Subnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Online Hard Example Mining</head><p>Mining hard examples is aimed at choosing better, more informative examples for training the model. Inspired by the effectiveness as well as easy integration of the online hard example mining strategy in Fast R-CNN image detection pipeline <ref type="bibr" target="#b25">[26]</ref>, we experiment with a similar strategy in our R-C3D network. While R-C3D with OHEM has a different strategy for choosing training examples than the original R-C3D, they follow the same prediction procedure. The original R-C3D chooses training examples using a fixed positive-tonegative example ratio in fixed sized batches. R-C3D with OHEM precomputes the loss for all of the candidate proposals and then chooses only the hard examples, i.e. the ones with high loss.</p><p>Hard training examples could have been mined for both the proposal and the classification subnets. However, the proposal subnet involves binary classification and thus the training examples are more evenly balanced between positive and negative classes. In this paper, we apply OHEM only in the classification subnet as it involves multi-class classification and thus suffers more from training data imbalance when the number of categories is large.</p><p>In the classification subnet, we add an extra read-only classification branch which shares the weights with the original classification subnet. The "read-only" classification branch is only used to compute loss for all the proposals generated by the proposal subnet and does not update its weights during the backpropagation stage. Specifically, the sum of the classification loss and regression loss for each proposal is computed in this read-only clone. The loss values represent how well the current network performs on each proposal. Proposals are sorted according to the sum of the losses in descending order and only top 128 are taken to form the training minibatch. After the hard examples are chosen, the backward pass is performed only with these hard examples in the original classification subnet. Since the read-only classification subnet shares weights with the original classification subnet, it also gets updated with the same weights in the next iteration and the losses are computed using the new weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Prediction</head><p>Activity prediction in R-C3D consists of two steps. First, the proposal subnet generates candidate proposals and predicts the start-end time offsets as well as proposal score for each. Then the proposals are refined via NMS with threshold value 0.7. After NMS, the selected proposals are fed to the classification network to be classified into specific activity classes, and the activity boundaries of the predicted proposals are further refined by the regression layer. The boundary prediction in both proposal subnet and classification subnet is in the form of relative displacement of center point and length of segments. In order to get the start time and end time of the predicted proposals or activities, inverse coordinate transformation to Equation 2 is performed.</p><p>R-C3D accepts variable length input videos. However, to take advantage of the vectorized implementation in fast deep learning libraries, we pad the last few frames of short videos with last frame, and break long videos into buffers (limited by memory only). NMS at a lower threshold (0.1 less than the mAP evaluation threshold) is applied to the predicted activities to get the final activity predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate R-C3D on three large-scale activity detection datasets -THUMOS'14 <ref type="bibr" target="#b28">[29]</ref>, Charades <ref type="bibr" target="#b30">[31]</ref> and Ac-tivityNet <ref type="bibr" target="#b29">[30]</ref>. In evaluating the proposals, we follow the evaluation paradigm for the temporal localization task in ActivityNet dataset (ref. IV-B) and use the Area Under the AR vs AN curve (AUC) at 100 proposals per video which is averaged across ten different tIoU thresholds uniformly distributed between 0.5 and 0.95. Activity detection results are shown in terms of mean Average Precision -mAP@α where α denotes different tIoU thresholds, as is the common practice in the literature. Section IV-D provides the detection speed in comparison to state-of-the-art activity detection approaches. Since the GPU memory is limited, we first create a buffer of 768 frames at 25 frames per second (fps) which means approximately 30 seconds of video. Our choice is motivated by the fact that 99.5% of all activity segments in the validation set (used here as the training set) are less than 30 seconds long. These buffers act as inputs to both streams of R-C3D. We can create the buffer by sliding from the beginning of the video to the end, denoted as the 'one-way buffer'. An additional pass from the end of the video to the beginning is used to increase the amount of training data, denoted as the 'two-way buffer'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on THUMOS'14</head><p>We initialize the 3D ConvNet part of our model with C3D weights trained on Sports-1M and finetuned on UCF101 released by the authors in <ref type="bibr" target="#b8">[9]</ref>. We allow all the layers of R-C3D to be trained on THUMOS'14 with a fixed learning rate of 0.0001. The number of anchor segments K chosen for this dataset is 10 with specific scale values <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>. The values are chosen according to the distribution of the activity durations in the training set. At 25 fps and temporal pooling factor of 8 (C tpn downsamples the input by 8 temporally), the anchor segments correspond to segments of duration between 0.64 and 5.12 seconds 1 . Note that, the predicted proposals or activities are relative to the anchor segments but not limited to the anchor boundaries, enabling our model to detect flexible-length activities. For the two-stream R-C3D, we explored two ways of fusing the RGB and flow streams. The first strategy is denoted as 'concat' and it concatenates both streams before the proposal subnet and before the final classification stage. The second strategy sums the two streams element-wise and is denoted as 'sum'. We still use the RGB C3D weights released in <ref type="bibr" target="#b8">[9]</ref> to initialize the RGB stream. In order to initialize the flow stream, we follow the pipeline proposed in <ref type="bibr" target="#b22">[23]</ref> to get a set of pretrained flow C3D weights on UCF101 flow images. Before finetuning flow C3D weights on UCF101 flow images, the flow C3D model is initialized with RGB C3D weights. The flow C3D have the same architecture as that of the RGB C3D network in all the layers except the first layer due to different channel dimensions for the flow and RGB inputs (2 vs 3). To make use of the weights in the first layer of RGB C3D model, we average the weights across channels in first convolution layer of RGB C3D model, and replicate this average weights in the two channels of the optical flow input. We test the efficiency of the C3D architecture for both the RGB as well as the flow streams. The RGB C3D shows around 80% activity recognition accuracy on UCF101 split-1 while the same for the flow C3D is around 70%. The learning strategy in flow stream is kept same as the RGB stream.</p><p>We apply OHEM on both the RGB single-stream R-C3D and two-stream R-C3D. These two experimental settings are denoted as 'Single-stream R-C3D + OHEM' and 'Two-stream R-C3D (Sum) + OHEM' respectively. Note that in the later setting we opt for the element-wise sum strategy of fusion as the performance corresponding to this strategy is better than the concatenation strategy for the Two-stream R-C3D without hard mining. The NMS threshold used for the OHEM setting is 0.7, and the batch size in the activity classification stage is set as 128 which is the same for the other datasets too. Results: We first evaluate the performance of the temporal proposal subnet. The proposals are first processed with NMS threshold 0.7. The proposal evaluation results in terms of average AUC with 100 proposals per video for single-stream R-C3D and two-stream R-C3D models are shown in <ref type="table" target="#tab_0">Table I</ref>. The average AUC for single-stream R-C3D is 26.36%. Both the two-stream extensions improve the proposal evaluation results, with two-stream R-C3D (Concat) having the average AUC 28.75% and two-stream R-C3D (Sum) 29.71%.</p><p>In <ref type="table" target="#tab_0">Table II</ref>, we present a comparative evaluation of the activity detection performance of our model with existing stateof-the-art approaches in terms of mAP at tIoU thresholds 0.1-0.5 (denoted as α). From the results corresponding to the single-stream R-C3D, we can see that the mAP@0.5 with the two-way buffer setting is better than the mAP@0.5 with the one-way buffer by 1.9%. So we take the two-way buffer setting as the data augmentation strategy for all the two-stream experiments unless otherwise mentioned. Both the two-stream R-C3D achieve better activity detection performance than the single-stream R-C3D. However the differences in performances between two-stream (sum) and two-stream (concat) are very minor in terms of mAP@α metric, with two-stream (sum) achieving slightly better results. When OHEM is applied to single-stream R-C3D and twostream R-C3D (sum), the performance improves significantly for both the scenarios. However, the relative improvement of OHEM from two-stream R-C3D (sum) is less than the single-stream R-C3D. With OHEM, single-stream R-C3D and two-stream R-C3D (sum) have almost the same results. The two-stream R-C3D (sum) architecture with OHEM achieves a new state-of-the-art in the mAP@0.5 metric, which requires a stringent overlap with the ground truth segment. The absolute improvement is 6.3% over the current state-of-the-art <ref type="bibr" target="#b41">[42]</ref>.</p><p>The Average Precision (AP) for each class on THUMOS'14 at tIoU threshold 0.5 is shown in <ref type="table" target="#tab_0">Table III</ref>. Compared to other published models, single-stream R-C3D outperforms previous methods in most classes and shows significant improvement (by more than 20% absolute AP over the next best) for activities e.g., Basketball Dunk, Cliff Diving, and Javelin Throw. With the addition of the optical flow stream, we can see two-stream R-C3D (sum) improves the detection performance significantly for activities with obvious motion patterns e.g., In single-stream R-C3D with OHEM, some hard activities with low class precision get further improvement e.g., Billiards, Cricket Shot and Shotput. <ref type="figure" target="#fig_3">Figure 4(a)</ref> shows some representative qualitative results from two videos on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on ActivityNet</head><p>The ActivityNet <ref type="bibr" target="#b29">[30]</ref> dataset consists of untrimmed videos and is released in three versions. We use the latest release (1.3) which has 10024, 4926 and 5044 videos containing 200 different types of activities in the train, validation and test sets respectively. Most videos contain activity instances of a single class covering a great deal of the video. Compared to THUMOS'14, this is a large-scale dataset both in terms of the number of activities involved and the amount of videos. Researchers have taken part in the ActivityNet challenge <ref type="bibr" target="#b57">[58]</ref> held on this dataset. The performances of the participating teams are evaluated on test videos for which the ground truth annotations are not public. In addition to evaluating on the validation set, we show our performance on the test set after evaluating it on the challenge server. Experimental Setup: Similar to THUMOS'14, the length of the input buffer is set to 768 but, as the videos are long, we sample frames at 3 fps to fit it into the GPU. This makes the duration of the buffer approximately 256 seconds covering over 99.99% training activities. The considerably long activity durations prompt us to set the number of anchor segments K to be as high as 20. Specifically, we chose the following scales - <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr">64]</ref>. Thus the shortest and the longest anchor segments are Considering the vast domain difference of the activities between Sports-1M and ActivityNet, we finetune the Sports-1M pretrained RGB 3D ConvNet model <ref type="bibr" target="#b8">[9]</ref> with the training videos of ActivityNet at 3 fps on the activity classification task. We initialize the RGB 3D ConvNet part of our model with these finetuned weights. We also use the Sports-1M pretrained RGB 3D ConvNet model to initialize the flow 3D ConvNet model and finetune it on the flow images of the ActivityNet training videos as is done for the THUMOS'14 dataset (ref.</p><p>Sec. IV-A). We test the efficiency of the C3D architecture for both the RGB and flow streams on ActivityNet. The RGB C3D shows around 60% activity recognition accuracy while the same for the flow C3D is around 32%. AcitivityNet being a large scale dataset, the training takes more epochs. As a speedefficiency trade-off, we freeze the first two convolutional layers in our model during training. The learning rate is kept fixed at 10 −4 for first 10 epochs and is decreased to 10 −5 for the last 5 epochs. Based on the improved results on the THUMOS'14, we choose the two-way buffer setting with horizontal flipping of frames for data augmentation.  Results: We first evaluate the proposal performance from the proposal subnet after the NMS step with threshold 0.7. The proposal evaluation results in terms of average AUC with 100 proposals per video for single-stream R-C3D and two-stream R-C3D models are shown in <ref type="table" target="#tab_0">Table IV</ref>. Both the two-stream extensions do not improve the proposal results and maintain almost the same performance as the single-stream R-C3D.  using other hand engineered features with a possible boost to performance and we keep this as a future task. UPC is a fair comparison to single stream R-C3D as it also uses only C3D features. However, it relies on a strong assumption that each video on ActivityNet just contains one activity class. Our approach obtains 4.3% improvement on the validation set and 4.5% improvement on the test set over UPC <ref type="bibr" target="#b13">[14]</ref> in terms of mAP@0.5 without such strong assumptions. When both training and validation sets are used for training, the performance improves further by 1.6%. As is the scenario in proposal evaluation, both the twostream extensions of R-C3D provide roughly the same detection performance as the single-stream R-C3D. One reason can be that the initial flow C3D branch of two-stream R-C3D has only around 32% classification accuracy on the ActivityNet validation set which is significantly lower than the classification performance (60%) by the initial RGB branch on the same dataset. As a result of the poor initialization of the flow C3D model, the two-stream extensions do not improve the results much. Since our proposed R-C3D activity detection model is based on low level 3D conv features learned by the C3D models, a better classification model with better initialization can give rise to better activity detection performance. We expect better performance by initializing with stronger classification architecture like I3D <ref type="bibr" target="#b20">[21]</ref> and pre-training with larger and more diverse video dataset like kinetics <ref type="bibr" target="#b20">[21]</ref> or Youtube-8M <ref type="bibr" target="#b59">[60]</ref>.</p><p>We only experiment with OHEM on single-stream R-C3D, since without OHEM, single-stream R-C3D performs almost the same as the two-stream R-C3D and a single-stream R-C3D with OHEM is relatively less expensive in terms of computation. Single-stream R-C3D with OHEM provides better performance with mAP@0.5 reaching 27.7% compared to single-stream R-C3D without OHEM. As further analysis, we divide the videos in the validation set of ActivityNet into three sets (Short/Medium/Long) in equal numbers according to the ground truth activity durations. Single-stream R-C3D with OHEM gets 39.2% mAP@0.5 on Long set (greater than 53 seconds), 23.2% mAP@0.5 on Medium set (13 seconds -53 seconds), and 8.3% mAP@0.5 on Short set (less than 13 seconds). This shows that our model is poor on the relatively short activities on this dataset. The reason might be that for ActivityNet, we sample frames at 3 fps to fit the long videos into the GPU memory. For short videos, this implies that only a few frames are sampled for proposal feature encoding. The poor feature encoding from limited frames might be the reason for the poor performance of these short activities on this dataset.</p><p>Our highest mAP@0.5 result is still lower than Zhao et. al. <ref type="bibr" target="#b41">[42]</ref> which uses an additional set of 200 separately trained binary classifiers to classify whether the predicted activity segments are complete or not. These completeness classifiers pay special attention to improve the tIoU of the predicted activity segments which is one of the main reasons to boost their detection results. Our model can also benefit from using such completeness classifiers at the cost of more computation.</p><p>The ActivityNet Challenge in 2017 introduced a new evaluation metric where mAP at 10 evenly distributed thresholds between [0.5, 0.95] are averaged to get the average mAP. We show the performance of single-stream R-C3D with and without OHEM in <ref type="table" target="#tab_0">Table VI</ref>. Training with videos from the training partition only, the average mAP for the validation and test set come to be 12.7% and 13.1% respectively. If both training and validation data are used during training, the average mAP for the test set increases to 16.7% showing the benefit of our jointly optimized single-stream model when more data is available for training. The OHEM extension also improves the single-stream results in terms of average mAP. <ref type="figure" target="#fig_3">Figure 4</ref>(b) shows some representative qualitative results of R-C3D from this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on Charades</head><p>Charades <ref type="bibr" target="#b30">[31]</ref> is a recently introduced dataset for activity classification and detection. The dataset consists of 7985 train and 1863 test videos from 157 classes. The videos are recorded by Amazon Mechanical Turk users based on provided scripts. Apart from low illumination, diversity and casual nature of the videos containing day-to-day activities, an additional challenge of this dataset is the abundance of overlapping activities, sometimes multiple activities having exactly the same start and end times (typical examples include pairs of activities like 'holding a phone' and 'playing with a phone' or 'holding a towel' and 'tidying up a towel'). Experimental Setup: For this dataset we sample frames at 5 fps, and the input buffer is set to contain 768 frames. This makes the duration of the buffer approximately 154 seconds covering all the ground truth activity segments in the train set. As the activity segments are longer, we choose the number of anchor segments K to be 18 with specific scale values <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48]</ref>. So the shortest anchor segment has a duration of 1.6 seconds and We first finetune the Sports-1M pretrained RGB C3D model <ref type="bibr" target="#b8">[9]</ref> on the Charades training set at 5 fps and initialize the 3D ConvNet for the RGB stream with these finetuned weights. The flow stream initialization follows the same pipeline mentioned in Sec. IV-A and Sec. IV-B. Both the RGB C3D model and flow C3D model have very low activity classification accuracy (9.6% and 8.8% respectively), due to the multi-label nature of the activity segments on this dataset. While training the full model, we freeze the first two convolutional layers in order to accelerate training. The learning rate is kept fixed at 0.0001 for the first 10 epochs and then decreased to 0.00001 for 5 further epochs. We augment the data by following the two-way buffer setting and horizontal flipping of frames. Results: The proposal evaluation results in terms of average AUC with 100 proposals per video for single-stream R-C3D and two-stream R-C3D models are shown in <ref type="table" target="#tab_0">Table VII</ref>. For this dataset also, both the two-stream extensions have almost the same performance as the single-stream R-C3D.</p><p>Table VIII provides a comparative evaluation with various baseline models reported in <ref type="bibr" target="#b60">[61]</ref>. This approach [61] trains a CRF based video classification model (asynchronous temporal fields) and evaluates the prediction performance on 25 equidistant frames by making a multi-label prediction for each frame. The activity localization result is reported in terms of mAP metric on these frames. For a fair comparison, we map our activity segment prediction to 25 equidistant frames and evaluate using the same mAP evaluation metric. A second evaluation strategy proposed in this work relies on a postprocessing stage where the frame level predictions are aver- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FPS S-CNN [3]</head><p>60 DAP <ref type="bibr" target="#b12">[13]</ref> 134.1 Single-stream R-C3D (Titan X Maxwell) <ref type="bibr" target="#b31">[32]</ref> 569 Single-stream R-C3D (Titan X Pascal) <ref type="bibr" target="#b31">[32]</ref> 1030 Two-stream R-C3D (Concat) (Titan X Pascal) * 656 Two-stream R-C3D (Sum) (Titan X Pascal) * 642 Single-stream R-C3D (OHEM) (Titan X Pascal) 1030 aged across 20 frames leading to more spatial consistency. As shown in <ref type="table" target="#tab_0">Table VIII</ref>, our jointly optimized single stream model outperforms the asynchronous temporal fields model <ref type="bibr" target="#b60">[61]</ref> as well as several baselines reported in the same paper <ref type="bibr" target="#b60">[61]</ref>. While the improvement over the standard method is as high as 2.8%, the improvement after the post-processing is not as high.</p><p>One possible reason could be that our jointly optimized fully convolutional model captures the spatial consistency implicitly without requiring any manually-designed post-processing. Similar to the proposal evaluation performance, the addition of the flow stream in the two-stream extensions does not affect the activity detection performance much on this dataset. The relatively weak underlying RGB and flow C3D architectures and the poor illumination conditions on the dataset may be the reasons behind the stalled performances of the two-stream extensions. Initialization with a better C3D classification model trained on indoor videos with these challenging conditions (e.g., the low illumination indoor scenes or the multi-label nature of the data) could possibly boost the performance. Employing hard mining improves the activity detection result of single-stream R-C3D by around 0.6% in both standard and post-processing settings. <ref type="figure" target="#fig_3">Figure 4</ref>(c) shows some representative qualitative results of R-C3D from one video in this dataset. One of the major challenges of this dataset is the presence of a large number of temporally overlapping activities. The results show that our model is capable of handling such scenarios to some extent. This is achieved by the ability of the proposal subnet to produce possibly overlapping activity proposals and is further facilitated by the segment offset regression per activity class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Activity Detection Speed</head><p>In this section, we compare detection speed of our model with two other state-of-the-art methods. The comparison results are shown in <ref type="table" target="#tab_0">Table IX</ref>. S-CNN <ref type="bibr" target="#b2">[3]</ref> uses a time-consuming sliding window strategy and predicts at 60 fps. DAP <ref type="bibr" target="#b12">[13]</ref> incorporates a proposal prediction step on top of LSTM and predicts at 134.1 fps. R-C3D constructs the proposal and classification pipeline jointly and these two stages share the features making it significantly faster. The speed of execution is 569 fps on a single Titan-X (Maxwell) GPU for the proposal and classification stages together. On the upgraded Titan-X (Pascal) GPU, our inference speed reaches even higher (1030 fps). One of the reasons of the speedup of R-C3D over DAP may come from the fact that the LSTM recurrent architecture in DAP takes time to unroll, while R-C3D directly accepts a wide range of frames as input and the convolutional features are shared by the proposal and classification subnets. The activity detection speed drops to 656 fps for two-stream concat extension and 642 fps for two-stream sum extension on pre-computed optical flow images, because of the additional computation for the flow stream 2 . OHEM extension of singlestream R-C3D maintains almost the same activity detection speed as single-stream R-C3D, since inference in both the models are essentially the same.</p><p>V. CONCLUSION In this paper, we explore a multi-stream network that augments RGB image features in R-C3D with motion features for temporal action detection in untrimmed videos. An online hard mining strategy improves the performance by intelligently training on hard examples. Our proposed method not only detects activities more accurately, but also detects them fast. Analysis of the experimental results and ablation studies indicates robustness of the method as well as significant result improvements over state-of-the-arts on three large-scale data sets with diverse characteristics. The future directions include investigating the related applications of our R-C3D framework in other computer vision tasks, e.g. dense video captioning and localizing moments in videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Single stream R-C3D model architecture with only RGB frames as input. The 3D ConvNet takes raw video frames as input and computes convolutional features. These are input to the Proposal Subnet that proposes candidate activities of variable length along with confidence scores. The Classification Subnet filters the proposals, pools fixed size features and then predicts activity labels along with refined segment boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Two stream R-C3D architecture. The top stream receives the stacked optical flow fields as input while the bottom stream receives the RGB frames as input. Two separate 3D convnets operate on the two inputs to produce the respective feature maps. These two feature maps are fused and fed to the proposal subnet which proposes candidate activity proposals along with their confidence scores. The classification subnet works on features from two separate streams but on the same set of proposals. The outputs of the two classification subnets are fused for the final activity classification and start-end time regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>THUMOS' 14</head><label>14</label><figDesc>activity detection dataset contains over 24 hours of video from 20 different sport activities. The training set contains 2765 trimmed videos while the validation and the test sets contain 200 and 213 untrimmed videos respectively. This dataset is particularly challenging as it consists of very long videos (up to a few hundreds of seconds) with multiple activity instances of very small duration (up to few tens of seconds). Most videos contain multiple activity instances of the same activity class. In addition, some videos contain activity segments from different classes. Experimental Setup: We divide 200 untrimmed videos from the validation set into 180 training and 20 held out videos to get the best hyperparameter setting. All 200 videos are used as the training set and the final results are reported on 213 test videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative visualization of the predicted activities by single-stream R-C3D (best viewed in color). Figure (a) and (b) show results for two videos each on THUMOS'14 and ActivityNet. (c) shows the result for one video from Charades. Ground truth activity segments are marked in black. Predicted activity segments are marked in green for correct predictions and in red for wrong ones. Predicted activities with tIoU more than 0.5 are considered as correct. Corresponding start-end times and confidence score are shown inside brackets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>PROPOSAL EVALUATION ON THUMOS'14 DATASET (IN PERCENTAGE). AVERAGE AUC OF 100 PROPOSALS PER VIDEO AT TIOU THRESHOLDS α ∈ (0.5, 0.95) WITH STEP 0.05 ARE REPORTED.</figDesc><table><row><cell></cell><cell>α ∈ (0.5, 0.95)</cell></row><row><cell>Single-stream R-C3D</cell><cell>26.36</cell></row><row><cell>Two-stream R-C3D (Concat)</cell><cell>28.75</cell></row><row><cell>Two-stream R-C3D (Sum)</cell><cell>29.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II .</head><label>II</label><figDesc>ACTIVITY DETECTION RESULTS ON THUMOS'14 (IN PERCENTAGE). MAP AT DIFFERENT TIOU THRESHOLDS α ARE REPORTED. TOP THREE PERFORMERS ON THUMOS'14 CHALLENGE LEADERBOARD AND OTHER RESULTS REPORTED IN EXISTING PAPERS ARE SHOWN. α 0.1 0.2 0.3 0.4 0.5 Karaman et. al. [1] 4.6 3.4 2.1 1.4 0.9 Wang et. al. [4] 18.2 17.0 14.0 11.7 8.3 Oneata et. al.</figDesc><table><row><cell>[2]</cell><cell cols="5">36.6 33.6 27.0 20.8 14.4</cell></row><row><cell>Heilbron et. al. [51]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.5</cell></row><row><cell>Escorcia et. al. [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.9</cell></row><row><cell>Richard et. al. [45]</cell><cell cols="5">39.7 35.7 30.0 23.2 15.2</cell></row><row><cell>Yeung et. al. [16]</cell><cell cols="5">48.9 44.0 36.0 26.4 17.1</cell></row><row><cell>Yuan et. al. [57]</cell><cell cols="5">51.4 42.6 33.6 26.1 18.8</cell></row><row><cell>Shou et. al. [3]</cell><cell cols="5">47.7 43.5 36.3 28.7 19.0</cell></row><row><cell>Shou et. al. [41]</cell><cell>-</cell><cell>-</cell><cell cols="3">40.1 29.4 23.3</cell></row><row><cell>Dai et. al. [52]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">33.3 25.6</cell></row><row><cell>Zhao et. al. [42]</cell><cell cols="5">66.0 59.4 51.9 41.0 29.8</cell></row><row><cell>Single-stream R-C3D [32] (one-way buffer)</cell><cell cols="5">51.6 49.2 42.8 33.4 27.0</cell></row><row><cell>Single-stream R-C3D [32] (two-way buffer)</cell><cell cols="5">54.5 51.5 44.8 35.6 28.9</cell></row><row><cell>Two-stream R-C3D (Concat)</cell><cell cols="5">54.5 52.2 46.9 40.0 33.1</cell></row><row><cell>Two-stream R-C3D (Sum)</cell><cell cols="5">56.6 54.2 48.9 40.6 33.4</cell></row><row><cell>Single-stream R-C3D + OHEM</cell><cell cols="5">57.4 54.9 51.1 43.1 35.8</cell></row><row><cell>Two-stream R-C3D (Sum) + OHEM</cell><cell cols="5">56.9 54.7 51.2 43.0 36.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III .</head><label>III</label><figDesc>PER-CLASS AP AT TIOU THRESHOLD α = 0.5 ON THUMOS'14 (IN PERCENTAGE).</figDesc><table><row><cell></cell><cell>[2]</cell><cell>[16]</cell><cell>[3]</cell><cell>Single-stream R-C3D (two-way buffer) [32]</cell><cell>Two-stream R-C3D (Sum)</cell><cell>Single-stream R-C3D + OHEM</cell></row><row><cell>Baseball Pitch</cell><cell cols="3">8.6 14.6 14.9</cell><cell>26.1</cell><cell>19.9</cell><cell>29.9</cell></row><row><cell>Basketball Dunk</cell><cell>1.0</cell><cell cols="2">6.3 20.1</cell><cell>54.0</cell><cell>55.3</cell><cell>48.6</cell></row><row><cell>Billiards</cell><cell>2.6</cell><cell>9.4</cell><cell>7.6</cell><cell>8.3</cell><cell>11.2</cell><cell>19.8</cell></row><row><cell>Clean and Jerk</cell><cell cols="3">13.3 42.8 24.8</cell><cell>27.9</cell><cell>33.2</cell><cell>37.7</cell></row><row><cell>Cliff Diving</cell><cell cols="3">17.7 15.6 27.5</cell><cell>49.2</cell><cell>54.0</cell><cell>59.4</cell></row><row><cell>Cricket Bowling</cell><cell cols="3">9.5 10.8 15.7</cell><cell>30.6</cell><cell>31.1</cell><cell>32.4</cell></row><row><cell>Cricket Shot</cell><cell>2.6</cell><cell cols="2">3.5 13.8</cell><cell>10.9</cell><cell>11.6</cell><cell>18.4</cell></row><row><cell>Diving</cell><cell cols="3">4.6 10.8 17.6</cell><cell>26.2</cell><cell>31.1</cell><cell>36.4</cell></row><row><cell>Frisbee Catch</cell><cell cols="3">1.2 10.4 15.3</cell><cell>20.1</cell><cell>21.5</cell><cell>16.9</cell></row><row><cell>Golf Swing</cell><cell cols="3">22.6 13.8 18.2</cell><cell>16.1</cell><cell>32.8</cell><cell>42.3</cell></row><row><cell>Hammer Throw</cell><cell cols="3">34.7 28.9 19.1</cell><cell>43.2</cell><cell>58.3</cell><cell>57.3</cell></row><row><cell>High Jump</cell><cell cols="3">17.6 33.3 20.0</cell><cell>30.9</cell><cell>37.9</cell><cell>37.8</cell></row><row><cell>Javelin Throw</cell><cell cols="3">22.0 20.4 18.2</cell><cell>47.0</cell><cell>47.2</cell><cell>59.2</cell></row><row><cell>Long Jump</cell><cell cols="3">47.6 39.0 34.8</cell><cell>57.4</cell><cell>62.1</cell><cell>63.9</cell></row><row><cell>Pole Vault</cell><cell cols="3">19.6 16.3 32.1</cell><cell>42.7</cell><cell>57.7</cell><cell>57.0</cell></row><row><cell>Shotput</cell><cell cols="3">11.9 16.6 12.1</cell><cell>19.4</cell><cell>20.0</cell><cell>31.0</cell></row><row><cell>Soccer Penalty</cell><cell>8.7</cell><cell cols="2">8.3 19.2</cell><cell>15.8</cell><cell>19.2</cell><cell>22.9</cell></row><row><cell>Tennis Swing</cell><cell>3.0</cell><cell cols="2">5.6 19.3</cell><cell>16.6</cell><cell>11.6</cell><cell>12.5</cell></row><row><cell>Throw Discus</cell><cell cols="3">36.2 29.5 24.4</cell><cell>29.2</cell><cell>41.0</cell><cell>22.1</cell></row><row><cell>Volleyball Spiking</cell><cell>1.4</cell><cell>5.2</cell><cell>4.6</cell><cell>5.6</cell><cell>11.3</cell><cell>11.2</cell></row><row><cell>mAP@0.5</cell><cell cols="3">14.4 17.1 19.0</cell><cell>28.9</cell><cell>33.4</cell><cell>35.8</cell></row></table><note>Hammer Throw, Golf Swing, Pole Vault and Throw Discus etc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV .</head><label>IV</label><figDesc>PROPOSAL EVALUATION ON ACTIVITYNET VALIDATION SET (IN PERCENTAGE). AVERAGE AUC OF 100 PROPOSALS PER VIDEO AT TIOU THRESHOLDS α ∈ (0.5, 0.95) WITH STEP 0.05 ARE REPORTED.</figDesc><table><row><cell></cell><cell>α ∈ (0.5, 0.95)</cell></row><row><cell>Single-stream R-C3D</cell><cell>56.7</cell></row><row><cell>Two-stream R-C3D (Concat)</cell><cell>56.2</cell></row><row><cell>Two-stream R-C3D (Sum)</cell><cell>55.6</cell></row><row><cell cols="2">of durations 2.7 and 170 seconds respectively covering 95.6%</cell></row><row><cell>of the training activities.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table V</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell>compares them with the published results from existing activity</cell></row><row><cell></cell><cell>detection approaches. In most experiments, the training set is</cell></row><row><cell></cell><cell>used for training and the performance is shown for either the</cell></row><row><cell></cell><cell>validation or test data or both. Some models in Table V make</cell></row><row><cell></cell><cell>use of sophisticated handcrafted features. The approach in [59]</cell></row><row><cell></cell><cell>also uses handcrafted motion features like MBH on top of</cell></row><row><cell></cell><cell>inception and C3D features in addition to dynamic program-</cell></row><row><cell>shows mAP@0.5 performance of our models and</cell><cell>ming based post processing. Our method is also capable of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V .</head><label>V</label><figDesc>DETECTION RESULTS ON ACTIVITYNET IN TERMS OF MAP@0.5 (IN %). THE TOP HALF OF THE TABLE SHOWS PERFORMANCE FROM METHODS USING ADDITIONAL HANDCRAFTED FEATURES WHILE THE BOTTOM HALF SHOWS APPROACHES USING DEEP FEATURES ONLY (INCLUDING OURS). RESULTS FOR [15] ARE TAKEN FROM<ref type="bibr" target="#b57">[58]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">train data validation</cell><cell>test</cell></row><row><cell cols="2">B. Singh et. al. [15]</cell><cell>train+val</cell><cell>-</cell><cell>28.8</cell></row><row><cell cols="2">G. Singh et. al. [59]</cell><cell>train</cell><cell>34.5</cell><cell>36.4</cell></row><row><cell cols="2">Dai et. al. [52]</cell><cell>train</cell><cell>36.2</cell><cell>37.5</cell></row><row><cell>UPC [14]</cell><cell></cell><cell>train</cell><cell>22.5</cell><cell>22.3</cell></row><row><cell cols="2">Zhao et. al. [42]</cell><cell>train</cell><cell>-</cell><cell>43.3</cell></row><row><cell cols="2">Single-stream R-C3D [32]</cell><cell>train</cell><cell>26.8</cell><cell>26.8</cell></row><row><cell cols="2">Single-stream R-C3D [32]</cell><cell>train+val</cell><cell>-</cell><cell>28.4</cell></row><row><cell cols="2">Two-stream R-C3D (Concat)</cell><cell>train</cell><cell>25.8</cell><cell>-</cell></row><row><cell cols="2">Two-stream R-C3D (Sum)</cell><cell>train</cell><cell>26.5</cell><cell>-</cell></row><row><cell cols="2">Single-stream R-C3D + OHEM</cell><cell>train</cell><cell>27.7</cell><cell>-</cell></row><row><cell>TABLE VI.</cell><cell cols="4">RESULTS ON ACTIVITYNET IN TERMS OF AVERAGE MAP</cell></row><row><cell cols="5">AT TIOU THRESHOLDS α ∈ (0.5, 0.95) WITH STEP 0.05 (IN %).</cell></row><row><cell></cell><cell></cell><cell cols="2">train data validation</cell><cell>test</cell></row><row><cell cols="2">Single-stream R-C3D [32]</cell><cell>train</cell><cell>12.7</cell><cell>13.1</cell></row><row><cell cols="2">Single-stream R-C3D [32]</cell><cell>train+val</cell><cell>-</cell><cell>16.7</cell></row><row><cell cols="2">Single-stream R-C3D + OHEM</cell><cell>train</cell><cell>15.4</cell><cell>15.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII .</head><label>VII</label><figDesc>PROPOSAL EVALUATION RESULTS ON CHARADES (IN PERCENTAGE). AVERAGE AUC OF 100 PROPOSALS PER VIDEO AT TIOU THRESHOLDS α ∈ (0.5, 0.95) WITH STEP 0.05 ARE REPORTED. 96% of the activities in the training set is under 76.8 seconds. For this dataset we, additionally, explore slightly different settings of the anchor segment scales, but find that our model is not very sensitive to this hyper-parameter.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">α ∈ (0.5, 0.95)</cell></row><row><cell cols="2">Single-stream R-C3D</cell><cell>70.0</cell></row><row><cell cols="2">Two-stream R-C3D (Concat)</cell><cell>69.2</cell></row><row><cell cols="2">Two-stream R-C3D (Sum)</cell><cell>69.6</cell></row><row><cell>TABLE VIII.</cell><cell cols="3">ACTIVITY DETECTION RESULTS ON CHARADES (IN %).</cell></row><row><cell cols="4">WE REPORT RESULTS USING THE SAME EVALUATION METRIC AS IN [61].</cell></row><row><cell></cell><cell></cell><cell cols="2">mAP</cell></row><row><cell></cell><cell></cell><cell cols="2">standard post-process</cell></row><row><cell cols="2">Random [61]</cell><cell>4.2</cell><cell>4.2</cell></row><row><cell>RGB [61]</cell><cell></cell><cell>7.7</cell><cell>8.8</cell></row><row><cell cols="2">Two-Stream [61]</cell><cell>7.7</cell><cell>10.0</cell></row><row><cell cols="2">Two-Stream+LSTM [61]</cell><cell>8.3</cell><cell>8.8</cell></row><row><cell cols="2">Sigurdsson et al. [61]</cell><cell>9.6</cell><cell>12.1</cell></row><row><cell cols="2">Single-stream R-C3D [32]</cell><cell>12.4</cell><cell>12.7</cell></row><row><cell cols="2">Two-stream R-C3D (Concat)</cell><cell>12.4</cell><cell>12.6</cell></row><row><cell cols="2">Two-stream R-C3D (Sum)</cell><cell>12.5</cell><cell>12.9</cell></row><row><cell cols="2">Single-stream R-C3D + OHEM</cell><cell>13.0</cell><cell>13.3</cell></row><row><cell cols="4">the longest anchor segment has a duration of 76.8 seconds.</cell></row><row><cell>Over 99.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX .</head><label>IX</label><figDesc>ACTIVITY DETECTION SPEED DURING INFERENCE. NOTE THAT, THE TWO-STREAM MODELS MARKED WITH ASTERISKS * DON'T INCLUDE THE OPTICAL FLOW EXTRACTION TIME.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">2 * 8/25 = 0.64 and 16 * 8/25 = 5.12</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Pre-computing the optical flow images takes extra time and runs at 32.4 fps on a single GPU using the code in<ref type="bibr" target="#b56">[57]</ref>. The total activity detection speed for the two-stream models could be limited by the bottleneck of flow extraction.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This work is supported in part by the NSF and DARPA. We would thank Lu He for the meaningful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast Saliency Based Pooling of Fisher Encoded Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The LEAR submission at Thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV THUMOS Workshop</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action Recognition and Detection by Combining Motion and Appearance Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV THUMOS Workshop</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video Action Detection with Relational Dynamic-Poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Two-stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DAPs: Deep Action Proposals for Action Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08128</idno>
		<title level="m">Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Multi-Stream Bi-Directional Recurrent Neural Network for Fine-Grained Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end Learning of Action Detection from Frame Glimpses in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CPMC: Automatic Object Segmentation using Constrained Parametric Min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient Two-Stream Motion and Appearance 3d CNNs for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quo Vadis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Action Recognition? A New Model and the Kinetics Dataset</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>IEEE CVPR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-Time Action Recognition With Enhanced Motion Vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond Short Snippets: Deep Networks for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training Region-Based Object Detectors With Online Hard Example Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning and Example Selection for Object and Pattern Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS Challenge: Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">R-C3D: Region Convolutional 3D Network for Temporal Activity Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Realistic Human Actions from Movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-view Action Recognition via Transferable Dictionary Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2542" to="2556" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to Track for Spatio-Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast Action Proposals for Human Action Detection and Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal Localization of Actions with Actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2782" to="2795" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning Activity Progression in LSTMs for Activity Detection and Early Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1942" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-End, Single-Stream Temporal Action Detection in Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal Action Detection With Structured Segment Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tube convolutional neural network (T-CNN) for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">UntrimmedNets for Weakly Supervised Action Recognition and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal Action Detection Using a Statistical Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">R-FCN: Object Detection via Regionbased Fully Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sequence to Sequence -Video to Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to See by Moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Flowing ConvNets for Human Pose Estimation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast Temporal Activity Proposals for Efficient Detection of Human Actions in Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal Context Network for Activity Localization in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6537</idno>
		<title level="m">Fracking Deep Convolutional Image Descriptors</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Online Batch Selection for Faster Training of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06343</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="214" to="223" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Temporal Action Localization with Pyramid of Score Distribution Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<ptr target="http://activity-net.org/challenges/2016/data/anetchallengesummary.pdf" />
	</analytic>
	<monogr>
		<title level="j">ActivitNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Untrimmed Video Classification for Activity Detection: submission to ActivityNet Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01979</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">YouTube-8M: A Large-Scale Video Classification Benchmark</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Asynchronous Temporal Fields for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06371</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Huijuan received her PhD degree from computer science department at Boston University in 2018, advised by Professor Kate Saenko. Before that, she received her Bachelor&apos;s degree from Hefei University of Technology in 2009, and Master&apos;s degree from University of Chinese Academy of Sciences in 2012. Her research focuses on deep learning, computer vision and natural language processing, particularly in the area of visual question answering</title>
	</analytic>
	<monogr>
		<title level="m">UC Berkeley</title>
		<imprint/>
	</monogr>
	<note>video language description and activity detection</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">India and the director of the Computer Vision and Intelligence Research (CVIR) group. He received his B.E. in Electrical Engineering from Jadavpur University, India in 2007 and M.S. and Ph.D. in the same subject from University of California</title>
		<imprint/>
		<respStmt>
			<orgName>Abir Das is an Assistant Professor of Computer Science and Engineering Department of IIT Kharagpur</orgName>
		</respStmt>
	</monogr>
	<note>He was a postdoctoral researcher in the Computer Science department at Boston University. His main research interests include computer vision, activity detection. person reidentification, explainable AI and bias in machine learning</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">director of the Computer Vision and Learning Group and co-founder of the Artificial Intelligence Research (AIR) initiative. She was previously an Assistant Professor at the Computer Science Department at UMass Lowell, a Postdoctoral Researcher at the International Computer Science Institute (ICSI), Visiting Scholar at UC Berkeley in EECS and a Visiting Postdoctoral Fellow in the School of Engineering and Applied Science (SEAS) at Harvard University. Prof. Saenko&apos;s research interests are in the broad area of Artificial Intelligence with a focus on Adaptive Machine Learning</title>
	</analytic>
	<monogr>
		<title level="m">Learning for Vision and Language Understanding, and Deep Learning</title>
		<imprint/>
		<respStmt>
			<orgName>Kate Saenko is an Associate Professor of Computer Science at Boston University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
