<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Residual-Dense Lattice Network for Speech Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Nikzad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Integrated and Intelligent Systems</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Nicolson</surname></persName>
							<email>aaron.nicolson@griffithuni.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Integrated and Intelligent Systems</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsheng</forename><surname>Gao</surname></persName>
							<email>yongsheng.gao@griffith.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Integrated and Intelligent Systems</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
							<email>jun.zhou@griffith.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Integrated and Intelligent Systems</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldip</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
							<email>k.paliwal@griffith.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Integrated and Intelligent Systems</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanhua</forename><surname>Shang</surname></persName>
							<email>fhshang@xidian.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Residual-Dense Lattice Network for Speech Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNNs) with residual links (ResNets) and causal dilated convolutional units have been the network of choice for deep learning approaches to speech enhancement. While residual links improve gradient flow during training, feature diminution of shallow layer outputs can occur due to repetitive summations with deeper layer outputs. One strategy to improve feature re-usage is to fuse both ResNets and densely connected CNNs (DenseNets). DenseNets, however, over-allocate parameters for feature reusage. Motivated by this, we propose the residual-dense lattice network (RDL-Net), which is a new CNN for speech enhancement that employs both residual and dense aggregations without over-allocating parameters for feature re-usage. This is managed through the topology of the RDL blocks, which limit the number of outputs used for dense aggregations. Our extensive experimental investigation shows that RDL-Nets are able to achieve a higher speech enhancement performance than CNNs that employ residual and/or dense aggregations. RDL-Nets also use substantially fewer parameters and have a lower computational requirement. Furthermore, we demonstrate that RDL-Nets outperform many state-of-the-art deep learning approaches to speech enhancement. Availability: https://github.com/nick-nikzad/RDL-SE. © 24 © 48 © © 72 96 120 (a) Dense topology (b) RDL topology Conv. unit 24 © 24 48 24 24</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep learning approaches to speech enhancement represent a significant leap in performance over previous approaches, such as the decision-directed (DD) approach <ref type="bibr" target="#b4">(Ephraim and Malah 1984)</ref>. Multi-layer perceptrons (MLPs) were amongst the first artificial neural networks (ANNs) used for speech enhancement <ref type="bibr" target="#b35">(Xu et al. 2017)</ref>. Recurrent neural networks (RNNs) employing long short-term memory (LSTM) cells provided a higher performance at the cost of parameter inefficiency and extensive training times <ref type="bibr" target="#b3">(Chen and Wang 2017)</ref>. Convolutional neural networks (CNNs) were able to match the performance of LSTM networks, with fewer parameters and a reduction in training time <ref type="bibr" target="#b20">(Park and Lee 2017)</ref>. LSTM networks were not outperformed until the introduction of residual <ref type="bibr" target="#b11">(He et al. 2016;</ref><ref type="bibr" target="#b23">Rethage, Pons, and Serra 2018)</ref> and densely connected <ref type="bibr" target="#b16">Li et al. 2019</ref>) CNNs, as well as causal dilated convolutional units <ref type="bibr" target="#b1">(Bai, Kolter, and Koltun 2018)</ref>. A residual CNN (ResNet) aggregates layer outputs via a summation operation, which is given as input to deeper layers. A densely connected CNN (DenseNet) differs by aggregating layer outputs via a concatenation operation. Other ANNs that have been successfully applied to speech enhancement include generative adversarial networks (GANs) and encoder-decoder CNNs (Pascual, Bonafonte, and Serra 2017).</p><p>Residual and dense aggregations of layer outputs have been found to benefit training. Residual links improve gradient flow during backpropagation <ref type="bibr" target="#b11">(He et al. 2016</ref>) and prevent the vanishing and exploding gradient problems <ref type="bibr" target="#b2">(Bengio, Simard, and Frasconi 1994)</ref>. This allows the training of very deep neural networks. Dense aggregations offer direct feature re-usage, as deeper layers have access to the outputs of shallower layers ). This allows a layer to explore a larger set of features during training. Despite the success of both ResNets and DenseNets, both aggregation types have drawbacks. For ResNets, information from the outputs of shallower layers can be lost after multiple summations with deeper layer outputs <ref type="bibr" target="#b36">(Zhu et al. 2018)</ref>. This restricts feature re-usage and limits feature exploration during training. For DenseNets, while the concatenation of all previous layer outputs yields total feature re-usage, a significant number of parameters are unexploited due to large input sizes <ref type="bibr" target="#b33">(Wang et al. 2018)</ref>. This is exemplified in <ref type="figure">Figure 1 (a)</ref>, where the input size increases with the depth of the block.</p><p>Combining the benefits of both aggregation types has also been investigated. Mixed link networks (MLNs) are CNNs that employ both residual and dense aggregations <ref type="bibr" target="#b33">(Wang et al. 2018)</ref>. A network that employs densely connected residual blocks (DenseRNet) was able to outperform both ResNets and DenseNets on a speech recognition task <ref type="bibr" target="#b29">(Tang et al. 2018)</ref>. As shown in <ref type="bibr" target="#b33">(Wang et al. 2018)</ref>, MLNs such as DenseRNets follow a similar dense aggregation strategy to DenseNets. For example, DenseRNet blocks have total feature re-usage between residual blocks. This indicates that current MLNs possess the same drawback inherent with DenseNets: too many parameters are allocated for feature reusage in each block. In this paper, we propose a new CNN for speech enhancement that takes advantage of both aggregation types, without over-allocating parameters for feature re-Figure 1: Comparison of (a) the dense topology and (b) the proposed RDL topology. The number of input features to each convolutional unit is indicated. Given identical kernel and output sizes, more parameters are consumed for a larger input size. c ○ represents the concatenation operation.</p><p>usage. This is achieved by using a topology that differs from the chain-structure of MLNs, such as DenseRNet. The topology is a triangular lattice of convolutional units, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Local dense aggregations of convolutional unit outputs are formed strictly over the height of the lattice. As can be seen by comparing <ref type="figure">Figure 1</ref> (b) to <ref type="figure">Figure 1</ref> (a), this reduces the maximum input size to a convolutional unit within a block. While RDL blocks do not allow for total feature re-usage, densely aggregating only a subset of previous outputs has been shown to be beneficial <ref type="bibr" target="#b36">(Zhu et al. 2018)</ref>. Local residual and global dense links are also adopted, to improve intra block gradient flow, and inter block feature re-usage, respectively. We refer to the framework of applying residual and dense aggregations over a triangular lattice of convolutional units as a residual-dense lattice (RDL). Moreover, we show that the proposed RDL network (RDL-Net) is able to produce a higher speech enhancement performance than networks that employ residual and/or dense aggregations. An ablation study of RDL-Nets is also performed over multiple aggregation configurations. We also show that RDL-Nets outperform many state-of-the-art deep learning approaches to speech enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related works</head><p>ANNs have been used for enhancing speech in both the time-and frequency-domain. In the time-domain, ANNs estimate clean speech frames from given noisy speech frames. A GAN was employed for speech enhancement in the timedomain (SEGAN), which used encoder-decoder CNNs for both the generator and discriminator <ref type="bibr" target="#b21">(Pascual, Bonafonte, and Serra 2017)</ref>. A CNN employing non-causal dilated convolutional units and residual links was also used for speech enhancement in the time-domain (Wavenet) <ref type="bibr" target="#b23">(Rethage, Pons, and Serra 2018)</ref>.</p><p>In the frequency-domain, ANNs are employed to estimate either the clean speech magnitude spectra, a time-frequency mask, or the a priori SNR from given noisy speech magnitude spectra. An MLP was used to estimate the clean speech log-power spectra (LPS) <ref type="bibr" target="#b34">(Xu et al. 2015)</ref>, with the framework later incorporating multi-objective learning, and ideal binary mask (IBM) post-processing (Xu2017) <ref type="bibr" target="#b35">(Xu et al. 2017)</ref>. A DenseNet was also used to estimate the clean speech LPS in <ref type="bibr" target="#b16">(Li et al. 2019)</ref>, and was able to outperform both MLP and LSTM networks in the same framework. Time-frequency masks, such as the ideal ratio mask (IRM), are applied as a suppression function to the noisy speech magnitude spectra. An LSTM network was used to estimate the IRM (LSTM-IRM) (Chen and Wang 2017), which was able to generalise to unseen speakers. A GAN with a regularised loss function was also used to estimate the IRM (MMSE-GAN), and was able to outperform SEGAN <ref type="bibr" target="#b26">(Soni, Shah, and Patil 2018)</ref>. This was outperformed by another GAN IRM estimator that used multiple objective measures during optimisation (Metric-GAN) <ref type="bibr" target="#b6">(Fu et al. 2019)</ref>.</p><p>A priori SNR estimates are used by minimum meansquare error (MMSE) estimators of the clean speech magnitude spectra <ref type="bibr" target="#b4">(Ephraim and Malah 1984)</ref>. Recently, a deep learning approach to a priori SNR estimation was proposed (Deep Xi) <ref type="bibr" target="#b18">(Nicolson and Paliwal 2019)</ref>. It used a residual LSTM network (ResLSTM) to estimate the a priori SNR directly from noisy speech magnitude spectra. By estimating the a priori SNR, different MMSE approaches can be used such as the MMSE log-spectral amplitude (MMSE-LSA) estimator <ref type="bibr" target="#b5">(Ephraim and Malah 1985)</ref> and the square-root Wiener filter (SRWF) <ref type="bibr" target="#b17">(Lim and Oppenheim 1979)</ref>. RDL-Nets are examined within the Deep Xi framework, due to its flexibility of MMSE estimator choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual-dense lattice networks</head><p>The proposed RDL-Net is used to estimate the a priori SNR from given noisy speech magnitude spectra, as shown in <ref type="figure">Fig</ref> The location of each convolutional unit within the lattice, ℎ , is specified by a height and length co-ordinate (ℎ, ), where ℎ = 1, 2, ..., , and, = 1, 2, ..., . The number of convolutional units in each block is denoted by , where is a square number, and ≥ 4. The height of the lattice is = √ , and the length is = 2 √ − 1. The following notation is used to indicate in which section of the lattice a convolutional unit exists:</p><formula xml:id="formula_0">Global dense links Residual-dense lattice block 1 C Residual-dense lattice block 2 C Residual-dense lattice block B C ⋯ O Concatenation operator C Noisy speech magnitude spectrogram A priori SNR Local residual links C C C</formula><p>Conv. unit <ref type="figure">Figure 3</ref>: The proposed RDL-Net for speech enhancement. The RDL-Net estimates the a priori SNR from the given noisy speech magnitude spectrum. The estimated a priori SNR is then used by an MMSE clean speech magnitude spectrum estimator.</p><formula xml:id="formula_1">ℎ = ⎧ ⎪ ⎨ ⎪ ⎩ ◿ ℎ , if ℎ ≤ , ≤ ◺ ℎ , if ℎ ≤ 2 − , &lt; ≤ ∅, otherwise,<label>(1)</label></formula><p>where ◿ ℎ and ◺ ℎ are convolutional units that exist in the left and right triangles of the lattice, respectively. ∅ indicates that the convolutional unit does not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional units</head><p>Each convolutional unit is a composite function, (⋅), consisting of three operations, including layer normalisation <ref type="bibr" target="#b0">(Ba, Kiros, and Hinton 2016)</ref>, followed by ReLU activation <ref type="bibr" target="#b10">(Glorot, Bordes, and Bengio 2011)</ref>, and 1D causal dilated convolution <ref type="bibr" target="#b1">(Bai, Kolter, and Koltun 2018)</ref>. The output of a convolutional unit is given by ( ℎ , ℎ ), where ℎ denotes the weights (and biases). Convolutional units within the RDL-Net are connected by both local and global links (i.e. intra and inter block links).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local dense aggregations</head><p>The input to a convolutional unit in the left triangle of the lattice, ◿ ℎ , is the dense aggregation of the outputs at length − 1, and heights ℎ, ℎ − 1, ..., 1:</p><formula xml:id="formula_2">◿ ℎ = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 11 , if = ℎ = 1 ℎ( −1) , if ◿ ℎ , &gt; 1, ℎ = 1 [ ℎ( −1) , (ℎ−1) ], if ◿ ℎ , &gt; ℎ, ℎ &gt; 1 (ℎ−1) , if ◿ ℎ , = ℎ, ℎ &gt; 1,<label>(2)</label></formula><p>where [.] denotes the concatenation operation, and ℎ is the local residual aggregation at (ℎ, ). The local dense aggregations in the left triangle of the lattice allow for multiple concise outputs to be progressively formed. The input to a convolutional unit in the right triangle of the lattice, ◺ ℎ , is the dense aggregation of the outputs at length − 1, and heights ℎ, ℎ + 1, ..., :</p><formula xml:id="formula_3">◺ ℎ = [ ℎ( −1) , (ℎ+1)( −1) ], if ◺ ℎ , ℎ = 2 − [ ℎ( −1) , (ℎ+1) ], if ◺ ℎ , ℎ &lt; 2 − .<label>(3)</label></formula><p>In the right triangle of the lattice, the outputs are progressively amalgamated into a single output. By densely aggregating outputs over the height of the lattice, the input size to deeper convolutional units within the block is limited. This enables RDL-Nets to avoid the drawback associated with other densely connected residual networks: the overallocation of parameters for feature re-usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local residual aggregations</head><p>To improve the flow of gradients over the length of the lattice, local residual links are adopted:</p><formula xml:id="formula_4">ℎ = ( ℎ , ℎ ) + ℎ( −1) , if ◿ ℎ or ◺ ℎ , &gt; ℎ ( ℎ , ℎ ), if ◿ ℎ or ◺ ℎ , ≤ ℎ.<label>(4)</label></formula><p>When the size of ℎ and ℎ( −1) are non-identical, the residual link is weighted so that ℎ( −1) is the same size as ℎ . Local residual links also help to stabilise the training process <ref type="bibr" target="#b11">(He et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global dense aggregations</head><p>Global dense links are adopted, to further enhance the propagation of information between RDL blocks:</p><formula xml:id="formula_5">+1 11 = [ 11 , 1 ],<label>(5)</label></formula><p>where the superscript is added to the notation to indicate the block index, = 1, 2, ..., . Utilising global dense links also enables feature re-usage between the RDL blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>The receptive field of an RDL block is controlled via the dilatation rate, = 2 ℎ−1 , and the kernel size, = 2ℎ − 1. However, this strategy can expend a large number of parameters. Hence, we alternate the kernel size of = 2ℎ − 1, with = 1 at each length, as depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. Moreover, we set the convolutional unit output size at each height to ℎ = 1 2 ℎ−1 , where 1 is the output size at ℎ = 1. This policy ensures that a reduced number of parameters are used for feature re-usage. In this work, the total number of convolutional units for each RDL block was set to = 16 (hence, = 4 and = 7). The output size of the first level (ℎ = 1) was 1 = 64. RDL-Nets with sizes of 0.53, 1.08, 1.48, 1.87, and 3.91 million parameters were formed by cascading 3, 6, 8, 10, and 18 blocks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment setup Network Configurations</head><p>The aforementioned RDL-Net configurations and the following network configurations were tasked with estimating the a priori SNR within the Deep Xi framework <ref type="bibr" target="#b18">(Nicolson and Paliwal 2019)</ref>. The estimated a priori SNR is then used by MMSE approaches to speech enhancement.</p><p>ResNet: Each residual block contained 2 causal dilated convolutional units with an output size of 64, and a kernel size of 3. For each block, was cycled from 1 to 8 (increasing by a power of 2). ResNets of sizes 0.53, 1.03, 1.53, and 2.03 million parameters were formed by cascading 20, 40, 60, and 80 residual blocks, respectively.</p><p>DenseNet: Each dense block contained 4 causal dilated convolutional units with an output size of 24, and a kernel size of 3. For each convolutional unit, was cycled from 1 to 8 (increasing by a power of 2). DenseNets of sizes 0.57, 0.97, 1.48, and 2.10 million parameters were formed by cascading 5, 7, 9, and 11 dense blocks, respectively.</p><p>DenseRNet: Each denseR block was composed of 4 densely connected residual blocks. Each residual block contained 2 causal dilated convolutional units with an output size of 24 and a kernel size of 3. For each residual block, was cycled from 1 to 8 (increasing by a power of 2). DenseR-Nets of sizes 0.60, 1.05, 1.44, and 2.02 million parameters were formed by cascading 2, 3, 4, and 6 denseR blocks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResLSTM:</head><p>The cell size and number of residual blocks were 170 and 4, 188 and 5, and 200 and 6, for the ResLSTMs of sizes 1.02, 1.51, and 2.03 million parameters, respectively. This was the original network used in the Deep Xi framework <ref type="bibr" target="#b18">(Nicolson and Paliwal 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech enhancement</head><p>For each frame of noisy speech, the 257-point single-sided magnitude spectrum was computed, which included both the DC frequency component and the Nyquist frequency component, forming the input to each of the five previously described networks. The estimated a priori SNR was used by an MMSE approach (MMSE-LSA estimator or SRWF approach) to estimate the clean speech magnitude spectrum. The short-time Fourier analysis, modification, and synthesis (AMS) framework was used to produce the final enhanced speech <ref type="bibr" target="#b18">(Nicolson and Paliwal 2019)</ref>. The Hamming window function was used for analysis and synthesis, with a frame length of 32 ms and a frame shift of 16 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training set</head><p>The train-clean-100 set from the Librispeech corpus <ref type="bibr" target="#b19">(Panayotov et al. 2015)</ref>, the CSTR VCTK corpus (recordings from speakers 232 and 257 were excluded as they are used in Test Set 2) <ref type="bibr" target="#b31">(Veaux et al. 2017)</ref>, and the * and * training sets from the TIMIT corpus <ref type="bibr" target="#b7">(Garofolo et al. 1993</ref>) were included in the training set (73 404 clean speech recordings). 5% of the clean speech recordings (3 667) were randomly selected and used as the validation set. The 2 382 recordings adopted in <ref type="bibr" target="#b18">(Nicolson and Paliwal 2019)</ref> were used for the noise training set. All clean speech and noise recordings were single-channel, with a sampling frequency of 16 kHz. The noise corruption procedure for the training set is described in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training strategy</head><p>Cross-entropy was used as the loss function. The Adam algorithm <ref type="bibr" target="#b15">(Kingma and Ba 2014)</ref> with default hyper-parameters was used for stochastic gradient descent optimisation. A mini-batch size of 10 noisy speech signals was used. The noisy speech signals were generated as follows: each clean speech recording selected for a mini-batch was mixed with a random section of a randomly selected noise recording at a randomly selected SNR level (-10 to 20 dB, in 1 dB increments). A total of 100 epochs were use to train all CNN architectures. A total of 10 epochs were used for the ResLSTM networks and the LSTM-IRM estimator <ref type="bibr" target="#b3">(Chen and Wang 2017)</ref>, as each epoch required eight hours of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test sets</head><p>The following two datasets were used for testing:</p><p>• Test set 1: The first test set was used to obtain the results in Figures 4 and 5, and Tables 1, 2, and 3. The four noise sources included voice babble, F16, and factory from the RSG-10 noise dataset <ref type="bibr" target="#b27">(Steeneken and Geurtsen 1988)</ref> and street music (recording no. 26 270) from the Urban Sound dataset <ref type="bibr" target="#b24">(Salamon, Jacoby, and Bello 2014)</ref>. 10 clean speech recordings were randomly selected (without replacement) from the TSP speech corpus <ref type="bibr" target="#b14">(Kabal 2002)</ref> for each of the four noise recordings. To generate the noisy speech, a random section of the noise recording was mixed with the clean speech at the following SNR levels: -5 to 15 dB, in 5 dB increments. This created a test set of 200 noisy speech signals. The noisy speech was single channel, with a sampling frequency of 16 kHz.</p><p>• Test set 2: The second test set was used to obtain the results in <ref type="table">Table 4</ref>. In order to make a direct comparison, the second test set is identical to those used in previous works. The test set included 824 clean speech recordings of two speakers from the Voice Bank corpus (393 from 232 and 431 from 257) <ref type="bibr" target="#b32">(Veaux, Yamagishi, and King 2013)</ref>. A total of 20 different conditions were used to create the noisy speech, including five noise types from the DEMAND dataset <ref type="bibr" target="#b30">(Thiemann, Ito, and Vincent 2013)</ref>, and four SNR levels: 2.5, 7.5, 12.5, and 17.5 dB. This corresponds to approximately 20 different sentences per condition for each speaker (824 noisy speech signals in the second test set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local and global aggregation study</head><p>In this section we conduct an ablation study on the effects of two aggregation types used in the RDL-Net topology, including local residual links (LR) and global dense links (GD). To this end, four RDL-Net configurations are examined, as shown in <ref type="table" target="#tab_0">Table 1</ref>. The convergence of each configuration during training is also depicted in <ref type="figure" target="#fig_2">Figure 4</ref>. The four  configurations were formed using the aforementioned hyperparameters, with 5 blocks. By adding either LR or GD to the baseline (no LR or GD), it can be seen that a lower validation error can be attained. While GD aggregations add more trainable parameters (0.23M) to the baseline, it achieved a lower validation error than <ref type="bibr">LR (141.82 vs. 142.14)</ref>. However, the GD configuration caused obvious fluctuations in the validation error during training. Utilising both LR and GD produced the lowest validation error, without the fluctuations in validation error exhibited by the GD configuration. This demonstrates that enhanced intra block gradient flow and inter block feature re-usage are both highly beneficial to the training of an RDL-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and validation error</head><p>The training and validation error curves for the RDL-Net, ResNet, DenseNet, and DenseRNet at a parameter sizes of approximately 2 million are shown in <ref type="figure" target="#fig_3">Figures 5 (a) and (b)</ref>, respectively. The RDL-Net was able to converge to a lower training and validation error than the other networks. This suggests that the proposed RDL-Net allocated an efficient number of parameters for feature re-usage. Conversely, the DenseNet and DenseRNet struggled at a parameter size of 2 million, indicating that too many parameters were wasted on feature re-usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter and computational efficiency</head><p>The lowest validation error as a function of the number of parameters and computations for RDL-Nets, ResNets, DenseNets, and DenseRNets are shown in Figures5 (c) and (d), respectively. RDL-Nets were able to achieve the same validation error as ResNets that employed significantly more parameters. For example, at a parameter size of 1 million, the RDL-Net attained the same lowest validation error as the ResNet with double the amount of parameters. A similar trend can be seen for the lowest validation error as a function of the number of FLOPs, (where FLOPs refers to the number of multiplication-addition operations during inference). For example, the RDL-Net that requires 2 million FLOPs achieved a lowest validation error similar to that of the ResNet that requires 4× as many FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech enhancement performance</head><p>The enhanced speech objective quality scores attained by each of the networks in the Deep Xi framework are presented in <ref type="table">Table 2</ref>. Each network estimated the a priori SNR for the MMSE-LSA estimator. It can be seen that RDL-Nets were able to achieve the highest objective quality scores for most of the tested conditions. The performance capability of RDL-Nets was demonstrated at a parameter size of 2 million for street music at 10 dB, where the RDL-Net achieved a MOS-LQO improvement of 0.22 over the ResNet. <ref type="table">Table 3</ref> shows the objective intelligibility scores obtained by each of the networks. It can be seen that RDL-Nets were able to achieve the highest objective intelligibility scores for most of the tested conditions. RDL-Nets demonstrated its performance at a parameter size of 2 million for factory noise at 0 dB, attaining an STOI improvement of 3.5% when compared to the equivalent ResNet. RDL-Nets in the Deep Xi framework were also able to produce enhanced speech with higher objective quality and intelligibility scores than two other widely known deep learning speech enhancement frameworks (LSTM-IRM and Xu2017) <ref type="bibr" target="#b35">(Xu et al. 2017;</ref><ref type="bibr" target="#b3">Chen and Wang 2017)</ref>.</p><p>We also compare RDL-Nets to recent deep learning approaches to speech enhancement. Here, RDL-Nets were used to estimate the a priori SNR for the SRWF approach and the MMSE-LSA estimator. As shown in <ref type="table">Table 4</ref>, RDL-Nets were able to attain the highest CSIG, CBAK, COVL, PESQ <ref type="table">Table 2</ref>: Enhanced speech objective quality scores. The mean opinion score of the listening quality objective (MOS-LQO) was used as the metric, where the wideband perceptual evaluation of quality (Wideband PESQ) was the objective model used to obtain the MOS-LQO score <ref type="bibr" target="#b22">(Rec 2005)</ref>. The tested conditions include clean speech mixed with real-world non-stationary (voice babble and street music) and coloured (F16 and factory) noise sources at multiple SNR levels. The highest MOS-LQO score attained at each condition and for each parameter size is shown in boldface. The standard error (SE) over all conditions for each network is provided in the last column.  <ref type="figure" target="#fig_0">07 1.15 1.35 1.71 1.04 1.06 1.11 1.27 1.58 1.03 1.06 1.11 1.25 1.52 1.04 1.04 1.09 1.24 1.54</ref>   <ref type="figure" target="#fig_0">1.10 1.29 1.65 2.15 2.62 1.13 1.32 1.66 2.11 2.53 1.25 1.48 1.73 2.17 2.62 1.15 1.39 1.73 2.10</ref>   <ref type="figure" target="#fig_0">36 1.79 2.46 2.98 1.19 1.42 1.83 2.27 2.74 1.26 1.53 1.86 2.31 2.78 1.19 1.46 1.83 2.26</ref> 2.74 0.045 <ref type="table">Table 3</ref>: Enhanced speech objective intelligibility scores (in %) as given by the short-time objective intelligibility (STOI) metric <ref type="bibr" target="#b28">(Taal et al. 2010)</ref>. The tested conditions include clean speech mixed with real-world non-stationary (voice babble and street music) and coloured (F16 and factory) noise sources at multiple SNR levels. The highest STOI score attained at each condition and for each parameter size is shown in boldface. The standard error (SE) over all conditions for each network is provided in the last column.  <ref type="figure" target="#fig_0">6 94.3 97.2 67.2 80.4 89.9 94.8 97.4 69.6 82.7 90.1 94.6 97.3 63.3 79.4 88.4 93.4</ref>   <ref type="figure" target="#fig_0">.2 80.82 89.0 94.7 97.4 71.6 82.9 90.7 95.0 97.5 72.6 83.9 91.0 95.4 97.8 67.1 81.7 89</ref>.5 93.9 96.8 0.008 <ref type="table">Table 4</ref>: Comparison to recent deep learning approaches to speech enhancement using the second test set. As in previous works, the objective scores are averaged over all tested conditions. CSIG, CBAK, and COVL are mean opinion score (MOS) predictors of the signal distortion, background-noise intrusiveness, and overall signal quality, respectively <ref type="bibr" target="#b12">(Hu and Loizou 2008)</ref>. PESQ is the perceptual evaluation of speech quality measure <ref type="bibr" target="#b12">(Hu and Loizou 2008)</ref>. STOI is the short-time objective intelligibility measure (in %) <ref type="bibr" target="#b28">(Taal et al. 2010</ref>). The highest scores attained for each measure are indicated in boldface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method CSIG CBAK COVL PESQ STOI</head><p>Noisy speech 3.35 2.44 2.63 1.97 92 (91.5) Wiener <ref type="bibr" target="#b25">(Scalart and J.V 1996)</ref> 3.23 2.68 2.67 2.22 -SEGAN <ref type="bibr" target="#b21">(Pascual, Bonafonte, and Serra 2017)</ref> 3.48 2.94 2.80 2.16 93 Wavenet <ref type="bibr" target="#b23">(Rethage, Pons, and Serra 2018)</ref> 3.62 3.23 2.98 --MMSE-GAN <ref type="bibr" target="#b26">(Soni, Shah, and Patil 2018)</ref> 3.80 3.12 3.14 2.53 93 Deep Feature Loss <ref type="bibr" target="#b9">(Germain, Chen, and Koltun 2018)</ref>   <ref type="figure" target="#fig_4">Figure 6 (d)</ref>. It can be seen that the RDL-Net demonstrated superior noise suppression with little formant distortion. As illustrated in <ref type="figure" target="#fig_4">Figure 6</ref> (c), Deep Feature Loss over-and under-estimated multiple spectral components. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel convolutional neural network (CNN) for speech enhancement, called a residual-dense lattice (RDL) network. Unlike other CNNs that use both residual and dense aggregations, RDL-Nets take advantage of both aggregation types without over-allocating parameters for feature re-usage. This enables RDL-Nets to produce a higher speech enhancement performance than other networks, such as ResLSTM networks, ResNets, DenseNets, and DenseRNets. We also show that RDL-Nets are able to outperform many state-of-the-art deep learning approaches to speech enhancement. In future work, the RDL-Net topology will be investigated for speech separation, speech recognition, computer vision, and image denoising.</p><p>1 Enhanced speech recordings and additional results are available at: https://github.com/nick-nikzad/RDL-SE. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An RDL block with a length of 5 and height of 3. The two coloured triangles indicate the left and right halves of the lattice. Here, the kernel size is denoted by .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ure 3. The network consists of RDL blocks, and a sigmoidal fully-connected output layer, O. The block topology is a triangular lattice of convolutional units, as shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Validation error attained by four RDL-Net configuration types: Baseline, LR, GD, and LR-GD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Training plots for RDL-Nets, ResNets, DenseNets and DenseRNets: (a) training error, (b) validation error, and lowest validation error as a function of the number of (c) parameters and (d) FLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>(a) Clean speech magnitude spectrogram (|S|) of female 257 uttering sentence 70, "The price cuts are really exciting". (b) Crowd noise mixed with (a) at an SNR level of 2.5 dB (|X|). Enhanced speech (|Ŝ|) produced by (c) Deep Feature Loss and (d) RDL-Net 3.91M (Deep Xi-MMSE-LSA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="6">: Ablation study of local residual (LR) and global</cell></row><row><cell cols="3">dense (GD) aggregations.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Different combinations of LR, GD</cell></row><row><cell>LR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6"># params. 0.39M 0.53M 0.62M 0.86M</cell></row><row><cell cols="6">Val. error 146.37 142.14 141.82 141.43</cell></row><row><cell>Validation error</cell><cell>144 146 148</cell><cell></cell><cell></cell><cell></cell><cell>Baseline LR GD LR-GD</cell></row><row><cell></cell><cell>142</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell cols="2">Epoch</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The RDL-Net demonstrated an improvement of 0.39, 0.25, 0.3, and 0.16 over Metric-GAN for CSIG, CBAK, COVL, and PESQ, respectively. The RDL-Net also demonstrated an improvement of 1% over MMSE-GAN for STOI. The enhanced speech produced by RDL-Net 3.91M is illustrated in</figDesc><table><row><cell></cell><cell>3.86</cell><cell>3.33</cell><cell>3.22</cell><cell>-</cell><cell>-</cell></row><row><cell>Metric-GAN (Fu et al. 2019)</cell><cell>3.99</cell><cell>3.18</cell><cell>3.42</cell><cell>2.86</cell><cell>-</cell></row><row><cell>Proposed RDL-Net 1.87M (Deep Xi -MMSE-LSA)</cell><cell>4.29</cell><cell>3.32</cell><cell>3.62</cell><cell>2.93</cell><cell>93 (93.4)</cell></row><row><cell>Proposed RDL-Net 1.87M (Deep Xi -SRWF)</cell><cell>4.27</cell><cell>3.23</cell><cell>3.56</cell><cell>2.84</cell><cell>93 (93.5)</cell></row><row><cell>Proposed RDL-Net 3.91M (Deep Xi -MMSE-LSA)</cell><cell>4.38</cell><cell>3.43</cell><cell>3.72</cell><cell>3.02</cell><cell>94 (93.8)</cell></row><row><cell>Proposed RDL-Net 3.91M (Deep Xi -SRWF)</cell><cell>4.36</cell><cell>3.35</cell><cell>3.67</cell><cell>2.94</cell><cell>94 (93.8)</cell></row><row><cell>and STOI scores.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning longterm dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory for speaker generalization in supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4705" to="4714" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimum mean-square error log-spectral amplitude estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="445" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Metric-GAN: Generative adversarial networks based black-box metric scores optimization for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2031" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST speech disc 1</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasa</forename><surname>Sti/Recon</surname></persName>
		</author>
		<idno>N 93</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10522</idno>
		<title level="m">Speech denoising with deep feature losses</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluation of objective quality measures for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kabal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>TSP speech database. McGill University, Database Version</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected network with time-frequency dilated convolution for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6860" to="6864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhancement and bandwidth compression of noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1586" to="1604" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning for minimum mean-square error approaches to speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fully convolutional neural network for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1993" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SEGAN: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<editor>IN-TERSPEECH</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wideband extension to recommendation P. 862 for the assessment of wideband telephone networks and speech codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Telecommunication Union</title>
		<imprint>
			<biblScope unit="volume">862</biblScope>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>CH-Geneva</orgName>
		</respStmt>
	</monogr>
	<note>P.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech enhancement based on a priori signal to noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scalart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="629" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Time-frequency masking-based speech enhancement using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5039" to="5043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Description of the RSG-10 noise database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Steeneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Geurtsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<pubPlace>Soesterberg, The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>TNO Institute for Perception</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Report IZF 1988-3</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A short-time objective intelligibility measure for timefrequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Acoustic modeling with densely connected residual network for multichannel speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Mcloughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1783" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The diverse environments multi-channel acoustic noise database: A database of multichannel environmental noise recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3591" to="3591" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. The Centre for Speech Technology Research (CSTR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The voice bank corpus: Design, collection and data analysis of a large regional accent speech database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
		<editor>O-COCOSDA/CASLRE</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mixed link networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2819" to="2825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-objective learning and mask-based post-processing for deep neural network based speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07172</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparsely aggregated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
