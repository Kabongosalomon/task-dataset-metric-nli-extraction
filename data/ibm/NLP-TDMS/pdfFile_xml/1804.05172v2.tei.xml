<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Morrison</surname></persName>
							<email>douglas.morrison@hdr.qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technology Brisbane</orgName>
								<address>
									<postCode>4000</postCode>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Corke</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technology Brisbane</orgName>
								<address>
									<postCode>4000</postCode>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Leitner</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technology Brisbane</orgName>
								<address>
									<postCode>4000</postCode>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a real-time, object-independent grasp synthesis method which can be used for closed-loop grasping. Our proposed Generative Grasping Convolutional Neural Network (GG-CNN) predicts the quality and pose of grasps at every pixel. This one-to-one mapping from a depth image overcomes limitations of current deep-learning grasping techniques by avoiding discrete sampling of grasp candidates and long computation times. Additionally, our GG-CNN is orders of magnitude smaller while detecting stable grasps with equivalent performance to current state-of-the-art techniques. The lightweight and single-pass generative nature of our GG-CNN allows for closed-loop control at up to 50Hz, enabling accurate grasping in non-static environments where objects move and in the presence of robot control inaccuracies. In our real-world tests, we achieve an 83% grasp success rate on a set of previously unseen objects with adversarial geometry and 88% on a set of household objects that are moved during the grasp attempt. We also achieve 81% accuracy when grasping in dynamic clutter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In order to perform grasping and manipulation tasks in the unstructured environments of the real world, a robot must be able to compute grasps for the almost unlimited number of objects it might encounter. In addition, it needs to be able to act in dynamic environments, whether that be changes in the robot's workspace, noise and errors in perception, inaccuracies in the robot's control, or perturbations to the robot itself.</p><p>Robotic grasping has been investigated for decades, yielding a multitude of different techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. Most recently, deep learning techniques have enabled some of the biggest advancements in grasp synthesis for unknown items. These approaches allow learning of features that correspond to good quality grasps that exceed the capabilities of humandesigned features <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>However, these approaches typically use adapted versions of Convolutional Neural Network (CNN) architectures designed for object recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>, and in most cases sample and rank grasp candidates individually <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>, resulting in long computation times in the order of a second <ref type="bibr" target="#b20">[21]</ref> to tens of seconds <ref type="bibr" target="#b16">[17]</ref>. As such, these techniques are rarely used in closed-loop grasp execution and rely on precise camera calibration and precise robot control to grasp successfully, even in static environments.</p><p>We propose a different approach to selecting grasp points for previously unseen items. Our Generative Grasping Convolutional Neural Network (GG-CNN) directly generates an antipodal grasp pose and quality measure for every pixel in an <ref type="figure">Fig. 1</ref>. Our real-time, generative grasping pipeline. A camera mounted to the wrist of the robot captures depth images containing an object to be grasped. Our Generative Grasping Convolutional Neural Network (GG-CNN) generates antipodal grasps -parameterised as a grasp quality, angle and gripper widthfor every pixel in the input image in a fraction of a second. The best grasp is calculated and a velocity command (v) is issued to the robot. The closed-loop system is capable of grasping dynamic objects and reacting to control errors. input depth image and is fast enough for closed-loop control of grasping in dynamic environments ( <ref type="figure">Fig. 1</ref>). We use the term "generative" to differentiate our direct grasp generation method from methods which sample grasp candidates.</p><p>The advantages of GG-CNN over other state-of-the-art grasp synthesis CNNs are twofold. Firstly, we do not rely on sampling of grasp candidates, but rather directly generate grasp poses on a pixelwise basis, analogous to advances in object detection where fully-convolutional networks are commonly used to perform pixelwise semantic segmentation rather than relying on sliding windows or bounding boxes <ref type="bibr" target="#b18">[19]</ref>. Secondly, our GG-CNN has orders of magnitude fewer parameters than other CNNs used for grasp synthesis, allowing our grasp detection pipeline to execute in only 19 ms on a GPU-equipped desktop computer, fast enough for closed-loop grasping.</p><p>We evaluate the performance of our system in different scenarios by performing grasping trials with a Kinova Mico robot, with static, dynamic and cluttered objects. In dynamic grasping trials, where objects are moved during the grasp attempt, we achieve 83% grasping success rate on a set of eight 3D-printed objects with adversarial geometry <ref type="bibr" target="#b20">[21]</ref> and 88% on a set of 12 household items chosen from standardised object sets. Additionally, we reproduce the dynamic clutter grasping experiments of <ref type="bibr" target="#b31">[32]</ref> and show an improved grasp success rate of 81%. We further illustrate the advantages of using a closed-loop method by reporting experimental results when artificial inaccuracies are added to the robot's control. <ref type="bibr" target="#b16">[17]</ref> [   </p><formula xml:id="formula_0">× - × × - × × × Adversarial Objects [21] × - × × - × × Clutter × × × × × Closed-loop × - × × - × Dynamic Objects × - × × - × × Code Available × × × × × *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Grasping Unknown Objects Grasp synthesis refers to the formulation of a stable robotic grasp for a given object, which is a topic which has been widely researched resulting in a plethora of techniques. Broadly, these can be classified into analytic methods and empirical methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>. Analytic methods use mathematical and physical models of geometry, kinematics and dynamics to calculate grasps that are stable <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>, but tend to not transfer well to the real world due to the difficultly in modelling physical interactions between a manipulator and an object <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In contrast, empirical methods focus on using models and experience-based approaches. Some techniques work with known items, associating good grasp points with an offline database of object models or shapes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref>, or familiar items, based on object classes <ref type="bibr" target="#b27">[28]</ref> or object parts <ref type="bibr" target="#b6">[7]</ref>, but are unable to generalise to new objects.</p><p>For grasping unknown objects, large advancements have been seen recently with a proliferation of vision-based deeplearning techniques <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. Many of these techniques share a common pipeline: classifying grasp candidates sampled from an image or point cloud, then ranking them individually using Convolutional Neural Networks (CNN). Once the best grasp candidate is determined, a robot executes the grasp open-loop (without any feedback) which requires precise calibration between the camera and the robot, precise control of the robot and a completely static environment.</p><p>Execution time is the primary reason that grasps are executed open-loop. In many cases, deep-learning approaches use large neural networks with millions of parameters <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> and process grasp candidates using a sliding window at discrete intervals of offset and rotation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>, which is computationally expensive and results in grasp planning times in the order of a second <ref type="bibr" target="#b20">[21]</ref> to tens of seconds <ref type="bibr" target="#b16">[17]</ref>.</p><p>Some approaches reduce execution time by pre-processing and pruning the grasp candidates <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref> or predicting the quality of a discrete set of grasp candidates simultaneously <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>, trading off execution time against the number of grasps which are sampled, but ignoring some potential grasps.</p><p>Instead of sampling grasp candidates, both <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b24">[25]</ref> use a deep CNN to regress a single best grasp pose for an input image. However, these regression methods are liable to output the average of the possible grasps for an object, which itself may not be a valid grasp <ref type="bibr" target="#b24">[25]</ref>.</p><p>Similar to our method, Varley et al. <ref type="bibr" target="#b30">[31]</ref> use a neural network to generate pixelwise heatmaps for finger placement in an image, but still rely on a grasp planner to determine the final grasp pose.</p><p>We address the issues of execution time and grasp sampling by directly generating grasp poses for every pixel in an image simultaneously, using a comparatively small neural network.</p><p>Closed-Loop Grasping Closed-loop control of a robot to a desired pose using visual feedback is commonly referred to as visual servoing. The advantages of visual servoing methods are that they are able to adapt to dynamic environments and do not necessarily require fully accurate camera calibration or position control. A number of works apply visual servoing directly to grasping applications, with a survey given in <ref type="bibr" target="#b13">[14]</ref>. However, the nature of visual servoing methods mean that they typically rely on hand-crafted image features for object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref> or object pose estimation <ref type="bibr" target="#b10">[11]</ref>, so do not perform any online grasp synthesis but instead converge to a pre-determined goal pose and are not applicable to unknown objects.</p><p>CNN-based controllers for grasping have very recently been proposed to combine deep learning with closed loop grasping <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref>. Rather than explicitly performing grasp synthesis, both systems learn controllers which map potential control commands to the expected quality of or distance to a grasp after execution of the control, requiring many potential commands to be sampled at each time step. In both cases, the control executes at no more than approximately 5 Hz. While both are closed-loop controllers, grasping in dynamic scenes is only presented in <ref type="bibr" target="#b31">[32]</ref> and we reproduce these experiments.</p><p>The grasp regression methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref> report real-time performance, but are not validated with robotic experiments.</p><p>Benchmarking for Robotic Grasping Directly comparing results between robotic grasping experiments is difficult due to the wide range of grasp detection techniques used, the lack of standardisation between object sets, and the limitations of different physical hardware, e.g. robot arms, grippers or cameras. Many people report grasp success rates on sets of "household" objects, which vary significantly in the number and types of objects used.</p><p>The ACRV Picking Benchmark (APB) <ref type="bibr" target="#b15">[16]</ref> and the YCB Object Set <ref type="bibr" target="#b4">[5]</ref> define item sets and manipulation tasks, but benchmark on tasks such as warehouse order fulfilment (APB) or table setting and block stacking (YCB) rather than raw grasp success rate as is typically reported. Additionally, many of the items from these two sets are impractically small, large or heavy for many robots and grippers, so have not been widely adopted for robotic grasping experiments.</p><p>We propose a set of 20 reproducible items for testing, comprising comprising 8 3D printed adversarial objects from <ref type="bibr" target="#b20">[21]</ref> and 12 items from the APB and YCB object sets, which we believe provide a wide enough range of sizes, shapes and difficulties to effectively compare results while not excluding use by any common robots, grippers or cameras.</p><p>In <ref type="table" target="#tab_2">Table I</ref> we provide a summary of the recent related work on grasping for unknown objects, and how they compare to our own approach. This is not intended to be a comprehensive review, but rather to highlight the most relevant work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRASP POINT DEFINITION</head><p>Like much of the related literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref>, we consider the problem of detecting and executing antipodal grasps on unknown objects, perpendicular to a planar surface, given a depth image of the scene <ref type="figure" target="#fig_0">(Fig. 2</ref>).</p><p>Let g = (p, φ, w, q) define a grasp, executed perpendicular to the x-y plane. The grasp is determined by its pose, i.e. the gripper's centre position p = (x, y, z) in Cartesian coordinates, the gripper's rotation φ around the z axis and the required gripper width w. A scalar quality measure q, representing the chances of grasp success, is added to the pose. The addition of the gripper width enables a better prediction and better performance over the more commonly used position and rotation only representation.</p><p>We want to detect grasps given a 2.5D depth image I = R H×W with height H and width W , taken from a camera with known intrinsic parameters. In the image I a grasp is described byg</p><formula xml:id="formula_1">= (s,φ,w, q),</formula><p>where s = (u, v) is the centre point in image coordinates (pixels),φ is the rotation in the camera's reference frame and w is the grasp width in image coordinates. A grasp in the image spaceg can be converted to a grasp in world coordinates g by applying a sequence of known transforms,</p><formula xml:id="formula_2">g = t RC (t CI (g))<label>(1)</label></formula><p>where t RC transforms from the camera frame to the world/robot frame and t CI transforms from 2D image coordinates to the 3D camera frame, based on the camera intrinsic parameters and known calibration between the robot and camera. We refer to the set of grasps in the image space as the grasp map, which we denote</p><formula xml:id="formula_3">G = (Φ, W, Q) ∈ R 3×H×W</formula><p>where Φ, W and Q are each ∈ R H×W and contain values of φ,w and q respectively at each pixel s.</p><p>Instead of sampling the input image to create grasp candidates, we wish to directly calculate a graspg for each pixel in the depth image I. To do this, we define a function M from a depth image to the grasp map in the image coordinates: M (I) = G. From G we can calculate the best visible grasp in the image spaceg * = max Q G, and calculate the equivalent best grasp in world coordinates g * via Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. GENERATIVE GRASPING CONVOLUTIONAL NEURAL NETWORK</head><p>We propose the use of a neural network to approximate the complex function M : I → G. M θ denotes a neural network with θ being the weights of the network.</p><p>We</p><formula xml:id="formula_4">show that M θ (I) = (Q θ , Φ θ , W θ ) ≈ M (I),</formula><p>can be learned with a training set of inputs I T and corresponding outputs G T and applying the L2 loss function L, such that</p><formula xml:id="formula_5">θ = argmin θ L(G T , M θ (I T )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Grasp Representation</head><p>G estimates the parameters of a set of grasps, executed at the Cartesian point p, corresponding to each pixel s. We represent the grasp map G as a set of three images, Q, Φ and W. The representations are as follows:</p><p>Q is an image which describes the quality of a grasp executed at each point (u, v). The value is a scalar in the range [0, 1] where a value closer to 1 indicates higher grasp quality, i.e. higher chance of grasp success.</p><p>Φ is an image which describes the angle of a grasp to be executed at each point. Because the antipodal grasp is symmetrical around ± π 2 radians, the angles are given in the range [− π 2 , π 2 ]. W is an image which describes the gripper width of a grasp to be executed at each point. To allow for depth invariance, values are in the range of [0, 150] pixels, which can be converted to a physical measurement using the depth camera parameters and measured depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Dataset</head><p>To train our network, we create a dataset ( <ref type="figure" target="#fig_1">Fig. 3</ref>) from the Cornell Grasping Dataset <ref type="bibr" target="#b16">[17]</ref>. The Cornell Grasping Dataset contains 885 RGB-D images of real objects, with 5110 human-labelled positive and 2909 negative grasps. While this is a relatively small grasping dataset compared to some more recent, synthetic datasets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, the data best suits Generation of training data used to train our GG-CNN. Left: The cropped and rotated depth and RGB images from the Cornell Grasping Dataset <ref type="bibr" target="#b16">[17]</ref>, with the ground-truth positive grasp rectangles representing antipodal grasps shown in green. The RGB image is for illustration and is not used by our system. Right: From the ground-truth grasps, we generate the Grasp Quality (Q T ), Grasp Angle (Φ T ) and Grasp Width (W T ) images to train our network. The angle is further decomposed into cos(2Φ T ) and sin(2Φ T ) for training as described in Section IV-B. our pixelwise grasp representation as multiple labelled grasps are provided per image. This is a more realistic estimate of the full pixel-wise grasp map, than using a single image to represent one grasp, such as in <ref type="bibr" target="#b20">[21]</ref>. We augment the Cornell Grasping Dataset with random crops, zooms and rotations to create a set of 8840 depth images and associated grasp map images G T , effectively incorporating 51,100 grasp examples.</p><p>The Cornell Grasping Dataset represents antipodal grasps as rectangles using pixel coordinates, aligned to the position and rotation of a gripper <ref type="bibr" target="#b34">[35]</ref>. To convert from the rectangle representation to our image-based representation G, we use the centre third of each grasping rectangle as an image mask which corresponds to the position of the centre of the gripper. We use this image mask to update sections of our training images, as described below and shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. We consider only the positive labelled grasps for training our network and assume any other area is not a valid grasp.</p><p>Grasp Quality: We treat each ground-truth positive grasp from the Cornell Grasping Dataset as a binary label and set the corresponding area of Q T to a value of 1. All other pixels are 0.</p><p>Angle: We compute the angle of each grasping rectangle in the range [− π 2 , π 2 ], and set the corresponding area of Φ T . We encode the angle as two vector components on a unit circle, producing values in the range [−1, 1] and removing any discontinuities that would occur in the data where the angle wraps around ± π 2 if the raw angle was used, making the distribution easier for the network to learn <ref type="bibr" target="#b8">[9]</ref>. Because the antipodal grasp is symmetrical around ± π 2 radians, we use use two components sin(2Φ T ) and cos(2Φ T ) which provides values which are unique within Φ T ∈ [− π 2 , π 2 ] and symmetrical at ± π 2 . Width: Similarly, we compute the width in pixels (maximum of 150) of each grasping rectangle representing the width of the gripper and set the corresponding portion of W T . During training, we scale the values of W T by 1 150 to put it in the range [0, 1]. The physical gripper width can be calculated using the parameters of the camera and the measured depth.</p><p>Depth Input: As the Cornell Grasping Dataset is captured with a real camera it already contains realistic sensor noise and therefore no noise addition is required. The depth images are inpainted using OpenCV <ref type="bibr" target="#b3">[4]</ref> to remove invalid values. We subtract the mean of each depth image, centring its value around 0 to provide depth invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture</head><p>Our GG-CNN is a fully convolutional topology, shown in <ref type="figure" target="#fig_2">Fig. 4a</ref>. It is used to directly approximate the grasp map G θ from an input depth image I. Fully convolutional networks have been shown to perform well at computer vision tasks requiring transfer between image domains, such image segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref> and contour detection <ref type="bibr" target="#b33">[34]</ref>.</p><p>The GG-CNN computes the function M θ (I) = (Q θ , Φ θ , W θ ), where I, Q θ , Φ θ and W θ are represented as 300×300 pixel images. As described in Section IV-B, the network outputs two images representing the unit vector components of 2Φ θ , from which we calculate the grasp angles by Φ θ = 1 2 arctan sin(2Φ θ ) cos(2Φ θ ) . Our final GG-CNN contains 62,420 parameters, making it significantly smaller and faster to compute than the CNNs used for grasp candidate classification in other works which contain hundreds of thousands <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref> or millions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> of parameters. Our code is available at https://github.com/dougsm/ggcnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training</head><p>We train our network on 80% of our training dataset, and keep 20% as an evaluation dataset. We trained 95 networks with similar architectures but different combinations of convolutional filters and stride sizes for 100 epochs each.</p><p>To determine the best network configuration, we compare relative performance between our trained networks by evaluating each on detecting ground-truth grasps in our 20% evaluation dataset containing 1710 augmented images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SET-UP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Physical Components</head><p>To perform our grasping trials we use a Kinova Mico 6DOF robot fitted with a Kinova KG-2 2-fingered gripper.</p><p>Our camera is an Intel RealSense SR300 RGB-D camera. The camera is mounted to the wrist of the robot, approximately 80 mm above the closed fingertips and inclined at 14 • towards the gripper. This set-up is shown in <ref type="figure" target="#fig_2">Fig. 4a</ref>.</p><p>The GG-CNN computations were performed on a PC running running Ubuntu 16.04 with a 3.6 GHz Intel Core i7-7700 CPU and NVIDIA GeForce GTX 1070 graphics card. On this platform, the GG-CNN takes 6 ms to compute for a single depth image, and computation of the entire grasping pipeline (Section V-C) takes 19 ms, with the code predominantly written in Python. Fig <ref type="figure">. 5</ref>. The objects used for grasping experiments. Left: The 8 adversarial objects from <ref type="bibr" target="#b20">[21]</ref>. Right: The 12 household objects selected from <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Limitations:</head><p>The RealSense camera has a specified minimum range of 200 mm. In reality, we find that the RealSense camera is unable to produce accurate depth measurements from a distance closer than 150 mm, as the separation between the camera's infra-red projector and camera causes shadowing in the depth image caused by the object. For this reason, when performing closed-loop grasping trials (Section V-D2), we stop updating the target grasp pose at this point, which equates to the gripper being approximately 70 mm from the object. Additionally, we find that the RealSense is unable to provide any valid depth data on many black or reflective objects.</p><p>The Kinova KG-2 gripper has a maximum stroke of 175 mm, which could easily envelop many of the test items. To encourage more precise grasps, we limit the maximum gripper width to approximately 70 mm. The fingers of the gripper have some built-in compliance and naturally splay slightly at the tips, so we find that objects with a height less than 15 mm (especially those that are cylindrical, like a thin pen) cannot be grasped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Test Objects</head><p>There is no set of test objects which are commonly used for robotic grasping experiments, with many people using random "household" objects which are not easily reproducible. We propose here two sets of reproducible benchmark objects <ref type="figure">(Fig. 5</ref>) on which we test the grasp success rate of our approach.</p><p>Adversarial Set The first set consists of eight 3D-printed objects with adversarial geometry, which were used by Mahler et al. <ref type="bibr" target="#b20">[21]</ref> to verify the performance of their Grasp Quality CNN. The objects all have complex geometry, meaning there is a high chance of a collision with the object in the case of an inaccurate grasp, as well as many curved and inclined surfaces which are difficult or impossible to grasp. The object models are available online as part of the released datatasets for Dex-Net 2.0 1 <ref type="bibr" target="#b20">[21]</ref>.</p><p>Household Set This set of items contains twelve household items of varying sizes, shapes and difficulty with minimal redundancy (i.e. minimal objects with similar shapes). The objects were chosen from the standard robotic grasping datasets the ACRV Picking Benchmark (APB) <ref type="bibr" target="#b15">[16]</ref> and the YCB Object Set <ref type="bibr" target="#b4">[5]</ref>, both of which provide item specifications and online purchase links. Half of the item classes (mug, screwdriver, marker pen, die, ball and clamp) appear in both data sets. We have made every effort to produce a balanced object set containing objects which are deformable (bear and cable), perceptually challenging (black clamp and screwdriver handle, thin reflective edges on the mug and duct tape, and clear packaging on the toothbrush), and objects which are small and require precision (golf ball, duck and die).</p><p>While both the APB and YCB object sets contain a large number of objects, many are physically impossible for our robot to grasp due to being too small and thin (e.g. screws, washers, envelope), too large (e.g. large boxes, saucepan, soccer ball) or too heavy (e.g. power drill, saucepan). While manipulating these objects is an open problem in robotics, we do not consider them for our experiments in order to compare our results to other work which use similar object classes to ours <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Grasp Detection Pipeline</head><p>Our grasp detection pipeline comprises three stages: image processing, evaluation of the GG-CNN and computation of a grasp pose. The depth image is first cropped to a square, and scaled to 300 × 300 pixels to suit the input of the GG-CNN. We inpaint invalid depth values using OpenCV <ref type="bibr" target="#b3">[4]</ref>.</p><p>The GG-CNN is then evaluated on the processed depth image, to produce the grasp map G θ . We filter Q θ with a Gaussian kernel, similar to <ref type="bibr" target="#b11">[12]</ref>, and find this helps improve our grasping performance by removing outliers and causing the local maxima of G θ to converge to regions of more robust grasps.</p><p>Finally, the best grasp pose in the image spaceg * θ is computed by identifying the maximum pixel s * in Q θ , and the rotation and width are computed from Φ θ | s * and W θ | s * respectively. The grasp in Cartesian coordinates g * θ is computed via Eq. (1) <ref type="figure" target="#fig_2">(Fig. 4b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Grasp Execution</head><p>We evaluate the performance of our system using two grasping methods. Firstly, an open-loop grasping method similar to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b20">21]</ref>, where the best grasp pose is calculated from a single viewpoint and executed by the robot openloop. Secondly, we implement a closed-loop visual servoing controller which we use for evaluating our system in dynamic environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Open Loop Grasping:</head><p>To perform open-loop grasps, the camera is positioned approximately 350 mm above and parallel to the surface of the table. An item is placed in the field of view of the camera. A depth image is captured and the pose of the best grasp is computed using the grasp detection pipeline. The robot moves to a pre-grasp position, with the gripper tips aligned with and approximately 170 mm above the computed grasp. From here, the robot moves straight down until the grasp pose is met or a collision is detected via force feedback in the robot. The gripper is closed and lifted, and the grasp is recorded as a success if the object is successfully lifted to the starting position.</p><p>2) Closed Loop Grasping: To perform closed-loop grasping, we implement a Position Based Visual Servoing (PBVS) controller <ref type="bibr" target="#b13">[14]</ref>. The camera is initially positioned approximately 400 mm above the surface of the table, and an object is placed in the field of view. Depth images are generated at a rate of 30 Hz and processed by the grasp detection pipeline to generate grasp poses in real time. There may be multiple similarly-ranked good quality grasps in an image, so to avoid rapidly switching between them, which would confuse the controller, we compute three grasps from the highest local maxima of G θ and select the one which is closest (in image coordinates) to the grasp used on the previous iteration. As the control loop is fast compared to the movement of the robot, there is unlikely to be a major change between frames. The system is initialised to track the global maxima of Q θ at the beginning of each grasp attempt. We represent the poses of the grasp T g * θ and the gripper fingers T f as 6D vectors comprising the Cartesian position and roll, pitch and yaw Euler angles (x, y, z, α, β, γ), and generate a 6D velocity signal for the end-effector: v = λ(T g * θ − T f ) where λ is a 6D scale for the velocity, which causes the gripper to converge to the grasp pose. Simultaneously, we control the gripper fingers to the computed gripper width via velocity control. Control is stopped when the grasp pose is reached or a collision is detected. The gripper is closed and lifted and the grasp is recorded as a success if the object is successfully lifted to the starting position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Object Placement</head><p>To remove bias related to object pose, objects are shaken in a cardboard box and emptied into the robot's workspace for each grasp attempt. The workspace is an approximately 250×300 mm area in the robot's field of view in which the robot's kinematics allow it to execute a vertical grasp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>To evaluate the performance of our grasping pipeline and GG-CNN, we perform several experiments comprising over 2000 grasp attempts. In order to compare our results to others, we aim to reproduce similar experiments where possible, and also aim to present experiments which are reproducible in themselves by using our defined set of objects (Section V-B) and defined dynamic motions.</p><p>Firstly, to most closely compare to existing work in robotic grasping, we perform grasping on singulated, static objects from our two object sets. Secondly, to highlight our primary contribution, we evaluate grasping on objects which are moved during the grasp attempt, to show the ability of our system to perform dynamic grasping. Thirdly, we show our system's ability to generalise to dynamic cluttered scenes by reproducing the experiments from <ref type="bibr" target="#b31">[32]</ref> and show improved results. Finally, we further show the advantage of our closedloop grasping method over open-loop grasping by performing grasps in the presence of simulated kinematic errors of our robot's control. <ref type="table" target="#tab_2">Table II</ref> provides a summary of our results in different grasping tasks and comparisons to other work where possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Static Grasping</head><p>To evaluate the performance of our GG-CNN under static conditions, we performed grasping trials using both the openand closed-loop methods on both sets of test objects, using the set-up shown in <ref type="figure" target="#fig_3">Fig. 6a</ref>. We perform 10 trials on each object. For the adversarial object set, the grasp success rates were 84% (67/80) and 81% (65/80) for the open-and closed-loop methods respectively. For the household object set, the openloop method achieved 92% (110/120) and the closed-loop 91% (109/120).</p><p>A comparison to other work is provided in <ref type="table" target="#tab_2">Table II</ref>. We note that the results may not be directly comparable due to the different objects and experimental protocol used, however we aim to show that we achieve comparable performance to other works which use much larger neural networks and have longer computation times. A noteworthy difference in method is <ref type="bibr" target="#b17">[18]</ref>, which does not require precise camera calibration, but rather learns the spatial relationship between the robot and the objects using vision. <ref type="bibr" target="#b16">[17]</ref> [  <ref type="bibr" target="#b20">[21]</ref> train their grasp network on the adversarial objects! </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Grasping</head><p>To perform grasps on dynamic objects we take inspiration from recent work in <ref type="bibr" target="#b31">[32]</ref>, where items are moved once by hand randomly during each grasp attempt. To assist reproducibility, we define this movement to consist of a translation of at least 100 mm and a rotation of at least 25 • after the grasp attempt has begun, shown in <ref type="figure" target="#fig_3">Fig. 6a</ref>-b, which we measure using a grid on the table.</p><p>We perform 10 grasp attempts on each adversarial and household object using our closed-loop method, and achieve grasp success rates of 83% (66/80) for the adversarial objects and 88% (106/120) for the household objects. These results are not significantly different to our results on static objects, and are within the 95% confidence bounds of our results on static objects, showing our method's ability to maintain a high level of accuracy when grasping dynamic objects.</p><p>We do not compare directly to an open-loop method as the object movement moves the object sufficiently far from the original position that no successful grasps would be possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dynamic Grasping in Clutter</head><p>Viereck et al. <ref type="bibr" target="#b31">[32]</ref> demonstrate a visuomotor controller for robotic grasping in clutter that is able to react to disturbances to the objects being grasped. As this work is closely related to our own, we have made an effort to recreate their experiments using objects as close as possible to their set of 10 ( <ref type="figure">Fig. 7)</ref> to <ref type="bibr">Fig. 7</ref>. Left: The objects used to reproduce the dynamic grasping in clutter experiment of <ref type="bibr" target="#b31">[32]</ref>. Right: The test objects used by <ref type="bibr" target="#b31">[32]</ref>. We have attempted to recreate the object set as closely as possible. perform a comparison. Even though our GG-CNN has not been trained on cluttered environments, we show here its ability to perform grasping in the presence of clutter. We recreate the three grasping experiments from <ref type="bibr" target="#b31">[32]</ref> as follows:</p><p>1) Isolated Objects: We performed 4 grasps on each of the 10 test objects <ref type="figure">(Fig. 7)</ref> in isolation, and achieved a grasp success rate of 100%, compared to 98% (39/40) in <ref type="bibr" target="#b31">[32]</ref>.</p><p>2) Cluttered Objects: The 10 test objects are shaken in a box and emptied in a pile below the robot <ref type="figure" target="#fig_3">(Fig. 6c</ref>). The robot attempts multiple grasps, and any objects that are grasped are removed. This continues until all objects are grasped, three consecutive grasps are failures or all objects are outside the workspace of the robot. We run this experiment 10 times.</p><p>Despite our GG-CNN not being trained on cluttered scenes, we achieved a grasp success rate of 87% (83/96) compared to 89% (66/74) in <ref type="bibr" target="#b31">[32]</ref>. Our most common failure cause was collision of the gripper with two objects that had fallen up against each other. 8 out of the 13 failed grasps were from two runs where objects had fallen into an ungraspable position and failed repeatedly. 8 out of the 10 runs finished with 0 or 1 grasp failures.</p><p>3) Dynamic Cluttered Objects: For dynamic scenes, we repeat the procedure as above with the addition of a random movement of the objects during the grasp attempt. Viereck et al. <ref type="bibr" target="#b31">[32]</ref> do not give specifications for their random movement, so we use the same procedures as in Section VI-B, where we move the objects randomly, at least 100 mm and 25 • during each grasp attempt <ref type="figure" target="#fig_3">(Fig. 6d</ref>).</p><p>In 10 runs of the experiment, we performed 94 grasp attempts of which 76 were successful (81%), compared to 77% (58/75) in <ref type="bibr" target="#b31">[32]</ref>. Like the static case, 8 of the 18 failed grasps were from two runs where the arrangement of the objects resulted in repeated failed attempts. In the other 8 runs, all available objects (i.e. those that didn't fall/roll out of the workspace) were successfully grasped with 2 or fewer failed grasps.</p><p>Despite not being trained on cluttered scenes, this shows our approach's ability to perform grasping in clutter and its ability to react to dynamic scenes, showing only a 5% decrease in performance for the dynamic case compared to 12% in <ref type="bibr" target="#b31">[32]</ref>.</p><p>For the same experiments, <ref type="bibr" target="#b31">[32]</ref> shows that an open-loop baseline approach on the same objects that is able to achieve 95% grasp success rate for the static cluttered scenes achieves only 23% grasp success rate for dynamic scenes as it is able to react to the change in item location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robustness to Control Errors</head><p>The control of a robot may not always be precise. For example, when performing grasping trials with a Baxter Research Robot, Lenz et al. <ref type="bibr" target="#b16">[17]</ref> found that positioning errors of up to 20 mm were typical. A major advantage of using a closedloop controller for grasping is the ability to perform accurate grasps despite inaccurate control. We show this by simulating an inaccurate kinematic model of our robot by introducing a cross-correlation between Cartesian (x, y and z) velocities:</p><formula xml:id="formula_6">v c = v ·   1 + c xx c xy c xz c yx 1 + c yy c yz c zx c zy 1 + c zz   where each c ∼ N (0, σ 2 )</formula><p>is sampled at the beginning of each grasp attempt. While a real kinematic error (e.g. a link length being incorrectly configured) would result in a more non-linear response, our noise model provides a good approximation which is independent of the robot's kinematic model, so has a deterministic effect with respect to end-effector positioning and is more easily replicated on a different robotic system. We test grasping on both object sets with 10 grasp attempts per object for both the open-and closed-loop methods with σ = 0.0 (the baseline case), 0.05, 0.1 and 0.15. In the case of our open-loop controller, where we only control velocity for 170 mm in the z direction from the pre-grasp pose (Section V-D1), this corresponds to having a robot with an endeffector precision described by a normal distribution with zero mean and standard deviation 0.0, 8.5, 17.0 and 25.5 mm respectively, by the relationship for scalar multiplication of the normal distribution: ∆x = ∆y = ∆z · N (0, σ 2 ) = N (0, ∆z 2 σ 2 ); ∆z = 170 mm</p><p>The results are illustrated in <ref type="figure" target="#fig_4">Fig. 8, and</ref> show that the closed-loop method outperforms the open-loop method in the presence of control error. This highlights a major advantage of being able to perform closed-loop grasping, as the openloop methods are unable to respond, achieving only 38% grasp success rate in the worst case. In comparison, the closed-loop method achieves 68% and 73% grasp success rate in the worst case on the adversarial and household objects respectively.</p><p>The decrease in performance of the closed-loop method is due to the limitation of our camera (Section V-A1), where we are unable to update the grasp pose when the gripper is within 70 mm of the object, so can not correct for errors in this range. The addition of control inaccuracy effects objects which require precise grasps (e.g. the adversarial objects, and small objects such as the die and ball) the most. Simpler objects which are more easily caged by the gripper, such as the pen, still report good grasp results in the presence of kinematic error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We present our Generative Grasping Convolutional Neural Network (GG-CNN), an object-independent grasp synthesis model which directly generates grasp poses from a depth image on a pixelwise basis, instead of sampling and classifying individual grasp candidates like other deep learning techniques. Our GG-CNN is orders of magnitude smaller than other recent grasping networks, allowing us to generate grasp poses at a rate of up to 50 Hz and perform closed-loop control. We show through grasping trials that our system is able to gain state-of-the-art results in grasping unknown, dynamic objects, including objects in dynamic clutter. Additionally, our closedloop grasping method significantly outperforms an open-loop method in the presence of simulated robot control error.</p><p>We encourage reproducibility in robotic grasping experiments by using two standard object sets, a set of eight 3D-printed objects with adversarial geometry <ref type="bibr" target="#b20">[21]</ref> plus a proposed set of twelve household items from standard robotic benchmark object sets, and by defining the parameters of our dynamic grasping experiments. On our two object sets we achieve 83% and 88% grasp success rate respectively when objects are moved during the grasp attempt, and 81% for objects in dynamic clutter.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Left: A grasp g is defined by its Cartesian position (x, y, z), rotation around the z-axis φ and gripper width w required for a successful grasp. Right: In the depth image the grasp poseg is defined by its centre pixel (u, v), its rotationφ around the image axis and perceived widthw.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(a) The Generative Grasping CNN (GG-CNN) takes an inpainted depth image (I), and directly generates a grasp pose for every pixel (the grasp map G θ ), comprising the grasp quality Q θ , grasp width W θ and grasp angle Φ θ . (b) From the combined network output, we can compute the best grasp point to reach for, g * θ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Grasping experiments. (a) Set-up for static grasping, and initial setup for dynamic grasping. (b) During a dynamic grasp attempt, the object is translated at least 100 mm and rotated at least 25 • , measured by the grid on the table. (c) Set-up for static grasping in clutter, and initial set-up for dynamic grasping in clutter. (d) During a dynamic grasp attempt, the cluttered objects are translated at least 100 mm and rotated at least 25 • , measured by the grid on the table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison of grasp success rates for open-loop and closed-loop control methods with velocity cross-correlation added to simulate kinematic errors (see Section VI-D for full details). The closed-loop method outperforms the open-loop method in all cases where kinematic errors are present. 10 trials were performed on each object in both the adversarial and household object sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>A comparison of our work to related deep learning approaches to grasp synthesis.</figDesc><table /><note>* Code is available at https://github.com/dougsm/ggcnn</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Results from grasping experiments with 95% confidence intervals, and comparison to other deep learning approaches where available. # Note that all experiments use different item sets and experimental protocol, so comparative performance is indicative only.</figDesc><table><row><cell></cell><cell></cell><cell>23]</cell><cell>[12]</cell><cell>[21]</cell><cell>[18]</cell><cell>[32]</cell><cell>Ours</cell></row><row><cell>Grasp Success Rate (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Household Objects (Static) #</cell><cell>89</cell><cell>73</cell><cell>80</cell><cell>80</cell><cell>80</cell><cell></cell><cell>92±5</cell></row><row><cell>Adversarial Objects (Static)</cell><cell></cell><cell></cell><cell></cell><cell>93*</cell><cell></cell><cell></cell><cell>84±8</cell></row><row><cell>Household Objects (Dynamic)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88±6</cell></row><row><cell>Adversarial Objects (Dynamic)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>83±8</cell></row><row><cell>Objects from [32] (Single)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>98</cell><cell>100</cell></row><row><cell>Objects from [32] (Clutter)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell>87±7</cell></row><row><cell>Objects from [32] (Clutter, Dynamic)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77</cell><cell>81±8</cell></row><row><cell>Network Parameters (approx.)</cell><cell></cell><cell cols="3">60M 60M 18M</cell><cell>1M</cell><cell></cell><cell>62k</cell></row><row><cell cols="2">Computation Time (to generate pose or command) 13.5s</cell><cell></cell><cell></cell><cell>0.8s</cell><cell>0.2-0.5s</cell><cell>0.2s</cell><cell>19ms</cell></row></table><note>*Contrary to our approach,</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://berkeleyautomation.github.io/dex-net/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS This research was supported by the Australian Research</head><p>Council Centre of Excellence for Robotic Vision (project number CE140100016).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<title level="m">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robotic Grasping and Contact: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bicchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data-Driven Grasp Synthesis -A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamim</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="309" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The OpenCV Library. Dr. Dobb&apos;s Journal of Software Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Benchmarking in Manipulation Research: Using the Yale-CMU-Berkeley Object and Model Set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics &amp; Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="36" to="52" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Object-specific Grasp Affordance Densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Detry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Baseski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mila</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younes</forename><surname>Touati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kroemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Development and Learning (ICDL)</title>
		<meeting>of the IEEE International Conference on Development and Learning (ICDL)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Handling Objects By Their Handles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anis</forename><surname>Sahbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Grasp Planning via Decomposition Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Goldfeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Peter K Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Lackner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pelossof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4679" to="4684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Designing Deep Convolutional Neural Networks for Continuous Object Orientation Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01499</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visually Guided Object Grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadi</forename><surname>Dornaika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Espiau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="532" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4461" to="4468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Playing Catch and Juggling with a Humanoid Robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Glisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mistry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE-RAS International Conference on Humanoid Robots (Humanoids)</title>
		<meeting>of the IEEE-RAS International Conference on Humanoid Robots (Humanoids)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="875" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Survey on Visual Servoing for Manipulation. Computational Vision and Active Perception Laboratory, Fiskartorpsv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robotic Grasp Detection using Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The ACRV Picking Benchmark: A Robotic Shelf Picking Benchmark to Foster Reproducible Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><forename type="middle">E</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">W</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Durham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Eich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Lehnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Mangels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4705" to="4712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning for detecting robotic grasps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<idno type="DOI">http:/journals.sagepub.com/doi/10.1177/0278364914549607</idno>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="705" to="724" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Hand-Eye Coordination for Robotic Grasping with Large-Scale Data Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-50115-4_16</idno>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dex-Net 1.0: A cloud-based network of 3D objects for robust grasp planning using a Multi-Armed Bandit model with correlated rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><forename type="middle">T</forename><surname>Pokorny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melrose</forename><surname>Roderick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Kroger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kuffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Mahler</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacky</forename><surname>Liang</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherdil</forename><surname>Niyaz</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laskey</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Doan</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Aparicio Ojea</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic Grasp Planning Using Shape Primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><forename type="middle">I</forename><surname>Knoop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter K</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1824" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supersizing selfsupervision: Learning to grasp from 50k tries and 700 robot hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerrel</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3406" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domenico</forename><surname>Prattichizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trinkle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grasping</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-540-30301-5_29</idno>
		<title level="m">Springer Handbook of Robotics, chapter 28</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="671" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-Time Grasp Detection Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1316" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the Relevance of Grasp Metrics for Predicting Grasp Success</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Rubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bohg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ International Conference of Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ International Conference of Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An overview of 3D object grasp synthesis algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anis</forename><surname>Sahbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>El-Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Bidaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="336" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robotic Grasping of Novel Objects using Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Driemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">http:/journals.sagepub.com/doi/10.1177/0278364907087172</idno>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robot Grasp Synthesis Algorithms: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shimoga</surname></persName>
		</author>
		<idno type="DOI">http:/journals.sagepub.com/doi/abs/10.1177/027836499601500302</idno>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="266" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual servoing for humanoid grasping and manipulation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vahrenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wieland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Humanoid Robots (Humanoids)</title>
		<meeting>of the International Conference on Humanoid Robots (Humanoids)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="406" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating Multi-Fingered Robotic Grasps via Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4415" to="4420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a visuomotor controller for real world robotic grasping using simulated depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Viereck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Pas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Robot Learning (CoRL)</title>
		<meeting>of the Conference on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robot grasp detection using multimodal deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">http:/ade.sagepub.com/lookup/doi/10.1177/1687814016668077</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Mechanical Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object Contour Detection with a Fully Convolutional Encoder-Decoder Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient Grasping from RGBD Images: Learning using a new Rectangle Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Moseson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
