<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to combine Grammatical Error Corrections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-10">10 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Kantor</surname></persName>
							<email>yoavka@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Katz</surname></persName>
							<email>katz@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
							<email>leshem.choshen@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edo</forename><surname>Cohen-Karlik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Liberman</surname></persName>
							<email>naftali.liberman@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Toledo</surname></persName>
							<email>assaf.toledo@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Menczel</surname></persName>
							<email>amir.menczel@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
							<email>noams@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to combine Grammatical Error Corrections</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-10">10 Jun 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The field of Grammatical Error Correction (GEC) has produced various systems to deal with focused phenomena or general text editing. We propose an automatic way to combine black-box systems. Our method automatically detects the strength of a system or the combination of several systems per error type, improving precision and recall while optimizing F score directly. We show consistent improvement over the best standalone system in all the configurations tested. This approach also outperforms average ensembling of different RNN models with random initializations.</p><p>In addition, we analyze the use of BERT for GEC -reporting promising results on this end. We also present a spellchecker created for this task which outperforms standard spellcheckers tested on the task of spellchecking. This paper describes a system submission to Building Educational Applications 2019 Shared Task:</p><p>Grammatical Error Correction <ref type="bibr" target="#b1">(Bryant et al., 2019)</ref>.</p><p>Combining the output of top BEA 2019 shared task systems using our approach, currently holds the highest reported score in the open phase of the BEA 2019 shared task, improving F 0.5 by 3.7 points over the best result reported.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike other generation tasks (e.g. Machine Translation and Text Summarization), Grammatical Error Correction (GEC) contains separable outputs, edits that could be extracted from sentences, categorized <ref type="bibr" target="#b2">(Bryant et al., 2017)</ref> and evaluated separately <ref type="bibr" target="#b3">(Choshen and Abend, 2018a)</ref>. Throughout the years different approaches were considered, some focused on specific error types <ref type="bibr" target="#b25">(Rozovskaya et al., 2014)</ref> and others adjusted systems from other tasks <ref type="bibr" target="#b30">(Zhao et al., 2019)</ref>. *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributed equally</head><p>While the first receive high precision, the latter often have high recall and differ in what they correct. To benefit from both worlds, pipelines <ref type="bibr" target="#b24">(Rozovskaya and Roth, 2016)</ref> and rescoring hybrids  were introduced.</p><p>Another suggested method for combining is average ensembling , used when several end to end neural networks are trained.</p><p>As single systems tend to have low recall <ref type="bibr" target="#b4">(Choshen and Abend, 2018b)</ref>, pipelining systems may propagate errors and may not benefit from more than one system per error. Rescoring reduces recall and may not be useful with many systems . We propose a new method for combining systems ( §4) that can combine many systems and relies solely on their output, i.e., it uses systems as a black-box. We show our system outperforms average ensembling, has benefits even when combining a single system with itself, and produces the new state of the art by combining several existing systems ( §5).</p><p>To develop a system we trained GEC systems and gathered outputs from black-box systems ( §3). One of the most frequent error types is spelling errors, we compared off of the shelf spellcheckers, systems developed for this error type specifically, to a new spellchecker ( §3.1), finding that our spellchecker outperforms common spellcheckers on the task of spellchecking.</p><p>Another system tested was modifications of BERT <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref> to correct errors, allowing for less reliance on parallel data and more generalizability across domains ( §3.4).</p><p>Lastly, we tested generating synthetic errors  as a way to replace data in an unsupervised scenario. While finding that mimicking the error distribution and generating errors on the same domain is better, we did not eventu-ally participate in the low-resource track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data 2.1 Preprocessing</head><p>Many systems assume the input is standard untokenized English sentences. In these cases, we detokenized the input data sets and then tokenized again to perform the combination and evaluation steps.</p><p>For training the Nematus network, we passed the data tokenization and truecasing <ref type="bibr">(Koehn et al., 2007)</ref> and trained BPE <ref type="bibr" target="#b28">(Sennrich et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Synthetic Error Generation</head><p>Generating training data for the GEC problem is expensive and slow when done manually by human annotators. Most machine-learning based systems today benefit from the quantity and richness of the training data, therefore, generating synthetic data has a lot of potential, as was also shown in previous work . We generate data with errors by applying corrections backwards. Meaning, if a correction adds a missing word X to a sentence, to produce the corresponding error we remove X from a sentence. And if a correction removes a redundant word X from a sentence, to produce the corresponding error we add word X in a random location in a sentence. And if a correction replaces word X with word Y in a sentence, to produce the corresponding error we replace word Y with word X in a sentence. In order to preserve the distribution of errors as found in the W&amp;I+LOCNESS train data set, we analyze it and measure the distribution of corrections in it. We measure the distribution of number of corrections in a sentence and distribution of specific corrections. Using these distributions and a corpus of gold (correct) sentences we produce errors with similar distributions. We first randomly select the number of corrections in a sentence according to the distribution measured before. Then, we randomly select specific corrections according to the distribution of corrections. We then find all sentences where all corrections can be applied backwards and pick one of them randomly. Lastly, we generate the errors in the sentence and add the gold sentence and error sentence to corresponding output files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constructing a spellchecker</head><p>Many tools are available for spelling correction. Yet, with a few heuristics we managed to get a comparatively high result. As by Errant <ref type="bibr" target="#b2">(Bryant et al., 2017)</ref>, our spellchecker receives a better F 0.5 score of spelling (type R:SPELL) than other leading open-source spell-checkers. A comparison can be found at §5.1.</p><p>Our method of correcting spelling mistakes is as follows. As a preprocessing stage, we go over a large monolingual corpus -specifically a 6 million sentences corpus taken from books in project Gutenberg 1 . We count the number of occurrences of each word (in it's surface form), skipping words with less than 3 characters and words that are not composed exclusively of letters. We also use an English dictionary (both US and GB) from Libre-Office site 2 for enriching our data with English words that are not in our books corpus. When correcting a sentence, we find words that are not in our word-count (or in it and have a count below 3) nor in the Dictionary. Skipping words with digits or if it was all upper case. These words are suspected to be misspelled and we try to correct them.</p><p>For every misspelled word we try to find a replacement word by going over the words in the word-count data (words with count greater than 20) in a descending order of occurrences. For each suggested word, we check if it can be considered as a correction for the misspelled word by two methods. First, we check if the original word and the candidate correction differ from each other by swapping two characters. If not, we calculate the distance between the two words using Levenshtein distance <ref type="bibr" target="#b20">(Levenshtein, 1966)</ref> and check if the distance is 1. We return the most frequent word that satisfies one of these conditions . If no candidate is found, we do the same with all words in the dictionary in a lexicographical order. If still no candidate is found, we check if we can split the misspelled word into two words that are in our word-count data or in the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Nematus</head><p>We trained 4 neural machine translation systems based on Nematus <ref type="bibr" target="#b27">(Sennrich et al., 2017)</ref> Transformer <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> implementation. All parameters used are the ones suggested for the 2017 Workshop on Machine Translation 3 . As training data we used all the restricted data, i.e., FCE <ref type="bibr" target="#b8">(Dale and Kilgarriff, 2011)</ref>, LANG8 <ref type="bibr" target="#b21">(Mizumoto et al., 2011)</ref>, NUCLE <ref type="bibr" target="#b7">(Dahlmeier et al., 2013)</ref> and W&amp;I+LOCNESS <ref type="bibr" target="#b1">(Bryant et al., 2019;</ref><ref type="bibr" target="#b14">Granger, 1998</ref>) (upsampled 10 times). Each of the four trained models was regarded as a separate correction method and all systems were combined using our method ( §4), this was especially beneficial as ensembling is not yet implemented for the transformer. See §5.4 for comparison of the two ensembling methods over RNN based Nematus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Off the shelf</head><p>LanguageTool. LanguageTool is a free grammar correction tool mainly based on spellchecking and rules. We used language tool programmatic API to obtain all the possible corrections and applied all the suggestions.</p><p>Grammarly. Grammarly is the company owning the world leading grammar correction product, as such it is the obvious candidate to be used as a component and to assess the potential of combining black box systems. We used their free web interface to correct the dev and test sets. Grammarly does not support a programmatic API, so this process was manual. We uploaded the texts after detokenization into the web interface. For each suggested correction, we took the top prediction without human discretion. The reason to choose the top prediction was to allow combining using a single reference of Grammarly.</p><p>Spelling correction. We tested Enchant, JamSpell and Norvig spellcheckers, finding our spellchecker outperforms those in terms of spelling correction (See §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">BERT</head><p>BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" target="#b9">(Devlin et al., 2018</ref>) is a language representation model. BERT is extremely effective in general purpose tasks, among its virtues, BERT holds a syntactic understanding of a language <ref type="bibr" target="#b13">(Goldberg, 2019)</ref>. Initial pre-training of BERT was performed over a large corpora jointly on two tasks: (1) Masked Language Model 3 https://github.com/EdinburghNLP/wmt17-transformer-scripts -randomly replace words with a predefined token, <ref type="bibr">[MASK]</ref>, and predict the missing word. (2) Next Sentence Prediction -given a pair of sentences A and B, does sentence B follow sentence A.</p><p>Our general approach for using BERT to solve the GEC task is by iteratively querying BERT as a black box language model, reminding former use of language models <ref type="bibr" target="#b6">(Dahlmeier and Ng, 2012;</ref><ref type="bibr" target="#b0">Bryant and Briscoe, 2018)</ref>. To detect missing words we add [MASK] between every two words, if BERT suggests a word with high confidence, we conclude that this word is missing in this gap. To detect unnecessary words, we replace words with the [MASK] token and if all the suggestions returned from BERT have a low probability, we conclude that the masked word was unnecessary. For replacing words, we perform the same procedure by replacing each word with [MASK] and checking if BERT returns a different word with high probability.</p><p>The described process produces many undesired replacements/deletions due to BERT's versatile nature, for example, given a sentence such as:</p><p>There are few ways to get there. BERT may suggest replacing few with many. Such a replacement preserves the grammatically soundness of the sentence, but alters the semantic meaning. Hence, although possibly improving fluency, arguably the true goal of GEC <ref type="bibr" target="#b22">(Napoles et al., 2017)</ref>, this behaviour does not align with the goals of GEC requiring semantic preservation <ref type="bibr" target="#b5">(Choshen and Abend, 2018c)</ref>. In order to focus the exploration space of BERT's suggestions, we limit replacements/deletions to operate within a predefined word set. The word sets considered included syntactically interchangeable words, often sharing some semantic properties. When considering a removal correction, we remove a word only if the returned values from BERT are not in the same word-set as the replaced word. Replacement is allowed only within the same word set. For example, a typical mistake which occurred frequently in the dataset is wrong usage of determiners such as a and an, given the word set {a, an} and the sentence:</p><p>Is that a armadillo?</p><p>The mechanism described limits the replacement correction options to suggest making a replacement-correction of a with an to result with the corrected sentence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is that an armadillo?</head><p>At each iteration of this process, a correction (addition/replacement/deletion) is performed and the resulting sentence is then used as the input to the next iteration. Each replacement/addition of the [MASK] token is a single candidate for a specific correction. Given an input sequence, each possible correction gives rise to a different candidate which is then sent to BERT. The most probable correction (above a minimal threshold) is then selected, this process accounts for one iteration. The resulting sentence is then processed again and the best correction is chosen until all corrections have a low probability in which case the sentence is assumed to be correct.</p><p>The above mechanism with threshold values between 0.6 and 0.98 did not yield satisfying results. For this reason, in the submitted system we limit the mechanism significantly, ignoring additions and deletions to focus solely on the replace corrections. Word sets were chosen from the most frequent errors in the training data across different error types (excluding punctuation marks R:PUNCT).</p><p>Another approach for using BERT is by finetuning BERT to the specific data at hand. Since the GEC task is naturally limited to specific types of errors, we fine-tuned the Masked Language Model task using synthetic data. Instead of randomly replacing words with the [MASK] token, we replace only specific words in a distribution which mimics the training data. This process should create a bias in the language model towards the prediction of words which we want to correct. Unfortunately, these efforts did not bear fruit. The authors believe a more extensive exploration of experimental settings may prove beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Combining systems</head><p>Combining the output of multiple systems has the potential to improve both recall and precision. Recall is increased because typically different systems focus on different aspects of the problem and can return corrections which are not identified by other systems <ref type="bibr" target="#b2">(Bryant et al., 2017)</ref>. Precision can be increased by utilizing the fact that if multiple systems predict the same annotations, we can be more confident that this correction is correct.</p><p>The outputs of Seq2Seq models, differing in training parameters, can be merged using an ensemble approach, where the predictions of the models for each possible word in the sequence are used to compute a merged prediction. It was shown that even an ensemble of models trained with the same hyperparameters but with different instances of random initialization can yield benefit .</p><p>The idea of automatically combining multiple system outputs is not new to other fields and was successfully used in the Named Entity Recognition (NER) and Entity linking (EL) tasks. <ref type="bibr" target="#b16">Jiang et al. (2016)</ref> evaluated multiple NER systems and based on these results, manually selected a rule for combining the two best systems, building a hybrid system that outperformed the standalone systems. <ref type="bibr" target="#b26">Ruiz and Poibeau (2015)</ref> used the precision calculated on a training corpus to calculate a weighted vote for each EL output on unseen data. <ref type="bibr" target="#b10">Dlugolinskỳ et al. (2013)</ref> used decision tree classifier to identify which output to accept. They used a feature set based on the overall text, NE surface form, the NE type and the overlap between different outputs. In GEC, combining was also proposed but was ad-hoc rather than automatic and general. Combining was done by either piping <ref type="bibr" target="#b24">(Rozovskaya and Roth, 2016)</ref>, where each system receives the output of the last system, or correction of specific phenomena per system <ref type="bibr" target="#b23">(Rozovskaya and Roth, 2011)</ref>, or more involved methods tailored to the systems used . This required manual adjustments and refinements for every set of systems.</p><p>Evaluating by a corpus level measure such as F score renders combining systems difficult. Systems developed towards F 0.5 tend to reduce recall improving precision <ref type="bibr" target="#b4">(Choshen and Abend, 2018b)</ref>, while avoiding catastrophic errors <ref type="bibr" target="#b5">(Choshen and Abend, 2018c)</ref> this behaviour might reduce the flexibility of the combination. It is possible to tune systems to other goals (e.g. recall) ) and thus achieve more versatile systems, but that is not the case when using black-box systems, and hence left for future inspection.</p><p>System pair. We propose a method to combine multiple systems by directly optimizing F β for a chosen β, in the field 0.5 is usually used. We begin by considering a combination of two systems 1. Given a development set, where E are the sentences with errors and G are the gold an-notations, generate M 2 gold file, which contains all the gold corrections to the sentences. which determine the probability an edit of the specific error type in a specific subset of edits will be used. According to the way subsets were built, each edit corresponds to exactly one subset (e.g. 1 \ 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Correct</head><p>8. For all error types and subset of edits, compute the optimal selection variables S error−type subset that maximize f β by solving</p><formula xml:id="formula_0">0 ≤ S error−type subset ≤ 1 total = t∈error−type T P t 1∩2 + F N t 1∩2 T P = t∈error−type,s∈subset T P t s * S t s F P = t∈error−type,s∈subset F P t s * S t s F N = total − T P Sopt = arg max S f β (T P, F P, F N )</formula><p>This is a convex optimization problem with linear constraints and pose no difficulty to standard solvers.</p><p>Sopt error−type subset need not be integer, although in practice they usually are. 4 . In our submission, for simplicity, we avoid these cases and round Sopt error−type subset to nearest integer value (either 0 or 1). But our implementation allows sampling.</p><p>A major concern is to what extent does the precision and recall statistics per error type and subset on the development set represent the actual distribution expected during inference on unseen data. Assuming the development set and the unseen are sampled from the same distributions, the confidence is correlated with the number of samples seen for each error-type and subset.</p><p>Assuming errors come from a binomial distribution, we try to estimate the conditional probability P (|prec test − prec dev | &lt; 0.15 | prec dev ). Given more than 20 samples, the probability for 15% difference in development and test precision is 14.5%, and if there are 50 samples, this probability drops to 2.8%. In the experiments, we ignore error-types where there are less than 2 samples.</p><p>The process of correcting an unseen set of sentences T is as follows: In <ref type="table" target="#tab_2">Table 1</ref>, we present the results of the most frequent error types when combining two systems, Nematus and Grammarly. As expected, the precision on corrections found by both systems is significantly higher than those found by a single system. For correction type 'R:OTHER', for example, the precision on common corrections is 0.67, compared to 0.17 and 0.28 of the respective standalone systems. Therefore, the optimal solution uses only the corrections produced by both systems. We can also see that in some error types (e.g., R:SPELL or R:DET) the precision of corrections identified by the Nematus system is low enough that the optimization algorithm selected only the corrections by Grammarly.</p><p>Multiple systems. When N &gt; 2 systems are available, it is possible to extend the above approach by creating more disjoint subsets, which include any of the 2 N subsets of corrections. When N is large, many of these subsets will be very small, and therefore may not contain meaningful statistics. We propose an iterative approach, where at each step two systems are combined. The results of this combination can be then combined with other systems. This approach works better when the development set is small, but can also suffers from over-fitting to the dev set, because subsequent combination steps are performed on the results of the previous merges steps, which were already optimized on the same data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>As our system is based on various parts and mainly focuses on the ability to smartly combine those, we experiment with how each of the parts work separately. A special focus is given to combining strong components, black-box components and single components as combining is a crucial part of the innovation in this system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Spell checkers' comparison</head><p>We've compared our home-brewed spell-checker with JamSpell 5 , Norvig 6 and ENCHANT 7 . When comparing the results over all error categories, our spell-checker has relatively low results (See <ref type="table" target="#tab_3">Table 2</ref>). However, when comparing the results in spelling (R:SPELL) category alone, our spellchecker excels (See <ref type="table" target="#tab_4">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Nematus</head><p>We trained Nematus using several different data sets. First, we trained using only the W&amp;I train set data, we then added Lang8, FCE and Nucle data sources. Since Lang8 is significantly larger than W&amp;I train set, inspired by , we upsampled W&amp;I 10 times so that it will have more significant effect on the training process. This procedure improved results significantly (See <ref type="table" target="#tab_6">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Synthetic Error Generation</head><p>We also tried training Nematus over synthetic errors data. We generated errors using data from two different domains. Books from project Gutenberg and gold sentences from W&amp;I train set. Additionally, we varied data sizes and observed the effect on the results (See <ref type="table" target="#tab_7">Table 5</ref>). These experiments show that relying on the source domain is crucial and it is best to generate data using text from similar domain. When using the synthetic W&amp;I train set we reached a score that is just a little lower than the score when training over W&amp;I train set directly (0.19 vs 0.23). This might suggest that there is potential in using synthetic data when combined with other data sets and promise for synthetic data methods for unsupervised GEC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Combining</head><p>The experiments regarding combining were performed on the dev set, which was not used for training the systems. The dev set was split to two randomly. The optimal selection of error-types and subsets to combine was done on one half, and we report system results on the second half. For example, when combining the output of the Nematus and Grammarly systems under 10 different fold partitions, the average F 0.5 improvement over the best of the two systems was 6.2 points, with standard deviation of 0.28 points.</p><p>Improvement of a single tool. Even given a single system, we are able to improve the system's performance by eschewing predictions on low performing error types. This filtering procedure has a minor effect and is exemplified in <ref type="table" target="#tab_8">Table 6</ref>. While such findings are known to exist implicitly by the cycles of development <ref type="bibr" target="#b4">(Choshen and Abend, 2018b)</ref>, and were suggested as beneficial for rule based and statistical machine translation systems when precision is 0 , to the best of our knowledge we are the first to report error-type    those results directly, on non trivial precision with neural network based systems. In explicitly filtering corrections by error types we gain two additional benefits over the mere score improvement. First, the weak spots of the system are emphasized, and work might be directed to improving components or combining with a relevant strong system. Second, the system itself is not discouraged or changed to stop producing those corrections. So, if future enhancement would improve this type of errors enough, it will show up in results, without discouraging smaller improvements done on the way.</p><formula xml:id="formula_1">Frequency S 1\2 P 1\2 R 1\2 S 1∩2 P 1∩2 R 1∩2 S 2\1 P 2\1 R 2\1 R:PUNCT</formula><p>Restricted track. In <ref type="table" target="#tab_9">Table 7</ref> we present the results of our shared task restricted track submission. The submission includes four Nematus models, our spellchecker, and Bert based system ( §3.4). This generated a 6 point improvement on       <ref type="table" target="#tab_12">Table 9</ref>). It is also important to note that while average ensemble improves precision, it reduces recall. Combination is balancing precision and recall, improving both, in a way that maximizes F 0.5 . The last observation is far from trivial as most ways to combine systems would emphasize one or the other, e.g., piping would support mainly recall perhaps reducing precision. Lastly, combining is based on the types of errors and is linguistically motivated, and hence could be further improved by smart categorization 8 Although some of the systems use only rules and nonparallel data, we did not include them in our submission to the restricted tracked, as we are not their originators. and perhaps improvements of automatic detection <ref type="bibr" target="#b2">(Bryant et al., 2017</ref>  Combining the shared task systems. After the completion of the competition test phase, several teams agreed to release their outputs on the dev and test set. We combined them using the entire dev set and submitted the results to the open phase of the restricted track for evaluation. This achieves a 3.7 point improvement in F 0.5 and a 6.5 point improvement in precision over the best standalone results (See <ref type="table" target="#tab_2">Table 10</ref>). This means this combination is the best result currently known in the field as assessed by the BEA 2019 shared task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we have shown how combining multiple GEC systems, using a pure black-box approach, can improve state of the art results in the error correction task. Additional variants of this combination approach can be further examined. The approach can work with any disjoint partition systems' corrections. We can consider combining more than 2 systems at the same time, or we can consider more refined subsets of two systems. For example, the set H 1\2 of all the suggested corrections of system1 which were not suggested by system2, can be split to the two sets: H 1overlapping2 and H 1non−overlapping2 , the former containing corrections of system 1 which have an overlapping (but different) corrections by system2, and the later corrections of system1 which have no overlap with any annotation of system2.</p><p>Several other approaches can be taken. The problem can be formulated as multiple-sequence to single sequence problem. The input sequences are the original text and n system corrections. The output sequence is the combined correction. During training, the gold correction is used. Given sufficient labeled data, it may be possible for such a system to learn subtle distinctions which may result in better combinations without relying on separating error types or iterative combinations.</p><p>In addition, we harnessed Bert for GEC and showed a simple spellchecking mechanism yields competitive results to the leading spellcheckers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>E with each of the systems, to receive corrected sentences hypothesis H i .3. Generate M 2 i for each system i by comparing the systems' output H i and the E input.</figDesc><table><row><cell cols="4">4. Split the annotations of the systems into three</cell></row><row><cell cols="4">subsets: H 1\2 -all the suggested annotations</cell></row><row><cell cols="4">of system1 which were not suggested by</cell></row><row><cell cols="4">system2; H 2\1 -all the suggested annota-</cell></row><row><cell cols="4">tions of system2 which were not suggested</cell></row><row><cell cols="4">by system1; and H 1∩2 -all the suggested an-</cell></row><row><cell cols="2">notations in common.</cell><cell></cell><cell></cell></row><row><cell cols="4">5. Generate M 2 files for each of the three sets:</cell></row><row><cell cols="2">M 2 1\2 , M 2 1\2 , M 2 1∩2 .</cell><cell></cell><cell></cell></row><row><cell cols="4">6. Evaluate the performance on each of the</cell></row><row><cell cols="4">three subsets of annotations, split by error</cell></row><row><cell cols="4">type, by comparing M 2 subset with M 2 gold . For</cell></row><row><cell cols="4">each subset and each error type, we obtain</cell></row><row><cell>T P error−type subset</cell><cell>, F P error−type subset</cell><cell>, F N error−type subset</cell><cell>.</cell></row><row><cell cols="3">7. Define selection variables S error−type subset</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. Correct T by every system i, to receive corrected sentences hypothesis H i .2. Generate M 2 i files for each system by comparing the systems' output H i and the T input.3. Split the annotations of the systems into three sets: H 1\2 , H 2\1 , and H 1∩2 .</figDesc><table><row><cell cols="2">4. Generate M 2 files for each of the three sets:</cell></row><row><cell>M 2 1\2 , M 2 2\1 , M 2 1∩2 .</cell><cell></cell></row><row><cell cols="2">5. Remove all annotations from the M 2 files for</cell></row><row><cell>which Sopt error−type subset</cell><cell>= 0.</cell></row><row><cell cols="2">6. Merge all the annotations from the modified</cell></row><row><cell cols="2">M 2 1\2 , M 2 2\1 , and M 2 1∩2 files to create M 2 f inal .</cell></row><row><cell cols="2">If there are overlapping annotations -we cur-</cell></row><row><cell cols="2">rently select an arbitrary annotation.</cell></row><row><cell cols="2">7. Apply all the corrections in M 2 f inal to T and</cell></row><row><cell cols="2">receive the final output.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Combination statistics of the most common error types over two systems -Nematus and Grammarly</figDesc><table><row><cell cols="2">All Categories P</cell><cell>R</cell><cell>F 0.5</cell></row><row><cell>Norvig</cell><cell cols="3">0.5217 0.0355 0.1396</cell></row><row><cell>Enchant</cell><cell cols="3">0.2269 0.0411 0.1192</cell></row><row><cell>Jamspell</cell><cell cols="3">0.4385 0.0449 0.1593</cell></row><row><cell>our</cell><cell cols="3">0.5116 0.0295 0.1198</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Comparison of Grammatical Error Perfor-</cell></row><row><cell cols="4">mance of Spellcheckers. Jamspell achieves the best</cell></row><row><cell cols="3">score as previously suggested.</cell></row><row><cell cols="2">R:SPELL P</cell><cell>R</cell><cell>F 0.5</cell></row><row><cell>Norvig</cell><cell cols="3">0.5775 0.6357 0.5882</cell></row><row><cell>Enchant</cell><cell>0.316</cell><cell cols="2">0.6899 0.3544</cell></row><row><cell>Jamspell</cell><cell cols="3">0.5336 0.6977 0.5599</cell></row><row><cell>our</cell><cell cols="3">0.6721 0.5297 0.6378</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of spellcheckers on spelling. Our method outperforms other methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Nematus performance on W&amp;I dev set by training data. The use of more data improves the system, but only when the training from the domain is upsampled.</figDesc><table><row><cell>Data Source</cell><cell>Size (sentences)</cell><cell>F 0.5</cell></row><row><cell>Gutenberg Books</cell><cell>650,000</cell><cell>0.1483</cell></row><row><cell>Gutenberg Books</cell><cell>7,000,000</cell><cell>0.1294</cell></row><row><cell>W&amp;I train set</cell><cell>1,300,000</cell><cell>0.1919</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Size of synthetic datasets and Nematus scores when trained on them.the dev set of f 0.5 when compared the best standalone Nematus model.</figDesc><table><row><cell cols="4">Off the shelf systems. As can be seen in Ta-</cell></row><row><cell cols="4">ble 8 when we combine the system with several</cell></row><row><cell cols="4">off the self systems, we get 3 point improvement</cell></row><row><cell cols="4">over the restricted baseline, and a 9 point improve-</cell></row><row><cell cols="4">ment over the best standalone system. This im-</cell></row><row><cell>System</cell><cell>P</cell><cell>R</cell><cell>F 0.5</cell></row><row><cell>Language Tool</cell><cell cols="3">0.2905 0.1004 0.2107</cell></row><row><cell cols="4">Filtered Language Tool 0.4005 0.0889 0.2355</cell></row><row><cell>Grammarly</cell><cell cols="3">0.4846 0.1808 0.3627</cell></row><row><cell>Filtered Grammarly</cell><cell cols="3">0.5342 0.1715 0.3754</cell></row><row><cell>Nematus</cell><cell>0.52</cell><cell cols="2">0.1751 0.373</cell></row><row><cell>Filtered Nematus</cell><cell cols="3">0.554 0.1647 0.3761</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Change in performance when avoiding hard errors.</figDesc><table><row><cell>System</cell><cell>P</cell><cell>R</cell><cell>F 0.5</cell></row><row><cell>(1) Nematus1</cell><cell cols="3">0.4788 0.1544 0.3371</cell></row><row><cell>(2) Nematus2</cell><cell cols="3">0.4839 0.1583 0.3429</cell></row><row><cell>(3) Nematus3</cell><cell cols="3">0.4842 0.1489 0.3338</cell></row><row><cell>(4) Nematus4</cell><cell cols="3">0.4843 0.1502 0.3352</cell></row><row><cell cols="4">(5) Spellchecker 0.5154 0.0308 0.1242</cell></row><row><cell>(6) Bert</cell><cell cols="3">0.0132 0.0147 0.0135</cell></row><row><cell>1+2</cell><cell cols="3">0.4972 0.1854 0.3721</cell></row><row><cell>1+2+3</cell><cell cols="3">0.5095 0.1904 0.3816</cell></row><row><cell>1+2+3+4</cell><cell cols="3">0.4926 0.2017 0.3824</cell></row><row><cell>1+2+3+4+5</cell><cell cols="3">0.5039 0.2233 0.4027</cell></row><row><cell>1+2+3+4+5+6</cell><cell cols="3">0.5029 0.2278 0.4051</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="4">: Performance of systems and iterative combi-</cell></row><row><cell cols="4">nation of them. Combination improves both precision</cell></row><row><cell cols="3">and recall even using low performing systems.</cell><cell></cell></row><row><cell cols="4">plies there is a promise in combining existing ap-</cell></row><row><cell cols="4">proaches which we can't improve ourselves to har-</cell></row><row><cell cols="3">ness some of their correction power. 8</cell><cell></cell></row><row><cell>System</cell><cell>P</cell><cell>R</cell><cell>F 0.5</cell></row><row><cell cols="4">(1) Restricted-best 0.5029 0.2278 0.4051</cell></row><row><cell cols="4">(2) Language Tool 0.2699 0.0955 0.1977</cell></row><row><cell>(3) Grammerly</cell><cell cols="3">0.4783 0.1825 0.3612</cell></row><row><cell>(4) Jamspell</cell><cell cols="3">0.423 0.0413 0.1484</cell></row><row><cell>1+2</cell><cell cols="3">0.5274 0.2175 0.4105</cell></row><row><cell>1+2+3</cell><cell cols="3">0.522 0.2656 0.4375</cell></row><row><cell>1+2+3+4</cell><cell cols="3">0.5221 0.2641 0.4367</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Combining with off the shelf systems helps.</figDesc><table><row><cell>Ensemble VS Combining models results. Ne-</cell></row><row><cell>matus has average ensembling built-in which en-</cell></row><row><cell>ables inference over several RNN models by per-</cell></row><row><cell>forming geometric average of the individual mod-</cell></row><row><cell>els' probability distributions. Combining outper-</cell></row><row><cell>forms the built-in ensemble by almost 4 points</cell></row><row><cell>(See</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>).</figDesc><table><row><cell>System</cell><cell>P</cell><cell>R</cell><cell>F 0.5</cell></row><row><cell>(1) Nematus RNN 1</cell><cell cols="3">0.4676 0.1157 0.2908</cell></row><row><cell>(2) Nematus RNN 2</cell><cell cols="3">0.4541 0.1223 0.2944</cell></row><row><cell>(3) Nematus RNN 3</cell><cell cols="3">0.484 0.1191 0.3002</cell></row><row><cell>(4) Nematus RNN 4</cell><cell cols="3">0.4839 0.1184 0.2991</cell></row><row><cell>1+2+3+4 ensemble</cell><cell cols="3">0.5577 0.1131 0.3122</cell></row><row><cell cols="4">1+2+3+4 combination 0.4861 0.166 0.3508</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Combining fares better compared to ensemble.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Test set results when combining systems from the competition used as black boxes. The combination is the new state of the art.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.gutenberg.org 2 https://cgit.freedesktop.org/libreoffice/dictionari</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Non integer value can occur when a 0 value yields high precision and low recall, and a 1 value yields low precision and high recall. In this case, randomly selecting a subset of the corrections will yield a medium recall and medium precision, which maximizes f β</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/bakwc/JamSpell 6 https://github.com/barrust/pyspellchecker 7 https://github.com/AbiWord/enchant</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language model based grammatical error correction without annotated training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-0529</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="247" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The BEA-2019 Shared Task on Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 14th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic annotation and evaluation of error types for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic metric validation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1372" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inherent biases in reference-based evaluation for grammatical error correction and text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Referenceless measure of faithfulness for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03824</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A beamsearch decoder for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner english: The nus corpus of learner english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth workshop on innovative use of NLP for building educational applications</title>
		<meeting>the eighth workshop on innovative use of NLP for building educational applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Helping our own: The hoo 2011 pilot shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Dlugolinskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Krammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Ciglan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Laclavík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Hluchỳ</surname></persName>
		</author>
		<title level="m">Combining named enitity recognition tools. Making Sense of Microposts (# MSM2013)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating artificial errors for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-0529</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="116" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kochmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Assessing bert&apos;s syntactic abilities. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>abs/1901.05287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The computerized learner corpus: a versatile new source of data for sla research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylviane</forename><surname>Granger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Near human-level performance in grammatical error correction with hybrid machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05945</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating and combining name entity recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ridong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">E</forename><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Named Entity Workshop</title>
		<meeting>the Sixth Named Entity Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Approaching neural grammatical error correction as a low-resource machine translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubha</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<pubPlace>Christine Moran</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</title>
		<meeting>the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mining revision log of language learning sns for automated japanese error correction of second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jfleg: A fluency corpus and benchmark for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Algorithm selection and model adaptation for esl correction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grammatical error correction: Machine translation and classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Correcting grammatical verb errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="358" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining open source annotators for entity linking through weighted voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Lexical and Computational Semantics (* SEM 2015)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="211" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadejde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00138</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
