<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Dependency-Based Neural Network for Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-07-16">16 Jul 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
							<email>lisujian@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<email>wanghf@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Dependency-Based Neural Network for Relation Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-07-16">16 Jul 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation classification aims to classify the semantic relations between two entities in a sentence. It plays a vital role in robust knowledge extraction from unstructured texts and serves as an intermediate step in a variety of natural language processing applications. Most existing approaches follow a machine learning based framework and focus on designing effective features to obtain better classification performance.</p><p>The effectiveness of using dependency relations between entities for relation classification has been reported in previous approaches <ref type="bibr" target="#b0">(Bach and Badaskar, 2007)</ref>. For example, <ref type="bibr" target="#b17">Suchanek et al. (2006)</ref> carefully selected a set of features from tokenization and dependency parsing, and extended some of them to * Contribution during internship at Microsoft Research. generate high order features in different ways. <ref type="bibr" target="#b4">Culotta and Sorensen (2004)</ref> designed a dependency tree kernel and attached more information including Part-of-Speech tag, word chunking tag to each node in the tree. Interestingly, <ref type="bibr" target="#b2">Bunescu and Mooney (2005)</ref> provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them.  developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for <ref type="bibr">detailed examples)</ref>. However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path (ADP) which attaches dependency subtrees to words on a shortest dependency path and focus on exploring the semantic representation behind the ADP structure.</p><p>Recently, deep learning techniques have been widely used in modeling complex structures. This provides us an opportunity to model the ADP structure in a neural network framework. Thus, we propose a dependency-based neural network where two sub-neural networks are used to model shortest dependency paths and dependency subtrees respectively. One convolutional neural network (CNN) is applied over the shortest dependency path, because CNN is suitable for capturing the most useful features in a flat structure. A recursive neural network (RNN) is used for extracting semantic representations from the dependency subtrees, since RNN is good at modeling hierarchical structures. To connect these two sub-A thief who tried to steal the truck broke the ignition with screwdriver.  <ref type="figure">Figure 2</ref>: The bold part is the shortest path between two entities in the undirected version of dependency tree, and some subtrees are attached to it. They two are combined as an augmented dependency path.</p><p>networks, each word on the shortest path is combined with a representation generated from its subtree, strengthening the semantic representation of the shortest path. In this way, the augmented dependency path is represented as a continuous semantic vector which can be further used for relation classification. The major contributions of the work presented in this paper are as follows.</p><p>1. We extend the shortest dependency path into the augmented dependency path to better model the relation between two entities.</p><p>2. We propose a dependency-based neural network, DepNN, to model the augmented dependency path. It combines the advantages of the convolutional neural network and the recursive neural network.</p><p>3. We conduct extensive experiments on the Se-mEval 2010 dataset and the experimental results show that DepNN outperforms baseline methods and yields state-of-the-art F1 measure on the relation classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition and Motivation</head><p>The task of relation classification can be defined as follows. Given a sentence S with a pair of entities e 1 and e 2 annotated, the task is to identify the semantic relation between e 1 and e 2 in accordance with a set of predefined relation</p><formula xml:id="formula_0">Relation Type Definition Cause-Effect X is the cause of Y Entity-Origin</formula><p>Y is the origin of an entity X , and X is coming or derived from that origin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message-Topic</head><p>X is a communicative message con-</p><formula xml:id="formula_1">taining information about Y Product-Producer X is a product of Y Entity-Destination Y is the destination of X in the sense of X moving toward Y Member-Collection X is a member of Y Instrument-Agency X is the instrument (tool) of Y or Y uses X Component-Whole</formula><p>X has an operating or usable purpose within Y Content-Container X is or was stored or carried inside Y <ref type="table">Table 1</ref>: Relation types of (X, Y ) and their definitions in SemEval-2010 task 8.</p><p>types. According to the the official guideline of SemEval-2010 task 8 <ref type="bibr" target="#b7">(Hendrickx et al., 2010)</ref>, there are 9 ordered relation types. We list them in <ref type="table">Table 1</ref> with their simplified definitions. Instances don't fall in any of these types are labeled as Other. For example, in <ref type="figure">Figure 2</ref>, the relation between two entities e 1 =thief and e 2 =screwdriver is Instrument-Agency. <ref type="bibr" target="#b2">Bunescu and Mooney (2005)</ref> reported that, for the relation classification task, the shortest dependency path between two entities plays a vital role. They pointed out that this kind of paths can capture the predicate-argument sequences, providing helpful information for relation classification. For example, in <ref type="figure">Figure 2a</ref>, the shortest path includes   <ref type="figure">Figure 3</ref>: Illustration of dependency-based neural networks.</p><p>the structure of "broke prep with screwdriver", helping judging the Instrument-Agency relation.</p><p>Although the shortest dependency paths prove useful for relation classification, there exists other information on the dependency tree that can be exploited to represent the relation more precisely. For example, <ref type="figure">Figure 2a</ref> and 2b show two instances which have similar shortest dependency paths but belong to different relation types. In this situation, if we only use the shortest dependency paths for judging relation types, it is difficult for us to distinguish these two instances. However, we notice that the subtrees attached to the shortest dependency paths such as "dobj→commandment" and "dobj→ignition" can provide supplemental information for relation classification. Based on many observations like this, we propose the idea that we should employ these subtrees and combine them with the shortest path to form a more precise structure for classifying relations. This combined structure is called "augmented dependency path (ADP)", as illustrated in <ref type="figure">Figure 2</ref>.</p><p>Next, our goal is to capture the semantic representation of the ADP structure between two entities. The key problem here is how to combine the two components of ADP to incorporate more information. We propose that on the augmented dependency path, a word should be represented by both itself and its attached subtree. This is because the word itself contains its general meaning while the subtree can provide semantic information about how this word functions in this specific sentence. With this idea, we adopt the re-cursive neural network (RNN) that is proved suitable for modeling hierarchical structures to build semantic embeddings for the words on the shortest path along with their subtrees. After obtaining these more precise word representations, a convolutional neural network (CNN) can be applied, since it is good at modeling flat structures and can generate a fix-sized vector containing the most relevant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dependency-Based Neural Networks</head><p>In this section, we will introduce how we use neural network techniques and dependency information to explore the semantic connection between two entities. We name our architecture of modeling ADP structures as dependency-based neural networks (DepNN). <ref type="figure">Figure 3</ref> illustrates DepNN with a concrete example. First, we associate each word w and dependency relation r with a vector representation x w , x r ∈ R dim . For each word w on the shortest dependency path, we develop an RNN from its leaf words up to the root to generate a subtree embedding c w and concatenate c w with x w to serve as the final representation of w.</p><p>Next, a CNN is designed to model the shortest dependency path based on the representation of its words and relations. Finally our framework can efficiently represent the semantic connection between two entities with consideration of more comprehensive dependency information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Dependency Subtree</head><p>The goal of modeling dependency subtrees is to find an appropriate representation for the words on the shortest path. As mentioned above, we assume that each word w can be interpreted by itself and its children on the dependency subtree. Then, for each word w on the subtree, its word embedding x w ∈ R dim and subtree representation c w ∈ R dimc are concatenated to form its final representation p w ∈ R dim+dimc . For a word that does not have a subtree, we set its subtree representation as c LEAF . The subtree representation of a word is derived through transforming the representations of its children words. During the bottom-up construction of the subtree, each word is associated with a dependency relation such as dobj as in <ref type="figure">Figure 3</ref>.</p><p>Take the ADP in <ref type="figure">Figure 3</ref> for example, we first compute leaves' representations like p the ,</p><formula xml:id="formula_2">p the = [x the , c LEAF ]<label>(1)</label></formula><p>Once all leaves are finished, we move to interior nodes with already processed children. In the example, continuing from "the" to its parent, "Sabbath", we compute</p><formula xml:id="formula_3">p Sabbath = [x Sabbath , c Sabbath ]</formula><p>(2)</p><formula xml:id="formula_4">c Sabbath = f (W det · p the + b)<label>(3)</label></formula><p>where f is a non-linear activation function such as tanh, W det is the transformation matrix associated with dependency relation det and b is a bias term. We repeat this process until we reach the root on the shortest path, which in this case is "broke",</p><formula xml:id="formula_5">p broke = [x broke , c broke ] c broke = f (W prep−on · p Sabbath + W dobj · p commandament )</formula><p>The composition equation for any word w with children Q(w) is,</p><formula xml:id="formula_6">c w = f ( q∈Children(w) W R (w,q) · p q + b) (4) p q = [x q , c q ]<label>(5)</label></formula><p>where R (w,q) denotes the dependency relation between word w and its child word q. This process continues recursively from leaves up to the root words on the shortest path. Each of these words will have a vector representation after this stage (p priests , p broke and p work in this example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Shortest Dependency Path</head><p>To classify the relation between two entities, we further explore the semantic representation behind their shortest dependency path, which can be seen as sequence of words interspersed with dependency relations. Take the shortest dependency path in last subsection for example. The sequence S will be,</p><formula xml:id="formula_7">S: [priests nsubj broke prep-with work] w 1 r 1 w 2 r 2 w 3</formula><p>As the convolutional neural network (CNN) is good at capturing the salient features from a sequence of objects, we design a CNN to tackle the shortest dependency path.</p><p>A CNN contains a convolution operation over windows of object representations, followed by a pooling operation. As we know, a word w on the shortest path is associated with the representation p w through modeling the subtree. For a dependency relation r on the shortest path, we set its representation as a vector x r ∈ R dim . As a sliding window is applied on the sequence, we set the window size as k. For example, when k = 3, the sliding windows of S are {[r s w 1 r 1 ], [r 1 w 2 r 2 ], [r 2 w 3 r e ]} where r s and r e are used to denote the beginning and end of a shortest dependency path between two entities.</p><p>We concatenate k neighboring word (or dependency relation) representations within one window into a new vector. Assume X i ∈ R dim·k+dimc·nw as the concatenated representation of the i-th window, where n w is the number of words in one window. A convolution operation involves a filter W 1 ∈ R l×(dim·k+dimc·nw) , which operates on X i to produce a new feature vector L i with l dimensions,</p><formula xml:id="formula_8">L i = W 1 X i<label>(6)</label></formula><p>where the bias term is ignored for simplicity. Then W 1 is applied to each possible window in the shortest dependency path to produce a feature map: [L 0 , L 1 , L 2 , · · · ]. Next, we adopt the widely-used max-over-time pooling operation <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref>, which can retain the most important features, to obtain the final representation L from the feature map. That is, L = max(L 0 , L 1 , L 2 , . . . ).</p><p>By this means, we are able to obtain the semantic representation of ADP with advantages of both RNN and CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>Like other relation classification systems, we also incorporate some lexical level features which are proved useful for this task. This includes named entity tags and WordNet hypernyms of e 1 and e 2 . We concatenate them with the ADP representation L to produce a combined vector M . We then pass M to a fully connected sof tmax layer whose output is the probability distribution y over relation labels.</p><formula xml:id="formula_9">M = [L, LEX]<label>(7)</label></formula><formula xml:id="formula_10">y = sof tmax(W 2 M )<label>(8)</label></formula><p>We define the ground-truth label vector t for each instance as a binary vector. If the instance belongs to the the i-th type, only t i is 1 and the other dimensions are set to 0. To learn the parameters, we optimize the cross-entropy error between y and t using stochastic gradient descent <ref type="bibr" target="#b1">(Bottou, 2004)</ref>.</p><p>For each training instance, we define the objective function as:</p><formula xml:id="formula_11">min θ (− ln j t j log(y j ))<label>(9)</label></formula><p>where θ represents the parameters. Gradients are computed using backpropagation <ref type="bibr" target="#b14">(Rumelhart et al., 1988)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments are performed on SemEval-2010 dataset <ref type="bibr" target="#b7">(Hendrickx et al., 2010)</ref>. The training part of the dataset includes 8000 instances, and the test part includes 2717 instances. <ref type="table" target="#tab_2">Table 2</ref> shows the statistics of the annotated relation types of this dataset. We can see that the distribution of relation types in the test set is similar to that in the training set. The official evaluation metric is the macro-averaged F1-score (excluding Other). We use dependency trees generated by the Stanford Parser <ref type="bibr" target="#b10">(Klein and Manning, 2003)</ref> with the "collapsed" option, which regards a preposition as a kind of dependency relation. As <ref type="bibr" target="#b5">de Marneffe and Manning (2008)</ref> pointed out, this option is more useful for event relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis of DepNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Contributions of different components</head><p>We first show the contributions from different components of DepNN. In our experiments, two   For evaluation, we first design a relation extraction system (named PATH) which only models the shortest dependency path with a CNN. Based on PATH, We consider to incorporate the two kinds of lexical features including named entity tags (NER) and WordNet hypernyms (WN). Then, we get two systems which are named PATH+WN and PATH+NER respectively. We also add the attached subtrees (SUB) modeled by an RNN to form the complete augmented dependency path. From <ref type="table">Table 4</ref>, we can verify the effectiveness of modeling the shortest dependency path with a CNN, since PATH can achieve a relatively high result. The experiment results also indicate that both the NER and WordNet features can improve the performance of relation extraction. WordNet seems less useful than NER, which conforms to the results of <ref type="bibr" target="#b19">Yu et al. (2014)</ref> , since a large number of WordNet hypernyms may cause overfitting. Furthermore, the attached subtrees, as we expect, can provide an obvious boost to DepNN. The NER tags, WordNet hypernyms and subtrees all contribute to the performance by providing supplemental information for words on the shortest path. The experiments show that the subtree information does a better job than the other two kinds of information and can help build more precise representations for words in a sentence. To get a deeper understanding of what semantic information can be captured behind the ADP structure, we will look into our model and analyze it with specific examples. Since the Gigaword embeddings, with its larger corpus and dimensions, can significantly improve the classification performance, the following experiments and analysis are all based on Gigaword embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Intuitive Analysis of Shortest Path</head><p>We take the output vector of the CNN layer as the distributed representation of a dependency path. In this way, we can calculate the cosine similarity between any two paths and illustrate some paths with high similarity. <ref type="table">Table 5</ref> shows three training instances with different relation types and their three most similar paths in the test set.</p><p>From <ref type="table">Table 5</ref>, we can see that our approach can capture the core meaning of the shortest dependency paths. For example, for the Instrument-Agency relation, we infer that the dependency relations "nsubj inv", "dobj" and "prep with" in the dependency path play a main role in the representation and our model can capture these similar paths. For the Product-Producer relation, our model focuses on representing the structure of "nsubj inv verb1 xcomp verb2 dobj" and exploits some words like "pencil" and "create" in the path representation. This is clearer for the Message-Topic relation, where the similarity of words like "point", "explore", "address" and "relate" are well learned.  <ref type="table">Table 5</ref>: Shortest dependency paths and their closest neighbours in the learned feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Influence of Attached Subtree</head><p>In this subsection, we will discuss the role of attached subtree (SUB) in relation classification. By comparing the results of DepNN before and after adding the subtree, we find the influence of this structure varies from different relation types. <ref type="table">Table 6</ref> shows the F1 measures of each relation type before and after adding the subtree.  <ref type="table">Table 6</ref>: Influence of the subtrees on each relation type.</p><p>We can see that the subtree information generally has a positive impact on all the relation types. It is especially salient for the Instrument-Agency and Product-Producer relations. With only using the shortest dependency paths, these two kinds of relation types are easily confused, as they both rely on the dependency paths such as ". . . verb prep-by/prep-with/using . . . ". But after considering the subtree information, we can better distinguish these two relation types. <ref type="figure">Figure 4</ref> lists two instances that can be classified correctly only af-    <ref type="figure">Figure 4</ref>: ADP of instances that can be classified correctly after adding the subtrees.</p><p>ter adding the subtrees. <ref type="figure">Figure 4a</ref> belongs to the Producer-Produce relation which can be reflected by the subtree structures like "conj-and→valves" and "amod→manufacturing". <ref type="figure">Figure 4b</ref> belongs to the Instrument-Agency relation, and the subtree structure attached to the word "scaled" provides more supplemental information to the shortest path as explained above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Baselines</head><p>In this subsection, we compare DepNN with several baseline approaches of relation classification. SVM <ref type="bibr" target="#b13">(Rink and Harabagiu, 2010)</ref>: This is the top performed system in SemEval-2010. It depends on the human compiled feature templates and then utilizes many external corpora to extract features for an SVM classifier.</p><p>MV-RNN <ref type="bibr" target="#b15">(Socher et al., 2012)</ref>: This model associates each word with a matrix. Based on the constituent parse tree structure, this model finds the path between two entities and learns the distributed representation of their highest parent node through the composition in a recursive neural network.</p><p>DT-RNN  : This model uses an RNN for modeling dependency trees. It assigns a composition matrix to each dependency relation. Different from our model DepNN, the embedding of each node is a linear combination of its children. The network is trained using the method provided by <ref type="bibr" target="#b8">(Iyyer et al., 2014)</ref>. We average the learned vectors of all nodes, stack it with the root node's embedding and additional features, and feed them into a sof tmax classifier.</p><p>CNN: <ref type="bibr" target="#b21">Zeng et al. (2014)</ref> build a convolutional model to learn a sentence representation over the words in a sentence. To represent each word, they use a special position vector to indicate the relative distances of current input word to two marked entities, concatenating the position vector with the corresponding word embedding. Then the sentence representation is staked with some lexical features and fed into a sof tmax classifier.</p><p>FCM <ref type="bibr" target="#b19">(Yu et al., 2014)</ref>: FCM decomposes a sentence into some substructures and learns substructure embedding from each of them. Then the substructure embeddings in a sentence are combined via a sum-pooling operation and put into a sof tmax classifier. <ref type="table" target="#tab_8">Table 7</ref> compares DepNN with the baseline approaches. Since many of our baselines are neural network models, it is convenient for them to use some features extracted with external resources or tools to enhance performance. We call these features "additional features" (AF) and list them in the second column. The F1-measures on SemEval-2010 dataset with/out these additional features are shown in the last two columns.</p><p>From <ref type="table" target="#tab_8">Table 7</ref>, we can see that DepNN achieves the best result (83.6) with the NER features. SVM achieves a comparable result, though the quality of feature engineering highly relies on human experience and external NLP resources. MV-RNN models the constituent parse trees with a recursive procedure and its F1-measures with/out AF are about 1.7 percent and 4.6 percent lower than those of DepNN. This to some extent indicates that our proposed ADP structure is more suitable for relation classification task. Meanwhile, MV-RNN is very slow to train, since each word is associated with a matrix. Both CNN and FCM use features from the whole sentence and achieve similar performance. DT-RNN is the worst of all baselines, though it also considers the information from shortest dependency paths and attached subtrees. As we analyze, shortest dependency paths and subtrees play different roles in relation classification. But, we can see that DT-RNN does not distinguish the modeling processes of shortest paths and subtrees, and deems the representation of each node as a linear combination of its children.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Relation classification is one traditional subproblem of Information Extraction (IE). It aims to detect and classify relations between the predefined types of objects in the corpus. These objects could be named entities or marked nominals 3 . Much research has been performed in this field, most of which considers it as a supervised multi-classification task. Depending on the input to the classifier, these approaches can be further divided into feature-based, tree kernel-based and composite kernel-based.</p><p>Feature-based methods extract various kinds of linguistic features, including both syntactic features and semantic cues. These features are combined to form a feature vector employed in a Max Entropy <ref type="bibr">(Kambhatla, )</ref> or an SVM  classifier. Feature-based methods usually need handcrafted features and lack the ability to represent structural information (e.g., parsing tree, word order).</p><p>Kernel methods use a more natural way of ex-ploring structural features by computing the inner product of two objects in the high-dimensional latent feature space. <ref type="bibr" target="#b20">Zelenko et al. (2003)</ref> designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees. <ref type="bibr" target="#b4">Culotta and Sorensen (2004)</ref> transferred this kernel to a dependency tree and attached more information including POS tag, word chunk tag to each node. <ref type="bibr">Zhou et al. (2007)</ref> proposed a contextsensitive convolution tree kernel that used context information beyond the local tree. In another view, Bunescu and Mooney <ref type="formula" target="#formula_6">(2005)</ref> provided an important insight that the shortest path between the two entities concentrates most of the information for identifying the relation between them.  used the dependency subtrees in a different manner by modeling the subtrees between entities and keywords of certain relations. <ref type="bibr" target="#b22">Zhang et al. (2006)</ref> further proposed composite kernels to combine a tree kernel and a feature-based kernel to promote the performance. Recently, Deep Neural Networks (DNN) have been developed to solve the relation classification problem. By associating each word a distributed representation, DNN can overcome the sparsity problem in traditional methods and automatically learn appropriate features. <ref type="bibr" target="#b15">Socher et al. (2012)</ref> proposed a recursive neural network model by constructing compositional semantics for the minimal constituent of a constituent parse tree including both marked entities. <ref type="bibr" target="#b21">Zeng et al. (2014)</ref> used a convolutional neural network over the whole sentence combined with some lexical features. They also pointed out that the position of each word in the sentence is very important for relation classification and concatenated a special position feature vector with the corresponding word embedding. <ref type="bibr" target="#b19">Yu et al. (2014)</ref> proposed the Factor-based Compositional Embedding Model which extracted features from the substructures of a sentence and combined them through a sum-pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose to classify relations between entities by modeling the augmented dependency path in a neural network framework. For a given instance, we generate its ADP by combining the shortest path between two entities and the attached subtrees. We present a novel approach, DepNN, to taking advantages of both convolu-tional neural network and recursive neural network to model this structure. Experiment results demonstrate that DepNN achieves state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the priests broke the commandment with priestly work. Augmented dependency path for S2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>priests</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of SemEval-2010 dataset.</figDesc><table><row><cell cols="5">kinds of word embeddings are used for initializa-</cell></row><row><cell cols="5">tion. One is the 50-d embeddings provided by</cell></row><row><cell cols="5">SENNA (Collobert et al., 2011). The second is the</cell></row><row><cell cols="5">200-d embeddings (Yu et al., 2014) trained on Gi-</cell></row><row><cell cols="5">gaword with word2vec 1 . The corresponding hy-</cell></row><row><cell cols="5">perparameters are set with 5-fold cross validation,</cell></row><row><cell cols="5">including window size k, learning rate λ, subtree</cell></row><row><cell cols="5">embedding's dimension dim c , and hidden layer</cell></row><row><cell cols="5">size l. The final settings are shown in Table 3.</cell></row><row><cell></cell><cell>k</cell><cell>λ</cell><cell>dim c</cell><cell>l</cell></row><row><cell>50-d</cell><cell>5</cell><cell>0.05</cell><cell>25</cell><cell>200</cell></row><row><cell>200-d</cell><cell>5</cell><cell>0.05</cell><cell>100</cell><cell>400</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Instrument-Agency master nsubj inv teaches dobj lesson prep with stick analyzer prep of inv core nsubj inv identifies dobj paths vmod using dobj method architect nn inv measures dep Sage prep with strip shop nsubj inv fixed prep with method</figDesc><table><row><cell>Product-Producer</cell></row><row><cell>factory nsubj inv began xcomp manufacture dobj ban-</cell></row><row><cell>duras</cell></row><row><cell>designer nsubj inv made dobj sets</cell></row><row><cell>writer rcmod pencilled dobj storyboard</cell></row><row><cell>student nsubj inv spent xcomp creating dobj application</cell></row><row><cell>Message-Topic</cell></row><row><cell>article prep-in-inv explores dobj impulsivity</cell></row><row><cell>article rcmod pointed dobj problems</cell></row><row><cell>speech vmod addressing dobjpractices</cell></row><row><cell>chapter nsubj inv relates dobj attempts</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Results of evaluation on the SemEval-2010 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://code.google.com/p/word2vec/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">MV-RNN achieves a higher F1-score (82.7) on SENNA embeddings reported in the original paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">ACE Evaluation uses the named entities while the Se-mEval evaluation is based on nominals.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey on relation extraction. Language Technologies Institute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Badaskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced lectures on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="146" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Shortest Path Dependency Kernel for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Bunescu and Mooney2005</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dependency Tree Kernels for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
	<note>Culotta and Sorensen2004</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2008] Marie-Catherine</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Guodong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hendrickx</surname></persName>
		</author>
		<title level="m">SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
	<note>Iyyer et al.2014</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Extracting Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate Unlexicalized Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2003] Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relation extraction from wikipedia using subtree mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1414</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma;</forename><surname>Cambridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>London</surname></persName>
		</author>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
	<note>Rink and Harabagiu2010</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning representations by back-propagating errors. Cognitive modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Rumelhart et al.1988</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Socher et al.2012</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining linguistic and statistical analysis to extract relations from web documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Ifrim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="712" to="717" />
		</imprint>
	</monogr>
	<note>Suchanek et al.2006</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A re-examination of dependency path kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="841" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernel Methods for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zelenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
		<respStmt>
			<orgName>August. Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring Various Knowledge in Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Zhou et al.2007] GuoDong Zhou, Min Zhang</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tree kernel-based relation extraction with contextsensitive structured parse tree information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP-CoNLL</title>
		<imprint>
			<biblScope unit="page">728</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
