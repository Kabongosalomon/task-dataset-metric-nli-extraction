<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rietzler</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stabinger</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Opitz</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Engl</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Innsbruck</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect-Target</head><p>Sentiment Classification (ATSC) is a subtask of Aspect-Based Sentiment Analysis (ABSA), which has many applications e.g. in e-commerce, where data and insights from reviews can be leveraged to create value for businesses and customers. Recently, deep transfer-learning methods have been applied successfully to a myriad of Natural Language Processing (NLP) tasks, including ATSC. Building on top of the prominent BERT language model, we approach ATSC using a two-step procedure: selfsupervised domain-specific BERT language model finetuning, followed by supervised task-specific finetuning. Our findings on how to best exploit domain-specific language model finetuning enable us to produce new state-of-the-art performance on the SemEval 2014 Task 4 restaurants dataset. In addition, to explore the real-world robustness of our models, we perform cross-domain evaluation. We show that a cross-domain adapted BERT language model performs significantly better than strong baseline models like vanilla BERT-base and XLNet-base. Finally, we conduct a case study to interpret model prediction errors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment Analysis (SA) is an active field of research in Natural Language Processing and deals with opinions in text. A typical application of classical SA in an industrial setting would be to classify a document like a product review into positive, negative or neutral sentiment polarity.</p><p>In constrast to SA, the more fine-grained task of Aspect Based Sentiment Analysis (ABSA) <ref type="bibr" target="#b6">(Hu and Liu, 2004;</ref><ref type="bibr" target="#b11">Pontiki et al., 2015)</ref> aims to find both the aspect of an entity like a restaurant, and the sentiment associated with this aspect.</p><p>It is important to note that ABSA comes in two variants. We will use the sentence "I love their dumplings" to explain these variants in detail. Both variants are implemented as a twostep procedure.</p><p>The first variant is comprised of Aspect-Category Detection (ACD) followed by Aspect-Category Sentiment Classification (ACSC). ACD is a multilabel classification task, where a sentence can be associated with a set of predefined aspect categories like "food" and "service" in the restaurants domain. In the second step, ACSC, the sentiment polarity associated to the aspect-category is classified. For our examplesentence the correct result is the tuple ("food", "positive").</p><p>The second variant consists of Aspect-Target Extraction (ATE) followed by Aspect-Target Sentiment Classification (ATSC). ATE is a sequence labeling task, where terms like "dumplings" are detected. In the second step, ATSC, the sentiment polarity associated to the aspect-target is determined. In our example the correct result is ("dumplings", "positive").</p><p>In this paper, we focus on ATSC. In recent years, specialized neural architectures <ref type="bibr">(Tang et al., 2016a,b)</ref> have been developed that substantially improved modeling of this target-context relationship. More recently, the Natural Language Processing community experienced a substantial shift towards using pre-trained language models <ref type="bibr" target="#b10">(Peters et al., 2018;</ref><ref type="bibr" target="#b12">Radford and Salimans, 2018;</ref><ref type="bibr" target="#b5">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b0">Devlin et al., 2019)</ref> as a base for many down-stream tasks, including ABSA <ref type="bibr" target="#b14">(Song et al., 2019;</ref><ref type="bibr" target="#b19">Xu et al., 2019;</ref><ref type="bibr" target="#b15">Sun et al., 2019)</ref>. We still see huge potential that comes with this trend, which is why we approach the ATSC task using the BERT architecture.</p><p>As shown by <ref type="bibr" target="#b19">Xu et al. (2019)</ref>, for the ATSC task the performance of models that were pretrained on general text corpora is improved substantially by finetuning the language model on domain-specific corpora -in their case review corpora -that have not been used for pre-training BERT, or other language models.</p><p>We extend the work by <ref type="bibr">Xu et al. by</ref> further investigating the behavior of finetuning the BERT language model in relation to ATSC performance. In particular, our contributions are:</p><p>1. Analysis of the influence of the amount of training-steps used for BERT language model finetuning on the Aspect-Target Sentiment Classification performance.</p><p>2. Findings on how exploiting BERT language model finetuning enables us to achieve new state-of-the-art performance on the SemEval 2014 restaurants dataset.</p><p>3. Analysis of cross-domain adaptation between the laptops and restaurants domains. Adaptation is tested by self-supervised finetuning of the BERT language model on the targetdomain and then supervised training on the ATSC task in the source-domain. In addition, the performance of training on the combination of both datasets is measured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>We separate our discussion of related work into two areas: first, neural methods applied to ATSC that have improved performance solely by model architecture improvements. Secondly, methods that additionally aim to transfer knowledge from semantically related tasks or domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture Improvements for Aspect-Target Sentiment Classification</head><p>The datasets typically used for Aspect-Target Sentiment Classification are the SemEval 2014 Task 4 datasets <ref type="bibr" target="#b11">(Pontiki et al., 2015)</ref> for the restaurants and laptops domain. Both datasets have only a small number of training examples. One common approach to compensate for insufficient training examples is to invent neural architectures that better model ATSC. For example, in the past a big leap in classification performance was achieved with the use of the Memory Network architecture <ref type="bibr" target="#b17">(Tang et al., 2016b)</ref>, which uses memory to remember context words and explicitly models attention over both the target word and context. It was found that making full use of context words improves their model compared to previous models <ref type="bibr" target="#b16">(Tang et al., 2016a</ref>) that make use of left-and right-sided context independently. <ref type="bibr" target="#b14">Song et al. (2019)</ref> proposed Attention Encoder Networks (AEN), a modification to the transformer architecture. The authors split the Multi-Head Attention (MHA) layers into Intra-MHA and Inter-MHA layers in order to model target words and context differently, which results in a more lightweight model compared to the transformer architecture.</p><p>Another recent performance leap was achieved by <ref type="bibr" target="#b21">Zhaoa et al. (2019)</ref>, who model dependencies between sentiment words explicitly in sentences with more than one aspect-target by using a graph convolutional neural network. They show that their architecture performs particularly well if multiple aspects are present in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Transfer for Aspect-Target Sentiment Classification Analysis</head><p>One approach to compensate for insufficient training examples is to transfer knowledge across domains or across similar tasks. <ref type="bibr" target="#b9">Li et al. (2019)</ref> proposed Multi-Granularity Alignment Networks (MGAN). They use this architecture to transfer knowledge from both an aspect-category classification task and also across different domains. They built a large scale aspectcategory dataset specifically for this. <ref type="bibr" target="#b2">He et al. (2018)</ref> transfer knowledge from a document-level sentiment classification task trained on the Amazon review dataset <ref type="bibr" target="#b3">(He and McAuley, 2016)</ref>. They successfully apply pretraining by reusing the weights of a Long Short Term Memory (LSTM) network <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997</ref>) that has been trained on the document-level sentiment task. In addition, they apply multi-task learning where aspect and document-level tasks are learned simultaneously by minimizing a joint loss function.</p><p>Similarly, <ref type="bibr" target="#b19">Xu et al. (2019)</ref> introduce a multitask loss function to simultaneously optimize on BERT model's <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref> pre-training objectives as well as a question answering task.</p><p>In contrast to the methods described above that aim to transfer knowledge from a different source task -like question answering or document-level sentiment classification -this paper aims at transferring knowledge across different domains by self-supervised finetuning of the BERT language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We approach the Aspect-Target Sentiment Classification task using a two-step procedure. We use the pre-trained BERT architecture as a basis. In the first step we finetune the pre-trained weights of the language model further in a self-supervised way on a domain-specific corpus. In the second step we train the finetuned language model in a supervised way on the ATSC end-task.</p><p>In the following subsections, we discuss the BERT architecture, how we finetune the language model, and how we transform the ATSC task into a BERT sequence-pair classification task <ref type="bibr" target="#b15">(Sun et al., 2019)</ref>. Subsequently, we discuss the different endtask training and domain-specific finetuning combinations we employ to evaluate our model's generalization performance not only in-domain but also cross-domain.</p><p>Finally, we describe how we apply input reduction, an interpretation method for neural NLP models, to the ATSC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BERT</head><p>The BERT model builds on many previous innovations: contextualized word representations <ref type="bibr" target="#b10">(Peters et al., 2018)</ref>, the transformer architecture <ref type="bibr" target="#b18">(Vaswani et al., 2017)</ref>, and pre-training on a language modeling task with subsequent end-to-end finetuning on a downstream task <ref type="bibr" target="#b12">(Radford and Salimans, 2018;</ref><ref type="bibr" target="#b5">Howard and Ruder, 2018)</ref>. Due to being deeply bidirectional, the BERT architecture creates powerful sequence representations that perform extremely well on many downstream tasks <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref>.</p><p>The main innovation of BERT is that instead of using the objective of next-word prediction, a different objective is used to train the language model. This objective consists of two parts.</p><p>The first part is the masked language model objective, where the model learns to predict randomly masked tokens from their context.</p><p>The second part is the next-sequence prediction objective, where the model needs to predict if a sequence B would naturally follow the previous sequence A. This objective enables the model to capture long-term dependencies better. Both objectives are discussed in more detail in the next section.</p><p>As a base for our experiments we use the BERT BASE model, which has been pre-trained by the Google research team. It has the following parameters: 12 layers, 768 hidden dimensions per token and 12 attention heads. It has 110 million parameters in total. For finetuning the BERT language model on a specific domain we use the weights of BERT BASE as a starting point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BERT Language Model Finetuning</head><p>As the first step of our procedure we perform language model finetuning of the BERT model using domain-specific corpora. Algorithmically, this is equivalent to pre-training. The domain-specific language model finetuning as an intermediate step to ATSC has been described by <ref type="bibr" target="#b19">Xu et al. (2019)</ref>.</p><p>As an extension to their paper we investigate the limits of language model finetuning in terms of how end-task performance is dependent on the amount of training steps.</p><p>The training input representation for language model finetuning consists of two sequences s A and</p><formula xml:id="formula_0">s B in the format of "[CLS] s A [SEP] s B [SEP]", where [CLS]</formula><p>is a dummy token used for downstream classification and [SEP] are separator tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Language Model Objective</head><p>The sequences A and B have tokens randomly masked out in order for the model to learn to predict them. The following example shows how domain-specific finetuning could alleviate the bias from pre-training on a Wikipedia corpus: "The touchscreen is an [MASK] device". In the factbased context of Wikipedia the [MASK] could be "input" and in the review domain a typical guess could be the general opinion word "amazing".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Next-Sentence Prediction</head><p>In order to train BERT to capture long-term dependencies better, the model is trained to predict whether sequence B follows sequence A. If this is the case, sequence A and sequence B are jointly sampled from the same document in the order they appear naturally. Otherwise the sequences are sampled randomly from the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aspect-Target Sentiment Classification</head><p>The ATSC task aims at classifying sentiment polarity into the three classes positive, negative, neutral with respect to an aspect-target. The input to the classifier is a tokenized sentence s = s 1:n and a target t = s j:j+m contained in the sentence, where j &lt; j + m ≤ n. Similar to previous work by <ref type="bibr" target="#b15">Sun et al. (2019)</ref>, we transform the input into a format compatible with BERT sequence-pair classification tasks:</p><formula xml:id="formula_1">"[CLS] s [SEP] t [SEP]".</formula><p>In the BERT architecture the position of the token embeddings is structurally maintained after each Multi-Head Attention layer. Therefore, we refer to the last hidden representation of the [CLS] token as h <ref type="bibr">[CLS]</ref> ∈ R 768×1 . The number of sentiment polarity classes is three. A distribution p ∈ [0, 1] 3 over these classes is predicted using a fully-connected layer with 3 output neurons on top of h <ref type="bibr">[CLS]</ref> , followed by a softmax activation func-</p><formula xml:id="formula_2">tion p = softmax(W · h [CLS] + b), where b ∈ R 3 and W ∈ R 3×768 .</formula><p>Cross-entropy is used as the training loss. The way we use BERT for classifying the sentiment polaritites is equivalent to how BERT is used for sequence-pair classification tasks in the original paper <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Domain Adaptation through Language Model Finetuning</head><p>In academia, it is common that the performance of a machine learning model is evaluated in-domain.</p><p>This means that the model is evaluated on a test set that comes from the same distribution as the training set. In real-world applications this setting is not always valid, as the trained model is used to predict previously unseen data.</p><p>In order to evaluate the performance of a machine learning model more robustly, its generalization error can be evaluated across different domains, i.e. cross-domain. To optimize crossdomain performance, the model itself can be adapted towards a target domain. This procedure is known as Domain Adaptation, which is a special case of Transductive Transfer Learning in the taxonomy of <ref type="bibr" target="#b13">Ruder (2019)</ref>. Here, it is typically assumed that supervised data for a specific task is only available for a source domain S, whereas only unsupervised data is available in the target domain T . The goal is to optimize performance of the task in the target domain while transferring task-specific knowledge from the source domain.</p><p>If we map this framework to our challenge, we define Aspect-Target Sentiment Classification as the transfer-task and BERT language model finetuning is used for domain adaptation. In terms of which domain is finetuned on, the full transferprocedure can be expressed in the following way:</p><formula xml:id="formula_3">D LM → D T rain → D T est .</formula><p>Here, D LM stands for the domain on which the language model is finetuned and can take on the values of Restaurants, Laptops or (Restaurants ∪ Laptops). The domain for training D T rain can take on the same values; for the joint case the training datasets for laptops and restaurants are simply combined. The domain for testing D T est can only take the value Restaurants or Laptops.</p><p>Combining finetuning and training steps gives us nine different evaluation scenarios, which we group into the following four categories:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-Domain Training</head><p>ATSC is trained on a domain-specific dataset and evaluated on the test set from the same domain. This can be expressed as D LM → T → T, where T is our target domain and can be either Laptops or Restaurants. It is expected that the performance of the model is high-</p><formula xml:id="formula_4">est if D LM = T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Domain Training</head><p>ATSC is trained on a domain-specific dataset and evaluated on the test set from the other domain. This can be expressed as D LM → S → T, where S = T are source and target domain and can be either Laptops or Restaurants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Domain Adaptation</head><p>As a special case of cross-domain training we expect performance to be optimal if D LM = T . This is the variant of Domain Adaptation and is written as</p><formula xml:id="formula_5">T → S → T.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint-Domain Training</head><p>ATSC is trained on both domain-specific datasets jointly and evaluated on both test sets independently. This can be expressed as D LM → (S ∪ T ) → T, where S = T are sourceand target domain and can either be Laptops or Restaurants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Input Reduction for Model Interpretation</head><p>Input reduction is an interpretation method for neural models introduced by Feng et al. <ref type="formula">(2018)</ref>, which tries to find a subset of the most important words of a document that contribute most to a prediction.</p><p>We use this interpretation method to illustrate the predictions of our models on the test set in order to find causes for classification errors, and also to find qualitative differences between our models and baseline models.</p><p>The input reduction method resembles a process that iteratively removes unimportant words from the input while the model's prediction is maintained. The idea is that the remaining set of words one iteration before the prediction flips are the most important ones. As pointed out by <ref type="bibr" target="#b1">Feng et al. (2018)</ref> for this method to work, a machine learning model needs to compute meaningful confidence values for unseen input. For our task, we find empirically that the predicted probabilities computed for our test set examples work well enough as a confidence approximation, which means that most of the reduced input for the examples discussed in subsection 4.5 allows for a meaningful interpretation.</p><p>Let</p><formula xml:id="formula_6">x = [x 1 , x 2 , .</formula><p>. . x n ] be the input sentence represented as a list of tokens and p(y|x) the predicted probability of label y, and y = argmaxŷ p(ŷ|x) the originally predicted label. The importance of a word is defined as</p><formula xml:id="formula_7">g(x i ) = p(y|x) − p(y|x −i ).</formula><p>Put differently, the importance of a word is the prediction probability towards the original label of a sentence containing the word minus the prediction probability of the sentence without the same word.</p><p>We apply this formula to iteratively remove the word with the lowest importance until the prediction changes to another label. Due to the nature of the ATSC task, we make an exception for words that are part of the aspect-target phrase, which we do not remove during an iteration. This allows us to maintain the context with respect to the aspecttarget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In our experiments we aim to answer the following research questions (RQs):</p><p>RQ1: How does the number of training iterations in the BERT language model finetuning stage influence the ATSC end-task performance?</p><p>At what point does performance start to improve, when does it converge?</p><p>RQ2: If trained in-domain, what ATSC endtask performance can be reached through fully exploited finetuning of the BERT language model? RQ3: If trained cross-domain in the special case of domain adaptation, what ATSC end-task performance can be reached if BERT language model finetuning is fully exploited?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets for Classification and Language Model Finetuning</head><p>We conduct experiments using the two SemEval 2014 Task 4 Subtask 2 datasets 1 <ref type="bibr" target="#b11">(Pontiki et al., 2015)</ref> for the laptops and the restaurants domain. The two datasets contain sentences with one or multiple marked aspect-targets that each have a 3level sentiment polarity (positive, neutral or negative) associated. In the original dataset the conflict class is also present. Here, the conflict labels are dropped for reasons of comparability with <ref type="bibr" target="#b19">Xu et al. (2019)</ref>. Detailed statistics for both datasets are shown in <ref type="table" target="#tab_1">Table 1</ref>. For BERT language model finetuning we prepare three corpora for the two domains of laptops and restaurants. For the restaurants domain we use Yelp Dataset Challenge reviews 2 and for the laptops domain we use Amazon Laptop reviews <ref type="bibr" target="#b3">(He and McAuley, 2016)</ref>. For the laptop domain we filtered out reviews that appear in the SemEval 2014 laptops dataset to avoid training bias for the test data. To be compatible with the next-sentence prediction task used during fine tuning, we removed reviews containing fewer than two sentences from the corpora.</p><p>For the laptop corpus, 1, 007, 209 sentences are left after pre-processing. For the restaurants domain, where more reviews are available, we sampled 10, 000, 000 sentences to have a sufficient amount of data for fully exploited language model finetuning. In order to compensate for the smaller amount of finetuning data in the laptops domain, we finetune for more epochs, 30 epochs in the case of the laptops domain compared to 3 epochs for the restaurants domain, so that the BERT model trains on about 30 million sentences in both cases. This means that a single sentence can appear multiple times with a different language model masking.</p><p>We also create a mixed corpus to jointly finetune on both domains. Here, we sample 1 million restaurant reviews and combine them with the laptop reviews. This results in about 2 million reviews that are finetuned for 15 epochs. The exact statistics for the three finetuning corpora are shown in the top of <ref type="table" target="#tab_1">Table 1</ref>.</p><p>We release code to reproduce generation of our finetuning corpora 3 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters</head><p>We use BERT BASE 4 (uncased) as the base for all of our experiments, with the exception of XLNet BASE (cased), which is used as one of the baseline models.</p><p>For the BERT language model finetuning we use 32 bit floating point computations using the Adam optimizer (Kingma and Ba, 2014). The batchsize is set to 32 while the learning rate is set to 3 · 10 −5 . The maximum input sequence length is set to 256 tokens, which amounts to about 4 sentences per sequence on average. As shown in Table 1, we finetune the language models on each domain so that the model trains a total of about 30 million sentences (≈ 7.5 million sequences).</p><p>For training the BERT and XLNet models on the down-stream task of ATSC we use mixed 16 bit and 32 bit floating point computations, the Adam optimizer, and a learning rate of 3 · 10 −5 and a batchsize of 32. We train the model for a total of 7 epochs. The validation accuracy converges after about 3 epochs of training on all datasets, but training loss still improves after that.</p><p>It is important to note that all our results reported are the average of 9 runs with different random initializations. This is needed to measure significance of improvements, as the standard deviation in accuray amounts to roughly 1% for all experiments (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared Methods</head><p>We compare in-domain results to current stateof-the-art methods, which we will now describe briefly. SDGCN-BERT <ref type="bibr" target="#b21">(Zhaoa et al., 2019)</ref> explicitly models sentiment dependencies for sentences with multiple aspects with a graph convolutional network. This method is current state-of-the-art on the SemEval 2014 laptops dataset. AEN-BERT <ref type="bibr" target="#b14">(Song et al., 2019)</ref> is an attentional encoder network. When used on top of BERT embeddings this method performs especially well on the laptops dataset. BERT-SPC <ref type="bibr" target="#b14">(Song et al., 2019)</ref> is BERT used in sentence-pair classification mode. This is exactly the same method as our BERT-base baseline and therefore, we can cross-check the authors' results. BERT-PT <ref type="bibr" target="#b19">(Xu et al., 2019)</ref> uses multi-task finetuning prior to downstream classification, where the BERT language model is finetuned jointly with a question answering task. It has state-of-the-art performance on the restaurants dataset prior to this paper.</p><p>To our knowledge, cross-and joint-domain training on the SemEval 2014 Task 4 datasets has not been analyzed so far. Thus, we compare our method to two very strong baseline models: BERT-base and XLNet-base. BERT-base <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref> is using the pretrained BERT BASE embeddings directly on the down-stream task without any domain specific language model finetuning. XLNet-base  is a method also based on general language model pre-training similar to BERT. Instead of randomly masking tokens for pre-training like BERT, a more general permutation objective is used, where all possible variants of masking are fully exploited.</p><p>Our models are BERT models whose language model has been finetuned on different domain corpora. BERT-ADA Lapt is the BERT language model finetuned on the laptop domain corpus.</p><p>BERT-ADA Rest is the BERT language model finetuned on the restaurant domain corpus.</p><p>BERT-ADA Joint is the BERT language model finetuned on the corpus containing an equal amount of laptops and restaurants reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results Analysis</head><p>The results of our experiments are shown in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="table" target="#tab_3">Table 2</ref> respectively. To answer RQ1, which is concerned with details of domain-specific language model finetuning, we can see in <ref type="figure" target="#fig_0">Figure 1</ref> that first of all, language model finetuning has a significant effect on ATSC endtask performance. Secondly, we see that in the restaurants domain the performance starts to increase immediately, whereas in the laptops domain it takes about 10 million finetuned sentences before a significant increase can be measured. After around 17 million sentences no significant improvement can be measured. In addition, we find that the different runs have a high variance, which necessitates averaging over 9 runs to measure differences in model performance reliably.</p><p>To answer RQ2, which is concerned with indomain ATSC performance, we see in <ref type="table" target="#tab_3">Table 2</ref> that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset and new state-of-the-art on the restaurants dataset with accuracies of 79.19% and 87.14%, respectively. On the restaurants dataset, this cor-responds to an absolute improvement of 2.2% compared to the previous state-of-the-art method BERT-PT. Language model finetuning produces a larger improvement on the restaurants dataset. We think that one reason for that might be that the restaurants domain is underrepresented in the pre-training corpora of BERT BASE . Generally, we find that language model finetuning helps even if the finetuning domain does not match the evaluation domain. We think the reason for this might be that the BERT-base model is pre-trained more on knowledge-based corpora like Wikipedia than on text containing opinions. We show some evidence for this hypothesis in subsection 4.5. In addition, we find that the XLNet-base baseline performs generally stronger than BERT-base, but only outperforms the BERT-ADA models on the laptops dataset with an accuracy of 79.89% .</p><p>To answer RQ3, which is concerned with domain adaptation, we can see from the grayed out cells in <ref type="table" target="#tab_3">Table 2</ref>, which correspond to the crossdomain adaption case where the BERT language model is trained on the target domain, that domain adaptation works well with 2.2% absolute accuracy improvement on the laptops test set and even 3.6% accuracy improvement on the restaurants test set compared to BERT-base.</p><p>In general, the ATSC task generalizes well cross-domain, with about a 2-3% drop in accuracy compared to in-domain training. We think the reason for this might be that syntactical relationships between the aspect-target and the phrase expressing sentiment polarity, as well as knowing the sentiment-polarity itself, are sufficient to solve the ATSC task in most cases.</p><p>For the joint-training case, we find that combining both training datasets improves performance on both test sets. This result is intuitive, as more training data generally leads to better performance if the domains do not confuse each other. Interestingly, for the joint-training case the BERT-ADA Joint model performs especially well when measured by the Macro-F1 metric. A reason for this might be that the SemEval 2014 datasets are imbalanced due to dominance of positive labels. It seems like through finetuning the language model on both domains the model learns to classify the neutral class much better, especially in the laptops domain.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>The goal of the case study is to find answers to the following questions:</p><p>• What specifically causes the finetuned language models BERT-ADA Lapt and BERT-ADA Rest to perform better than BERT-base?</p><p>• What reasons can we find by comparing conflicting predictions made by these models?</p><p>• What are specific reasons for erroneous classifications?</p><p>• What error types prevent us from performing at human expert level on ATSC?</p><p>To answer these questions we performed input reduction, which allows for a better interpretation of sample predictions from the SemEval 2014 Restaurant and Laptops test set, see <ref type="table" target="#tab_5">Table 3</ref>. The input reduction technique tries to isolate a set of words from the sentence that contribute most to the prediction. The theoretical details of input reduction are discussed in subsection 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Samples predicted correctly by the target-domain adapted model</head><p>In the following, we will discuss a selection of examples that are classified correctly by the best performing in-domain BERT-ADA and incorrectly by BERT-base. The error types for BERT-base are mentioned for all the examples next to their reference label. RC2 -restaurant-domain context needed: "not . . . sweet" -the negated sentiment contained in this phrase is identified correctly by BERT-base and could be interpreted as negative in isolation, but an adjective like "fluffy" carries a stronger positive sentiment for BERT-ADA Rest. RC4 -general review-domain context needed: We think that "should be" is an expression often found in text with opinions, thus both BERT-ADA Lapt and Rest, which both have been finetuned on review-specific text, predict this example correctly. BERT-base is strongly influenced by "friendly" and cannot detect the sentimentnegating function of "should be". RC5 -restaurant-domain context needed: The reduced input "gratuity" is detected as positive for the BERT-ADA Laptop and BERT-base model. In contrast, the BERT-ADA Rest model reveals reduced input words "automatically" and "bill" to detect the negative sentiment correctly. LC2 -laptop-domain context needed: "very quiet" is classified as negative by BERTbase whereas the same expression is classified positive by the BERT-ADA Lapt model. BERT-ADA Rest classifies "aren't audible" as negative.  ). In addition, samples which are predicted falsely by the target-domain adapted model are shown (RE1-2, LE1-2). The abbrevations stand for: B -BERT-base, L -BERT-ADA Lapt(op), R -BERT-ADA Rest(aurant)all the language models used for prediction. The used down-stream-classifiers are trained in-domain. The reduced input (set of words that influence prediction strongest) is formatted with underline and the subscript denotes the corresponding model (B, L, R) used for computing the reduced input. If viewed in color, the corresponding predicted sentiment polarity of the reduced input corresponds to: green -positive, red -negative, gray -neutral, alternating green and red -both negative and positive for different models. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref. Restaurant Samples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Samples predicted incorrectly by the target-domain adapted model</head><p>In the following, we investigate examples that are classified incorrectly by the BERT-ADA models. This helps us to understand the remaining error types and shows a way forward for future work. The majority of incorrect predictions come from the ground-truth neutral class, which in most cases is confused with the positive class for restaurants and with the negative class for laptop reviews. RE1 -influenced by sentiment towards a different aspect-target: This examples was classified correctly only by the BERT-ADA Laptop model. The reduced input for this model is the word "'had", which is used a lot in fact based formulations like for example "the CPU had 3 GHz". From experience, we think that this type of formulation appears more often in the laptops than in the restaurant domain. The BERT-ADA Restaurant and BERT-base model both seem to be influenced by the sentiment associated to another aspect-target. RE2 -influenced by sentiment towards a different aspect-target: Words indicating a certain kind of relation to the aspect-target like "with" in this example could be used to separate the aspect-target specific sentiment from the general sentiment. We think that with more supervised data this case should be solvable by learning these relations in a general way. LE1 -absence of something like a part classified as negative:</p><p>"not" is classified as negative by BERT-ADA Lapt. In the laptops domain the largest remaining confusion are neutral examples classified as neg-ative examples by our algorithm. It seems if absences of parts like a "disk drive" are mentioned, the algorithm tends to classify this as negative.</p><p>In other examples these statements of absence of things actually imply a negative sentiment. LE2 -possibly incorrect ground truth: A handful of examples like this one are, in our opinion, labelled incorrectly. We think the word "only" indicates negative sentiment in this examples.</p><p>To summarize, we find that in order to correctly predict aspect-target based sentiment, the context sensitivity of the sentiment expression plays a important role in difficult examples. By finetuning the language model on domain-specific text the model is able to capture this knowledge most of the time, even if such expressions are not directly observed in the training set used for downstreamclassification.</p><p>We see that especially neutral examples are more difficult to classify correctly. Some of these examples could be solved for an applied realworld case with more supervised data that allows to learn more abstract relationships between entities like sauce and its ingredients in example RS7 and contain more fact-based formulations to discriminate the neutral class better. We also think that selecting finetuning corpora more carefully with these error types in mind could also lead to improvements of classification performance on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We performed experiments on the task of Aspect-Target Sentiment Classification by first finetuning a pre-trained BERT model on a domain specific corpus with subsequent training on the downstream classification task.</p><p>We analyzed the behavior of the number of domain-specific BERT language model finetuning steps in relation to the end-task performance.</p><p>With the findings on how to best exploit BERT language model finetuning we were able to train high performing models, of which the one trained on SemEval 2014 Task 4 restaurants dataset achieves new state-of-the-art performance.</p><p>We further evaluated our models cross-domain to explore the robustness of Aspect-Target Sentiment Classification. We found that with our setup, this task transfers well between the laptops and the restaurants domain.</p><p>As a special case we ran a cross-domain adaptation experiments, where the BERT language model is specifically finetuned on the target domain. We achieve significant improvement over unadapted models: one cross-domain adapted model performs even better than a BERT-base model that is trained in-domain.</p><p>Overall, our findings reveal promising directions for follow-up work. The XLNet-base model performs strongly on the ATSC task. Here, domain-specific finetuning could probably bring significant performance improvements. Another interesting direction for future work would be to investigate cross-domain behavior for an additional domain like hotels, which is more similar to the restaurants domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Absolute accuracy improvement of Aspect-Target Sentiment Classification as a function of the number of sentences the BERT language model has been finetuned on. Markers ( , ) connected through the lines are the averages (µ) over 9 runs, a single run is marked as either a cross (× for restaurants) or a plus (+ for laptops). The standard deviation (σ) curves are also drawn (µ ± σ). The model is trained on the SemEval 2014 Task 4 datasets and evaluated in-domain. The language models are finetuned on the target-domain corpora. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Top: Detailed statistics of the corpora for</cell></row><row><cell>BERT language model finetuning. Bottom: Number</cell></row><row><cell>of labels for each category of the SemEval 2014 Task</cell></row><row><cell>4 Subtask 2 laptop and restaurant datasets for Aspect-</cell></row><row><cell>Target Sentiment Classification.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Summary of results for Aspect-Target Sentiment Classification for in-domain, cross-domain, and joint-</cell></row><row><cell>domain training on SemEval 2014 Task 4 Subtask 2 datasets. The cells with gray background correspond to the</cell></row><row><cell>cross-domain adaptation case, where the language model is finetuned on the target domain. As evaluation metrics</cell></row><row><cell>accuracy (Acc) and Macro-F1 (MF1) are used.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>the best R sushi in New York, however, it is always fresh, and the place is very clean L , sterile B .</figDesc><table><row><cell></cell><cell>Aspect</cell><cell>Base</cell><cell>Lapt</cell><cell>Rest</cell><cell>Gold</cell></row><row><cell cols="2">RC1 Certainly not place</cell><cell cols="4">-+ + +</cell></row><row><cell>RC2 the icing L MADE this cake, it was fluffy R , not B ultra</cell><cell>cake</cell><cell cols="4">-+ + +</cell></row><row><cell>sweet B , creamy and light.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RC3 The sangria's -watered B,L,R down L .</cell><cell>sangria</cell><cell cols="4">+ ---</cell></row><row><cell>RC4 The staff should L,R be a bit more L friendly B .</cell><cell>staff</cell><cell cols="4">+ ---</cell></row><row><cell>RC5 15% B gratuity automatically R,L added to the bill R .</cell><cell>gratuity</cell><cell cols="4">+ + --</cell></row><row><cell>RE1 My friend had L a burger and I had these wonderful B,R blue-</cell><cell>burger</cell><cell cols="4">o + + o</cell></row><row><cell>berry pancakes.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RE2 The sauce is excellent B,L,R (very fresh) with dabs of real</cell><cell>dabs of real</cell><cell cols="4">+ + + o</cell></row><row><cell>mozzarella.</cell><cell>mozzarella</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Laptop Samples</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LC1 the retina display display make pictures i R took B,L years B</cell><cell>retina dis-</cell><cell cols="4">-+ + +</cell></row><row><cell>ago jaw R dropping.</cell><cell>play display</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LC2 The Mac mini is about 8x smaller than my old computer</cell><cell>fans</cell><cell cols="4">-+ -+</cell></row><row><cell>which is a huge bonus and runs B very quiet B,L , actually B</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the fans aren't audible R unlike my old pc</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LE1 the latest version does not B,R,L have a disc drive.</cell><cell>disc drive</cell><cell cols="4">---o</cell></row><row><cell>LE2 Which it did not B,R have, only L 3 USB 2 ports.</cell><cell cols="5">USB 2 ports ---o</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Shown are text samples from SemEval 2014 Restaurants and Laptops test-set that are predicted correctly for the language model adapted to the target domain but predicted falsely with the bert-base model (RC1-5, LC1-LC2</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://alt.qcri.org/semeval2014/task4 2 https://www.yelp.com/dataset/ challenge</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/deepopinion/ domain-adapted-atsc4  We make use of both BERT-base-uncased and XLNetbase-cased models as part of the pytorch-transformers library:https://github.com/huggingface/ pytorch-transformers</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pathologies of neural models make interpretations difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Grissom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1407</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3719" to="3728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting document knowledge for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-2092</idno>
	</analytic>
	<monogr>
		<title level="m">ACL 2018 -56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="579" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ups and Downs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883037</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web -WWW &apos;16</title>
		<meeting>the 25th International Conference on World Wide Web -WWW &apos;16<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2018 -56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/1014052.1014073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;04</title>
		<imprint>
			<biblScope unit="page">168</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4253" to="4260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SemEval-2014 Task 4: Aspect Based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/s14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural Transfer Learning for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Attentional encoder network for targeted sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09314</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective LSTMs for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016 -26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aspect Level Sentiment Classification with Deep Memory Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="5999" to="6009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2324" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Modeling sentiment dependencies with graph convolutional networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinlong</forename><surname>Zhaoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Houb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Wua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04501</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
