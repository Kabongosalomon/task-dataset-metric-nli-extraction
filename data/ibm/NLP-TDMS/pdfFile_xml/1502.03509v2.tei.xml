<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MADE: Masked Autoencoder for Distribution Estimation Google DeepMind</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Germain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Germain2@usherbrooke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ca</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
							<email>karol.gregor@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Ac</forename><surname>Murray@ed</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uk</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle@usherbrooke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ca</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Université de Sherbrooke</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Université de Sherbrooke</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MADE: Masked Autoencoder for Distribution Estimation Google DeepMind</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with stateof-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Distribution estimation is the task of estimating a joint distribution p(x) from a set of examples {x (t) } T t=1 , which is by definition a general problem. Many tasks in machine learning can be formulated as learning only specific properties of a joint distribution. Thus a good distribution estimator can be used in many scenarios, including classification (Schmah Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&amp;CP volume 37. Copyright 2015 by the author(s). <ref type="bibr">et al., 2009)</ref>, denoising or missing input imputation <ref type="bibr" target="#b11">(Poon &amp; Domingos, 2011;</ref><ref type="bibr" target="#b4">Dinh et al., 2014)</ref>, data (e.g. speech) synthesis <ref type="bibr" target="#b18">(Uria et al., 2015)</ref> and many others. The very nature of distribution estimation also makes it a particular challenge for machine learning. In essence, the curse of dimensionality has a distinct impact because, as the number of dimensions of the input space of x grows, the volume of space in which the model must provide a good answer for p(x) exponentially increases.</p><p>Fortunately, recent research has made substantial progress on this task. Specifically, learning algorithms for a variety of neural network models have been proposed <ref type="bibr" target="#b1">(Bengio &amp; Bengio, 2000;</ref><ref type="bibr" target="#b10">Larochelle &amp; Murray, 2011;</ref><ref type="bibr" target="#b7">Gregor &amp; LeCun, 2011;</ref><ref type="bibr" target="#b16">Uria et al., 2013;</ref><ref type="bibr" target="#b9">Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b12">Rezende et al., 2014;</ref><ref type="bibr" target="#b2">Bengio et al., 2014;</ref><ref type="bibr" target="#b8">Gregor et al., 2014;</ref><ref type="bibr" target="#b6">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b4">Dinh et al., 2014)</ref>. These algorithms are showing great potential in scaling to high-dimensional distribution estimation problems. In this work, we focus our attention on autoregressive models (Section 3). Computing p(x) exactly for a test example x is tractable with these models. However, the computational cost of this operation is still larger than typical neural network predictions for a D-dimensional input. For previous deep autoregressive models, evaluating p(x) costs O(D) times more than a simple neural network point predictor. This paper's contribution is to describe and explore a simple way of adapting autoencoder neural networks that makes them competitive tractable distribution estimators that are faster than existing alternatives. We show how to mask the weighted connections of a standard autoencoder to convert it into a distribution estimator. The key is to use masks that are designed in such a way that the output is autoregressive for a given ordering of the inputs, i.e. that each input dimension is reconstructed solely from the dimensions preceding it in the arXiv:1502.03509v2 <ref type="bibr">[cs.</ref>LG] 5 Jun 2015 ordering. The resulting Masked Autoencoder Distribution Estimator (MADE) preserves the efficiency of a single pass through a regular autoencoder. Implementation on a GPU is straightforward, making the method scalable.</p><p>The single hidden layer version of MADE corresponds to the previously proposed autoregressive neural network of <ref type="bibr" target="#b1">Bengio &amp; Bengio (2000)</ref>. Here, we go further by exploring deep variants of the model. We also explore training MADE to work simultaneously with multiple orderings of the input observations and hidden layer connectivity structures. We test these extensions across a range of binary datasets with hundreds of dimensions, and compare its statistical performance and scaling to comparable methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Autoencoders</head><p>A brief description of the basic autoencoder, on which this work builds upon, is required to clearly grasp what follows. In this paper, we assume that we are given a training set of examples {x (t) } T t=1 . We concentrate on the case of binary observations, where for every D-dimensional input x, each input dimension x d belongs in {0, 1}. The motivation is to learn hidden representations of the inputs that reveal the statistical structure of the distribution that generated them.</p><p>An autoencoder attempts to learn a feed-forward, hidden representation h(x) of its input x such that, from it, we can obtain a reconstruction x which is as close as possible to x. Specifically, we have</p><formula xml:id="formula_0">h(x) = g(b + Wx) (1) x = sigm(c + Vh(x)) ,<label>(2)</label></formula><p>where W and V are matrices, b and c are vectors, g is a nonlinear activation function and sigm(a) = 1/(1 + exp(−a)). Thus, W represents the connections from the input to the hidden layer, and V represents the connections from the hidden to the output layer.</p><p>To train the autoencoder, we must first specify a training loss function. For binary observations, a natural choice is the cross-entropy loss:</p><formula xml:id="formula_1">(x) = D d=1 −x d log x d − (1−x d ) log(1− x d ) .<label>(3)</label></formula><p>By treating x d as the model's probability that x d is 1, the cross-entropy can be understood as taking the form of a negative log-likelihood function. Training the autoencoder corresponds to optimizing the parameters {W, V, b, c} to reduce the average loss on the training examples, usually with (mini-batch) stochastic gradient descent.</p><p>One advantage of the autoencoder paradigm is its flexibility. In particular, it is straightforward to obtain a deep autoencoder by inserting more hidden layers between the input and output layers. Its main disadvantage is that the representation it learns can be trivial. For instance, if the hidden layer is at least as large as the input, hidden units can each learn to "copy" a single input dimension, so as to reconstruct all inputs perfectly at the output layer. One obvious consequence of this observation is that the loss function of Equation 3 isn't in fact a proper log-likelihood function. Indeed, since perfect reconstruction could be achieved,</p><formula xml:id="formula_2">the implied data 'distribution' q(x) = d x x d d (1− x d ) 1−x d<label>could</label></formula><p>be learned to be 1 for any x and thus not be properly normalized ( x q(x) = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Distribution Estimation as Autoregression</head><p>An interesting question is what property we could impose on the autoencoder, such that its output can be used to obtain valid probabilities. Specifically, we'd like to be able to write p(x) in such a way that it could be computed based on the output of a properly corrected autoencoder.</p><p>First, we can use the fact that, for any distribution, the probability product rule implies that we can always decompose it into the product of its nested conditionals</p><formula xml:id="formula_3">p(x) = D d=1 p(x d | x &lt;d ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">x &lt;d = [x 1 , . . . , x d−1 ] .</formula><p>By defining p(x d = 1 | x &lt;d ) =x d , and thus p(x d = 0 | x &lt;d ) = 1−x d , the loss of Equation 3 becomes a valid negative log-likelihood:</p><formula xml:id="formula_5">− log p(x) = D d=1 − log p(x d | x &lt;d ) = D d=1 −x d log p(x d = 1 | x &lt;d ) − (1−x d ) log p(x d = 0 | x &lt;d ) = (x) .<label>(5)</label></formula><p>This connection provides a way to define autoencoders that can be used for distribution estimation. Each output x d = p(x d | x &lt;d ) must be a function taking as input x &lt;d only and outputting the probability of observing value x d at the d th dimension. In particular, the autoencoder forms a proper distribution if each output unitx d only depends on the previous input units x &lt;d , and not the other units</p><formula xml:id="formula_6">x ≥d = [x d , . . . , x D ] .</formula><p>We refer to this property as the autoregressive property, because computing the negative log-likelihood (5) is equivalent to sequentially predicting (regressing) each dimension of input x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Masked Autoencoders</head><p>The question now is how to modify the autoencoder so as to satisfy the autoregressive property. Since outputx d must depend only on the preceding inputs x &lt;d , it means that there must be no computational path between output unit x d and any of the input units x d , . . . , x D . In other words, for each of these paths, at least one connection (in matrix W or V) must be 0.</p><p>A convenient way of zeroing connections is to elementwisemultiply each matrix by a binary mask matrix, whose entries that are set to 0 correspond to the connections we wish to remove. For a single hidden layer autoencoder, we write</p><formula xml:id="formula_7">h(x) = g(b + (W M W )x) (6) x = sigm(c + (V M V )h(x))<label>(7)</label></formula><p>where M W and M V are the masks for W and V respectively. It is thus left to the masks M W and M V to satisfy the autoregressive property.</p><p>To impose the autoregressive property we first assign each unit in the hidden layer an integer m between 1 and D−1 inclusively. The k th hidden unit's number m(k) gives the maximum number of input units to which it can be connected. We disallow m(k) = D since this hidden unit would depend on all inputs and could not be used in modelling any of the conditionals p(x d | x &lt;d ). Similarly, we exclude m(k) = 0, as it would create constant hidden units.</p><p>The constraints on the maximum number of inputs to each hidden unit are encoded in the matrix masking the connections between the input and hidden units:</p><formula xml:id="formula_8">M W k,d = 1 m(k)≥d = 1 if m(k) ≥ d 0 otherwise,<label>(8)</label></formula><p>for d ∈ {1, . . . , D} and k ∈ {1, . . . , K}. Overall, we need to encode the constraint that the d th output unit is only connected to x &lt;d (and thus not to x ≥d ). Therefore the output weights can only connect the d th output to hidden units with m(k) &lt; d, i.e. units that are connected to at most d−1 input units. These constraints are encoded in the output mask matrix:</p><formula xml:id="formula_9">M V d,k = 1 d&gt;m(k) = 1 d &gt;m(k) 1 m(k)≥d . (10)</formula><p>If d ≤ d, then there are no values for m(k) such that it is both strictly less than d and greater or equal to d. Thus</p><formula xml:id="formula_10">M V,W d ,d is indeed 0.</formula><p>Constructing the masks M V and M W only requires an assignment of the m(k) values to each hidden unit. One could imagine trying to assign an (approximately) equal number of units to each legal value of m(k). In our experiments, we instead set m(k) by sampling from a uniform discrete distribution defined on integers from 1 to D−1, independently for each of the K hidden units.</p><p>Previous work on autoregressive neural networks have also found it advantageous to use direct connections between the input and output layers <ref type="bibr" target="#b1">(Bengio &amp; Bengio, 2000)</ref>. In this context, the reconstruction becomes:</p><formula xml:id="formula_11">x = sigm(c + (V M V )h(x) + (A M A )x) ,<label>(11)</label></formula><p>where A is the parameter connection matrix and M A is its mask matrix. To satisfy the autoregressive property, M A simply needs to be a strictly lower diagonal matrix, filled otherwise with ones. We used such direct connections in our experiments as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Deep MADE</head><p>One advantage of the masked autoencoder framework described in the previous section is that it naturally generalizes to deep architectures. Indeed, as we'll see, by assigning a maximum number of connected inputs to all units across the deep network, masks can be similarly constructed so as to satisfy the autoregressive property.</p><p>For networks with L &gt; 1 hidden layers, we use superscripts to index the layers. The first hidden layer matrix (previously W) will be denoted W 1 , the second hidden layer matrix will be W 2 , and so on. The number of hidden units (previously K) in each hidden layer will be similarly indexed as K l , where l is the hidden layer index. We will also generalize the notation for the maximum number of connected inputs of the k th unit in the l th layer to m l (k).</p><p>We've already discussed how to define the first layer's mask matrix such that it ensures that its k th unit is connected to at most m(k) (now m 1 (k)) inputs. To impose the same property on the second hidden layer, we must simply make sure that each unit k is only connected to first layer units Input in the bottom is passed through fully connected layers and point-wise nonlinearities. In the final top layer, a reconstruction specified as a probability distribution over inputs is produced. As this distribution depends on the input itself, a standard autoencoder cannot predict or sample new data. Right: MADE. The network has the same structure as the autoencoder, but a set of connections is removed such that each input unit is only predicted from the previous ones, using multiplicative binary masks</p><formula xml:id="formula_12">Autoencoder Masks MADE x W 1 W 2 V M W 1 M W 2 M V = = = p(x 2 ) p(x 3 |x 2 ) p(x 1 |x 2 , x 3 ) 3 1 2 2 1 2 2 1 2 2 1 3 1 2 x 1 x 2 x 3 x 1 x 2 x 3 x 1 x 2 x 3</formula><formula xml:id="formula_13">(M W 1 , M W 2 , M V ).</formula><p>In this example, the ordering of the input is changed from 1,2,3 to 3,1,2. This change is explained in section 4.2, but is not necessary for understanding the basic principle. The numbers in the hidden units indicate the maximum number of inputs on which the k th unit of layer l depends. The masks are constructed based on these numbers (see Equations 12 and 13). These masks ensure that MADE satisfies the autoregressive property, allowing it to form a probabilistic model, in this example p(x) = p(x2) p(x3|x2) p(x1|x2, x3). Connections in light gray correspond to paths that depend only on 1 input, while the dark gray connections depend on 2 inputs. connected to at most m 2 (k ) inputs, i.e. the first layer units such that m 1 (k) ≤ m 2 (k ).</p><p>One can generalize this rule to any layer l, as follows:</p><formula xml:id="formula_14">M W l k ,k = 1 m l (k )≥m l−1 (k) = 1 if m l (k ) ≥ m l−1 (k) 0 otherwise.<label>(12)</label></formula><p>Also, taking l = 0 to mean the input layer and defining m 0 (d) = d (which is intuitive, since the d th input unit indeed takes its values from the d first inputs), this definition also applies for the first hidden layer weights. As for the output mask, we simply need to adapt its definition by using the connectivity constraints of the last hidden layer m L (k) instead of the first:</p><formula xml:id="formula_15">M V d,k = 1 d&gt;m L (k) = 1 if d &gt; m L (k) 0 otherwise.<label>(13)</label></formula><p>Like for the single hidden layer case, the values for m l (k) for each hidden layer l ∈ {1, . . . , L} are sampled uniformly.</p><p>To avoid unconnected units, the value for m l (k) is sampled to be greater than or equal to the minimum connectivity at the previous layer, i.e. min k m l−1 (k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Order-agnostic training</head><p>So far, we've assumed that the conditionals modelled by MADE were consistent with the natural ordering of the dimensions of x. However, we might be interested in modelling the conditionals associated with an arbitrary ordering of the input's dimensions.</p><p>Specifically, <ref type="bibr" target="#b17">Uria et al. (2014)</ref> have shown that training an autoregressive model on all orderings can be beneficial.</p><p>We refer to this approach as order-agnostic training. It can be achieved by sampling an ordering before each stochastic/minibatch gradient update of the model. There are two advantages of this approach. Firstly, missing values in partially observed input vectors can be imputed efficiently: we invoke an ordering where observed dimensions are all before unobserved ones, making inference straightforward. Secondly, an ensemble of autoregressive models can be constructed on the fly, by exploiting the fact that the conditionals for two different orderings are not guaranteed to be exactly consistent (and thus technically correspond to slightly different models). An ensemble is then easily obtained by sampling a set of orderings, computing the probability of x under each ordering and averaging.</p><p>Conveniently, in MADE, the ordering is simply represented by the vector m 0 = [m 0 (1), . . . , m 0 (D)]. Specifically, m 0 (d) corresponds to the position of the original d th dimension of x in the product of conditionals. Thus, a random ordering can be obtained by randomly permuting the ordered vector [1, . . . , D]. From these values of each m 0 , the first hidden layer mask matrix can then be created. During order-agnostic training, randomly permuting the last value of m 0 again is sufficient to obtain a new random ordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Connectivity-agnostic training</head><p>One advantage of order-agnostic training is that it effectively allows us to train as many models as there are orderings, using a common set of parameters. This can be exploited by creating ensembles of models at test time.</p><p>In MADE, in addition to choosing an ordering, we also have to choose each hidden unit's connectivity constraint m l (k). Thus, we could imaging training MADE to also be agnostic of the connectivity pattern generated by these constraints. To achieve this, instead of sampling the values of m l (k) for all units and layers once and for all before training, we actually resample them for each training example or minibatch. This is still practical, since the operation of creating the masks is easy to parallelize. Denoting m l = [m l (1), . . . , m l (K l )], and assuming an element-wise and parallel implementation of the operation 1 a≥b for vectors, such that 1 a≥b is a matrix </p><formula xml:id="formula_16">L do M W l ← 1 m l ≥m l−1 end for M V ← 1 m 0 &gt;m L # Computing p(x) h 0 (x) ← x for l from 1 to L do h l (x) ← g(b l + (W l M W l )h l−1 (x)) end for x ← sigm(c + (V M V )h L (x)) p(x) ← exp D d=1 x d log x d + (1−x d ) log(1− x d ) # Computing gradients of − log p(x) tmp ← x − x δc ← tmp δV ← tmp h L (x) M V tmp ← (tmp (V M V )) for l from L to 1 do tmp ← tmp g (b l + (W l M W l )h l−1 (x)) δb l ← tmp δW l ← tmp h l−1 (x) M W l tmp ← (tmp (W l M W l ))</formula><p>end for return p(x), δb 1 , . . . , δb L , δW 1 , . . . , δW L , δc, δV whose i, j element is 1 ai≥bj , then the hidden layer masks are simply M W l = 1 m l ≥m l−1 .</p><p>By resampling the connectivity of hidden units for every update, each hidden unit will have a constantly changing number of incoming inputs during training. However, the absence of a connection is indistinguishable from an instantiated connection to a zero-valued unit, which could confuse the neural network during training. In a similar situation, <ref type="bibr" target="#b17">Uria et al. (2014)</ref> informed each hidden unit which units were providing input with binary indicator variables, connected with additional learnable weights. We considered applying a similar strategy, using companion weight matrices U l , that are also masked by M W l but connected to a constant one-valued vector:</p><formula xml:id="formula_17">h l (x) = g(b l + (W l M W l )h l−1 (x) + (U l M W l )1)</formula><p>(14) An analogous parametrization of the output layer was also employed. These connectivity conditioning weights were only sometimes useful. In our experiments, we treated the choice of using them as a hyperparameter.</p><p>Moreover, we've found in our experiments that sampling masks for every example could sometimes over-regularize MADE and provoke underfitting. To fix this issue, we also considered sampling from only a finite list of masks. During training, MADE cycles through this list, using one for every update. At test time, we then average probabilities obtained for all masks in the list.</p><p>Algorithm 1 details how p(x) is computed by MADE, as well as how to obtain the gradient of (x) for stochastic gradient descent training. For simplicity, the pseudocode assumes order-agnostic and connectivity-agnostic training, doesn't assume the use of conditioning weight matrices or of direct input/output connections. <ref type="figure" target="#fig_0">Figure 1</ref> also illustrates an example of such a two-layer MADE network, along with its m l (k) values and its masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>There has been a lot of recent work on exploring the use of feed-forward, autoencoder-like neural networks as probabilistic generative models. Part of the motivation behind this research is to test the common assumption that the use of models with probabilistic latent variables and intractable partition functions (such as the restricted Boltzmann machine <ref type="bibr" target="#b13">(Salakhutdinov &amp; Murray, 2008)</ref>), is a necessary evil in designing powerful generative models for high-dimensional data.</p><p>The work on the neural autoregressive distribution estimator or NADE <ref type="bibr" target="#b10">(Larochelle &amp; Murray, 2011)</ref> has illustrated that feed-forward architectures can in fact be used to form stateof-the-art and even tractable distribution estimators.</p><p>Recently, a deep extension of NADE was proposed, improving even further the state-of-the-art in distribution estimation <ref type="bibr" target="#b17">(Uria et al., 2014)</ref>. This work introduced a randomized training procedure, which (like MADE) has nearly the same cost per iteration as a standard autoencoder. Unfortunately, deep NADE models still require D feed-forward passes through the network to evaluate the probability p(x) of a D-dimensional test vector. The computation of the first hidden layer's activations can be shared across these passes, <ref type="table">Table 1</ref>. Complexity of the different models in <ref type="table" target="#tab_6">Table 6</ref>, to compute an exact test negative log-likelihood. R is the number of orderings used, D is the input size, and K is the hidden layer size (assuming equally sized hidden layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>ONLL</p><formula xml:id="formula_18">RBM 25 CD steps O(min(2 D K, D2 K )) DARN O(2 K D) NADE (fixed order) O(DK) EoNADE 1hl, R ord. O(RDK) EoNADE 2hl, R ord. O(RDK 2 ) MADE 1hl, 1 ord. O(DK +D 2 ) MADE 2hl, 1 ord. O(DK +K 2 +D 2 ) MADE 1hl, R ord. O(R(DK +D 2 )) MADE 2hl, R ord. O(R(DK +K 2 +D 2 ))</formula><p>although is slower in practice than evaluating a single pass in a standard autoencoder. In deep networks with K hidden units per layer, it costs O(DK 2 ) to evaluate a test vector.</p><p>Deep AutoRegressive Networks (DARN, <ref type="bibr" target="#b8">Gregor et al., 2014)</ref>, also provide probabilistic models with roughly the same training costs as standard autoencoders. DARN's latent representation consist of binary, stochastic hidden units. While simulating from these models is fast, evaluation of exact test probabilities requires summing over all configurations of the latent representation, which is exponential in computation. Monte Carlo approximation is thus recommended.</p><p>The main advantage of MADE is that evaluating probabilities retains the efficiency of autoencoders, with minor additional cost for simple masking operations. <ref type="table">Table 1</ref> lists the computational complexity for exact computation of probabilities for various models. DARN and RBMs are exponential in dimensionality of the hiddens or data, whereas NADE and MADE are polynomial. MADE only requires one pass through the autoencoder rather than the D passes required by NADE. In practice, we also observe that the single-layer MADE is an order of magnitude faster than a one-layer NADE, for the same hidden layer size, despite NADE sharing computation to get the same asymptotic scaling. NADE's computations cannot be vectorized as efficiently. The deep versions of MADE also have better scaling than NADE at test time. The training costs for MADE, DARN, and deep NADE will all be similar.</p><p>Before the work on NADE, <ref type="bibr" target="#b1">Bengio &amp; Bengio (2000)</ref> proposed a neural network architecture that corresponds to the special case of a single hidden layer MADE model, without randomization of input ordering and connectivity. A contribution of our work is to go beyond this special case, exploring deep variants and order/connectivity-agnostic training. An interesting interpretation of the autoregressive mask sampling is as a structured form of dropout regularization <ref type="bibr" target="#b15">(Srivastava et al., 2014)</ref>. Specifically, it bears similarity with the masking in dropconnect networks <ref type="bibr" target="#b19">(Wan et al., 2013)</ref>. The exception is that the masks generated here must guaranty the autoregressive property of the autoencoder, while in <ref type="bibr" target="#b19">Wan et al. (2013)</ref>, each element in the mask is generated independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>To test the performance of our model we considered two different benchmarks: a suite of UCI binary datasets, and the binarized MNIST dataset. The code to reproduce the experiments of this paper is available at https://github.com/mgermain/MADE/releases/tag/ICML2015. The results reported here are the average negative loglikelihood on the test set of each respective dataset. All experiments were made using stochastic gradient descent (SGD) with mini-batches of size 100 and a lookahead of 30 for early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">UCI evaluation suite</head><p>We use the binary UCI evaluation suite that was first put together in <ref type="bibr" target="#b10">Larochelle &amp; Murray (2011)</ref>. It's a collection of 7 relatively small datasets from the University of California, Irvine machine learning repository and the OCR-letters dataset from the Stanford AI Lab. <ref type="table" target="#tab_1">Table 2</ref> gives an overview of the scale of those datasets and the way they were split.</p><p>The experiments were run with networks of 500 units per hidden layer, using the adadelta learning update <ref type="bibr" target="#b20">(Zeiler, 2012)</ref> with a decay of 0.95. The other hyperparameters were varied as <ref type="table" target="#tab_2">Table 3</ref> indicates. We note as # of masks the number of different masks through which MADE cycles during training. In the no limit case, masks are sampled on the fly and never explicitly reused unless re-sampled by chance. In this situation, at validation and test time, 300 and 1000 sampled masks are used for averaging probabilities.  <ref type="bibr">,</ref><ref type="bibr">8,</ref><ref type="bibr">16,</ref><ref type="bibr">32,</ref><ref type="bibr">No Limit</ref> The results are reported in <ref type="table" target="#tab_5">Table 4</ref>. We see that MADE is among the best performing models on half of the datasets and is competitive otherwise. To reduce clutter, we have not reported standard deviations, which were fairly small and consistent across models. However, for completeness we report standard deviations in a separate table in the supplementary materials.</p><p>An analysis of the hyperparameters selected for each dataset reveals no clear winner. However, we do see from <ref type="table" target="#tab_5">Table 4</ref> that when the mask sampling helps, it helps quite a bit and when it does not, the impact is negligible on all but OCRletters. Another interesting note is that the conditioning weights had almost no influence except on NIPS-0-12 where it helped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Binarized MNIST evaluation</head><p>The version of MNIST we used is the one binarized by <ref type="bibr" target="#b13">Salakhutdinov &amp; Murray (2008)</ref>. MNIST is a set of 70,000 hand written digits of 28×28 pixels. We use the same split as in <ref type="bibr" target="#b10">Larochelle &amp; Murray (2011)</ref>, consisting of 50,000 for the training set, 10,000 for the validation set and 10,000 for the test set.</p><p>Experiments were run using the adagrad learning update <ref type="bibr" target="#b5">(Duchi et al., 2010)</ref>, with an epsilon of 10 −6 . Since MADE is much more efficient than NADE, we considered varying the hidden layer size from 500 to 8000 units. Building on what we learned on the UCI experiments, we set the activation function to be ReLU and the conditioning weights were not used. The hyperparameters that were varied are in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>The results are reported in <ref type="table" target="#tab_6">Table 6</ref>, alongside other results  <ref type="figure">Figure 2</ref>. Impact of the number of masks used with a single hidden layer, 500 hidden units network, on binarized MNIST. taken from the literature. Again, despite its tractability, MADE is competitive with other models. Of note is the fact that the best MADE model outperforms the single layer NADE network, which was otherwise the best model among those requiring only a single feed-forward pass to compute log probabilities.</p><p>In these experiments, we clearly observed the overregularization phenomenon from using too many masks. When more than four orderings were used, the deeper variant of MADE always yielded better results. For the two layer model, adding masks during training helped up to 64, at which point the negative log-likelihood started to increase. We observed a similar pattern for the single layer model, but in this case the dip was around 8 masks. <ref type="figure">Figure 2</ref> illustrates this behaviour more precisely for a single layer MADE with 500 hidden units, trained by only varying the number of masks used and the size of the mini-batches (83, 100, 128).</p><p>We randomly sampled 100 digits from our best performing model from <ref type="table" target="#tab_6">Table 6</ref> and compared them with their nearest neighbor in the training set <ref type="figure" target="#fig_1">(Figure 3</ref>), to ensure that the generated samples are not simple memorization. Each row of digits uses a different mask that was seen at training time by the network.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed MADE, a simple modification of autoencoders allowing them to be used as distribution estimators. MADE demonstrates that it is possible to get direct, cheap estimates of high-dimensional joint probabilities, from a single pass through an autoencoder. Like standard autoencoders, our extension is easy to vectorize and implement on GPUs. MADE can evaluate high-dimensional probably distributions with better scaling than before, while maintaining state-of-the-art statistical performance.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Left: Conventional three hidden layer autoencoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Left: Samples from a 2 hidden layer MADE. Right: Nearest neighbour in binarized MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Computation of p(x) and learning gradients for MADE with order and connectivity sampling. D is the size of the input, L the number of hidden layers and K the number of hidden units.Input: training observation vector x Output: p(x) and gradients of − log p(x) on parameters</figDesc><table><row><cell># Sampling m l vectors</cell></row><row><cell>m 0 ← shuffle([1, . . . , D]) for l from 1 to L do</cell></row><row><cell>for k from 1 to K l do</cell></row><row><cell>m l (k) ← Uniform([min k m l−1 (k ), . . . , D−1]) end for</cell></row><row><cell>end for</cell></row><row><cell># Constructing masks for each layer</cell></row><row><cell>for l from 1 to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Number of input dimensions and numbers of examples in the train, validation, and test splits.</figDesc><table><row><cell>Name</cell><cell># Inputs</cell><cell>Train</cell><cell>Valid.</cell><cell>Test</cell></row><row><cell>Adult</cell><cell>123</cell><cell>5000</cell><cell>1414</cell><cell>26147</cell></row><row><cell>Connect4</cell><cell cols="2">126 16000</cell><cell>4000</cell><cell>47557</cell></row><row><cell>DNA</cell><cell>180</cell><cell>1400</cell><cell>600</cell><cell>1186</cell></row><row><cell>Mushrooms</cell><cell>112</cell><cell>2000</cell><cell>500</cell><cell>5624</cell></row><row><cell>NIPS-0-12</cell><cell>500</cell><cell>400</cell><cell>100</cell><cell>1240</cell></row><row><cell>OCR-letters</cell><cell cols="3">128 32152 10000</cell><cell>10000</cell></row><row><cell>RCV1</cell><cell cols="4">150 40000 10000 150000</cell></row><row><cell>Web</cell><cell cols="2">300 14000</cell><cell>3188</cell><cell>32561</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>UCI Grid Search</figDesc><table><row><cell>Hyperparameter</cell><cell>Values tried</cell></row><row><cell># Hidden Layer</cell><cell>1, 2</cell></row><row><cell>Activation function</cell><cell>ReLU, Softplus</cell></row><row><cell>Adadelta epsilon</cell><cell>10 −5 , 10 −7 , 10 −9</cell></row><row><cell cols="2">Conditioning Weights True, False</cell></row><row><cell># of orderings</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Binarized MNIST Grid Search</figDesc><table><row><cell cols="2">Hyperparameter Values tried</cell></row><row><cell># Hidden Layer</cell><cell>1, 2</cell></row><row><cell>Learning Rate</cell><cell>0.1, 0.05, 0.01, 0.005</cell></row><row><cell># of masks</cell><cell>1, 2, 4, 8, 16, 32, 64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Negative log-likelihood test results of different models on multiple datasets. The best result as well as any other result with an overlapping confidence interval is shown in bold. Note that since the variance of DARN was not available, we considered it to be zero.</figDesc><table><row><cell>Model</cell><cell cols="6">Adult Connect4 DNA Mushrooms NIPS-0-12 OCR-letters</cell><cell>RCV1</cell><cell>Web</cell></row><row><cell>MoBernoullis</cell><cell>20.44</cell><cell>23.41</cell><cell>98.19</cell><cell>14.46</cell><cell>290.02</cell><cell>40.56</cell><cell>47.59</cell><cell>30.16</cell></row><row><cell>RBM</cell><cell>16.26</cell><cell>22.66</cell><cell>96.74</cell><cell>15.15</cell><cell>277.37</cell><cell>43.05</cell><cell>48.88</cell><cell>29.38</cell></row><row><cell>FVSBN</cell><cell>13.17</cell><cell>12.39</cell><cell>83.64</cell><cell>10.27</cell><cell>276.88</cell><cell>39.30</cell><cell>49.84</cell><cell>29.35</cell></row><row><cell>NADE (fixed order)</cell><cell>13.19</cell><cell>11.99</cell><cell>84.81</cell><cell>9.81</cell><cell>273.08</cell><cell>27.22</cell><cell>46.66</cell><cell>28.39</cell></row><row><cell cols="2">EoNADE 1hl (16 ord.) 13.19</cell><cell>12.58</cell><cell>82.31</cell><cell>9.69</cell><cell>272.39</cell><cell>27.32</cell><cell>46.12</cell><cell>27.87</cell></row><row><cell>DARN</cell><cell>13.19</cell><cell>11.91</cell><cell>81.04</cell><cell>9.55</cell><cell>274.68</cell><cell>≈28.17</cell><cell cols="2">≈46.10 ≈28.83</cell></row><row><cell>MADE</cell><cell>13.12</cell><cell>11.90</cell><cell>83.63</cell><cell>9.68</cell><cell>280.25</cell><cell>28.34</cell><cell>47.10</cell><cell>28.53</cell></row><row><cell cols="2">MADE mask sampling 13.13</cell><cell>11.90</cell><cell>79.66</cell><cell>9.69</cell><cell>277.28</cell><cell>30.04</cell><cell>46.74</cell><cell>28.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Negative log-likelihood test results of different models on the binarized MNIST dataset.</figDesc><table><row><cell>Model</cell><cell>− log p</cell><cell></cell></row><row><cell>RBM (500 h, 25 CD steps) DBM 2hl DBN 2hl DARN n h =500 DARN n h =500, adaNoise</cell><cell>≈ 86.34 ≈ 84.62 ≈ 84.55 ≈ 84.71 ≈ 84.13</cell><cell>Intractable</cell></row><row><cell>MoBernoullis K=10</cell><cell>168.95</cell><cell></cell></row><row><cell>MoBernoullis K=500</cell><cell>137.64</cell><cell></cell></row><row><cell>NADE 1hl (fixed order) EoNADE 1hl (128 orderings) EoNADE 2hl (128 orderings) MADE 1hl (1 mask)</cell><cell>88.33 87.71 85.10 88.40</cell><cell>Tractable</cell></row><row><cell>MADE 2hl (1 mask)</cell><cell>89.59</cell><cell></cell></row><row><cell>MADE 1hl (32 masks)</cell><cell>88.04</cell><cell></cell></row><row><cell>MADE 2hl (32 masks)</cell><cell>86.64</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Negative log-likelihood and 95% confidence intervals forTable 4in the main document. Binarized MNIST negative log-likelihood and 95% confidence intervals forTable 6in the main document.</figDesc><table><row><cell></cell><cell cols="2">MADE</cell><cell>EoNADE</cell></row><row><cell>Dataset</cell><cell cols="2">Fixed mask Mask sampling</cell><cell>16 ord.</cell></row><row><cell>Adult</cell><cell>13.12±0.05</cell><cell>13.13±0.05</cell><cell>13.19±0.04</cell></row><row><cell>Connect4</cell><cell>11.90±0.01</cell><cell>11.90±0.01</cell><cell>12.58±0.01</cell></row><row><cell>DNA</cell><cell>83.63±0.52</cell><cell>79.66±0.63</cell><cell>82.31±0.46</cell></row><row><cell>Mushrooms</cell><cell>9.68±0.04</cell><cell>9.69±0.03</cell><cell>9.69±0.03</cell></row><row><cell>NIPS-0-12</cell><cell>280.25±1.05</cell><cell>275.92±1.01</cell><cell>272.39±1.08</cell></row><row><cell>Ocr-letters</cell><cell>28.34±0.22</cell><cell>30.04±0.22</cell><cell>27.32±0.19</cell></row><row><cell>RCV1</cell><cell>47.10±0.11</cell><cell>46.74±0.11</cell><cell>46.12±0.11</cell></row><row><cell>Web</cell><cell>28.53±0.20</cell><cell>28.25±0.20</cell><cell>27.87±0.20</cell></row><row><cell cols="2">Model</cell><cell></cell><cell></cell></row><row><cell cols="2">MADE 1hl (1 mask)</cell><cell>88.40±0.45</cell><cell></cell></row><row><cell cols="2">MADE 2hl (1 mask)</cell><cell>89.59±0.46</cell><cell></cell></row><row><cell cols="3">MADE 1hl (32 masks) 88.04±0.44</cell><cell></cell></row><row><cell cols="3">MADE 2hl (32 masks) 86.64±0.44</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">if d &gt; m(k) 0 otherwise,(9)again for d ∈ {1, . . . , D} and k ∈ {1, . . . , K}. Notice that, from this rule, no hidden units will be connected to the first output unitx 1 , as desired.From these mask constructions, we can easily demonstrate that the corresponding masked autoencoder satisfies the autoregressive property. First, we note that, since the masks M V and M W represent the network's connectivity, their matrix product M V,W = M V M W represents the connectivity between the input and the output layer. Specifically,M V,W d ,dis the number of network paths between output unit x d and input unit x d . Thus, to demonstrate the autoregressive property, we need to show that M V,W is strictly lower diagonal, i.e. M V,W d ,d is 0 if d ≤ d. By definition of the matrix product, we have:M V,W d ,d = K k=1 M V d ,k M W k,d = K k=1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Marc-Alexandre Côté for helping to implement NADE in Theano and the whole Theano <ref type="bibr" target="#b0">(Bastien et al., 2012;</ref><ref type="bibr" target="#b3">Bergstra et al., 2010)</ref> team of contributors. We also thank NSERC, Calcul Québec and Compute Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arnaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling highdimensional discrete data with multi-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 12 (NIPS 1999)</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="400" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep generative stochastic networks trainable by backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th Annual International Conference on Machine Learning</title>
		<meeting>the 31th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<publisher>Oral Presentation</publisher>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">NICE: non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516v2</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-03" />
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS 2014)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1108.1169v1</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep AutoRegressive Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andriy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th Annual International Conference on Machine Learning</title>
		<meeting>the 31th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1242" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
		<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR 2014</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics<address><addrLine>Ft. Lauderdale, USA</addrLine></address></meeting>
		<imprint>
			<publisher>JMLR W&amp;CP</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sum-product networks: A new deep architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence (UAI 2011)</title>
		<meeting>the 20th Conference on Uncertainty in Artificial Intelligence (UAI 2011)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International Conference on Machine Learning</title>
		<meeting>the 25th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative versus discriminative training of RBMs for classification of fMRI images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Schmah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RNADE: The real-valued neural autoregressive densityestimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benigno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS 2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2175" to="2183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benigno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modelling acoustic feature dependencies with artificial neural networks: Trajectory-RNADE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benigno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 40th IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sixin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701v1</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
