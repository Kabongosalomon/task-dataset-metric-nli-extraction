<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Faithful to the Original: Fact Aware Neural Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong Polytechnic University Shenzhen Research Institute</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong Polytechnic University Shenzhen Research Institute</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
							<email>lisujian@pku.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Faithful to the Original: Fact Aware Neural Abstractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. Our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. The dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions. Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%. Notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The exponentially growing online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization <ref type="bibr" target="#b21">(Rush, Chopra, and Weston 2015a)</ref> which generates a shorter version of a given sentence while attempting to preserve its original meaning. This task is different from documentlevel summarization since it is hard to apply the common extractive techniques <ref type="bibr" target="#b17">(Over and Yen 2004)</ref>. Selecting existing sentences to form the sentence summary is impossible. Early studies on sentence summarization involve handcrafted rules <ref type="bibr" target="#b26">(Zajic et al. 2007</ref>), syntactic tree pruning <ref type="bibr" target="#b13">(Knight and Marcu 2002)</ref> and statistical machine translation techniques <ref type="bibr" target="#b4">(Banko, Mittal, and Witbrock 2000)</ref>. Recently, the application of the attentional sequence-tosequence (s2s) framework has attracted growing attention in this area <ref type="bibr" target="#b21">(Rush, Chopra, and Weston 2015a;</ref><ref type="bibr" target="#b9">Chopra et al. 2016;</ref><ref type="bibr" target="#b17">Nallapati et al. 2016)</ref>.</p><p>Source the repatriation of at least #,### bosnian moslems was postponed friday after the unhcr pulled out of the first joint scheme to return refugees to their homes in northwest bosnia . Target repatriation of bosnian moslems postponed s2s</p><p>bosnian moslems postponed after unhcr pulled out of bosnia <ref type="table">Table 1</ref>: An example of fake summaries generated by the state-of-the-art s2s model. "#" stands for a digit masked during preprocessing.</p><p>As we know, sentence summarization inevitably needs to fuse different parts in the source sentence and is abstractive. Consequently, the generated summaries often mismatch with the original relations and yield fake facts. Our preliminary study reveals that nearly 30% of the outputs from a state-of-the-art s2s system suffer from this problem. Previous researches are usually devoted to increasing summary informativeness. However, one of the most essential prerequisites for a practical abstractive summarization system is that the generated summaries must accord with the facts expressed in the source. We refer to this aspect as summary faithfulness in this paper. A fake summary may greatly misguide the comprehension of the original text. Look at an illustrative example of the generation result using the state-of-the-art s2s model <ref type="bibr" target="#b17">(Nallapati et al. 2016)</ref> in <ref type="table">Table 1</ref>. The actual subject of the verb "postponed" is "repatriation". Nevertheless, probably because the entity "bosnian moslems" is closer to "postponed" in the source sentence, the summarization system wrongly regards "bosnian moslems" as the subject and counterfeits a fact "bosnian moslems postponed". Meanwhile, the s2s system generates another fake fact: "unhcr pulled out of bosnia" and puts it into the summary. Consequently, although the informativeness (ROUGE-1 F1=0.57) and readability of this summary are high, its meaning departs far from the original. This sort of summaries is nearly useless in practice.</p><p>Since the fact fabrication is a serious problem, intuitively, encoding existing facts into the summarization system should be an ideal solution to avoid fake generation. To achieve this goal, the first step is to extract the facts from the source sentence. In the relatively mature task of Open Information Extraction (OpenIE) <ref type="bibr" target="#b3">(Banko et al. 2007</ref>), a fact is usually represented by a relation triple consisting of (subject; predicate; object). For example, given the source sentence in <ref type="table">Table 1</ref>, the popular OpenIE tool <ref type="bibr" target="#b0">(Angeli, Premkumar, and Manning 2015)</ref> generates two relation triples including (repatriation; was postponed; friday) and (unhcr; pulled out of; first joint scheme). Obviously, these triples can help rectify the mistakes made by the s2s model. However, the relation triples are not always extractable, e.g., from the imperative sentences. Hence, we further adopt a dependency parser and supplement with the (subject; predicate) and (predicate; object) tuples identified from the parse tree of the sentence. This is also inspired by the work of parse tree based sentence compression (e.g., <ref type="bibr" target="#b13">(Knight and Marcu 2002)</ref>). We represent a fact through merging words in a triple or tuples to form a short sentence, defined as a fact description. Fact descriptions actually form the skeletons of sentences. Thus we incorporate them as an additional input source text in our model. Our experiments reveal that the words in the extracted fact descriptions are 40% more likely to be included in the actual summaries than the entire words in the source sentences. That is, fact descriptions clearly provide the right guidance for summarization. Next, using both source sentence and fact descriptions as input, we extend the state-of-the-art attentional s2s model <ref type="bibr" target="#b17">(Nallapati et al. 2016)</ref> to fully leverage their information. Specially, we use two Recurrent Neural Network (RNN) encoders to read the sentence and fact descriptions in parallel. With respective attention mechanisms, our model computes the sentence and fact context vectors. It then merges the two vectors according to their relative reliabilities. Finally, a RNN decoder makes use of the integrated context to generate the summary wordby-word. Since our summarization system encodes facts to enhance faithfulness, we call it FTSum.</p><p>To verify the effectiveness of FTSum, we conduct extensive experiments on the Gigaword sentence summarization benchmark dataset <ref type="bibr" target="#b22">(Rush, Chopra, and Weston 2015b)</ref>. The results show that our model greatly reduces the fake summaries by 80% compared to the state-of-the-art s2s framework. Due to the compression nature of fact descriptions, the use of them also brings the significant improvement in terms of automatic informativeness evaluation. The contributions of our work can be summarized as follows:</p><p>• To the best of our knowledge, we are the first to explore the faithfulness problem of abstractive summarization. • We propose a dual-attention s2s model to push the generation to follow the original facts. • Since the fact descriptions often condense the meaning of the source sentence, they also bring the significant benefit to promote informativeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact Description Extraction</head><p>Based on our observation, 30% of summaries generated by state-of-the-art s2s models suffer from fact fabrication, such as the mismatch between the predicate and its subject or object. Therefore, we propose to explicitly encode existing fact descriptions into the model. We leverage popular tools of Open Information Extraction (OpenIE) and dependency   <ref type="table" target="#tab_1">Table 2</ref> is reserved. When different fact descriptions are extracted at the end, we use a special separator "|||" to concatenate them to accelerate the encoding process, which is explained by Eq. 2 and 3. OpenIE is able to give a complete description of the entity relations. However, it is worth noting that, the relation triples are not always extractable, e.g., from the imperative sentences. In fact, about 15% of the OpenIE outputs are empty on our dataset. These empty instances are likely to damage the robustness of our model. As observed, although the complete relation triples are not always available, the (subject; predicate) or (predicate; object) tuples are almost present in each sentence. Therefore, we leverage the dependency parser to dig out the appropriate tuples to supplement the fact descriptions. A dependency parser converts a sentence into the labeled (governor; dependent) tuples. We extract the predicate-related tuples according to the labels: nsubj, nsubjpass, csubj, csubjpass and dobj. To acquire more complete fact descriptions, we also reserve the important modifiers including the adjectival (amod), numeric (nummod) and noun compound (compound). We then merge the tuples containing the same words, and order words based on the original sentence to form the fact descriptions. Take the dependency tree in <ref type="figure">Fig. 1</ref> as an example. The output of OpenIE is empty for this sentence. Based on the dependency parser, we firstly filter the following predicate-related tuples: (prices; opened) (opened; tuesday) (dealers; said) and the modify-head tuples: (taiwan; price) (share; price) (lower; tuesday). These tuples are then merged to form two fact descriptions: taiwan share prices opened lower tuesday ||| dealers said.</p><p>In the experiments, we employ the popular NLP pipeline Stanford CoreNLP  to handle Ope-nIE and dependency parse at the same time. We combine the fact descriptions derived from both parts, and screen out the fact descriptions with the pattern "somebody said/declared/announced", which are usually meaningless Figure 1: A dependency tree example. The meaning of the dependency labels can be referred to <ref type="bibr" target="#b10">(De Marneffe and Manning 2008)</ref>. We extract the following two fact descriptions: taiwan share prices opened lower tuesday ||| dealers said and insignificant. Referring to the copy ratios in <ref type="table" target="#tab_3">Table 3</ref>, words in fact descriptions are 40% more likely to be used in the summary than the words in the original sentence. It indicates that fact descriptions truly condense the meaning of sentences to a large extent. The above statistics also supports the practice of dependency parse based compressive summarization <ref type="bibr" target="#b13">(Knight and Marcu 2002)</ref>. However, the length sum of extracted fact descriptions is shorter than the actual summary in 20% of the sentences, and 4% of the sentences even hold empty fact descriptions. In addition, from <ref type="table" target="#tab_3">Table 3</ref> we can find that on average one key source word is missing in the fact descriptions. Thus, without the source sentence, we cannot reply on fact descriptions alone to generate summaries.  Fact Aware Neural Summarization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Framework</head><p>As shown in <ref type="figure">Figure 2</ref>, our model consists of three modules including two encoders and a dual-attention decoder equipped with a context selection gate network. The sentence encoder reads the input words x = (x 1 , · · · x n ) and builds its corresponding representation (h x 1 , · · · h x n ). Likewise, the relation encoder converts the fact descriptions r = (r 1 , · · · r k ) into hidden states (h r 1 , · · · h r k ). With the respective attention mechanisms, our model computes the sentence and relation context vectors (c x t and c r t ) at each decoding time step t. The gate network is followed to merge the context vectors according to their relative associations with the current generation. The decoder produces summaries y = (y 1 , · · · y l ) word-by-word conditioned on the tailored context vector which embeds the semantics of both source sentence and fact descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoders</head><p>The input includes the source sentence x and the fact descriptions r. For each sequence, we employ the bidirectional Gated Recurrent Unit (BiGRU) encoder , to construct its semantic representation. Take the sentence</p><p>x as an example. The GRU at the time step i is defined as follows:</p><formula xml:id="formula_0">h i = GRU(x i , h i−1 )<label>(1)</label></formula><p>The BiGRU consists of a forward GRU and a backward GRU. Suppose the corresponding outputs are (</p><formula xml:id="formula_1">→ h 1 , · · · → h n ) and ( ← h 1 , · · · ← h n ),</formula><p>respectively. Then, the composite hidden state of a word is the concatenation of the two GRU repre-</p><formula xml:id="formula_2">sentations, i.e., h i = [ → h i ; ← h i ].</formula><p>For the relation sequence r, since it contains multiple independent fact descriptions, we introduce boundary indicators γ to separate their hidden states. Specially, the value of γ is defined as follows:</p><formula xml:id="formula_3">γ i = 0, r i is "|||" 1, otherwise<label>(2)</label></formula><p>Then, γ is used to reset the GRU state in Eq. 1:</p><formula xml:id="formula_4">h i = γ i h i<label>(3)</label></formula><p>In this way, all the fact descriptions will start with the same zero vector. In other words, they are encoded independently. Finally, both sentence hidden states {h x i } and relation hidden states {h r i } are fed to the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual-Attention Decoder</head><p>Previous s2s models have developed some task-specific modifications on the decoder, such as to incorporate the copying mechanism <ref type="bibr" target="#b11">(Gu et al. 2016</ref>) and coverage mechanism <ref type="bibr" target="#b23">(See, Liu, and Manning 2017)</ref>. As this paper focuses on the faithfulness problem, we use the most popular decoder, i.e., GRU with attentions . At each decoding time step t, GRU reads the previous output y t−1 and context vector c t−1 as inputs to compute new hidden state s t :</p><formula xml:id="formula_5">s t = GRU(y t−1 , c t , s t−1 )<label>(4)</label></formula><p>Since we have both sentence and relation representations as input, we develop two attentional layers to construct the overall context vector c t . For instance, the context representation of the sentence at time step t is computed as <ref type="bibr" target="#b15">(Luong, Pham, and Manning 2015)</ref>: where MLP stands for multi-layer perceptrons. The context vector of the relation c r can be computed similarly. We combine c x t and c r t to build the overall context vector c t . We explore two alternative combination approaches. The first one is called "FTSum c ", which simply concatenates two context vectors:</p><formula xml:id="formula_6">e x t,i = MLP(s t , h x i ) (5) α x t,i = exp(e x t,i ) j exp(e x t,j )<label>(6)</label></formula><formula xml:id="formula_7">c x t = i α x t,i h x i ,<label>(7)</label></formula><formula xml:id="formula_8">c t = [c x t ; c r t ]<label>(8)</label></formula><p>The other approach is denoted as "FTSum g ", where we also use MLP to build a gate network and combine context vectors with the weighted sum:</p><formula xml:id="formula_9">g t = MLP(c x t , c r t ) (9) c t = g t c x t + (1 − g t ) c r t ,<label>(10)</label></formula><p>where " " means the element-wise dot. Experiments show that FTSum g significantly outperforms FTSum c , and the gate values apparently reflect the relative reliability of sentence and fact descriptions. Finally, the softmax layer is introduced to generate the next word based on previous word y t−1 , context vector c t and current decoder state s t .</p><formula xml:id="formula_10">o t = W w [y t−1 ] + W c c t + W s s t (11) p(y t |y &lt;t ) = softmax(W o o t )<label>(12)</label></formula><p>where W . stands for a weight matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning</head><p>The learning goal is to maximize the estimated probability of the actual summary. We adopt the common negative loglikelihood (NLL) as the loss function.</p><formula xml:id="formula_11">J(θ) = − 1 |D| (x,r,y)∈D log(p(y|x, r)),<label>(13)</label></formula><p>where D denotes the training dataset and θ stands for the model parameters. We use Adam <ref type="bibr" target="#b12">(Kingma and Ba 2014)</ref> with mini-batches as the optimization algorithm. We set the learning rate α = 0.001 and the mini-batch size to 32. Similar to <ref type="bibr" target="#b27">(Zhou et al. 2017</ref>  learning rate if the cost increases for 10 consecutive validations. In addition, we apply gradient clipping <ref type="bibr" target="#b18">(Pascanu, Mikolov, and Bengio 2013)</ref> with range [−5, 5] during training to enhance the stability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>We conduct experiments on the Annotated English Gigaword corpus, as with <ref type="bibr" target="#b22">(Rush, Chopra, and Weston 2015b)</ref>. This parallel corpus is produced by pairing the first sentence in the news article and its headline as the summary with heuristic rules. The training and development datasets are built through the script 1 released by <ref type="bibr" target="#b22">(Rush, Chopra, and Weston 2015b)</ref>. The script also performs various basic text normalization, including tokenization, lower-casing, replacing all digit characters with #, and mask the words appearing less than 5 times with a UNK tag. It comes up with about 3.8M sentence-headline pairs as the training set and 189K pairs as the development set. We use the same Gigaword test set as <ref type="bibr" target="#b22">(Rush, Chopra, and Weston 2015b)</ref>. It contains 2000 sentence-headline pairs. Following (Rush, Chopra, and Weston 2015a), we remove pairs with empty titles, leading to slightly different accuracy compared with <ref type="bibr" target="#b22">(Rush, Chopra, and Weston 2015b)</ref>. The statistics of the Gigaword corpus is presented in <ref type="table" target="#tab_5">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metric</head><p>We adopt ROUGE <ref type="bibr" target="#b14">(Lin 2004)</ref> for automatic evaluation. ROUGE has been the standard evaluation metric for DUC shared tasks since 2004. It measures the quality of summary by computing overlapping lexical units between the candidate summary and actual summaries, such as unigram, bigram and longest common subsequence (LCS). Following the common practice, we report ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) F1 scores 2 in the following experiments. ROUGE-1 and ROUGE-2 mainly consider informativeness while ROUGE-L is supposed to be linked to readability. In addition, we manually inspect whether the generated summaries accord with the facts in the original sentences. We mark summaries into three categories: FAITH-FUL, FAKE and UNCLEAR. The last one refers to the case where a generated summary is too incomplete to judge its faithfulness, such as just producing a UNK tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Since the dataset has already masked infrequent words with the UNK tag, we reserve all the rest words in the training set. As a result, the sizes of source and target vocabularies are 120k and 69k, respectively. With reference to <ref type="bibr" target="#b17">(Nallapati et al. 2016)</ref>, we leverage the popular s2s framework dl4mt 3 as the starting point, and set the size of word embeddings to 200. We initialize word embeddings with GloVe (Pennington, Socher, and Manning 2014). All the GRU hidden state dimensions are fixed to 400. We use dropout <ref type="bibr" target="#b24">(Srivastava et al. 2014</ref>) with probability p = 0.5. With the decoder, we use the beam search of size 6 to generate the summary, and restrict the maximal length of a summary to 20 words. We find that the average system summary length from all our models (about 8.0 words) is very much consistent with that of the ground truth on the development set, without any special tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare our proposed model with the following six state-of-the-art baselines: ABS (Rush, Chopra, and Weston 2015a) used an attentive CNN encoder and NNLM decoder to summarize the sentence. ABS+ <ref type="bibr" target="#b21">(Rush, Chopra, and Weston 2015a)</ref> further tuned the ABS model with additional features to balance the abstractive and extractive tendency. RAS-Elman As the extension of the ABS model, it used a convolutional attention-based encoder and an RNN decoder <ref type="bibr" target="#b9">(Chopra et al. 2016</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Informativeness Evaluation</head><p>At first, look at the final cost values during training in Table 5. We can see that our model achieves the lowest perplexity compared against the state-of-the-art systems. It is also noted that, FTSum g largely outperforms FTSum c , which verifies the importance of context selection. The ROUGE F1 scores are then reported in <ref type="table" target="#tab_8">Table 6</ref>. Although the focus of our model focuses is to improve faithfulness, the ROUGE scores it receives are also much higher than the other methods. Note that, ABS+ and Feats2s have utilized a series of hand-crafted features, but our model is totally data-driven. Even though, our model surpasses Feats2s by 13% and ABS+ by 56% on ROUGE-2. When fact descriptions are ignored, our model is equivalent to the standard attentional s2s model s2s+att. Therefore, it is safe to conclude that, fact descriptions have significant contribute to the increase of ROUGE scores. One probable reason is that fact descriptions are much more informative than the original sentence, as shown in <ref type="table" target="#tab_3">Table 3</ref>. It also largely explains why FTSum g is superior to FTSum c . FTSum c treats the source sentence and relations equally, while FTSum g tells the fact descriptions are often more reliable, as discussed in more detail later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Faithfulness Evaluation</head><p>Next, we conduct manual evaluation to inspect the faithfulness of the generated summaries. Specially, we randomly select 100 sentences from the test set. Then, we classify the generated summaries as FAITHFUL, FAKE or UNCLEAR. For the sake of a complete comparison, we present the results of our system FTSum g together with the the attentional s2s model s2s+att. As shown in <ref type="table" target="#tab_10">Table 7</ref>, about 30% of the  s2s-att outputs gives disinformation. This number greatly reduces to 6% by our model. Nearly 90% of summaries generated by our model is faithful, which makes our model far more practical. We find that s2s-att tends to copy the words closer to the predicate and regard them as its subject and object. However, this is not always reasonable and thus it is actually counterfeiting messages. In comparison, the fact descriptions indeed designate the relations between a predicate and its subject and object. As a result, generation in line with the fact descriptions is usually able to keep the faithfulness.</p><p>We illustrate the examples of defective outputs in Table 8. As shown, att-s2s often attempts to fuse different parts in the source sentence to form the summary, no matter whether these phrases are relevant or not. For instance, att-s2s treats "bosnian moslems" as the subject of "postponed" and "bosnia" as the object of "pulled out of" in Example 1. By contract, since the fact description point out the actual subject and object, the output of our model is faithful. In fact, it is exactly the same as the target summary. In Example 2, neither att-s2s nor our model achieves satisfactory performance. att-s2s again mismatches the object while our model fails to produce a complete sentence. To take a closer look, we find the target summary of this sentence is somewhat strange -it merely focuses on the prepositional phrase (after taking a ## stoke...), rather than the main clause as usual. Since the main clause is hard to summarize and there is no high-quality fact description extracted, our model fails to give a complete summary.</p><p>It is also noteworthy that, given multiple long fact descriptions, the generation of our model sometimes traps into one item. For instance, there are two long fact descriptions in Example 3 and our model only utilizes the first one for generation. As a result, despite the high faithfulness, the informativeness is somewhat damaged. Therefore, it seems more reliable to introduce the coverage mechanism <ref type="bibr" target="#b23">(See, Liu, and Manning 2017)</ref> to handle the cases like this one. We leave it as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gate Analysis</head><p>As shown in <ref type="table" target="#tab_8">Table 6</ref>, FTSum g achieves much higher ROUGE scores than FTSum c . Now, we investigate what the gate network (Eq. 9) actually learns. The changes of the gate values on the development set during training are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. At the beginning, the average gate value exceeds 0.5, which means the generation is biased to the source sentence. As training proceeds, the model realizes that the fact descriptions are more reliable, resulting in a consecutive drop of the gate value. Finally, the average gate value is gradu- ally stabilized to 0.415. Interestingly, the ratio of sentence and relation gate values i.e., (1 − 0.415)/0.415 ≈ 1.41, is extremely close to the ratio of copying proportions shown in <ref type="table" target="#tab_3">Table 3</ref> i.e., 0.17/0.12 ≈ 1.42. It seems that our model predicts the copy proportion and normalizes it as the gate value. Then, look at the standard deviation of gates. To our surprise, its change is nearly anti-symmetric to the mean value. The final standard deviation reaches about 90% of the mean gate value. Thus, still many sentences can dominate the generation. This strange observation urges us to carefully check the summaries with top/bottom-100 gate values in the development set. We find 10 fact descriptions in the top-100 cases are empty, and nearly 60% contains the UNK tag. Our model believes these fact descriptions have not much worth to guide generation. Instead, there is no empty fact descriptions and only 1 UNK tag in the bottom 100 cases. Hence these fact descriptions are usually informative enough. In addition, we find the instances with the lowest gate values often hold the following (target summary; fact description) pair:</p><p>Target COUNTRY share prices close/open #.# percent higher/lower</p><p>Fact COUNTRY share prices slumped/dropped/rose #.# percent</p><p>The extracted fact description itself is already a proper summary. That is why fact descriptions are particularly preferred in generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Abstractive sentence summarization <ref type="bibr" target="#b9">(Chopra et al. 2016)</ref> aims to produce a shorter version of a given sentence while preserving its meaning. Unlike document-level summarization, it is impossible for this task to apply the common extractive techniques (e.g., <ref type="bibr" target="#b5">(Cao et al. 2015a;</ref>). Early studies for sentence summarization included rule-based methods <ref type="bibr" target="#b26">(Zajic et al. 2007</ref>), syntactic tree pruning <ref type="bibr" target="#b13">(Knight and Marcu 2002)</ref> and statistical machine translation techniques <ref type="bibr" target="#b4">(Banko, Mittal, and Witbrock 2000)</ref>.</p><p>Recently, the application of encoder-decoder structures has attracted growing attention in this area. <ref type="bibr" target="#b21">(Rush, Chopra, and Weston 2015a)</ref> proposed the ABS model which consisted of an attentive Convolutional Neural Network (CNN)  encoder and an neural network language model decoder. <ref type="bibr" target="#b9">(Chopra et al. 2016</ref>) extended their work by replacing the decoder with Recurrent Neural Network (RNN). <ref type="bibr" target="#b17">(Nallapati et al. 2016</ref>) followed this line and developed a full RNN based sequence-to-sequence (s2s) framework <ref type="bibr" target="#b25">(Sutskever, Vinyals, and Le 2014)</ref>. Experiments on the Gigaword test set <ref type="bibr" target="#b21">(Rush, Chopra, and Weston 2015a)</ref> show that the above models achieve state-of-the-art performance.</p><p>In addition to the direct application of the general s2s framework, researchers attempted to import various properties of summarization. For example, <ref type="bibr" target="#b17">(Nallapati et al. 2016)</ref> enriched the encoder with hand-crafted features such as named entities and POS tags. These features played important roles in traditional feature based summarization systems. <ref type="bibr" target="#b11">(Gu et al. 2016)</ref> found that a large proportion of words in the summary were copied from the source text. Therefore, they proposed CopyNet which considered the copying mechanism during generation. Later, <ref type="bibr" target="#b7">(Cao et al. 2017</ref>) extended this work by directly measuring the copying mechanism within neural attentions. Meanwhile, they modified the decoder to reflect the rewriting behavior in summarization. Recently, <ref type="bibr" target="#b23">(See, Liu, and Manning 2017)</ref> used the coverage mechanism to discourage repetition. There were also studies to modify the loss function to fit the evaluation metrics. For instance, <ref type="bibr" target="#b1">(Ayana, Liu, and Sun 2016)</ref> applied Minimum Risk Training strategy to maximize the ROUGE scores of generated summaries. <ref type="bibr" target="#b19">(Paulus, Xiong, and Socher 2017)</ref> used reinforcement learning algorithm to optimize a mixed objective function of likelihood and ROUGE scores.</p><p>Notably, previous researches usually focused on the improvement of summary informativeness. To the best of our knowledge, we are the first to explore the faithfulness problem of abstractive summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>This paper investigates the faithfulness problem in abstractive summarization. We employ popular OpenIE and dependency parse tools to extract fact descriptions in the source sentence. Then, we propose the dual-attention s2s framework to force the generation conditioned on both source sentence and the fact descriptions. Experiments on the Gigaword benchmark demonstrate that our model greatly reduce fake summaries by 80%. In addition, since the fact descriptions often condense the meaning of the sentence, the import of them also brings significant improvement on informativeness.</p><p>We believe our work can be extended in various aspects. On the one hand, we plan to improve our decoder with the copying mechanism and coverage mechanism, which is further adapted to summarization. On the other hand, we are interested in the automatic evaluation of summary faithfulness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Model framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Gates change during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Examples of OpenIE triples in different granular-</cell></row><row><cell>ities. We extract the following fact description: I saw cat</cell></row><row><cell>sitting on desk</cell></row><row><cell>parser for this purpose. OpenIE refers to the extraction of</cell></row><row><cell>entity relations from the open-domain text. In OpenIE, a fact</cell></row><row><cell>is typically interpreted as a relation triple consisting of (sub-</cell></row><row><cell>ject; predicate; object). We join all the items in a triple (i.e.,</cell></row><row><cell>subject + predicate + object) since it usually acts as a concise</cell></row><row><cell>sentence. An example of the OpenIE outputs is presented in</cell></row><row><cell>Table 2. As we can see, OpenIE may extract multiple triples</cell></row><row><cell>to reflect an identical fact in different granularities. In some</cell></row><row><cell>extreme cases, one relation can yield over 50 triple variants,</cell></row><row><cell>which brings high redundancy and burdens the computation</cell></row><row><cell>cost of the model. To balance redundancy and fact complete-</cell></row><row><cell>ness, we remove a relation triple if all its words are covered</cell></row><row><cell>by another one. For example, only the last fact description</cell></row><row><cell>(i.e., I saw cat sitting on desk) in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between source sentences and relations. AvgLen is the average number of tokens. Copy% means the proportion of source tokens can be found in the summary.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Data statistics for the English Gigaword. Avg- SourceLen is the average input sentence length and AvgTar- getLen is the average headline length.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Final perplexity on the development set. † indicates the value is cited from the corresponding paper. ABS+, Feats2s and Luong-NMT do not provide this value. RAS-Elman † 33.78 * 15.97 * 31.15 * Luong-NMT † 33.10 * 14.45 * 30.71 * s2s+att 34.23 * 15.52 * 31.57</figDesc><table><row><cell>Model</cell><cell>RG-1</cell><cell>RG-2</cell><cell>RG-L</cell></row><row><cell>ABS  †</cell><cell cols="3">29.55  34.13</cell></row><row><cell>FTSum g</cell><cell>37.27</cell><cell>17.65</cell><cell>34.24</cell></row></table><note>* 11.32* 26.42* ABS+† 29.78* 11.89* 26.97* Feats2s† 32.67* 15.59* 30.64** FTSumc 35.73* 16.02*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>ROUGE F1 performance. " * " indicates statistical significance of the corresponding model with respect to the baseline model on the 95% confidence interval in the official ROUGE script. RG refers to ROUGE for short.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Faithfulness performance on the test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Source the repatriation of at least #,### bosnian moslems was postponed friday after the unhcr pulled out of the first joint scheme to return refugees to their homes in northwest bosnia . Relations unhcr pulled out of first joint scheme ||| repatriation was postponed friday ||| unhcr return refugees to their homes Target repatriation of bosnian moslems postponed att-s2s (FAKE) bosnian moslems postponed after unhcr pulled out of bosnia FTSum (FAITHFUL) repatriation of bosnian moslems postponed Example 2 Source davis love said he was thinking of making the world cup of golf a full time occupation after taking a ## stroke lead over japan in the event with us partner fred couples here on saturday . Relations making world cup full time occupation ||| taking ## stroke lead Source the us space shuttle atlantis separated from the orbiting russian mir space station early saturday , after three days of test runs for life in a future space facility , nasa announced . Relations us space shuttle atlantis separated from orbiting russian mir space station ||| us space shuttle atlantis runs after three days of test for line in future space facility</figDesc><table><row><cell></cell><cell>Example 1</cell></row><row><cell>Target</cell><cell>americans lead UNK by ## strokes</cell></row><row><cell>att-s2s</cell><cell>(FAKE) davis love says he is thinking of the world cup</cell></row><row><cell>FTSum</cell><cell>(UNCLEAR) love in the world cup of golf</cell></row><row><cell></cell><cell>Example 3</cell></row><row><cell>Target</cell><cell>atlantis mir part ways after three-day space collaboration by emmanuel UNK</cell></row><row><cell>att-s2s</cell><cell>(UNCLEAR) space shuttle atlantis separated after # days of test runs for life</cell></row><row><cell>FTSum</cell><cell>(FAITHFUL) space shuttle atlantis separated from mir</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Examples of defective outputs. We use bold font to indicate the problematic parts.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebook/NAMAS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the ROUGE evaluation option: -m -n 2 -w 1.2 3 https://github.com/kyunghyuncho/ dl4mt-material</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work described in this paper was supported by Research Grants Council of Hong Kong (PolyU 152036/17E), National Natural Science Foundation of China (61672445 and 61572049) and The Hong Kong Polytechnic University (G-YBP6, 4-BCDV).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Leveraging linguistic structure for open domain information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural headline generation with minimum risk training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Ayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01904</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Headline generation based on statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="318" to="325" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ranking with recursive neural networks and its application to multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning summary prior representation for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: Short Papers</title>
		<meeting>ACL: Short Papers</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="829" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint copying and restricted generation for paraphrase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harvard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT16 93-98</title>
		<meeting>NAACL-HLT16 93-98</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stanford typed dependencies manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">, M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Summarization beyond sentence extraction: A probabilistic approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="107" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop</title>
		<meeting>the ACL Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: System Demonstrations</title>
		<meeting>ACL: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Introduction to duc-2001: an intrinsic evaluation of generic news text summarization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DUC</title>
		<meeting>DUC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Abstractive text summarization using sequence-to-sequence rnns and beyond</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<title level="m">A deep reinforced model for abstractive summarization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-candidate reduction: Sentence compression as a tool for document summarization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1549" to="1570" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07073</idno>
		<title level="m">Selective encoding for abstractive sentence summarization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
