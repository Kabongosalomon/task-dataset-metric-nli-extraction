<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
							<email>adosovitskiy@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a deep convolutional decoder architecture that can generate volumetric 3D outputs in a compute-and memory-efficient manner by using an octree representation. The network learns to predict both the structure of the octree, and the occupancy values of individual cells. This makes it a particularly valuable technique for generating 3D shapes. In contrast to standard decoders acting on regular voxel grids, the architecture does not have cubic complexity. This allows representing much higher resolution outputs with a limited memory budget. We demonstrate this in several application domains, including 3D convolutional autoencoders, generation of objects and whole scenes from high-level representations, and shape from a single image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Up-convolutional 1 decoder architectures have become a standard tool for tasks requiring image generation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21]</ref> or per-pixel prediction <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9]</ref>. They consist of a series of convolutional and up-convolutional (upsam-pling+convolution) layers operating on regular grids, with resolution gradually increasing towards the output of the network. The architecture is trivially generalized to volumetric data. However, because of cubic scaling of computational and memory requirements, training up-convolutional decoders becomes infeasible for high-resolution threedimensional outputs.</p><p>Poor scaling can be resolved by exploiting structure in the data. In many learning tasks, neighboring voxels on a voxel grid share the same state -for instance, if the voxel grid represents a binary occupancy map or a multi-class labeling of a three-dimensional object or a scene. In this case, data can be efficiently represented with octrees -data structures with adaptive cell size. Large regions of space sharing the same value can be represented with a single <ref type="bibr" target="#b0">1</ref> Also known as deconvolutional  <ref type="figure">Figure 1</ref>. The proposed OGN represents its volumetric output as an octree. Initially estimated rough low-resolution structure is gradually refined to a desired high resolution. At each level only a sparse set of spatial locations is predicted. This representation is significantly more efficient than a dense voxel grid and allows generating volumes as large as 512 3 voxels on a modern GPU in a single forward pass.</p><p>large cell of an octree, resulting in savings in computation and memory compared to a fine regular grid. At the same time, fine details are not lost and can still be represented by small cells of the octree. We present an octree generating network (OGN) -a convolutional decoder operating on octrees. The coarse structure of the network is illustrated in <ref type="figure">Figure 1</ref>. Similar to a usual up-convolutional decoder, the representation is gradually convolved with learned filters and up-sampled. The difference is that, starting from a certain layer in the network, dense regular grids are replaced by octrees. Therefore, the OGN predicts large uniform regions of the output space already at early decoding stages, saving the computation for the subsequent high-resolution layers. Only regions containing fine details are processed by these more computationally demanding layers.</p><p>In this paper, we focus on generating shapes represented as binary occupancy maps. We thoroughly compare OGNs to standard dense nets on three tasks: auto-encoding shapes, generating shapes from a high-level description, and reconstructing 3D objects from single images. OGNs yield the same accuracy as conventional dense decoders while consuming significantly less memory and being much faster at high resolutions. For the first time, we can generate shapes of resolution as large as 512 3 voxels in a single forward pass. Our OGN implementation is publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The majority of deep learning approaches generate volumetric data based on convolutional networks with feature maps and outputs represented as voxel grids. Applications include single-and multi-view 3D object reconstruction trained in supervised <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18]</ref> and unsupervised <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref> ways, probabilistic generative modeling of 3D shapes <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30]</ref>, semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> and shape deformation <ref type="bibr" target="#b39">[40]</ref>. A fundamental limitation of these approaches is the low resolution of the output. Memory and computational requirements of approaches based on the voxel grid representation scale cubically with the output size. Thus, training networks with resolutions higher than 64 3 comes with memory issues on the GPU or requires other measures to save memory, such as reducing the batch size or generating the volume part-by-part. Moreover, with growing resolution, training times become prohibitively slow.</p><p>Computational limitations of the voxel grid representation led to research on alternative representations of volumetric data in deep learning. Tatarchenko et al. <ref type="bibr" target="#b32">[33]</ref> predict RGB images and depth maps for multiple views of an object, and fuse those into a single 3D model. This approach is not trainable end to end because of the postprocessing fusion step, and is not applicable to objects with strong self-occlusion. Sinha et al. <ref type="bibr" target="#b30">[31]</ref> convert shapes into two-dimensional geometry images and process those with conventional CNNs -an approach only applicable to certain classes of topologies. Networks producing point clouds have been applied to object generation <ref type="bibr" target="#b10">[11]</ref> and semantic segmentation <ref type="bibr" target="#b26">[27]</ref>. By now, these architectures have been demonstrated to generate relatively low-resolution outputs. Scaling these networks to higher resolution is yet to be explored. Tulsiani et al. <ref type="bibr" target="#b33">[34]</ref> assemble objects from volumetric primitives. Yi et al. <ref type="bibr" target="#b38">[39]</ref> adapt the idea of graph convolutions in the spectral domain to semantic segmentation of 3D shapes. Their approach requires all samples to have aligned eigenbasis functions, thus limiting possible application domains.</p><p>Promising alternative representations that are not directly applicable to generating 3D outputs have been explored on analysis tasks. Masci et al. <ref type="bibr" target="#b23">[24]</ref> proposed geodesic CNNs for extracting local features in non-Euclidean domains. Our approach is largely inspired by Graham's sparse convolutional networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, which enable efficient shape analysis by storing a sparse set of non-trivial features instead of dense feature maps. The 2 https://github.com/lmb-freiburg/ogn OGN essentially solves the inverse problem by predicting which regions of the output contain high-resolution information and by restricting extensive calculations only to those regions.</p><p>The recent pre-print by Riegler et al. <ref type="bibr" target="#b28">[29]</ref> builds on the same general idea as our work: designing convolutional networks that operate on octrees instead of voxel grids. However, the implementation and the application range of the method is very different from our work. When generating an octree, Riegler et al. assume the octree structure to be known at test time. This is the case, for example, in semantic segmentation, where the structure of the output octree can be set to be identical to that of the input. However, in many important scenarios -any kind of 3D reconstruction, shape modeling, RGB-D fusion, superresolutionthe structure of the octree is not known in advance and must be predicted. The method of Riegler et al. is not applicable in these cases. Moreover, the OGN is more flexible in that it allows for octrees with an arbitrary number of levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Octrees</head><p>An octree <ref type="bibr" target="#b24">[25]</ref> is a 3D grid structure with adaptive cell size, which allows for lossless reduction of memory consumption compared to a regular voxel grid. Octrees have a long history in classical 3D reconstruction and depth map fusion <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref>. A function defined on a voxel grid can be converted into a function defined on an octree. This can be done by starting from a single cell representing the entire space and recursively partitioning cells into eight octants. If every voxel within a cell has the same function value, this cell is not subdivided and becomes a leaf of the tree. The set of cells at a certain resolution is referred to as an octree level. The recursive subdivision process can also be started not from the whole volume, but from some initial coarse resolution. Then the maximal octree cell size is given by this initial resolution. The most straightforward way of implementing an octree is to store in each cell pointers to its children. In this case, the time to access an element scales linearly with the tree's depth, which can become costly at high resolutions. We use a more efficient implementation that exploits hash tables. An octree cell with spatial coordinates x = (x, y, z) at level l is represented as an index-value pair (m, v), where v can be any kind of discrete or continuous signal. m is calculated from (x, l) using Z-order curves <ref type="bibr" target="#b13">[14]</ref> </p><formula xml:id="formula_0">m = Z(x, l),<label>(1)</label></formula><p>which is a computationally cheap transformation implemented using bit shifts. An octree O is, hence, a set of all pairs</p><formula xml:id="formula_1">O = {(m, v)}.<label>(2)</label></formula><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>...</head><p>OGNConv (one or more)</p><p>...  <ref type="figure">Figure 2</ref>. Single block of an OGN illustrated as 2D quadtree for simplicity. After convolving features F l−1 of the previous block with weight filters, we directly predict the occupancy values of cells at level l using 1 3 convolutions. Features corresponding to "filled" and "empty" cells are no longer needed and thus not propagated, which yields F l as the final output of this block.</p><p>Storing this set as a hash table allows for constant-time element access.</p><p>When training networks, we will need to compare two different octrees O 1 and O 2 , i.e. for each cell (x, l) from O 1 , query the corresponding signal value v in O 2 . Since different octrees have different structure, two situations are possible. If Z(x, k) is stored at a level k in O 2 , which is the same or lower than l, the signal value of this cell can be uniquely determined. If Z(x, k) is stored at one of the later levels, the cell is subdivided in O 2 , and the value of the whole cell is not defined. To formalize this, we introduce a function f for querying the signal value of an arbitrary cell with index m = Z(x, l) from octree O:</p><formula xml:id="formula_2">f (m, O) = v, if ∃k ≤ l : (Z(x, k), v) ∈ O ∅, otherwise ,<label>(3)</label></formula><p>where ∅ denotes an unavailable value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Octree Generating Networks</head><p>An Octree Generating Network (OGN) is a convolutional decoder that yields an octree as output: both the structure, i.e. which cells should be subdivided, and the signal value of each cell. In this work we concentrate on binary occupancy values v ∈ {0, 1}, but the proposed framework can be easily extended to support arbitrary signals. As shown in <ref type="figure">Figure 1</ref>, an OGN consists of a block operating on dense regular grids, followed by an arbitrary number of hash-table-based octree blocks.</p><p>The dense block is a set of conventional 3D convolutional and up-convolutional layers producing a feature map of size d 1 × d 2 × d 3 × c as output, where {d i } are the spatial dimension and c is the number of channels.</p><p>From here on, the representation is processed by our custom layers operating on octrees. The regular-grid-based feature map produced by the dense block is converted to a set of index-value pairs stored as a hash table (with values being feature vectors), and is further processed in this format. We organize octree-based layers in blocks, each responsible for predicting the structure and the content of a single level of the generated octree. <ref type="figure">Figure 2</ref> illustrates the functioning of a single such block that predicts level l of an octree. For the sake of illustration, we replaced three-dimensional octrees by two-dimensional quadtrees. Feature maps in <ref type="figure">Figure 2</ref> are shown as dense arrays only for simplicity; in fact the green cells are stored in hash maps, and the white cells are not stored at all. We now give a high-level overview of the block and then describe its components in more detail.</p><p>Input to the block is a sparse hash-table-based convolutional feature map F l−1 of resolution (d 1 ·2 l−1 , d 2 ·2 l−1 , d 3 · 2 l−1 ) produced by the previous block. First this feature map is processed with a series of custom convolutional layers and one up-convolutional layer with stride 2, all followed by non-linearities.</p><p>This yields a new feature mapF l of resolution (d 1 ·2 l , d 2 · 2 l , d 3 · 2 l ). Based on this feature map, we directly predict the content of level l. For each cell, there is a two-fold decision to be made: should it be kept at level l, and if yes, what should be the signal value in this cell? In our case making this decision can be formulated as classifying the cell as being in one of three states: "empty", "filled" or "mixed". These states correspond to the outputs of statequerying function f from eq. (3), with "empty" and "filled" being the signal values v, and "mixed" being the state where the value is not determined. We make this prediction using a convolutional layer with 1 3 filters followed by a three-way softmax. This classifier is trained in a supervised manner with targets provided by the ground truth octree.</p><p>Finally, in case the output resolution has not been reached, features fromF l that correspond to "mixed" cells are propagated to the next layer <ref type="bibr" target="#b2">3</ref> and serve as an input feature map F l to the next block.</p><p>In the following subsections, we describe the components of a single octree block in more detail: the octreebased convolution, the loss function, and the feature propagation mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Convolution</head><p>We implemented a custom convolutional layer OGN-Conv, which operates on feature maps represented as hash tables instead of usual dense arrays. Our implementation supports strided convolutions and up-convolutions with arbitrary filter sizes. It is based on representing convolution as a single matrix multiplication, similar to standard caffe <ref type="bibr" target="#b18">[19]</ref> code for dense convolutions.</p><p>In the dense case, the feature tensor is converted to a matrix with the im2col operation, then multiplied with the weight matrix of the layer, and the result is converted back into a dense feature tensor using the col2im operation. In OGN, instead of storing full dense feature tensors, only a sparse set of relevant features is stored at each layer. These features are stored in a hash table, and we implemented custom operations to convert a hash table to a feature matrix and back. The resulting matrices are much smaller than those in the dense case. Convolution then amounts to multiplying the feature matrix by the weight matrix. Matrix multiplication is executed on GPU with standard optimized functions, and our conversion routines currently run on CPU. Even with this suboptimal CPU implementation, computation times are comparable to those of usual dense convolutions at 32 3 voxel resolution. At higher resolutions, OGNConv is much faster than dense convolutions (see section 5.2).</p><p>Unlike convolutions on regular grids, OGN convolutions are not shift invariant by design. This is studied in Section E of the Appendix. <ref type="bibr" target="#b2">3</ref> Additional neighboring cells may have to be propagated if needed by subsequent convolutional layers. This is described in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Loss</head><p>The classifier at level l of the octree outputs the probabilities of each cell from this level being "empty", "filled" or "mixed", that is, a three-component prediction vector p m = (p 0 m , p 1 m , p 2 m ) for cell with index m. We minimize the cross-entropy between the network predictions and the cell states of the ground truth octree O GT , averaged over the set M l of cells predicted at layer l:</p><formula xml:id="formula_3">L l = 1 |M l | m∈M l 2 i=0 h i (f (m, O GT )) log p i m ,<label>(4)</label></formula><p>where function h yields a one-hot encoding (h 0 , h 1 , h 2 ) of the cell state value returned by f from eq. (3). Loss computations are encapsulated in our custom OGNLoss layer. The final OGN objective is calculated as a sum of loss values from all octree levels</p><formula xml:id="formula_4">L = L l=1 L l .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature propagation</head><p>At the end of each octree block there is an OGNProp layer that propagates to the next octree block features from cells in the "mixed" state, as well as from neighboring cells if needed to compute subsequent convolutions. Information about the cell state can either be taken from the ground truth octree, or from the network prediction. This spawns two possible propagation modes: using the known tree structure (Prop-known) and using the predicted tree structure (Proppred). Section 4.4 describes use cases for these two modes.</p><p>The set of features to be propagated depends on the kernel size in subsequent OGNConv layers. The example illustrated in <ref type="figure">Figure 2</ref> only holds for 2 3 up-convolutions which do not require any neighboring elements to be computed. To use larger convolutional filters or multiple convolutional layers, we must propagate not only the features of the "mixed" cells, but also the features of the neighboring cells required for computing the convolution at the locations of the "mixed" cells. The size of the required neighborhood is computed based on the network architecture, before the training starts. Details are provided in Section C of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training and testing</head><p>The OGN decoder is end-to-end trainable using standard backpropagation. The only subtlety is in selecting the feature propagation modes during training and testing. At training time the octree structure of the training samples is always available, and therefore the Prop-known mode can be used. At test time, the octree structure may or may not be available. We have developed two training regimes for these two cases.</p><p>If the tree structure is available at test time, we simply train the network with Prop-known and test it the same way. This regime is applicable for tasks like semantic segmentation, or, more generally, per-voxel prediction tasks, where the structure of the output is exactly the same as the structure of the input.</p><p>If the tree structure is not available at test time, we start by training the network with Prop-known, and then finetune it with Prop-pred. This regime is applicable to any task with volumetric output.</p><p>We have also tested other regimes of combining Proppred and Prop-known and found those to perform worse than the two described variants. This is discussed in more detail in Section B of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In our experiments we verified that the OGN architecture performs on par with the standard dense voxel grid representation, while requiring significantly less memory and computation, particularly at high resolutions. The focus of the experiments is on showcasing the capabilities of the proposed architecture. How to fully exploit the new architecture in practical applications is a separate problem that is left to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup</head><p>For all OGN decoders used in our evaluations, we followed the same design pattern: 1 or 2 up-convolutional layers interleaved with a convolutional layer in the dense block, followed by multiple octree blocks depending on the output resolution. In the octree blocks we used 2 3 up-convolutions. We also evaluated two other architecture variants, presented in section 5.3.1. ReLU non-linearities were applied after each (up-)convolutional layer. The number of channels in the up-convolutional layers of the octree blocks was set to 32 in the outermost layer, and was increased by 16 in each preceding octree block. The exact network architectures used in individual experiments are shown in Section F of the Appendix.</p><p>The networks were trained using ADAM <ref type="bibr" target="#b22">[23]</ref> with initial learning rate 0.001, β 1 = 0.9, β 2 = 0.999. The learning rate was decreased by a factor of 10 after 30K and 70K iterations. We did not apply any additional regularization.</p><p>For quantitative evaluations, we converted the resulting octrees back to regular voxel grids, and computed the Intersection over Union (IoU) measure between the ground truth model and the predicted model. To quantify the importance of high-resolution representations, in some experiments we upsampled low-resolution network predictions to high-resolution ground truth using trilinear interpolation, and later binarization with a threshold of 0.5. We explicitly specify the ground truth resolution in all experiments where this was done.</p><p>If not indicated otherwise, the results were obtained in the Prop-pred mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>In our evaluations we used three datasets:</p><p>ShapeNet-all Approximately 50.000 CAD models from 13 main categories of the ShapeNet dataset <ref type="bibr" target="#b3">[4]</ref>, used by Choy et al. <ref type="bibr" target="#b5">[6]</ref>. We also used the renderings provided by Choy et al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>ShapeNet-cars A subset of ShapeNet-all consisting of 7497 car models.</p><p>BlendSwap A dataset of 4 scenes we manually collected from blendswap.com, a website containing a large collection of Blender models.</p><p>All datasets were voxelized in multiple resolutions from 32 3 to 512 3 using the binvox 4 tool, and were converted into octrees. We set the interior parts of individual objects to be filled, and the exterior to be empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Computational efficiency</head><p>We start by empirically demonstrating that OGNs can be used at high resolutions when the voxel grid representation becomes impractical both because of the memory requirements and the runtime.</p><p>The number of elements in a voxel grid is uniquely determined by its resolution, and scales cubically as the latter increases. The number of elements in an octree depends on the data, leading to variable scaling rates: from constant for cubic objects aligned with the grid, to cubic for pathological shapes such as a three-dimensional checkerboard. In practice, octrees corresponding to real-world objects and scenes scale approximately quadratically, since they represent smooth two-dimensional surfaces in a threedimensional space.   We empirically compare the runtime and memory consumption values for a dense network and OGN, for varying output resolution. Architectures of the networks are the same as used in Section 5.4 -three fully connected layers followed by an up-convolutional decoder. We performed the measurements on an NVidia TitanX Maxwell GPU, with 12Gb of memory. To provide actual measurements for dense networks at the largest possible resolution, we performed the comparison with batch size 1. The 512 3 dense network does not fit into memory even with batch size 1, so we extrapolated the numbers by fitting cubic curves. <ref type="figure" target="#fig_2">Figure 3</ref> and <ref type="table" target="#tab_1">Table 1</ref> show the results of the comparison. The OGN is roughly as efficient as its dense counterpart for low resolutions, but as the resolution grows, it gets drastically faster and consumes far less memory. At 512 3 voxel resolution, the OGN consumes almost two orders of magnitude less memory and runs 20 times faster. In Section A of the Appendix we provide a more detailed analysis and explicitly demonstrate the near-cubic scaling of dense networks against the near-quadratic scaling of OGNs.</p><p>To put these numbers into perspective, training OGN at 256 3 voxel output resolution takes approximately 5 days. Estimated training time of its dense counterpart would be almost a month. Even if the 512 3 voxel dense network would fit into memory, it would take many months to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Autoencoders</head><p>Autoencoders and their variants are commonly used for representation learning from volumetric data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>. Therefore, we start by comparing the representational power of the OGN to that of dense voxel grid networks on the task of auto-encoding volumetric shapes.</p><p>We used the decoder architecture described in section 5.1 both for the OGN and the dense baseline. The architecture of the encoder is symmetric to the decoder. Both encoders operate on a dense voxel grid representation <ref type="bibr" target="#b4">5</ref> .</p><p>We trained the autoencoders on the ShapeNet-cars dataset in two resolutions: 32 3 and 64 3 . We used 80% of the data for training, and 20% for testing. Quantitative results are summarized in <ref type="table">Table 2</ref>. With predicted octree structure, there is no significant difference in performance between the OGN and the dense baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Flexibility of architecture choice</head><p>To show that OGNs are not limited to up-convolutional layers with 2 3 filters, we evaluated two alternative 64 3 OGN auto-encoders: one with 4 3 up-convolutions and one with 2 3 up-convolutions interleaved with 3 3 convolutions. The results are summarized in <ref type="table" target="#tab_5">Table 7</ref>. There is no significant difference between the architectures for this task. With larger filters, the network is roughly twice slower in our current implementation, so we used 2 3 filters in all further experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Using known structure</head><p>Interestingly, OGN with known tree structure outperforms the network based on a dense voxel grid, both qualitatively and quantitatively. An example of this effect can be seen in <ref type="figure">Figure 4</ref>: the dense autoencoder and our autoencoder with predicted propagation struggle with properly reconstructing the spoiler of the car. Intuitively, the known tree structure provides additional shape information to the decoder, thus simplifying the learning problem. In the autoencoder scenario, however, this may be undesirable if one aims to encode all information about a shape in a latent vector. In tasks like semantic segmentation, the input octree structure could help introduce shape features implicitly in the learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">3D shape from high-level information</head><p>We trained multiple OGNs for generating shapes from high-level parameters similar to Dosovitskiy et al. <ref type="bibr" target="#b9">[10]</ref>. In all cases the input of the network is a one-hot encoded object ID, and the output is an octree with the object shape.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">ShapeNet-cars</head><p>First, we trained on the whole ShapeNet-cars dataset in three resolutions: 64 3 , 128 3 and 256 3 . Example outputs are shown in <ref type="figure">Figure 5</ref> and quantitative results are presented in <ref type="table">Table 4</ref>. Similar to the two-dimensional case <ref type="bibr" target="#b9">[10]</ref>, the outputs are accurate in the overall shape, but lack some fine details. This is not due to the missing resolution, but due to general limitations of the training data and the learning task. <ref type="table">Table 4</ref> reveals that a resolution of 128 3 allows the reconstruction of a more accurate shape with more details than a resolution of 64 3 . At an even higher resolution of 256 3 , the overall performance decreased again. Even though the higher-resolution network is architecturally capable of performing better, it is not guaranteed to train better. Noisy gradients from outer high-resolution layers may hamper learning of deeper layers, resulting in an overall decline in performance. This problem is orthogonal to the issue of designing computationally efficient architectures, which we aim to solve in this paper. We further discuss this in the Appendix.</p><p>Notably, the network does not only learn to generate objects from the training dataset, but it can also generalize to unseen models. We demonstrate this by interpolating be-tween pairs of one-hot input ID vectors. <ref type="figure">Figure 6</ref> shows that for all intermediate input values the network produces consistent output cars, with the style being smoothly changed between the two training points.  <ref type="figure">Figure 6</ref>. Our networks can generate previously unseen cars by interpolating between the dataset points, which demonstrates their generalization capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">BlendSwap</head><p>To additionally showcase the benefit of using higher resolutions, we trained OGNs to fit the BlendSwap dataset containing 4 whole scenes. In contrast to the ShapeNet-cars dataset, such amount of training data does not allow for any generalization. The experiment aims to show that OGNs provide sufficient resolution to represent such high-fidelity shape data. <ref type="figure">Figure 7</ref> shows the generated scenes. In both examples, 64 3 and 128 3 resolutions are inadequate for representing the details. For the bottom scene, even the 256 3 resolution still struggles with fine-grained details. This example demonstrates that tasks like end-to-end learning of scene reconstruction requires high-resolution representations, and the OGN is an architecture that can provide such resolutions.</p><p>These qualitative observations are confirmed quantitatively in <ref type="table">Table 4</ref>. Higher output resolutions allow for more accurate reconstruction of the samples in the dataset. More results for this experiment are shown in Section D of the Appendix, and the accompanying video 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Single-image 3D reconstruction</head><p>In this experiment we trained networks with our OGN decoder on the task of single-view 3D reconstruction. To demonstrate that our dense voxel grid baseline, as already used in the autoencoder experiment, is a strong baseline, we compare to the approach by Choy et al. <ref type="bibr" target="#b5">[6]</ref>. This approach operates on 32 3 voxel grids, and we adopt this resolution for our first experiment. To ensure a fair comparison, we trained networks on ShapeNet-all, the exact dataset used by Choy et al. <ref type="bibr" target="#b5">[6]</ref>. Following the same dataset splitting strategy, we used 80% of the data for training, and 20% for testing. As a baseline, we trained a network with a dense 256 3 512 3 GT 512 3 <ref type="figure">Figure 7</ref>. OGN is used to reproduce large-scale scenes from the dataset, where high resolution is crucial to generate fine-grained structures. decoder which had the same configuration as our OGN decoder. <ref type="table" target="#tab_4">Table 5</ref> shows that compared to single-view reconstructions from <ref type="bibr" target="#b5">[6]</ref>, both the OGN and the baseline dense network compare favorably for most of the classes. In conclusion, the OGN is competitive with voxel-grid-based networks on the complex task of single-image class-specific 3D reconstruction. We also evaluated the effect of resolution on the ShapeNet-cars dataset. <ref type="figure">Figure 8</ref> shows that OGNs learned to infer the 3D shapes of cars in all cases, and that highresolution predictions are clearly better than the 32 3 models commonly used so far. This is backed up by quantitative results shown in <ref type="table">Table 6</ref>: 32 3 results are significantly worse than the rest. At 256 3 performance drops again for the same reasons as in the decoder experiment in section 5.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have presented a novel convolutional decoder architecture for generating high-resolution 3D outputs represented as octrees. We have demonstrated that this architecture is flexible in terms of the exact layer configuration, and  <ref type="table">Table 6</ref>. Single-image 3D reconstruction results on ShapeNet-cars. Low-resolution predictions are upsampled to 256 3 . Commonly used 32 3 models are significantly worse than the rest.</p><p>that it provides the same accuracy as dense voxel grids in low resolution. At the same time, it scales much better to higher resolutions, both in terms of memory and runtime.</p><p>This architecture enables end-to-end deep learning to be applied to tasks that appeared unfeasible before. In particular, learning tasks that involve 3D shapes, such as 3D object and scene reconstruction, are likely to benefit from it.</p><p>While in this paper we have focused on shapes and binary occupancy maps, it is straightforward to extend the framework to multi-dimensional outputs attached to the octree structure; for example, the output of the network could be a textured shape or a signed distance function. This will allow for an even wider range of applications.</p><p>In the main paper we have shown that with a practical architecture our networks scale much better than their dense counterparts both in terms of memory consumption and computation time. The numbers were obtained for the "houses" scene from the BlendSwap dataset.   In order to further study this matter, we have designed a set of slim decoder networks that fit on a GPU in every resolution, including 512 3 , both with an OGN and a dense representation. The architectures of those networks are similar to those from <ref type="table" target="#tab_1">Table 13</ref>, but with only 1 channel in every convolutional layer, and a single fully-connected layer with 64 units in the encoder. The resulting measurements are shown in <ref type="figure" target="#fig_6">Figure 9</ref> for memory consumption and <ref type="figure" target="#fig_8">Figure 10</ref> for runtime. To precisely quantify the scaling, we subtracted the constant amount of memory reserved on a GPU by caffe (190 MB) from all numbers.</p><p>Both plots are displayed in the log-log scale, i.e., functions from the family y = ax k are straight lines. The slope of this line is determined by the exponent k, and the vertical shift by the coefficient a. In this experiment we are mainly interested in the slope, that is, how do the approaches scale with increasing output resolution. As a reference, we show dashed lines corresponding to perfect cubic and perfect quadratic scaling.</p><p>Starting from 64 3 voxel resolution both the runtime and the memory consumption scale almost cubically in case of dense networks. For this particular example, OGN scales even better than quadratically, but in general scaling of the octree-based representation depends on the specific data it is applied to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Train/test modes</head><p>In Section 4.4 of the main paper, we described how we use the two propagation modes (Prop-known and Proppred) during training and testing. Here we motivate the proposed regimes, and show additional results with other combinations of propagation modes.</p><p>When the structure of the output tree is not known at test time, we train the networks until convergence with Propknown, and then additionally fine-tune with Prop-pred -line 4 in <ref type="table" target="#tab_5">Table 7</ref>. Without this fine-tuning step (line 2), there is a decrease in performance, which is more significant when using larger convolutional filters. Intuitively, this happens because the network has never seen erroneous propagations during training, and does not now how to deal with them at test time.</p><p>When the structure of the output is known at test time, the best strategy is to simply train in Prop-known, and test the same way (line 1). Additional fine-tuning in the Proppred mode slightly hurts performance in this case (line 3). The overall conclusion is not surprising: the best results are obtained when training networks in the same propagation modes, in which they are later tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature propagation</head><p>In the main paper we mentioned that the number of features propagated by an OGNProp layer depends on the sizes of the convolutional filters in all subsequent blocks. In case of 2 3 up-convolutions with stride 2, which were used in most of our experiments, no neighboring features need to be propagated. This situation is illustrated in <ref type="figure" target="#fig_9">Figure 11</ref>-A in a one-dimensional case. Circles correspond to cells of an octree. The green cell in the input is the only one for which the value was predicted to be "mixed". Links between the circles indicate which features of the input are required to compute the result of the operation (convolution or up-convolution) for the corresponding output cell. In this case, we can see that the output cells in the next level are only affected by their parent cell from the previous level. A more general situation is shown in <ref type="figure" target="#fig_9">Figure 11</ref>-B. The input is processed with an up-convolutional layer with 4 3 filters and stride 2, which is followed by a convolutional layer with 3 3 filters and stride 1. Again, only one cell was predicted to be "mixed", but in order to perform convolutions and up-convolutions in subsequent layers, we additionally must propagate some of its neighbors (marked red). Therefore, with this particular filter configuration, two cells in the output are affected by four cells in the input.</p><p>Generally, the number of features that should be propagated by each OGNProp layer is automatically calculated based on the network architecture before starting the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. 3D shape from high-level information: additional experiments D.1. MPI-FAUST</head><p>To additionally showcase the benefit of using higher resolutions, we trained OGNs to fit the MPI-FAUST dataset <ref type="bibr" target="#b0">[1]</ref>. It contains 300 high-resolution scans of human bodies of 10 different people in 30 different poses. Same as with the BlendSwap, the trained networks cannot generalize to new samples due to the low amount of training data. <ref type="figure" target="#fig_11">Figure 12</ref> and <ref type="table">Table 8</ref> demonstrate qualitative and quantitative results respectively. Human models from MPI-FAUST include finer details than cars from ShapeNet, and therefore benefit from the higher resolution.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Fitting reduced ShapeNet-cars</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full</head><p>Subset GT 256 3 <ref type="figure" target="#fig_2">Figure 13</ref>. When training on a subset of the Shapenet-cars datset, higher resolution models contain more details.</p><p>just 500 first models from the dataset. Quantitative results for different resolutions, along with the results for the full dataset, are shown in <ref type="table">Table 9</ref>. Interestingly, when training on the reduced dataset, high resolution is beneficial. This is further supported by examples shown in <ref type="figure" target="#fig_2">Figure 13</ref> -when training on the reduced dataset, the higher-resolution model contain more fine details. Overall, these results support our hypothesis that the performance drop at higher resolution is not due to the OGN architecture, but due to the difficulty of fitting a large dataset at high resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Shift invariance</head><p>The convolution operation on a voxel grid is perfectly shift invariant by design. This is no longer true for convolutions on octrees: a shift by a single pixel in the original voxel grid can change the structure of the octree significantly. To study the effect of shifts, we trained two fully convolutional autoencoders -one with an OGN decoder, and one with a dense decoder -on 64 3 models, with lowest feature map resolution 4 3 (so the networks should be perfectly invariant to shifts of 16 voxels). Both were trained on non-shifted Shapenet-Cars, and tested in the Prop-pred mode on models shifted by a different number of voxels along the z-axis. The results are summarized in <ref type="table" target="#tab_1">Table 10</ref>. There is no significant difference between OGN and the dense network. A likely reason is that different training models have different octree structures, which acts as an implicit regularizer. The network learns the shape, but remains robust to the exact octree structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Network architectures</head><p>In this section, we provide the exact network architectures used in the experimental evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Autoencoders</head><p>The architectures of OGN autoencoders are summarized in <ref type="table" target="#tab_1">Table 12</ref>. For the dense baselines, we used the same layer configurations with usual convolutions instead of OGN-Conv, and predictions being made only after the last layer of the network. All networks were trained with batch size 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. 3D shape from high-level information</head><p>OGN decoders used on the Shapenet-cars dataset are shown in <ref type="table" target="#tab_1">Table 13</ref>. Encoders consisted of three fullyconnected layers, with output size of the last encoder layer being identical to the input size of the corresponding decoder.</p><p>For FAUST and BlendSwap the 256 3 output octrees had four levels, not five like those in <ref type="table" target="#tab_1">Table 13</ref>. Thus, the dense block had an additional deconvolution-convolution layer pair instead of one octree block. The 512 3 decoder on BlendSwap had one extra octree block with 32 output channels.</p><p>All 64 3 and 128 3 networks were trained with batch size 16, 256 3 -with batch size 4, 512 3 -with batch size 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Single-image 3D reconstruction</head><p>In this experiment we again used decoder architectures shown in <ref type="table" target="#tab_1">Table 13</ref>. The architecture of the convolutional encoder is shown in <ref type="table" target="#tab_1">Table 11</ref>. The number of channels in the last encoder layer was set identical to the number of input channels of the corresponding decoder.    <ref type="table" target="#tab_1">Table 13</ref>. OGN decoder architectures used in shape from ID, and single-image 3D reconstruction experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Memory consumption (left) and iteration time (right) of OGN and a dense network at different output resolutions. Forward and backward pass, batch size 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Figure 5 .</head><label>35</label><figDesc>Training samples from the ShapeNet-cars dataset generated by our networks. Cells at different octree levels vary in size and are displayed in different shades of gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 Figure 8 .</head><label>38</label><figDesc>Single-image 3D reconstruction on the ShapeNet-cars dataset using OGN in different resolutions. 3D 0.641 0.771 0.782 0.766</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Memory consumption for very slim networks, forward and backward pass, batch size 1. Shown in log-log scale -lines with smaller slope correspond to better scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Iteration time for very slim networks, forward and backward pass, batch size 1. Shown in log-log scale -lines with smaller slope correspond to better scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>To better understand the performance drop at 256 3 resolution observed in section 5.4.1 of the main paper, we upThe OGNProp layer propagates the features of "mixed" cells together with the features of the neighboring cells required for computations in subsequent layers. We show the number of neighbors that need to be propagated in two cases: 2 3 upconvolutions (A), and 4 3 up-convolutions followed by 3 3 convolutions (B). Visualized in 1D for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Training samples from the FAUST dataset reconstructed by OGN.performed an additional experiment on the ShapeNet-Cars dataset. We trained an OGN for generating car shapes from their IDs on a reduced version of ShapeNet-Cars,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Memory consumption and iteration time of OGN and a dense network at different output resolutions. Batch size 1.</figDesc><table><row><cell></cell><cell cols="2">Memory, GB</cell><cell cols="2">Iteration time, s</cell></row><row><cell cols="5">Resolution Dense OGN Dense OGN</cell></row><row><cell>32 3</cell><cell>0.33</cell><cell>0.29</cell><cell>0.015</cell><cell>0.016</cell></row><row><cell>64 3</cell><cell>0.50</cell><cell>0.36</cell><cell>0.19</cell><cell>0.06</cell></row><row><cell>128 3</cell><cell>1.62</cell><cell>0.43</cell><cell>0.56</cell><cell>0.18</cell></row><row><cell>256 3</cell><cell>9.98</cell><cell>0.54</cell><cell>3.89</cell><cell>0.64</cell></row><row><cell>512 3</cell><cell cols="3">(74.28) 0.88 (41.3)</cell><cell>2.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Single-view 3D reconstruction results on the 32 3 version of ShapeNet-all from Choy et al. [6] compared to OGN and a dense baseline. OGN is competitive with voxel-grid-based networks.</figDesc><table><row><cell>Category</cell><cell cols="2">R2N2 [6] OGN Dense</cell></row><row><cell>Plane</cell><cell>0.513</cell><cell>0.587 0.570</cell></row><row><cell>Bench</cell><cell>0.421</cell><cell>0.481 0.481</cell></row><row><cell>Cabinet</cell><cell>0.716</cell><cell>0.729 0.747</cell></row><row><cell>Car</cell><cell>0.798</cell><cell>0.816 0.828</cell></row><row><cell>Chair</cell><cell>0.466</cell><cell>0.483 0.481</cell></row><row><cell>Monitor</cell><cell>0.468</cell><cell>0.502 0.509</cell></row><row><cell>Lamp</cell><cell>0.381</cell><cell>0.398 0.371</cell></row><row><cell>Speaker</cell><cell>0.662</cell><cell>0.637 0.650</cell></row><row><cell>Firearm</cell><cell>0.544</cell><cell>0.593 0.576</cell></row><row><cell>Couch</cell><cell>0.628</cell><cell>0.646 0.668</cell></row><row><cell>Table</cell><cell>0.513</cell><cell>0.536 0.545</cell></row><row><cell>Cellphone</cell><cell>0.661</cell><cell>0.702 0.698</cell></row><row><cell>Watercraft</cell><cell>0.513</cell><cell>0.632 0.550</cell></row><row><cell>Mean</cell><cell>0.560</cell><cell>0.596 0.590</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Training Testing 2 3 filters 4 3 filters IntConv Reconstruction quality for autoencoders with different decoder architectures: 2 3 up-convolutions, 4 3 up-convolutions, and 2 3 up-convolutions interleaved with 3 3 convolutions, using different configurations of Prop-known and Prop-pred propagation modes.</figDesc><table><row><cell>Known</cell><cell>Known</cell><cell>0.904</cell><cell>0.907</cell><cell>0.907</cell></row><row><cell>Known</cell><cell>Pred</cell><cell>0.862</cell><cell>0.804</cell><cell>0.823</cell></row><row><cell>Pred</cell><cell>Known</cell><cell>0.898</cell><cell>0.896</cell><cell>0.897</cell></row><row><cell>Pred</cell><cell>Pred</cell><cell>0.884</cell><cell>0.885</cell><cell>0.885</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>3D shape from high-level information on the FAUST dataset. Lower-resolution predictions were upsampled to 256 3 ground truth. There is no drop in performance in higher resolution, when training on a subset of the Shapenet-cars dataset.</figDesc><table><row><cell>Dataset</cell><cell>128 3</cell><cell>256 3</cell></row><row><cell>Shapenet-cars (full)</cell><cell cols="2">0.901 0.865</cell></row><row><cell cols="3">Shapenet-cars (subset) 0.922 0.931</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .</head><label>11</label><figDesc>Convolutional encoder used in the single-image 3D reconstruction experiment.</figDesc><table><row><cell>32 3</cell><cell>64 3 (2 3 filters)</cell><cell>64 3 (4 3 filters)</cell><cell>64 3 (InvConv)</cell></row><row><cell></cell><cell></cell><cell>[64 3 × 1]</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Conv (3 3 )</cell><cell></cell></row><row><cell>[32 3 × 1]</cell><cell></cell><cell>[32 3 × 32]</cell><cell></cell></row><row><cell>Conv (3 3 )</cell><cell></cell><cell>Conv (3 3 )</cell><cell></cell></row><row><cell>[16 3 × 32]</cell><cell></cell><cell>[16 3 × 48]</cell><cell></cell></row><row><cell>Conv (3 3 )</cell><cell></cell><cell>Conv (3 3 )</cell><cell></cell></row><row><cell>[8 3 × 48]</cell><cell></cell><cell>[8 3 × 64]</cell><cell></cell></row><row><cell>Conv (3 3 )</cell><cell></cell><cell>Conv (3 3 )</cell><cell></cell></row><row><cell>[4 3 × 64]</cell><cell></cell><cell>[4 3 × 80]</cell><cell></cell></row><row><cell>FC</cell><cell></cell><cell>FC</cell><cell></cell></row><row><cell>[1024]</cell><cell></cell><cell>[1024]</cell><cell></cell></row><row><cell>FC</cell><cell></cell><cell>FC</cell><cell></cell></row><row><cell>[1024]</cell><cell></cell><cell>[1024]</cell><cell></cell></row><row><cell>FC</cell><cell></cell><cell>FC</cell><cell></cell></row><row><cell>[4 3 × 80]</cell><cell></cell><cell>[4 3 × 96]</cell><cell></cell></row><row><cell>Deconv (2 3 )</cell><cell></cell><cell>Deconv (2 3 )</cell><cell></cell></row><row><cell>[8 3 × 64]</cell><cell></cell><cell>[8 3 × 80]</cell><cell></cell></row><row><cell>Conv (3 3 )→ l1</cell><cell></cell><cell>Conv (3 3 )</cell><cell></cell></row><row><cell>[8 3 × 64]</cell><cell></cell><cell>[8 3 × 80]</cell><cell></cell></row><row><cell>OGNProp</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OGNConv(2 3 ) → l2</cell><cell></cell><cell>Deconv (2 3 )</cell><cell></cell></row><row><cell>[16 3 × 48]</cell><cell></cell><cell>[16 3 × 64]</cell><cell></cell></row><row><cell>OGNProp</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OGNConv(2 3 ) → l3</cell><cell></cell><cell>Conv (3 3 )→ l1</cell><cell></cell></row><row><cell>[32 3 × 32]</cell><cell></cell><cell>[16 3 × 64]</cell><cell></cell></row><row><cell></cell><cell>OGNProp</cell><cell>OGNProp</cell><cell>OGNProp</cell></row><row><cell></cell><cell cols="2">OGNConv(2 3 ) → l2 OGNConv(4 3 ) → l2</cell><cell>OGNConv(2 3 )</cell></row><row><cell></cell><cell>[32 3 × 48]</cell><cell>[32 3 × 48]</cell><cell>[32 3 × 48]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>OGNConv*(3 3 ) → l2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[32 3 × 48]</cell></row><row><cell></cell><cell>OGNProp</cell><cell>OGNProp</cell><cell>OGNProp</cell></row><row><cell></cell><cell cols="2">OGNConv(2 3 ) → l3 OGNConv(4 3 ) → l3</cell><cell>OGNConv(2 3 )</cell></row><row><cell></cell><cell>[64 3 × 32]</cell><cell>[64 3 × 32]</cell><cell>[64 3 × 32]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>OGNConv*(3 3 ) → l3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[64 3 × 32]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>OGN architectures used in our experiments with autoencoders. OGNConv denotes up-convolution, OGNConv* -convolution. Layer name followed by '→ lk' indicates that level k of an octree is predicted by a classifier attached to this layer. ) → l2 OGNConv (2 3 ) → l2 OGNConv (2 3 ) → l2 [32 3 × 48] [32 3 × 64] [32 3 × 64] OGNProp OGNProp OGNProp OGNConv (2 3 ) → l3 OGNConv (2 3 ) → l3 OGNConv (2 3 ) → l3 [64 3 × 32] [64 3 × 48] [64 3 × 48] OGNProp OGNProp OGNConv (2 3 ) → l4 OGNConv (2 3 ) → l4 [128 3 × 32] [128 3 × 32] OGNProp OGNConv (2 3 ) → l5 [256 3 × 32]</figDesc><table><row><cell>32 3</cell><cell>64 3</cell><cell>128 3</cell><cell>256 3</cell></row><row><cell>[4 3 × 80]</cell><cell>[4 3 × 96]</cell><cell>[4 3 × 112]</cell><cell>[4 3 × 112]</cell></row><row><cell>Deconv (2 3 )</cell><cell>Deconv (2 3 )</cell><cell>Deconv (2 3 )</cell><cell>Deconv (2 3 )</cell></row><row><cell>[8 3 × 64]</cell><cell>[8 3 × 80]</cell><cell>[8 3 × 96]</cell><cell>[8 3 × 96]</cell></row><row><cell>Conv (3 3 ) → l1</cell><cell>Conv (3 3 )</cell><cell>Conv (3 3 )</cell><cell>Conv (3 3 )</cell></row><row><cell>[8 3 × 64]</cell><cell>[8 3 × 80]</cell><cell>[8 3 × 96]</cell><cell>[8 3 × 96]</cell></row><row><cell>OGNProp</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OGNConv (2 3 ) → l2</cell><cell>Deconv (2 3 )</cell><cell>Deconv (2 3 )</cell><cell>Deconv (2 3 )</cell></row><row><cell>[16 3 × 48]</cell><cell>[16 3 × 64]</cell><cell>[16 3 × 80]</cell><cell>[16 3 × 80]</cell></row><row><cell>OGNProp</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OGNConv (2 3 ) → l3</cell><cell>Conv (3 3 ) → l1</cell><cell>Conv (3 3 ) → l1</cell><cell>Conv (3 3 ) → l1</cell></row><row><cell>[32 3 × 32]</cell><cell>[16 3 × 64]</cell><cell>[16 3 × 80]</cell><cell>[16 3 × 80]</cell></row><row><cell></cell><cell>OGNProp</cell><cell>OGNProp</cell><cell>OGNProp</cell></row><row><cell></cell><cell>OGNConv (2 3</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.patrickmin.com/binvox</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In this paper, we focus on generating 3D shapes. Thus, we have not implemented an octree-based convolutional encoder. This could be done along the lines of Riegler et al.<ref type="bibr" target="#b28">[29]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://youtu.be/kmMvKNNyYF4</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the Excellence Initiative of the German Federal and State Governments: BIOSS Centre for Biological Signalling Studies (EXC 294). We would like to thank Benjamin Ummenhofer for valuable discussions and technical comments. We also thank Nikolaus Mayer for his help with 3D model visualization and manuscript preparation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Computational efficiency</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FAUST: Dataset and evaluation for 3D mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SSD: Smooth Signed Distance Surface Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Calakli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">net: Learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>3d u-</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Voxresnet: Deep voxelwise residual networks for volumetric brain segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<idno>abs/1608.05895</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cumulative generation of octree models from range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Connolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep shape from a low number of silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1612.00603</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fusion of depth maps with multiple scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">3d shape induction from 2d views of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1612.05872</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linear octtrees for fast processing of threedimensional objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gargantini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="374" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spatially-sparse convolutional neural networks. CoRR, abs/1409</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6070</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Sparse 3d convolutional neural networks. In BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep disentangled representations for volumetric reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Gerven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding. CoRR, abs/1408</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5093</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<idno>NIPS. 2016. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SGP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<title level="m">Geodesic convolutional neural networks on riemannian manifolds</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICCV Workshops</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Octree encoding: A new technique for the representation, manipulation and display of arbitrary 3-d objects by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meagher</surname></persName>
		</author>
		<idno>IPL-TR-80-111</idno>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Octnet: Learning deep 3d representations at high resolutions. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>abs/1611.05009</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vconv-dae: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning 3d shape surfaces using geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Volumetric 3d mapping in real-time on a cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Steinbrücker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-view 3d models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning shape abstractions by assembling volumetric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1612.00404</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Global, dense multiscale reconstruction for a billion points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Syncspeccnn: Synchronized spectral CNN for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1612.00606</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning semantic deformation flows with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
