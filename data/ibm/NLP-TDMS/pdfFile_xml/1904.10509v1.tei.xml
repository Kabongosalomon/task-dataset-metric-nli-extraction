<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Long Sequences with Sparse Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
						</author>
						<title level="a" type="main">Generating Long Sequences with Sparse Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n √ n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating complex, high-dimensional data distributions is a central problem in unsupervised learning, as many downstream applications of interest involve generation of text, images, audio, and other data. Additionally, it is believed to be a key component of unsupervised representation learning.</p><p>Recently, neural autoregressive models have achieved impressive results in this domain, achieving state-of-the-art in modeling natural language <ref type="bibr" target="#b13">(Jozefowicz et al., 2016)</ref>  <ref type="bibr" target="#b22">(Radford et al., 2018)</ref>  , raw audio (Van Den <ref type="bibr" target="#b20">Oord et al., 2016)</ref>  <ref type="bibr" target="#b17">(Mehri et al., 2016)</ref>, and images   <ref type="bibr" target="#b18">(Menick &amp; Kalchbrenner, 2018)</ref>  <ref type="bibr" target="#b24">(Salimans et al., 2017)</ref>  <ref type="bibr" target="#b23">(Reed et al., 2017)</ref>  .</p><p>These methods decompose a joint probability distribution into a product of conditional ones. Modeling these conditional distributions is extremely challenging, however, as they contain many complex, long-range dependencies and require a suitably expressive model architecture to learn them.</p><p>Architectures based off CNNs  have made <ref type="figure">Figure 1</ref>. Unconditional samples from our neural autoregressive model on ImageNet 64 and a classical music dataset. We used the same self-attention based architecture for audio, images, and text. The samples above were generated with softmax temperature 1.0, and had lengths 12,288 and 65,536. Audio samples be listened to at https://openai.com/blog/sparse-transformer great progress in this direction, but require significant depth to expand their receptive field. To address this, WaveNet (Van Den <ref type="bibr" target="#b20">Oord et al., 2016)</ref> introduced dilated convolutions, which allowed the network to model long-range dependencies in a logarithmic number of layers.</p><p>Separately, the Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> has been shown to excel on many natural language tasks, which may be in part due to its ability to model arbitrary dependencies in a constant number of layers. As each self-attention layer has a global receptive field, the network can allocate representational capacity to the input regions for which it is arXiv:1904.10509v1 <ref type="bibr">[cs.</ref>LG] 23 Apr 2019 most useful. Thus the architecture may be more flexible at generating diverse data types than networks with fixed connectivity patterns.</p><p>However, the memory and computational requirements of such networks grows quadratically with sequence length, which excludes their use on long sequences.</p><p>The main contribution of this work is to introduce several sparse factorizations of the attention matrix, which scale as O(n p √ n) with the sequence length without sacrificing performance. These work by separating the full attention computation into several faster attention operations which, when combined, can approximate the dense attention operation. We use this to apply self-attention to sequences of unprecedented length.</p><p>Additionally, we introduce several other changes to the Transformer, including:</p><p>• A restructured residual block and weight initialization to improve training of very deep networks</p><p>• A set of sparse attention kernels which efficiently compute subsets of the attention matrix</p><p>• Recomputation of attention weights during the backwards pass to reduce memory usage</p><p>We empirically validate that models augmented in this manner can achieve state-of-the-art compression and generation of natural language, raw audio, and natural images. The simplicity of the architecture leads us to believe it may be useful for many problems of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The most related work involves other techniques for scaling up autoregressive generative models. For images, <ref type="bibr" target="#b23">(Reed et al., 2017)</ref> models conditional independence between the pixels in order to generate many locations in parallel, and <ref type="bibr" target="#b18">(Menick &amp; Kalchbrenner, 2018)</ref> imposes an ordering and multi-scale upsampling procedure to generate high fidelity samples. <ref type="bibr" target="#b21">(Parmar et al., 2018)</ref> uses blocks of local attention to apply Transformers to images. For text,  introduces a state reuse "memory" for modeling long-term dependencies. And for audio, in addition to <ref type="bibr" target="#b26">(Van Den Oord et al., 2016)</ref>, <ref type="bibr" target="#b17">(Mehri et al., 2016)</ref> used a hierarchical structure and RNNs of varying clock-rates to use long contexts during inference, similar to <ref type="bibr" target="#b15">(Koutnik et al., 2014)</ref>. <ref type="bibr" target="#b12">(Huang et al., 2018)</ref> apply Transformers to MIDI generation with an efficient relative attention.</p><p>Our work is simpler than many of the techniques above and can be applied equally across images, text, and audio. Many of the above techniques are orthogonal to ours, moreover, and could be used in conjunction with ours.</p><p>Outside of generative modeling, there are several works relevant to improving the efficiency of attention based off chunking <ref type="bibr" target="#b5">(Chiu &amp; Raffel, 2017)</ref> or using fixed length representations <ref type="bibr" target="#b2">(Britz et al., 2017)</ref>. Other works have investigated attention with multiple "hops", such as <ref type="bibr" target="#b25">(Sukhbaatar et al., 2015)</ref> and <ref type="bibr" target="#b8">(Gehring et al., 2017)</ref>.</p><p>It is worth noting that the Gated Pixel CNN  and WaveNet (Van Den <ref type="bibr" target="#b20">Oord et al., 2016)</ref> use multiplicative interactions in their networks, which are related to self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>We consider the task of autoregressive sequence generation, where the joint probability of a sequence x = {x 1 , x 2 , ..., x n } is modeled as the product of conditional probability distributions and parameterized by a network θ.</p><formula xml:id="formula_0">p(x) = n i=1 p(x i |x 1 , ..., x i−1 ; θ)<label>(1)</label></formula><p>We treat images, text, and audio as a sequence of discrete tokens, typically raw bytes. The network θ takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary. The training objective is to maximize the log-probability of the data with respect to θ.</p><p>A simple and powerful choice for model θ is a Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> in decoder-only mode, as demonstrated by <ref type="bibr" target="#b22">(Radford et al., 2018)</ref> and <ref type="bibr" target="#b16">(Liu et al., 2018)</ref>. These models transform the input sequence with blocks of multihead self-attention over the entire sequence, followed by dense transformations over each sequence element. The selfattention portion of the network must compute n weightings for each of n elements, however, which can quickly become intractable as the sequence length grows.</p><p>In the following sections, we describe our modifications to the Transformer architecture which make it more suitable for modeling long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Factorized Self-Attention</head><p>Sparse Transformers separate the full self-attention operation across several steps of attention, as visualized in <ref type="figure" target="#fig_1">Figure  3</ref>(b) and 3(c). To motivate our approach, we first perform a qualitative assessment of attention patterns learned by a standard Transformer on an image dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Qualitative assessment of learned attention patterns</head><p>We visualized the attention patterns learned by a 128-layer self-attention network on CIFAR-10, and present several examples in <ref type="figure" target="#fig_0">Figure 2</ref>. Visual inspection showed that most layers had sparse attention patterns across most data points, suggesting that some form of sparsity could be introduced without significantly affecting performance. Several layers ( <ref type="figure" target="#fig_0">Figure 2c</ref>) clearly exhibited global patterns, however, and others exhibited data-dependent sparsity <ref type="figure" target="#fig_0">(Figure 2d</ref>), both of which would be impacted by introducing a predetermined sparsity pattern into all of the attention matrices.</p><p>In this paper, we restricted our investigation to a class of sparse attention patterns that have connectivity between all positions over several steps of attention. These methods can be more efficient than full attention while still providing global context to any given position. We aimed to empirically validate the performance of these factorized patterns on a range of tasks, given that they are unable to learn the exact same mappings as those in <ref type="figure" target="#fig_0">Figure 2</ref>. We present the formulation of factorized attention below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Factorized self-attention</head><p>A self-attention layer maps a matrix of input embeddings X to an output matrix and is parameterized by a connectivity pattern S = {S 1 , ..., S n }, where S i denotes the set of indices of the input vectors to which the ith output vector attends. The output vector is a weighted sum of transformations of the input vectors:</p><formula xml:id="formula_1">Attend(X, S) = a(x i , S i ) i∈{1,...,n} (2) a(x i , S i ) = softmax (W q x i )K T Si √ d V Si (3) K Si = W k x j j∈Si V Si = W v x j j∈Si<label>(4)</label></formula><p>Here W q , W k , and W v represent the weight matrices which transform a given x i into a query, key, or value, and d is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.</p><p>Full self-attention for autoregressive models defines S i = {j : j ≤ i}, allowing every element to attend to all previous positions and its own position. . We are chiefly interested in efficient choices for the subset A, where |A</p><formula xml:id="formula_2">(m) i | ∝ p √ n.</formula><p>Additionally, for the time being we consider valid choices of A, where all input positions are connected to all future output positions across the p steps of attention.</p><p>For every j ≤ i pair, we set every A such that i can attend to j through a path of locations with maximum length p + 1.</p><formula xml:id="formula_3">Specifically, if (j, a, b, c, ..., i) is the path of indices, then j ∈ A (1) a , a ∈ A (2) b , b ∈ A<label>(3)</label></formula><p>c , and so forth.</p><p>These two criteria allow us keep the ability of Transformers to propagate signals from arbitrary input positions to arbitrary output positions in a constant number of steps, while reducing the total effective computation to O(n p √ n). We also note that softening the validity criterion (for instance, having a series of only locally connected layers) may be a useful inductive bias for certain domains.</p><p>In this work, we explore two factorizations for p = 2, which we describe in the following section, though we note that the same techniques can be easily extended to higher dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Two-dimensional factorized attention</head><p>A natural approach to defining a factorized attention pattern in two dimensions is to have one head attend to the previous l locations, and the other head attend to every lth location, where l is the stride and chosen to be close to √ n, a method we call strided attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formally, A</head><p>(1) i = {t, t + 1, ..., i} for t = max(0, i − l) and A</p><p>(2) i = {j : (i − j) mod l = 0}. This pattern can be visualized in <ref type="figure" target="#fig_1">Figure 3</ref></p><formula xml:id="formula_4">(b).</formula><p>This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, we find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future.</p><p>In those cases, we instead use a fixed attention pattern <ref type="figure" target="#fig_1">(Figure 3(c)</ref>), where specific cells summarize previous locations and propagate that information to all future cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formally, A</head><p>(1) i = {j : ( j/l = i/l )}, where the brackets denote the floor operation, and A Concretely, if the stride is 128 and c = 8, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth.</p><p>A fixed-attention pattern with c = 1 limits the expressivity of the network significantly, as many representations in the network are only used for one block whereas a small number of locations are used by all blocks. We instead found choosing c ∈ {8, 16, 32} for typical values of l ∈ {128, 256} to perform well, although it should be noted that this increases the computational cost of this method by c in comparison to the strided attention.</p><p>Additionally, we found that when using multiple heads, having them attend to distinct subblocks of length c within the block of size l was preferable to having them attend to the same subblock.</p><p>In the subsequent section, we describe how to incorporate factorized attention into the Sparse Transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sparse Transformer</head><p>Here we fully describe the Sparse Transformer architecture, which is a modified version of the Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Factorized attention heads</head><p>Standard dense attention simply performs a linear transformation of the attend function defined in Equation 2:</p><formula xml:id="formula_5">attention(X) = W p · attend(X, S)<label>(5)</label></formula><p>where W p denotes the post-attention weight matrix. The simplest technique for integrating factorized self-attention is to use one attention type per residual block, and interleave them sequentially or at a ratio determined as a hyperparameter:</p><p>attention(X) = W p · attend(X, A (r mod p) ) (6)</p><p>Here r is the index of the current residual block and p is the number of factorized attention heads.</p><p>A second approach is to have a single head attend to the locations of the pixels that both factorized heads would attend to, which we call a merged head:</p><formula xml:id="formula_6">attention(X) = W p · attend(X, p m=1 A (m) )<label>(7)</label></formula><p>This is slightly more computationally intensive, but only by a constant factor. A third approach is to use multi-head attention <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>, where n h attention products are computed in parallel, then concatenated along the feature dimension:  Here, the A can be the separate attention patterns, the merged patterns, or interleaved as in Eq. 2. Also, the dimensions of the weight matrices inside the attend function are reduced by a factor of 1/n h , such that the number of parameters are invariant across values of n h .</p><formula xml:id="formula_7">attention(X) = W p attend(X, A) (i) i∈{1,...,n h }<label>(8)</label></formula><p>We typically find multiple heads to work well, though for extremely long sequences where the attention dominates the computation time, it is more worthwhile to perform them one at a time and sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Scaling to hundreds of layers</head><p>We found that Transformers were difficult to train with many layers, as noted by <ref type="bibr" target="#b0">(Al-Rfou et al., 2018)</ref>. Instead of incorporating auxillary losses, we adopted the following architectural changes.</p><p>First, we use the pre-activation residual block of <ref type="bibr" target="#b10">(He et al., 2016)</ref>, defining a network of N layers in the following way:</p><formula xml:id="formula_8">H 0 = embed(X, W e )<label>(9)</label></formula><formula xml:id="formula_9">H k = H k−1 + resblock(H k−1 ) (10) y = softmax(norm(H N )W out )<label>(11)</label></formula><p>where embed is a function we describe in the next section, W out is a weight matrix, and resblock(h) normalizes the input to the attention block and a positionwise feedforward network in the following way:</p><formula xml:id="formula_10">a(H) = dropout(attention(norm(H)))<label>(12)</label></formula><p>b(H) = dropout(ff(norm(H + a(H)))) (13)</p><formula xml:id="formula_11">resblock(H) = a(H) + b(H)<label>(14)</label></formula><p>The norm function denotes Layer Normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>, and ff(</p><formula xml:id="formula_12">x) = W 2 f (W 1 x + b 1 ) + b 2 .</formula><p>Our choice of f is the Gaussian Error Linear Unit <ref type="bibr" target="#b11">(Hendrycks &amp; Gimpel, 2016)</ref>, f (X) = X sigmoid(1.702 · X), as used in <ref type="bibr" target="#b22">(Radford et al., 2018)</ref>. The output dimension of W 1 is 4.0 times the input dimension, unless otherwise noted.</p><p>Observe that H N is the sum of N applications of functions a and b, and thus each function block receives a gradient directly from the output layer . We scale the initialization of W 2 and W p in Eq. 5 by 1 √ 2N to keep the ratio of input embedding scale to residual block scale invariant across values of N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Modeling diverse data types</head><p>In addition to the embedding of input symbols, positional embeddings are typically used in Transformers and other location-agnostic architectures to encode the spatial relationships of data <ref type="bibr" target="#b8">(Gehring et al., 2017)</ref>, <ref type="bibr" target="#b21">(Parmar et al., 2018)</ref>.</p><p>We found using learned embeddings which either encoded the structure of the data or the factorized attention patterns were important for performance of our models.</p><p>We added either n emb = d data or n emb = d attn embeddings to each input location, where d data refers to the number of dimensions of the data, and d attn is the number of dimensions of the factorized attention. If x i is the one-hot encoded ith element in the sequence, and o (j) i represents the one-hot encoded position of x i in the jth dimension (1 ≤ j ≤ n emb ), then:</p><formula xml:id="formula_13">embed(X, W e ) =   x i W e + n emb j=1 o (j) i W j   xi∈X<label>(15)</label></formula><p>For images, we used data embeddings, where d data = 3 for the row, column, and channel location of each input byte. For text and audio, we used two-dimensional attention embeddings, where d attn = 2 and the index corresponds to each position's row and column index in a matrix of width equal to the stride.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Saving memory by recomputing attention weights</head><p>Gradient checkpointing has been shown to be effective in reducing the memory requirements of training deep neural networks <ref type="bibr" target="#b3">(Chen et al., 2016)</ref>, <ref type="bibr" target="#b9">(Gruslys et al., 2016)</ref>. It is worth noting, however, that this technique is particularly effective for self-attention layers when long sequences are processed, as memory usage is high for these layers relative to the cost of computing them.</p><p>Using recomputation alone, we are able to train dense attention networks with hundreds of layers on sequence lengths of 16,384, which would be infeasible on modern hardware otherwise.</p><p>In our experiments, we recompute the attention and feedforward blocks during the backwards pass. To simplify our implementation, we do not apply dropout within the attention blocks, as in <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>, and instead only apply it at the end of each residual addition, as seen in <ref type="figure" target="#fig_5">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Efficient block-sparse attention kernels</head><p>The sparse attention masks in 3(b) and 3(c) can be efficiently computed by slicing out sub-blocks from the query, key, and value matrices and computing the product in blocks. Attention over a local window can be computed as-is, whereas attention with a stride of k can be computed by transposing the matrix and computing a local window. Fixed attention positions can be aggregated and computed in blocks.</p><p>In order to ease experimentation, we implemented a set of GPU kernels which efficiently perform these operations. The softmax operation is fused into a single kernel and also uses registers to eliminate loading the input data more than once, allowing it to run at the same speed as a simple nonlinearity. The upper triangle of the attention matrix is never computed, moreover, removing the need for the negative bias term of <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> and halving the number of operations to be performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Mixed-precision training</head><p>We store network weights in single-precision floating-point, but otherwise compute network activations and gradients in half-precision, as in <ref type="bibr" target="#b19">(Micikevicius et al., 2017)</ref>. This accelerates our training due to the usage of Tensor Core operations on the V100 GPU. During the gradient calculation, we use <ref type="figure">Figure 5</ref>. Unconditional samples from ImageNet 64x64, generated with an unmodified softmax temperature of 1.0. We are able to learn long-range dependencies directly from pixels without using a multi-scale architecture. dynamic loss scaling to reduce numerical underflow, and we communicate half-precision gradients when averaging across multiple GPUs. When sampling, we cast the queries and keys to single-precision, as the query-key product can sometimes overflow the max value of half-precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Training</head><p>We use the Adam optimizer with a linear warmup of 5000 iterations and a gradient clipping of 1.0, both of which we found important for model stability. We use a weight decay penalty of 0.01. We annealed the learning rate according to a cosine decay as in <ref type="bibr" target="#b22">(Radford et al., 2018)</ref>. We train on 8 V100 GPUs unless otherwise noted.</p><p>All embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless we use "half-size" transformations, where it is 2d. Additionally, sometimes we halve the size of the query and key transformations.</p><p>We initialize the token embedding W e from N (0, 0.125 √ d ) and the position embeddings from N (0, 0.125</p><formula xml:id="formula_14">√ dn emb</formula><p>). Within the attention and feedforward components, all biases are initial-ized to 0 and all weights are initialized from N (0, 0.125 √ din ) where d in is the fan-in dimension. The weight matrix for the output logits was initialized to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>We empirically test our architecture on density modeling tasks including natural images, text, and raw audio. A summary of the results is available in <ref type="table">Table 1</ref>. We found that, in addition to running significantly faster than full attention, sparse patterns also converged to lower error, as shown in <ref type="table" target="#tab_0">Table 2</ref>. This may point to a useful inductive bias from the sparsity patterns we introduced, or an underlying optimization issue with full attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">CIFAR-10</head><p>We train strided Sparse Transformers on CIFAR-10 images represented as sequences of 3072 bytes. Models have 2 heads, 128 layers, d = 256, half-size feedforward network and query-key projections, and are trained for 120 epochs with a learning rate of 0.00035 and a dropout rate of 0.25 until validation error stops decreasing.</p><p>We use 48000 examples for training and 2000 examples for validation, evaluating the performance of our best models on <ref type="table">Table 1</ref>. Summary of our findings for density modeling tasks. Results are reported in bits per byte, which is equivalent to bits per dim for image tasks. M refers to millions of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Bits per byte</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>PixelCNN  3.03 PixelCNN++ <ref type="bibr" target="#b24">(Salimans et al., 2017)</ref> 2.92 Image Transformer <ref type="bibr" target="#b21">(Parmar et al., 2018)</ref> 2.90 PixelSNAIL  2.85 Sparse Transformer 59M (strided) 2.80</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enwik8</head><p>Deeper Self-Attention <ref type="bibr" target="#b0">(Al-Rfou et al., 2018)</ref> 1.06 Transformer-XL 88M  1.03 Transformer-XL 277M  0.99 Sparse Transformer 95M (fixed) 0.99</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet 64x64</head><p>PixelCNN  3.57 Parallel Multiscale <ref type="bibr" target="#b23">(Reed et al., 2017)</ref> 3.7 Glow <ref type="bibr" target="#b14">(Kingma &amp; Dhariwal, 2018)</ref> 3.81 SPN 150M <ref type="bibr" target="#b18">(Menick &amp; Kalchbrenner, 2018)</ref> 3.52 Sparse Transformer 152M (strided) 3.44</p><p>Classical music, 5 seconds at 12 kHz Sparse Transformer 152M (strided) 1.97</p><p>the test set. The model achieves 2.80 bits per dim (2.798 ± 0.004 over seeds 1, 2, 3) versus the previous 2.85 state of the art . We also compare performance of different attention patterns in <ref type="table" target="#tab_0">Table 2</ref>. The strided attention reaches the lowest error in the shortest amount of time, surpassing the error of dense attention at 2.82 bits per dim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Text</head><p>In order to assess Sparse Transformers on datasets without a strong two-dimensional structure, we trained models on the EnWik8 dataset, which represents the first 10 8 bytes of Wikipedia and contains a great degree of variability in periodic structure. We trained with a context length of 12,288, which is longer than previous approaches.</p><p>We trained on the first 90 million tokens and reserved the last 10 million for validation and test. We used 30-layer fixed Sparse Transformers with 8 heads, d = 512, and a dropout rate of 0.40. We trained for 80 epochs until validation loss stopped decreasing. We used a stride of 128, c = 32, and merged the factorized attention heads.</p><p>Our best model reached 0.99 bits per dim (0.992 ± 0.001 over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for a similarly-sized Transformer-XL  and matching the 0.99 of a model trained with more than double the number of parameters. Strided attention failed to do well on this dataset, whereas fixed patterns were able to recover and surpass the performance of dense attention, as listed in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>Additionally, during evaluation of the test set, we modified the minimum context length the network could use by evaluating fewer tokens in parallel. We saw monotonic increases in performance with more tokens used, up to 12,160 out of the 12,288 tokens used for training (see <ref type="table">Table 3</ref>), which suggests the network is effectively incorporating long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">ImageNet 64x64</head><p>In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by  and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs.</p><p>Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 <ref type="bibr" target="#b18">(Menick &amp; Kalchbrenner, 2018)</ref>.</p><p>Additionally, we generate unconditional samples ( <ref type="figure">Figure  5</ref>) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Classical music from raw audio</head><p>To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by <ref type="bibr" target="#b7">(Dieleman et al., 2018)</ref>. As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism.</p><p>Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 √ 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million).</p><p>Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz. The samples clearly demonstrate global coherence over the sampled period, and exhibit a variety of play styles and tones, swapping from rhythmic playing to forceful. To listen to samples, visit https://openai.com/blog/ sparse-transformer. Sample quality quickly degrades for greater sequence lengths due to reduced model capacity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We introduced Sparse Transformers and showed they attain equivalent or better performance on density modeling of long sequences than standard Transformers while requiring significantly fewer operations. This performance is stateof-the-art in images and text and is easily adaptable to raw audio. The model demonstrates usage of long-term context and generates globally coherent samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Learned attention patterns from a 128-layer network on CIFAR-10 trained with full attention. White highlights denote attention weights for a head while generating a given pixel, and black denotes the autoregressive mask. Layers are able to learn a variety of specialized sparse structures, which may explain their ability to adapt to different domains. a) Many early layers in the network learn locally connected patterns, which resemble convolution. b) In layers 19 and 20, the network learned to split the attention across a row attention and column attention, effectively factorizing the global attention calculation. c) Several attention layers showed global, data-dependent access patterns. d) Typical layers in layers 64-128 exhibited high sparsity, with positions activating rarely and only for specific input patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Two 2d factorized attention schemes we evaluated in comparison to the full attention of a standard Transformer (a). The top row indicates, for an example 6x6 image, which positions two attention heads receive as input when computing a given output. The bottom row shows the connectivity matrix (not to scale) between all such outputs (rows) and inputs (columns). Sparsity in the connectivity matrix can lead to significantly faster computation. In (b) and (c), full connectivity between elements is preserved when the two heads are computed sequentially. We tested whether such factorizations could match in performance the rich connectivity patterns ofFigure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Factorized self-attention instead has p separate attention heads, where the mth head defines a subset of the indices A (m) i ⊂ {j : j ≤ i} and lets S i = A (m) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>=</head><label></label><figDesc>{j : j mod l ∈ {t, t + 1, ..., l}, where t = l − c and c is a hyperparameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Diagram depicting one residual block of the Sparse Transformer. The shaded background indicates tensors which are checkpointed<ref type="bibr" target="#b3">(Chen et al., 2016)</ref> and stored in GPU memory. The other tensors, including the attention weights and feedforward network activations, are recomputed during the calculation of gradients, reducing memory usage substantially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Sparse patterns showed increased speed and also better loss on the datasets where we could compare both, which may point to a useful inductive bias in the patterns we learned or an underlying optimization issue with full attention.</figDesc><table><row><cell>Model</cell><cell cols="2">Bits per byte Time/Iter</cell></row><row><cell>Enwik8 (12,288 context)</cell><cell></cell><cell></cell></row><row><cell>Dense Attention</cell><cell>1.00</cell><cell>1.31</cell></row><row><cell>Sparse Transformer (Fixed)</cell><cell>0.99</cell><cell>0.55</cell></row><row><cell>Sparse Transformer (Strided)</cell><cell>1.13</cell><cell>0.35</cell></row><row><cell>CIFAR-10 (3,072 context)</cell><cell></cell><cell></cell></row><row><cell>Dense Attention</cell><cell>2.82</cell><cell>0.54</cell></row><row><cell>Sparse Transformer (Fixed)</cell><cell>2.85</cell><cell>0.47</cell></row><row><cell>Sparse Transformer (Strided)</cell><cell>2.80</cell><cell>0.38</cell></row><row><cell cols="3">Table 3. We observe increased compression of Enwik8 with longer</cell></row><row><cell cols="3">contexts, suggesting the Sparse Transformer can effectively incor-</cell></row><row><cell>porate long-term dependencies.</cell><cell></cell><cell></cell></row><row><cell cols="3">Minimum context length during evaluation Bits per byte</cell></row><row><cell>6,144 tokens</cell><cell></cell><cell>0.9952</cell></row><row><cell>9,216 tokens</cell><cell></cell><cell>0.9936</cell></row><row><cell>10,752 tokens</cell><cell></cell><cell>0.9932</cell></row><row><cell>11,904 tokens</cell><cell></cell><cell>0.9930</cell></row><row><cell>12,096 tokens</cell><cell></cell><cell>0.9922</cell></row><row><cell>12,160 tokens</cell><cell></cell><cell>0.9908</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Performance of a strided Sparse Transformer on a classical audio dataset (µ-law encoded at 12 kHz) as a function of sequence length and model size.</figDesc><table><row><cell cols="3">Sequence length Parameters Bits per byte</cell></row><row><cell>65,536</cell><cell>152M</cell><cell>1.97</cell></row><row><cell>262,144</cell><cell>25M</cell><cell>2.17</cell></row><row><cell>1,048,576</cell><cell>3M</cell><cell>2.99</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgements</head><p>We would like to thank Ashish Vaswani for insightful discussions during the genesis of the project. We also thank Joshua Meier and Mark Chen for helpful discussions, and Johannes Otterbach, Prafulla Dhariwal, and David Luan for feedback on drafts of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper selfattention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04444</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient attention using a fixed-size memory representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00110</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pixelsnail</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09763</idno>
		<title level="m">An improved autoregressive generative model</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05382</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Monotonic chunkwise attention. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transformer-xl: Language modeling with longer-term dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The challenge of realistic music generation: modelling raw audio at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8000" to="8010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memory-efficient backpropagation through time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4125" to="4133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An improved relative self-attention mechanism for transformer with application to music generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04281</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clockwork Rnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3511</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10198</idno>
		<title level="m">Generating wikipedia by summarizing long sequences</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samplernn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07837</idno>
		<title level="m">An unconditional end-to-end neural audio generation model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01608</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Mixed precision training</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ku</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">A. Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.ama-zonaws.com/openai-assets/research-covers/language-unsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03664</idno>
		<title level="m">Parallel multiscale autoregressive density estimation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pixelcnn++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<title level="m">Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavenet</surname></persName>
		</author>
		<idno>abs/1609.03499</idno>
		<title level="m">A generative model for raw</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
