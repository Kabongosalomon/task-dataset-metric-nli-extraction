<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Information Aggregation via Dynamic Routing for Sequence Encoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Gong</surname></persName>
							<email>jjgong15@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojing</forename><surname>Wang</surname></persName>
							<email>sjwang17@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Information Aggregation via Dynamic Routing for Sequence Encoding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While much progress has been made in how to encode a text sequence into a sequence of vectors, less attention has been paid to how to aggregate these preceding vectors (outputs of RNN/CNN) into fixed-size encoding vector. Usually, a simple max or average pooling is used, which is a bottom-up and passive way of aggregation and lack of guidance by task information. In this paper, we propose an aggregation mechanism to obtain a fixed-size encoding with a dynamic routing policy. The dynamic routing policy is dynamically deciding that what and how much information need be transferred from each word to the final encoding of the text sequence. Following the work of Capsule Network, we design two dynamic routing policies to aggregate the outputs of RNN/CNN encoding layer into a final encoding vector. Compared to the other aggregation methods, dynamic routing can refine the messages according to the state of final encoding vector. Experimental results on five text classification tasks show that our method outperforms other aggregating models by a significant margin. Related source code is released on our github page 1 . * Corresponding Author 1 https://github.com/FudanNLP/Capsule4TextClassification</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning the distributed representation of text sequences, such as sentences or documents, is crucial to a wide range of important natural language processing applications. A primary challenge is how to encode the variable-length text sequence into a fixed-size vector, which should fully capture the semantics of text.</p><p>Many successful text encoding methods usually contain three key steps: (1) converting each word in a text sequence into its embedding; (2) taking as input the sequence of word embeddings, and computing the context-aware representation for each word with a recurrent neural network (RNN) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b2">Chung et al., 2014)</ref> or convolutional neural network (CNN) <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b6">Kim, 2014)</ref>; (3) summarizing the sentence meaning into a fixed-size vector by an aggregation operation. Then, these models are trained by combining a downstream task in a supervised or unsupervised way.</p><p>Currently, much attention is paid to the first two steps, while the aggregation step is less emphasized on. Some simple aggregation methods, such as max (or average) pooling, is used to sum the RNN hidden states or convolved vectors, computed in the previous step, into a single vector. This kind of methods aggregate information in a bottom-up and passive way and are lack of the guide of task information. Recently, several works employ self-attention mechanism <ref type="bibr" target="#b10">(Lin et al., 2017;</ref><ref type="bibr" target="#b23">Yang et al., 2016)</ref> on top of the recurrent or convolutional encoding layer to replace simple pooling. A basic assumption is that the words (or even sentences) are not equally important. One or several task-specific context vectors are used to assign a different weight to each word and select task-specific encodings. The context vectors are parameters learned jointly with other parameters during the training process. These attentive aggregation can select task-dependent information. However, the context vectors are fixed once learned.</p><p>In this paper, we regard the aggregation as a routing problem of how to deliver the messages from source nodes to target nodes. In our setting, the source nodes are the outputs of a recurrent or convolu-tional encoding layer, and the target nodes are one or several fixed-size encoding vectors to represent the meaning of the text sequence.</p><p>From this viewpoint, both the pooling and attentive aggregations are a fixed routing policy without considering the state of the final encoding vectors. For example, the final encoding vectors could receive some redundancy information from different words. The fixed routing policy cannot avoid this issue. Therefore, we wish for a new way to aggregate information according to the state of the final encoding.</p><p>In recent promising work of capsule network <ref type="bibr" target="#b13">(Sabour et al., 2017)</ref>, a dynamic routing policy is proposed and proven to be more effective than the max-pooling routing. Inspired by their idea, we introduce a text sequence encoding model with dynamic routing mechanism. Specifically, we propose two kinds of dynamic routing policies. One is the standard dynamic routing policy same as the capsule network, in which the source node decides what and how many messages are sent to different target nodes. The other is the reversed dynamic routing policy, in which the target node decides what and how many messages may be received from different source nodes.</p><p>Experimental results on five text classification tasks show that the dynamic routing policy outperforms other aggregation methods, such as max pooling, average pooling, and self-attention by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: general sequence encoding for text classification</head><p>In this section, we are going to introduce a general text classification framework. It consists of an Embedding Layer, Encoding Layer, Aggregation Layer and Prediction Layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding Layer</head><p>Given a text sequence with words S = w 1 , w 2 , · · · , w L . Since the words are symbols that could not be processed directly using prominent neural architectures, so we first map each word into a d dimensional embedding vector,</p><formula xml:id="formula_0">X = [x 1 , x 1 , x 2 , · · · , x L ].</formula><p>(1)</p><p>In order to transfer knowledge from a vast unlabeled corpus, the embeddings can be taken from the pre-trained word embedding, such as Glove <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoding Layer</head><p>However, each word representation in X is still independent with each other. To gain some dependency between adjacent words, we then build a bi-directional LSTM (BiLSTM) layer <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref> to incorporate forward and backward context information of a sequence. Then we can get phrase-level encoding h t of a word by concatenating forward h f t and backward output vector h b t correspond to the target word.</p><formula xml:id="formula_1">h f t = LSTM(h f t−1 , x t ),<label>(2)</label></formula><formula xml:id="formula_2">h b t = LSTM(h b t+1 , x t ),<label>(3)</label></formula><formula xml:id="formula_3">h t = [h f t ; h b t ].<label>(4)</label></formula><p>Thus, the outputs of BiLSTM encoder are a sequence of vectors</p><formula xml:id="formula_4">H = [h 1 , h 2 , · · · , h L ].<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Aggregation Layer</head><p>Encoding layer only models dependency between adjacent words, but the final prediction of the text requires a fix-length vector. Therefore we need aggregate information from variable length sequence to a single fix-length vector. There are several different ways of aggregation such as max or average pooling, and context-attention.</p><p>Max or Average Pooling Max or Average pooling is a simple way of aggregating information, which does not require extra parameters and is computationally efficient <ref type="bibr" target="#b6">(Kim, 2014;</ref><ref type="bibr" target="#b24">Zhao et al., 2015;</ref><ref type="bibr" target="#b10">Lin et al., 2017)</ref>. In the process of modeling natural language, max or average pooling is performed along the time dimension.</p><formula xml:id="formula_5">e max = max([h 1 , h 2 , · · · , h L ]),<label>(6)</label></formula><formula xml:id="formula_6">e avg = 1 L L i=1 h i ,<label>(7)</label></formula><p>For example,in Equation 6 the max operation is performed on each dimension of h along time dimension. And in Equation 7 the average operation is performed along time dimension. Max pooling is empirically better at aggregating long sentences than average pooling. We assume it's because that, the actual word that contributes to the classification problem is far less than the number of words that contain in a long sentence. Information from important words is weakened by a large population of "boring" words.</p><p>Self-Attention As has been stated previously, average pooling is prone to weaken important words when the sentence is longer. Self-Attention assigns each word a weight to indicate the importance of a word depending on the task on hand. A few words that are crucial to the task will be emphasized while the "boring" words are ignored. The self-attention process is formulated as follows:</p><formula xml:id="formula_7">u i = q T h i ,<label>(8)</label></formula><formula xml:id="formula_8">a i = exp(u i ) k exp(u k ) ,<label>(9)</label></formula><formula xml:id="formula_9">e attn = L i=1 a i · h i<label>(10)</label></formula><p>First, we need a task-specific trainable query q ∈ R d to calculate similarity weight between query and each contextually encoded word. Then the corresponding weights are normalized across time dimension using softmax normalization function Eq. 9, after that the aggregated vector is simply a weighted sum of the input sequence in Eq. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Prediction Layer</head><p>Then we feed the encoding e to the input of a multi-layer perceptron (MLP), followed by a softmax classifier.</p><formula xml:id="formula_10">p(·|e) = softmax(MLP(e))</formula><p>where p(·|e) is the predicted distribution of different classes given the representation vector m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Aggregation via Dynamic Routing</head><p>In this section, we will formally introduce dynamic routing in detail. The goal of dynamic routing is to encode the meaning of X into M fix-length vectors</p><formula xml:id="formula_11">V = [v 1 , v 2 , · · · , v M ].<label>(11)</label></formula><p>To transfer information from a variable number of representation H to a fixed number of vectors V , a key problem we need to slove is to properly design a routing policy of information transfer. In other words, what and how much information is to be transferred from h i to v j .</p><p>Although self-attention has been applied in aggregation, the notion of summing up elements in the attention mechanism is still very primitive. Inspired by the capsule networks <ref type="bibr" target="#b13">(Sabour et al., 2017)</ref>, we propose a dynamic routing aggregation (DR-AGG) mechanism to compute the final encoding of text sequence.  <ref type="formula">a)</ref> is the overall dynamic routing diagram, the width of edges between capsules indicate the amount of information transfered, which is refined iteratively. (b) is a detailed iterative process of transferring information from capsule h i to capsule v j , where is a inner product operation and ⊗ is a element-wise product.</p><p>Following the definition of capsule networks, we call each encoding vector, or a group of neurons, as a capsule. Thus, H denotes the input capsules, and V denotes the output capsules.</p><p>A message vector m i→j denotes the information to be transferred from h i to v j .</p><formula xml:id="formula_12">m i→j = c ij f (h i , θ j ),<label>(12)</label></formula><p>where c ij indicates proportionally how much information is to be transferred, and f (h i , θ j ) is a one-layer fully-connected network parameterized by θ j , indicating which aspect of information is to be transferred. The output capsule v j first aggregates all the incoming messages</p><formula xml:id="formula_13">s j = L i=1 m i→j ,<label>(13)</label></formula><p>and then squashes s j to confine |s j | ∈ (0, 1) to a probability,</p><formula xml:id="formula_14">v j = s j 2 1 + s j 2 s j s j<label>(14)</label></formula><p>Dynamic Routing Process The dynamic routing process is implemented by an iterative process of refining the coupling coefficient c ij , which define proportionally how much information is to be transferred from h i to v j . The coupling coefficient c ij is computed by</p><formula xml:id="formula_15">c ij = exp(b ij ) k exp(b ik ) ,<label>(15)</label></formula><formula xml:id="formula_16">b ij ← b ij + v T j f (h i , θ j ),<label>(16)</label></formula><p>where b ij is the log probabilities, initialized with 0.</p><p>The coefficients c ij is computed using a softmax function, and M j=1 c ij = 1. Therefore, the total amount of information transferred from capsule h i is proportionally summed to one.</p><p>When an output capsule v j receives the incoming messages, its state will be updated and the coefficient c ij is also re-computed for each input capsule. Thus, we iteratively refine the route of information passing, towards an instance dependent and context aware encoding of a sequence. After the text sequence is encoded into M capsules, We map these capsules into vector representation by simply concatenating all capsules, <ref type="figure">Figure 1</ref> gives an illustration of the dynamic routing mechanism. The detailed dynamic routing algorithm is further described in detail in Algorithm 1.</p><formula xml:id="formula_17">e = [v 1 ; . . . ; v M ].<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Dynamic Routing Algorithm</head><p>Data: Input Capsules: h 1 , h 2 , · · · , h L , Maximum number of Iterations: </p><formula xml:id="formula_18">T Result: Output Capsules: v 1 , v 2 , · · · , v M Initialize b ij ← 0 ; for t = 1 to T do</formula><formula xml:id="formula_19">c ij = exp(b ij ) k exp(b kj )<label>(18)</label></formula><p>Other detail of reversed dynamic routing is the same as the standard dynamic routing. The reversed DR-AGG works like the multi-hop memory network in iteratively aggregating information <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b8">Kumar et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Analysis</head><p>The DR-AGG is somewhat like attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b21">Vaswani et al., 2017)</ref>.however, there are differences.</p><p>In standard DR-AGG, each input capsule (encoding of each word) is employed as query vector to assign a proportion weight to each output capsule, and then sends messages to the output capsules in proportion. Thus, for all input capsules the total amount of messages sent from an input capsule are the same.</p><p>In reversed DR-AGG, each output capsule is used as query vector to assign a proportion weight to each input capsule and then receives messages from the input capsules in proportion. Thus, for all output capsules the total amount of message received by an output capsule is same.</p><p>The major difference between DR-AGG and self-attention <ref type="bibr" target="#b10">(Lin et al., 2017;</ref><ref type="bibr" target="#b23">Yang et al., 2016)</ref> is that the query vector of self-attention is task dependent trainable parameters learned during the training phase, while the query vector of DR-AGG is each input or output capsule which is instance dependent and dynamically updated. Additionally, the self-attention aggregation collects information in a bottom-up way, without considering the state of the final encoding. It is hard to avoid the problems of information redundancy and information loss. While in the standard DR-AGG, each word can iteratively decide what and how much information is to be sent to the final encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hierarchical Dynamic Routing for Long Text</head><p>The dynamic routing mechanism can aggregate the text sequence with any length, therefore it is able to handle long texts directly, such as the whole paragraphs or documents.</p><p>To further enhance the efficiency and scalability of information aggregation, we adopt a hierarchical dynamic routing mechanism to handle the long text. The hierarchical routing strategy can exploit more parallelization and speed up training and inference process. A similar strategy is also used in <ref type="bibr" target="#b23">(Yang et al., 2016)</ref>.</p><p>Concretely, we split a document into sentences, and apply the proposed dynamic routing mechanism on word and sentence levels separately. We first encode each sentence into a fixed-length vector, then convert the sentence encodings into document encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We test the empirical performance of our proposed model on 5 benchmark datasets for document and sentence level classification and compare our proposed model to other competitor models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>To evaluate the effectiveness of our proposed aggregation method, we have conducted experiments on 5 datasets, the statistics of experimented datasets are shown in <ref type="table">Table 1</ref>. As shown in the table, Yelp-2013, Yelp-2014, and IMDB are document level datasets, while SST-1 and SST-2 are sentence level datasets. Note that we use the same document level datasets provided in <ref type="bibr" target="#b20">(Tang et al., 2015)</ref>.</p><p>Yelp reviews Yelp-2013 and Yelp-2014 are reviews from Yelp, each example consists of several review sentences and a rating score range from 1 to 5 (higher is better).</p><p>IMDB is a movie review dataset extracted from IMDB website. It is a multi-sentence dataset that for each example there are several review sentences. A rating score range from 1 to 10 is also associated with each example.</p><p>SST-1 Stanford Sentiment Treebank is a movie review dataset which has been parsed and further splited to train/dev/test set <ref type="bibr" target="#b17">(Socher et al., 2013)</ref>. For each example in the dataset, there exists only one sentence and a label associated with it. And the labels can be one of {negative, somewhat negative, neutral, somewhat positive, positive}.</p><p>SST-2 This dataset is a binary-class version of SST-1, with neutral reviews removed and the remaining reviews categorized to either negative or positive.</p><p>Yelp-2013 Yelp-2014 IMDB SST-1 SST- <ref type="table" target="#tab_2">2   Embedding size  300  300  300  300  300  LSTM hidden unit  200  200  200  200  200  Capsule dimension  200  200  200  200  200  Capsule number  5  5  5  5  5  Iteration number  3  3  3  3  3  Regularization</ref>   </p><formula xml:id="formula_20">(i) , t (i) } N i=1 , where x (i)</formula><p>is an example of the training set and t (i) is the corresponding label, the goal is to minimize the cross-entropy loss J (θ):</p><formula xml:id="formula_21">J (θ) = − 1 N i log p(t (i) |x (i) ; θ)+λ||θ|| 2 2 ,<label>(19)</label></formula><p>where θ represents all of the parameters. The Adam optimizer is applied to update the parameters <ref type="bibr" target="#b7">(Kingma and Ba, 2014)</ref>. <ref type="table" target="#tab_2">Table 2</ref> displays the detailed hyper-parameter settings. To prevent overfitting, the L2 regularization term is introduced to our loss function. We also adopt early stop strategy, The training process will be stopped after seven epochs of no improvement on development set is observed.To further avoid overfitting, dropout is applied before the biLSTM encoder and hidden layer of classifier MLP.</p><p>The mini-batch size is set to 32 for document level dataset, 64 for sentence level dataset, examples are sampled from a sliding bucket to speed up the training process. Data is sorted by the length of sentence, and we first sample a window on the sorted data, we call the window "sliding bucket" and then sample a batch of examples from the sliding bucket, we double the window size after an epoch of no improvement on development set, through such a strategy, we are able to considerably speed up training while retaining randomness. Also, batch size is halved after an epoch of no improvement on development set until it reaches the low bound batch size. We also utilize a data preparation queue to parallelize data preparation and training.</p><p>Word embedding is initialized from pre-trained Glove <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref>. We randomly initialize word vectors for words that doesn't appear in Glove. Network weights are initialized with Xavier Normalization <ref type="bibr" target="#b4">(Glorot and Bengio, 2010)</ref>. A more detailed hyper-parameter setting can be referred to hyper-parameter <ref type="table" target="#tab_2">Table 2</ref>. And hyper-parameters are determined using grid search strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>We evaluate several aggregation methods on five text classification datasets, in which Yelp-2013, Yelp-2014 and IMDB are document-level datasets, and SST-1 and SST-2 are sentence-level datasets. Since max pooling, average pooling and self-attention are most related to our proposed DR-AGG, we mainly compare DR-AGG to these three methods. <ref type="table" target="#tab_4">Table 3</ref> gives the results for different methods, the last two rows are our model ( standard DR-AGG and reversed DR-AGG), the table shows that our proposed dynamic routing performed the best on all datasets. In document-level text classification, specifically Yelp 2013 Yelp 2014 and IMDB, DR-AGG outperforms previous models best results by 2.5%, 3.0% and 1.6% respectively. In sentence-level text Yelp-2013 Yelp-2014 IMDB SST-1 SST-2 RNTN+Recurrent <ref type="bibr" target="#b17">(Socher et al., 2013)</ref> 57.4 58.2 40.0 --CNN-non-static <ref type="bibr" target="#b6">(Kim, 2014)</ref> ---48.0 87.2 Paragraph-Vec <ref type="bibr" target="#b9">(Le and Mikolov, 2014)</ref> ---48.7 87.8 MT-LSTM (F2S)  ---49.1 87.2 UPNN(np UP) <ref type="bibr" target="#b20">(Tang et al., 2015)</ref> 57.7 58.5 40.5 --UPNN(full) <ref type="bibr" target="#b20">(Tang et al., 2015)</ref> 59.6 60.8 43.5 --Cached LSTM <ref type="bibr" target="#b22">(Xu et al., 2016)</ref> 59.4 59.  (1) so relentlessly wholesome it made me want to swipe something .</p><p>(2) so relentlessly wholesome it made me want to swipe something .</p><p>(3) so relentlessly wholesome it made me want to swipe something . classification, such as SST-1 SST-2, our model also achieves better results. Compared to max pooling, average pooling and self-attention, which are closely related to our model, DR-AGGs significantly improves the performance. For example the standard DR-AGG outperforms the max pooling approach by 1%, 1.8%, 4%,2.5% and 0.4% on Yelp 2013,Yelp 2014, IMDB, SST-1 and SST-2. It empirically shows that our proposed dynamic routing policy is the most effective method on aggregating information. It is worth to note the reversed DR-AGG is inferior to the standard DR-AGG by a small margin, although it has also achieved better results than the other aggregation methods and SOTA approaches. As discussion before, the reversed DR-AGG have much resemblance with the attention using output capsule as query vector. Not all of the input capsules would be selected by the reversed DR-AGG, while in the standard DR-AGG, the information of all the input capsules need be sent to the output capsules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Iterative Routing</head><p>We also study how the iteration number affect the performance of aggregation on the SST-2 dataset. <ref type="figure">Figure 2</ref> shows the comparison of 1 -5 iterations in the standard DR-AGG. The capsule number is set to 1, 2, 3 and 4 for each comparison respectively. We found that the performances on several different capsule number setting reach the best when iteration is set to 3. The results indicate the dynamic routing is contributing to improve the performance.</p><p>Visualization Additionally, we visualize how much information each input capsule sends to the output capsules. As shown in <ref type="table" target="#tab_5">Table 4</ref>, the visualization experiment was conducted with the setting on three output capsules. The i-th column represents the i-th input capsule, while the j-th row is the j-th output capsule. The color density of each word denotes the proportion c ij in equation 15. A deeper color indicates more information of the concerned word is routed to the output capsule.</p><p>Intuitively, the different part of the sentence is routed to three different capsules. In another word, each capsule has a different perspective or focus of the sequence. Therefore, DR-AGG can avoid the problem of information redundancy and information missing. Currently, much attention has been paid to how developing a sophisticated encoding models to capture the long and short term dependency information in a sequence. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN <ref type="bibr" target="#b16">(Socher et al., 2012)</ref>, RNTN <ref type="bibr" target="#b17">(Socher et al., 2013)</ref>, CNN <ref type="bibr" target="#b6">(Kim, 2014)</ref>, AdaSent <ref type="bibr" target="#b24">(Zhao et al., 2015)</ref>, and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task. There are also many works <ref type="bibr" target="#b22">Xu et al., 2016;</ref><ref type="bibr" target="#b1">Cheng et al., 2016)</ref> to improve LSTM's ability to carrying information for a long distance. A line of orthogonal researches <ref type="bibr" target="#b10">(Lin et al., 2017;</ref><ref type="bibr" target="#b23">Yang et al., 2016;</ref><ref type="bibr" target="#b14">Shen et al., 2018a;</ref><ref type="bibr" target="#b15">Shen et al., 2018b)</ref> is to introduce attention mechanism <ref type="bibr" target="#b21">(Vaswani et al., 2017)</ref> to weighted average the outputs of CNN/RNN layer. The attention mechanism can effectively reduce the burden of CNN/RNN. The CNN/RNN encoding layer is only expected to extract local context information for each word, while the global semantics of text sequence can be aggregated from the local encoding vectors.</p><p>The attention based aggregation collects information in a bottom-up way, without considering the state of the final encoding. It is hard to avoid the problems of information redundancy or information lost. An improved idea is to use multi-hop attention, like memory network <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b8">Kumar et al., 2015)</ref>, to iterative aggregate information. This idea is equivalent to our proposed reversed dynamic routing mechanism.</p><p>Different from the attention based aggregation methods, aggregation via dynamic routing is iteratively deciding that what and how much information need be transfer to the final encoding of each word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we focus on how to obtain a fixed-size encoding of text sequence by aggregating the encodings of each word. Although we use LSTM hidden states as word encoding in this paper, the other word encodings, such as convolved n-gram, could be alternatively used. We introduced a fixed-size encoding of text sequence with dynamic routing mechanism. Experimental results of five text classification tasks show that the model outperforms other encoding models by a significant margin.</p><p>In the future, we would like to investigate more sophisticated routing policy for better encoding the text sequence. Besides, dynamic routing should also be useful to improve the encoder in the sequenceto-sequence tasks <ref type="bibr" target="#b19">(Sutskever et al., 2014)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Detialed dynamic routing process Figure 1: Diagram of dynamic routing for sequence encoding. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Compute the routing coefficients c ij for all i ∈ [1, L], j ∈ [1, M ] ; / * Eq.15 * / Update all the output capsule v j , j ∈ [1, M ] ; / * Eq. 13 and 14 * / Update b ij for alli ∈ [1, L], j ∈ [1, M ] ; / * Eq. 16 * / end for return v 1 , v 2 , · · · , v MReversed Dynamic Routing Process In standard DR-AGG, an input capsule decides what proportion of information can be transferred to an output capsule. We also explore a reversed dynamic routing, in which the output capsule decides what proportion of information should be received from an input capsule. The only difference between reversed dynamic routing and standard dynamic routing is how the softmax function was applied to the log probabilities [b ij ] L×M . Instead of normalizing each row of [b ij ] L×M as is done in standard DR-AGG, reverse dynamic routing normalizes each column of [b ij ] L×M ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Detailed hyper-parameter settings</cell></row><row><cell>5.2 Training</cell></row><row><cell>Given a training set {x</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experimental result comparison on five datasets. For the document-level datasets, hierarchical aggregation is used for both self-attention and DR-AGGs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>A visualization to show the perspective of a sentence from 3 different upper level capsule. A deeper color indicates more information of the associated word is routed to the corresponding capsule.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-timescale long short-term memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="2326" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DISAN: Directional self-attention network for RNN/CNN-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bi-directional block self-attention for fast and memory-efficient sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc Vv</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cached long short-term memory neural networks for document-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1610.04989</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05070</idno>
		<title level="m">Self-adaptive hierarchical sentence model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
