<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Random Erasing Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
							<email>zhunzhong007@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Cognitive Science Department</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
							<email>liangzheng06@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
							<email>guoliang.kang@student.uts.edu.auszlig@xmu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cognitive Science Department</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yee.i.yang@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Random Erasing Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person reidentification. Code is available at: https://github. com/zhunzhong07/Random-Erasing.</p><p>1 In Section 5.1.2, we show erasing with random values achieves approximately equal performance to the ImageNet mean pixel value.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to generalize is a research focus for the convolutional neural network (CNN). When a model is excessively complex, such as having too many parameters compared to the number of training samples, over-fitting might happen and weaken its generalization ability. A learned model may describe random error or noise instead of the underlying data distribution <ref type="bibr" target="#b34">[35]</ref>. In bad cases, the CNN model may exhibit good performance on the training data, but fail drastically when predicting new data. To improve the generalization ability of CNNs, many data augmentation and regularization approaches have been proposed, such as random cropping <ref type="bibr" target="#b13">[14]</ref>, flipping <ref type="bibr" target="#b20">[21]</ref>, dropout <ref type="bibr" target="#b21">[22]</ref>, and batch normalization <ref type="bibr" target="#b10">[11]</ref>.</p><p>Occlusion is a critical influencing factor on the generalization ability of CNNs. It is desirable that invariance to various levels of occlusion is achieved. When some parts of an object are occluded, a strong classification model should be able to recognize its category from the overall object structure. However, the collected training samples usually exhibit limited variance in occlusion. In an extreme case when all the training objects are clearly visible, i.e., no occlusion happens, the learned CNN will probably work well on testing images without occlusion, but, due to the limited generalization ability of the CNN model, may fail to recognize objects which are partially occluded. While we can manually add occluded natural images to the training set, it is costly and the levels of occlusion might be limited.</p><p>To address the occlusion problem and improve the generalization ability of CNNs, this paper introduces a new data augmentation approach, Random Erasing. It can be easily implemented in most existing CNN models. In the training phase, an image within a mini-batch randomly undergoes either of the two operations: 1) kept unchanged; 2) we randomly choose a rectangle region of an arbitrary size, and assign the pixels within the selected region with random values (or the ImageNet <ref type="bibr" target="#b4">[5]</ref> mean pixel value) <ref type="bibr" target="#b0">1</ref> . During Operation 2), an image is partially occluded in a random posi-tion with a random-sized mask. In this manner, augmented images with various occlusion levels can be generated. Examples of Random Erasing are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Two commonly used data augmentation approaches, i.e., random flipping and random cropping, also work on the image level and are closely related to Random Erasing. Both techniques have demonstrated the ability to improve the image recognition accuracy. In comparison with Random Erasing, random flipping does not incur information loss during augmentation. Different from random cropping, in Random Erasing, 1) only part of the object is occluded and the overall object structure is preserved, 2) pixels of the erased region are re-assigned with random values, which can be viewed as adding block noise to the image.</p><p>Working primarily on the fully connected (FC) layer, Dropout <ref type="bibr" target="#b21">[22]</ref> is also related to our method. It prevents overfitting by discarding (both hidden and visible) units of the CNN with a probability p. Random Erasing is somewhat similar to performing Dropout on the image level. The difference is that in Random Erasing, 1) we operate on a continuous rectangular region, 2) no pixels (units) are discarded, and 3) we focus on making the model more robust to noise and occlusion. The recent A-Fast-RCNN <ref type="bibr" target="#b26">[27]</ref> proposes an occlusion invariant object detector by training an adversarial network that generates examples with occlusion. Comparison with A-Fast-RCNN, Random Erasing does not require any parameter learning, can be easily applied to other CNN-based recognition tasks and still yields competitive accuracy with A-Fast-RCNN in object detection. To summarize, Random Erasing has the following advantages:</p><p>• A lightweight method that does not require any extra parameter learning or memory consumption. It can be integrated with various CNN models without changing the learning strategy.</p><p>• A complementary method to existing data augmentation and regularization approaches. When combined, Random Erasing further improves the recognition performance.</p><p>• Consistently improving the performance of recent state-of-the-art deep models on image classification, object detection, and person re-ID.</p><p>• Improving the robustness of CNNs to partially occluded samples. When we randomly adding occlusion to the CIFAR-10 testing dataset, Random Erasing significantly outperforms the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Regularization is a key component in preventing overfitting in the training of CNN models. Various regularization methods have been proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Dropout <ref type="bibr" target="#b13">[14]</ref> randomly discards (setting to zero) the output of each hidden neuron with a probability during the training and only considers the contribution of the remaining weights in forward pass and back-propagation. Latter, Wan et al. <ref type="bibr" target="#b25">[26]</ref> propose a generalization of dropout approach, DropConect, which instead randomly selects weights to zero during training. In addition, Adaptive dropout <ref type="bibr" target="#b0">[1]</ref> is proposed where the dropout probability for each hidden neuron is estimated through a binary belief network. Stochastic Pooling <ref type="bibr" target="#b33">[34]</ref> randomly selects activation from a multinomial distribution during training, which is parameter free and can be applied with other regularization techniques. Recently, a regularization method named "Distur-bLabel" <ref type="bibr" target="#b29">[30]</ref> is introduced by adding noise at the loss layer. DisturbLabel randomly changes the labels of small part of samples to incorrect values during each training iteration. PatchShuffle <ref type="bibr" target="#b11">[12]</ref> randomly shuffles the pixels within each local patch while maintaining nearly the same global structures with the original ones, it yields rich local variations for training of CNN.</p><p>Data augmentation is an explicit form of regularization that is also widely used in the training of deep CNN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref>. It aims at artificially enlarging the training dataset from existing data using various translations, such as, translation, rotation, flipping, cropping, adding noises, etc. The two most popular and effective data augmentation methods in training of deep CNN are random flipping and random cropping. Random flipping randomly flips the input image horizontally, while random cropping extracts random sub-patch from the input image. As an analogous choice, Random Erasing may discard some parts of the object. For random cropping, it may crop off the corners of the object, while Random Erasing may occlude some parts of the object. Random Erasing maintains the global structure of object. Moreover, it can be viewed as adding noise to the image. The combination of random cropping and Random Erasing can produce more various training data. Recently, Wang et al. <ref type="bibr" target="#b26">[27]</ref> learn an adversary with Fast-RCNN <ref type="bibr" target="#b6">[7]</ref> detection to create hard examples on the fly by blocking some feature maps spatially. Instead of generating occlusion examples in feature space, Random Erasing generates images from the original images with very little computation which is in effect, computationally free and does not require any extra parameters learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>For image classification, we evaluate on three image classification datasets, including two well-known datasets, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b12">[13]</ref>, and a new dataset Fashion-MNIST <ref type="bibr" target="#b27">[28]</ref>. CIFAR-10 and CIFAR-100 contain 50,000 training and 10,000 testing 32×32 color images drawn from 10 and 100 classes, respectively. Fashion-MNIST consists of 60,000 training and 10,000 testing 28x28 gray-scale images. Each image is associated with a label from 10 classes. We evaluate top-1 error rates in the format "mean ± std" based on 5 runs.</p><p>For object detection, we use the PASCAL VOC 2007 <ref type="bibr" target="#b5">[6]</ref> dataset which contains 9,963 images of 24,640 annotated objects in training/validation and testing sets. We use the "trainval" set for training and "test" set for testing.</p><p>For person re-identification (re-ID), the Market-1501 dataset <ref type="bibr" target="#b37">[38]</ref> contains 32,668 labeled bounding boxes of 1,501 identities captured from 6 different cameras. The dataset is split into two parts: 12,936 images with 751 identities for training and 19,732 images with 750 identities for testing. In testing, 3,368 hand-drawn images with 750 identities are used as probe set to identify the correct identities on the testing set. DukeMTMC-reID <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b18">19]</ref>   <ref type="bibr" target="#b40">[41]</ref> to evaluate the multi-shot re-ID performance. There are 767 identities in the training set and 700 identities in the testing set. We conduct experiment on both "detected" and "labeled" sets. We evaluate using rank-1 accuracy and mean average precision (mAP) on these three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Approach</head><p>This section presents the Random Erasing data augmentation method for training the convolutional neural network (CNN). We first describe the detailed procedure of Random Erasing. Next, the implementation of Random Erasing in different tasks is introduced. Finally, we analyze the differences between Random Erasing and random cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Random Erasing</head><p>In training, Random Erasing is conducted with a certain probability. For an image I in a mini-batch, the probability of it undergoing Random Erasing is p, and the probability of it being kept unchanged is 1 − p. In this process, training images with various levels of occlusion are generated.</p><p>Random Erasing randomly selects a rectangle region I e in an image, and erases its pixels with random values. Assume that the size of the training image is W × H. The area of the image is S = W × H. We randomly initialize the area of erasing rectangle region to S e , where Se S is in range specified by minimum s l and maximum s h . The aspect ratio of erasing rectangle region is randomly initialized between r 1 and r 2 , we set it to r e . The size of I e is H e = √ S e × r e and W e = Se re . Then, we randomly initialize a point P = (x e , y e ) in I. If x e + W e ≤ W and y e + H e ≤ H, we set the region, I e = (x e , y e , x e + W e , y e + H e ), as the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Random Erasing for Object Detection</head><p>Object detection aims at detecting instances of semantic objects of a certain class in images. Since the location of each object in the training image is known, we implement Random Erasing with three schemes:1) Image-aware Random Erasing (IRE): selecting erasing regions on the whole image, the same as image classification and person re-identification; 2) Object-aware Random Erasing (ORE): selecting erasing regions in the bounding box of each object. In the latter, if there are multiple objects in the image, Random Erasing is applied on each object separately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Random Cropping</head><p>Random cropping is an effective data augmentation approach, it reduces the contribution of the background in the CNN decision, and can base learning models on the presence of parts of the object instead of focusing on the whole object. In comparison to random cropping, Random Erasing retains the overall structure of the object, only occluding some parts of object. In addition, the pixels of erased region are re-assigned with random values, which can be viewed as adding noise to the image. In our experiment (Section 5.1.2), we show that these two methods are complementary to each other for data augmentation. The examples of Random Erasing, random cropping, and the combination of them are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>  deep architecture, all the models are trained from the same weight initialization. Note that some popular regularization techniques (e.g., weight decay, batch normalization and dropout) and various data augmentations (e.g., flipping, padding and cropping) are employed. The compared CNN architectures are summarized as below:</p><p>Architectures.</p><p>Four architectures are adopted on CIFAR-10, CIFAR-100 and Fashion-MNIST: ResNet <ref type="bibr" target="#b7">[8]</ref>, pre-activation ResNet <ref type="bibr" target="#b8">[9]</ref>, ResNeXt <ref type="bibr" target="#b30">[31]</ref>, and Wide Residual Networks <ref type="bibr" target="#b32">[33]</ref>. We use the 20, 32, 44, 56, 110-layer network for ResNet and pre-activation ResNet. The 18layer network is also adopted for pre-activation ResNet. We use ResNeXt-29-8×64 and WRN-28-10 in the same way as <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b32">[33]</ref>, respectively. The training procedure follows <ref type="bibr" target="#b7">[8]</ref>. Specially, the learning rate starts from 0.1 and is divided by 10 after the 150th and 225th epoch. We stop training by the 300th epoch. If not specified, all models are trained with data augmentation: randomly performs horizontal flips, and takes a random crop with 32×32 for CIFAR-10 and CIFAR-100 (28×28 for Fashion-MNIST) from images padded by 4 pixels on each side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Classification Evaluation</head><p>Classification accuracy on different datasets. The results of applying Random Erasing on CIFAR-10 ,CIFAR-100 and Fashion-MNIST with different architectures are shown in <ref type="table" target="#tab_1">Table 1</ref>. We set p = 0.5, s l = 0.02, s h = 0.4, and r 1 = 1 r2 = 0.3. Results indicate that models trained with Random Erasing have significant improvement, demonstrating that our method is applicable to various CNN architectures. For CIFAR-10, our method improves the accuracy by 0.49% and 0.33% using ResNet-110 and ResNet-110-PreAct, respectively. In particular, our approach obtains 3.08% error rate using WRN-28-10, which improves the accuracy by 0.72% and achieves new state of the art. For CIFAR-100, our method obtains 17.73% error rate which gains 0.76% than the WRN-28-10 baseline. Our method also works well for gray-scale images: Random erasing im-  proves WRN-28-10 from 4.01% to 3.65% in top-1 error on Fashion-MNIST.</p><p>The impact of hyper-parameters. When implementing Random Erasing on CNN training, we have three hyperparameters to evaluate, i.e., the erasing probability p, the area ratio range of erasing region s l and s h , and the aspect ratio range of erasing region r 1 and r 2 . To demonstrate the impact of these hyper-parameters on the model performance, we conduct experiment on CIFAR-10 based on ResNet18 (pre-act) under varying hyper-parameter settings. To simplify experiment, we fix s l to 0.02, r 1 = 1 r2 and evaluate p, s h , and r 1 . We set p = 0.5, s h = 0.4 and r 1 = 0.3 as the base setting. When evaluating one of the parameters, we fixed the other two parameters. Results are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>Notably, Random Erasing consistently outperforms the ResNet18 (pre-act) baseline under all parameter settings. For example, when p ∈ [0.2, 0.8] and s h ∈ [0.2, 0.8], the average classification error rate is 4.48%, outperforming the baseline method (5.17%) by a large margin. Random Erasing is also robust to the aspect ratios of the erasing region. Specifically, our best result (when r 1 = 0.3, error rate = 4.31%) reduces the classification error rate by 0.86% compared with the baseline. In the following experiment for image classification, we set p = 0.5, s l = 0.02, s h = 0.4, and r 1 = 1 r2 = 0.3, if not specified.</p><p>Four types of random values for erasing. We evaluate Random Erasing when pixels in the selected region are erased in four ways: 1) each pixel is assigned with a random value ranging in [0, 255], denoted as RE-R; 2) all pixels are assign with the mean ImageNet pixel value i.e., <ref type="bibr">[125,</ref><ref type="bibr">122,</ref><ref type="bibr">114]</ref>, denoted as RE-M; 3) all pixels are assigned with 0, denoted as RE-0; 4) all pixels are assigned with 255, denoted as RE-255. <ref type="table" target="#tab_4">Table 2</ref> presents the result with different erasing values on CIFAR10 using ResNet18 (pre-act). We observe that, 1) all erasing schemes outperform the baseline, 2) RE-R achieves approximately equal performance to RE-M, and 3) both RE-R and RE-M are superior to RE-0 and RE-255. If not specified, we use RE-R in the following experiment.</p><p>Comparison with Dropout and random noise. We compare Random Erasing with two variant methods applied on image layer. 1) Dropout: we apply dropout on image layer with probability λ 1 . 2) Random noise: we add different levels of noise on the input image by changing the pixel to a random value in [0, 255] with probability λ 2 . The probability of whether an image undergoes dropout or random noise is set to 0.5 as Random Erasing. Results are presented in <ref type="table">Table 3</ref>. It is clear that applying dropout or adding random noise at the image layer fails to improve the accuracy. As the probability λ 1 and λ 2 increase, performance drops quickly. When λ 2 = 0.4, the number of noise pix-  <ref type="table">Table 3</ref>. Comparing Random Erasing with dropout and random noise on CIFAR-10 with using ResNet18 (pre-act). els for random noise is approximately equal to the number of erasing pixels for Random Erasing, the error rate of random noise increases from 5.17% to 6.52%, while Random Erasing reduces the error rate to 4.31%.</p><p>Comparing with data augmentation methods. We compare our method with random flipping and random cropping in <ref type="table" target="#tab_6">Table 4</ref>. When applied alone, random cropping (6.33%) outperforms the other two methods. Importantly, Random Erasing and the two competing techniques are complementary. Particularly, combining these three methods achieves 4.31% error rate, a 7% improvement over the baseline without any augmentation.</p><p>Robustness to occlusion. Last, we show the robustness of Random Erasing against occlusion. In this experiment, we add different levels of occlusion to the CIFAR-10 dataset in testing. We randomly select a region of area and fill it with random values. The aspect ratio of the region is randomly chosen from the range of [0.3, 3.33]. Results as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Obviously, the baseline performance drops quickly when increasing the occlusion level l. In comparison, the performance of the model training with Random Erasing decreases slowly. Our approach achieves 56.36% error rate when the occluded area is half of the image (l = 0.5), while the baseline rapidly drops to 75.04%. It demonstrates that Random Erasing improves the robustness of CNNs against occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Experiment Settings</head><p>Experiment is conducted based on the Fast-RCNN <ref type="bibr" target="#b6">[7]</ref> detector. The model is initialized by the ImageNet classification models, and then fine-tuned on the object detection data. We experiment with VGG16 <ref type="bibr" target="#b20">[21]</ref> architecture. We follow A-Fast-RCNN <ref type="bibr" target="#b26">[27]</ref> for training. We apply SGD for 80K to train all models. The training rate starts with 0.001   and decreases to 0.0001 after 60K iterations. With this training procedure, the baseline mAP is slightly better than the report mAP in <ref type="bibr" target="#b6">[7]</ref>. We use the selective search proposals during training. For Random Erasing, we set p = 0.5, s l = 0.02, s h = 0.2, and r 1 = 1 r2 = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Detection Evaluation</head><p>We report results with using IRE, ORE and I+ORE during training Fast-RCNN in   <ref type="table">Table 6</ref>. Person re-identification performance with Random Erasing (RE) on Market-1501, DukeMTMC-reID, and CUHK03 based on different models. We evaluate CUHK03 under the new evaluation protocol in <ref type="bibr" target="#b40">[41]</ref>.</p><p>which the mAP of IRE is improved by 0.8% and ORE is improved by 1.0%. When applying I+ORE during training, the mAP of Fast-RCNN increases to 76.2%, surpassing the baseline by 1.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Person Re-identification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Experiment Settings</head><p>Three baselines are used in person re-ID, i.e., the IDdiscriminative Embedding (IDE) <ref type="bibr" target="#b38">[39]</ref>, TriNet <ref type="bibr" target="#b9">[10]</ref>, and SVDNet <ref type="bibr" target="#b23">[24]</ref>. IDE and SVDNet are trained with the Softmax loss, while TriNet is trained with the triplet loss. The input images are resized to 256 × 128. For IDE, we basically follow the training strategy in <ref type="bibr" target="#b38">[39]</ref>. We further add a fully connected layer with 128 units after the Pool5 layer, followed by batch normalization, ReLU and Dropout. The Dropout probability is set to 0.5. We use SGD to train IDE. The learning rate starts with 0.01 and is divided by 10 after each 40 epochs. We train 100 epochs in total. In testing, we extract the output of Pool5 as feature for Market-1501 and DukeMTMC-reID datasets, and the fully connected layer with 128 units as feature for CUHK03.</p><p>For TriNet and SVDNet, we use the same model as proposed in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b23">[24]</ref>, respectively, and follow the same training strategy. In testing, we extract the last fully connected layer with 128 units as feature for TriNet and extract the output of Pool5 for SVDNet. Note that, we use 256 × 128 as the input size to train SVDNet which achieves higher performance than the original paper using size 224 × 224.</p><p>We use the ResNet-18, ResNet-34, and ResNet-50 architectures for IDE and TriNet, and ResNet-50 for SVDNet. We fine-tune them on the model pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref>. We also perform random cropping and random horizontal flipping during training. For Random Erasing, we set p = 0.5, s l = 0.02, s h = 0.2, and r 1 = 1 r2 = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Person Re-identification Performance</head><p>Baseline Evaluation. The results of Random Erasing on Market-1501, DukeMTMC-reID, and CUHK03 with different baselines and architectures are shown in <ref type="table">Table 6</ref>. For Market-1501 and DukeMTMC-reID, the IDE <ref type="bibr" target="#b38">[39]</ref> and SVDNet <ref type="bibr" target="#b23">[24]</ref> baselines outperform the TriNet baseline <ref type="bibr" target="#b9">[10]</ref>.</p><p>Since there exists plenty of samples in each ID, the mod-  <ref type="bibr" target="#b24">[25]</ref> 65.88 39.55 IDE <ref type="bibr" target="#b38">[39]</ref> 72.54 46.00 MSCAN <ref type="bibr" target="#b14">[15]</ref> 80.31 57.53 DF <ref type="bibr" target="#b36">[37]</ref> 81.0 63.4 SSM <ref type="bibr" target="#b2">[3]</ref> 82.21 68.80 SVDNet <ref type="bibr" target="#b23">[24]</ref> 82.3 62.1 GAN <ref type="bibr" target="#b39">[40]</ref> 83.97 66.07 PDF <ref type="bibr" target="#b22">[23]</ref> 84 Random Erasing improves different baseline models. When implementing Random Erasing in these baseline models, we can observe that, Random Erasing consistently improves the rank-1 accuracy and mAP. Specifically, for Market-1501, Random Erasing improves the rank-1 by 3.10% and 2.67% for IDE and SVDNet with using ResNet-50. For DukeMTMC-reID, Random Erasing increases the rank-1 accuracy from 71.99% to 74.24% for IDE (ResNet-50) and from 76.82% to 79.31% for SVDNet (ResNet-50). For CUHK03, TriNet gains 8.28% and 5.0% in rank-1 accuracy when applying Random Erasing on the labeled and detected settings with ResNet-50, respectively. We note that, due to lack of adequate training data, over-fitting tend to occur on CUHK03. For example, a deeper architecture, such as ResNet-50, achieves lower performance than ResNet-34 when using the IDE mode on the detected subset. However, with our method, IDE (ResNet-50) outperforms IDE (ResNet-34). This indicates that our method can reduce the risk of over-fitting and improves the re-ID performance.</p><p>Comparison with the state-of-the-art methods. We compare our method with the state-of-the-art methods on  <ref type="bibr" target="#b38">[39]</ref> 65.22 44.99 GAN <ref type="bibr" target="#b39">[40]</ref> 67.68 47.13 OIM <ref type="bibr" target="#b28">[29]</ref> 68.1 47.4 TriNet <ref type="bibr" target="#b9">[10]</ref> 72.44 53.50 ACRN <ref type="bibr" target="#b19">[20]</ref> 72.58 51.96 SVDNet <ref type="bibr" target="#b23">[24]</ref> 76.7 56.8</p><p>SVDNet+Ours 79.31 62.44 SVDNet+Ours+re <ref type="bibr" target="#b40">[41]</ref> 84.02 78.28  <ref type="table">Table 9</ref>. Comparison of our method with state-of-the-art methods on the CUHK03 dataset using the new evaluation protocol in <ref type="bibr" target="#b40">[41]</ref>.</p><p>We use ResNet-50 as backbone.</p><p>Market-1501, DukeMTMC-reID, and CUHK03 in <ref type="table" target="#tab_10">Table 7</ref>, <ref type="table" target="#tab_12">Table 8</ref>, and <ref type="table">Table 9</ref>, respectively. Our method achieves competitive results with the state of the art. Specifically, based on SVDNet, our method obtains rank-1 accuracy = 87.08% for Market-1501, and rank-1 accuracy = 79.31% for DukeMTMC-reID. On CUHK03, based on TriNet, our method achieves rank-1 accuracy = 58.14% for the labeled setting, and rank-1 accuracy = 55.50% for the detected setting. When we further combine our system with re-ranking <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b1">2]</ref>, the final rank-1 performance arrives at 89.13% for Market-1501, 84.02% for DukeMTMC-reID, and 64.43% for CUHK03 under the detected setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a new data augmentation approach named "Random Erasing" for training the convolutional neural network (CNN). It is easy to implemented: Random Erasing randomly occludes an arbitrary region of the input image during each training iteration. Experiment conducted on CIFAR10, CIFAR100, and Fashion-MNIST with various architectures validate the effectiveness of our method. Moreover, we obtain reasonable improvement on object detection and person re-identification, demonstrating that our method has good performance on various recognition tasks. In the future work, we will apply our approach to other CNN recognition tasks, such as, image retrieval and face recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of Random Erasing. In CNN training, we randomly choose a rectangle region in the image and erase its pixels with random values or the ImageNet mean pixel value. Images with various levels of occlusion are thus generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>3) Image and object-aware Random Erasing (I+ORE): selecting erasing regions in both the whole image and each object bounding box. Examples of Random Erasing for object detection with the three schemes are shown in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Examples of Random Erasing, random cropping, and the combination of them. When combining these two augmentation methods, more various images can be generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Test errors (%) under different hyper-parameters on CIFAR-10 with using ResNet18 (pre-act).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Test errors (%) under different levels of occlusion on CIFAR-10 based on ResNet18 (pre-act).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Similar to Market-1501, it contains 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images. CUHK03 [16] contains 14,096 images of 1,467 identities. We use the new training/testing protocol proposed in</figDesc><table><row><cell>contains</cell></row><row><cell>36,411 images of 1,812 identities shot by 8 high-resolution</cell></row><row><cell>cameras.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Random Erasing Procedure Input : Input image I; Image size W and H; Area of image S; Erasing probability p; Erasing area ratio range s l and s h ; Erasing aspect ratio range r 1 and r 2 . Output: Erased image I * . Initialization: p 1← Rand (0, 1).</figDesc><table><row><cell></cell><cell>Image-aware Random Erasing</cell></row><row><cell>input image</cell><cell>Object-aware Random Erasing</cell></row><row><cell></cell><cell>Image and object-aware Random Erasing</cell></row><row><cell cols="2">Figure 2. Examples of Random Erasing for object detection</cell></row><row><cell cols="2">with Image-aware Random Erasing (IRE), Object-aware Ran-</cell></row><row><cell cols="2">dom Erasing (ORE) and Image and object-aware Random Erasing</cell></row><row><cell>(I+ORE).</cell><cell></cell></row></table><note>1 if p 1 ≥ p then2 I* ← I;3 return I* .4 else5 while True do6 Se ← Rand (s l , s h )×S;7 re ← Rand (r 1 , r 2 );8 He ← √ Se × r e , W e ← Se re ;9 x e ← Rand (0, W ), y e ← Rand (0, H);10 if x e + W e ≤ W and y e + H e ≤ H then11 Ie ← (x e , y e , x e + W e , y e + H e );12 I(I e ) ← Rand (0, 255);13 I* ← I;14 return I* .15 end 16 end 17 end selected rectangle region. Otherwise repeat the above pro- cess until an appropriate I e is selected. With the selected erasing region I e , each pixel in I e is assigned to a random value in [0, 255], respectively. The procedure of selecting the rectangle area and erasing this area is shown in Alg. 1.4.2. Random Erasing for Image Classification and Person Re-identification</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Experiment SettingsIn all of our experiment, we compare the CNN models trained with or without Random Erasing. For the same</figDesc><table><row><cell></cell><cell>Random Croping</cell></row><row><cell>5. Experiment 5.1. Image Classification 5.1.1 input image</cell><cell>Random Erasing Random Croping + Random Erasing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>± 0.17 6.73 ± 0.09 30.84 ± 0.19 29.97 ± 0.11 4.39 ± 0.08 4.02 ± 0.07 ResNet-32 6.41 ± 0.06 5.66 ± 0.10 28.50 ± 0.37 27.18 ± 0.32 4.16 ± 0.13 3.80 ± 0.05 ResNet-44 5.53 ± 0.08 5.13 ± 0.09 25.27 ± 0.21 24.29 ± 0.16 4.41 ± 0.09 4.01 ± 0.14 ResNet-56 5.31 ± 0.07 4.89 ± 0.07 24.82 ± 0.27 23.69 ± 0.33 4.39 ± 0.10 4.13 ± 0.42 Test errors (%) with different architectures on CIFAR-10, CIFAR-100 and Fashion-MNIST.</figDesc><table><row><cell></cell><cell cols="2">Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Baseline</cell><cell cols="5">CIFAR-10 Random Erasing</cell><cell></cell><cell></cell><cell cols="3">Baseline</cell><cell cols="5">CIFAR-100 Random Erasing</cell><cell></cell><cell></cell><cell cols="9">Fashion-MNIST Baseline Random Erasing</cell></row><row><cell></cell><cell cols="11">ResNet-20 7.21 ResNet-110 5.10 ± 0.07</cell><cell cols="4">4.61 ± 0.06</cell><cell></cell><cell cols="5">23.73 ± 0.37</cell><cell></cell><cell cols="3">22.10 ± 0.41</cell><cell></cell><cell></cell><cell cols="3">4.40 ± 0.10</cell><cell></cell><cell></cell><cell cols="3">4.01 ± 0.13</cell></row><row><cell></cell><cell cols="5">ResNet-20-PreAct</cell><cell></cell><cell></cell><cell cols="4">7.36 ± 0.11</cell><cell cols="4">6.78 ± 0.06</cell><cell></cell><cell cols="5">30.58 ± 0.16</cell><cell></cell><cell cols="3">30.18 ± 0.13</cell><cell></cell><cell></cell><cell cols="3">4.43 ± 0.19</cell><cell></cell><cell></cell><cell cols="3">4.02 ± 0.09</cell></row><row><cell></cell><cell cols="5">ResNet-32-PreAct</cell><cell></cell><cell></cell><cell cols="4">6.42 ± 0.11</cell><cell cols="4">5.79 ± 0.10</cell><cell></cell><cell cols="5">29.04 ± 0.25</cell><cell></cell><cell cols="3">27.82 ± 0.28</cell><cell></cell><cell></cell><cell cols="3">4.36 ± 0.02</cell><cell></cell><cell></cell><cell cols="3">4.00 ± 0.05</cell></row><row><cell></cell><cell cols="5">ResNet-44-PreAct</cell><cell></cell><cell></cell><cell cols="4">5.54 ± 0.16</cell><cell cols="4">5.09 ± 0.10</cell><cell></cell><cell cols="5">25.22 ± 0.19</cell><cell></cell><cell cols="3">24.10 ± 0.26</cell><cell></cell><cell></cell><cell cols="3">4.92 ± 0.30</cell><cell></cell><cell></cell><cell cols="3">4.23 ± 0.15</cell></row><row><cell></cell><cell cols="5">ResNet-56-PreAct</cell><cell></cell><cell></cell><cell cols="4">5.28 ± 0.12</cell><cell cols="4">4.84 ± 0.09</cell><cell></cell><cell cols="5">24.14 ± 0.25</cell><cell></cell><cell cols="3">22.93 ± 0.27</cell><cell></cell><cell></cell><cell cols="3">4.55 ± 0.30</cell><cell></cell><cell></cell><cell cols="3">3.99 ± 0.08</cell></row><row><cell></cell><cell cols="5">ResNet-110-PreAct</cell><cell></cell><cell></cell><cell cols="4">4.80 ± 0.09</cell><cell cols="4">4.47 ± 0.11</cell><cell></cell><cell cols="5">22.11 ± 0.20</cell><cell></cell><cell cols="3">20.99 ± 0.11</cell><cell></cell><cell></cell><cell cols="3">5.11 ± 0.55</cell><cell></cell><cell></cell><cell cols="3">4.19 ± 0.15</cell></row><row><cell></cell><cell cols="5">ResNet-18-PreAct</cell><cell></cell><cell></cell><cell cols="4">5.17 ± 0.18</cell><cell cols="4">4.31 ± 0.07</cell><cell></cell><cell cols="5">24.50 ± 0.29</cell><cell></cell><cell cols="3">24.03 ± 0.19</cell><cell></cell><cell></cell><cell cols="3">4.31 ± 0.06</cell><cell></cell><cell></cell><cell cols="3">3.90 ± 0.06</cell></row><row><cell></cell><cell cols="3">WRN-28-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">3.80 ± 0.07</cell><cell cols="4">3.08 ± 0.05</cell><cell></cell><cell cols="5">18.49 ± 0.11</cell><cell></cell><cell cols="3">17.73 ± 0.15</cell><cell></cell><cell></cell><cell cols="3">4.01 ± 0.10</cell><cell></cell><cell></cell><cell cols="3">3.65 ± 0.03</cell></row><row><cell></cell><cell cols="4">ResNeXt-8-64</cell><cell></cell><cell></cell><cell></cell><cell cols="4">3.54 ± 0.04</cell><cell cols="4">3.24 ± 0.03</cell><cell></cell><cell cols="5">19.27 ± 0.30</cell><cell></cell><cell cols="3">18.84 ± 0.18</cell><cell></cell><cell></cell><cell cols="3">4.02 ± 0.05</cell><cell></cell><cell></cell><cell cols="3">3.79 ± 0.06</cell></row><row><cell></cell><cell>5.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Random Erasing Baseline</cell><cell>5.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Random Erasing Baseline</cell><cell>5.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Random Erasing Baseline</cell></row><row><cell>Test Error Rate (%)</cell><cell>4.5 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Error Rate (%)</cell><cell>4.5 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Error Rate (%)</cell><cell>4.5 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>p</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>3.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>sh</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>3.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>r1</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(a) probability p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) area ratio s h</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(c) aspect ratio r 1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Test errors (%) on CIFAR-10 based on ResNet18 (pre-act) with four types of erasing value. Baseline: Baseline model, RE-R: Random Erasing model with random value, RE-M: Random Erasing model with mean value of ImageNet 2012, RE-0: Random Erasing model with 0, RE-255: Random Erasing model with 255.</figDesc><table><row><cell cols="2">Types of Erasing Value</cell><cell>Baseline</cell><cell>RE-R</cell><cell>RE-M</cell><cell>RE-0</cell><cell>RE-255</cell></row><row><cell>Test error rate (%)</cell><cell></cell><cell>5.17 ± 0.18</cell><cell>4.31 ± 0.07</cell><cell>4.35 ± 0.12</cell><cell>4.62 ± 0.09</cell><cell>4.85 ± 0.13</cell></row><row><cell>Method</cell><cell>Test error (%)</cell><cell>Method</cell><cell>Test error (%)</cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>5.17 ± 0.18</cell><cell>Baseline</cell><cell>5.17 ± 0.18</cell><cell></cell><cell></cell></row><row><cell>Random Erasing</cell><cell>4.31 ± 0.07</cell><cell>Random Erasing</cell><cell>4.31 ± 0.07</cell><cell></cell><cell></cell></row><row><cell>Dropout</cell><cell>Test error (%)</cell><cell>Random Noise</cell><cell>Test error (%)</cell><cell></cell><cell></cell></row><row><cell>λ1 = 0.001</cell><cell>5.37 ± 0.12</cell><cell>λ2 = 0.01</cell><cell>5.38 ± 0.07</cell><cell></cell><cell></cell></row><row><cell>λ1 = 0.005</cell><cell>5.48 ± 0.15</cell><cell>λ2 = 0.05</cell><cell>5.79 ± 0.14</cell><cell></cell><cell></cell></row><row><cell>λ1 = 0.01</cell><cell>5.89 ± 0.14</cell><cell>λ2 = 0.1</cell><cell>6.13 ± 0.12</cell><cell></cell><cell></cell></row><row><cell>λ1 = 0.05</cell><cell>6.23 ± 0.11</cell><cell>λ2 = 0.2</cell><cell>6.25 ± 0.09</cell><cell></cell><cell></cell></row><row><cell>λ1 = 0.1</cell><cell>6.38 ± 0.18</cell><cell>λ2 = 0.4</cell><cell>6.52 ± 0.12</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="9">Test errors (%) with different data augmentation methods</cell></row><row><cell cols="12">on CIFAR-10 based on ResNet18 (pre-act). RF: Random flipping,</cell></row><row><cell cols="10">RC: Random cropping, RE: Random Erasing.</cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell cols="3">Baseline Random Erasing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Error Rate (%)</cell><cell>30 40 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell>0.25</cell><cell>0.3</cell><cell>0.35</cell><cell>0.4</cell><cell>0.45</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Occlusion Level</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>The IRE and ORE schemes give similar results, in Method train set mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv FRCN [7] 07 66.9 74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8 FRCN [27] 07 69.1 75.4 80.8 67.3 59.9 37.6 81.9 80.0 84.5 50.0 77.1 68.2 81.0 82.5 74.3 69.9 28.4 71.1 70.2 75.8 66.6 ASDN [27] 07 71.0 74.4 81.3 67.6 57.0 46.6 81.0 79.3 86.0 52.9 75.9 73.7 82.6 83.2 77.7 72.7 37.4 66.3 71.2 78.2 74.3 Ours (IRE) 07 70.5 75.9 78.9 69.0 57.7 46.4 81.7 79.5 82.9 49.3 76.9 67.9 81.5 83.3 76.7 73.2 40.7 72.8 66.9 75.4 74.2</figDesc><table><row><cell>. The detector is trained</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>VOC 2007 test detection average precision (%). FRCN refers to FRCN with training schedule in<ref type="bibr" target="#b26">[27]</ref>.</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell>RE</cell><cell cols="2">Market-1501 Rank-1 mAP</cell><cell cols="2">DukeMTMC-reID Rank-1 mAP</cell><cell cols="2">CUHK03 (labeled) Rank-1 mAP</cell><cell cols="2">CUHK03 (detected) Rank-1 mAP</cell></row><row><cell></cell><cell>ResNet-18</cell><cell>No Yes</cell><cell>79.87 82.36</cell><cell>57.37 62.06</cell><cell>67.73 70.60</cell><cell>46.87 51.41</cell><cell>28.36 36.07</cell><cell>25.65 32.58</cell><cell>26.86 34.21</cell><cell>25.04 31.20</cell></row><row><cell>IDE</cell><cell>ResNet-34</cell><cell>No Yes</cell><cell>82.93 84.80</cell><cell>62.34 65.68</cell><cell>71.63 73.56</cell><cell>49.71 54.46</cell><cell>31.57 40.29</cell><cell>28.66 35.50</cell><cell>30.14 36.36</cell><cell>27.55 33.46</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>No Yes</cell><cell>83.14 85.24</cell><cell>63.56 68.28</cell><cell>71.99 74.24</cell><cell>51.29 56.17</cell><cell>30.29 41.46</cell><cell>27.37 36.77</cell><cell>28.36 38.50</cell><cell>26.74 34.75</cell></row><row><cell></cell><cell>ResNet-18</cell><cell>No Yes</cell><cell>77.32 79.84</cell><cell>58.43 61.68</cell><cell>67.50 71.81</cell><cell>46.27 51.84</cell><cell>43.00 48.29</cell><cell>39.16 43.80</cell><cell>40.50 46.57</cell><cell>37.36 43.20</cell></row><row><cell>TriNet</cell><cell>ResNet-34</cell><cell>No Yes</cell><cell>80.73 83.11</cell><cell>62.65 65.98</cell><cell>72.04 72.89</cell><cell>51.56 55.38</cell><cell>46.00 53.07</cell><cell>43.79 48.80</cell><cell>45.07 53.21</cell><cell>42.58 48.03</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>No Yes</cell><cell>82.60 83.94</cell><cell>65.79 68.67</cell><cell>72.44 72.98</cell><cell>53.50 56.60</cell><cell>49.86 58.14</cell><cell>46.74 53.83</cell><cell>50.50 55.50</cell><cell>46.47 50.74</cell></row><row><cell>SVDNet</cell><cell>ResNet-50</cell><cell>No Yes</cell><cell>84.41 87.08</cell><cell>65.60 71.31</cell><cell>76.82 79.31</cell><cell>57.70 62.44</cell><cell>42.21 49.43</cell><cell>38.73 45.07</cell><cell>41.85 48.71</cell><cell>38.24 43.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Comparison of our method with state-of-the-art methods on the Market-1501 dataset. We use ResNet-50 as backbone. The best, second and third highest results are in red, blue and green, respectively. els with using the Softmax loss can learn better features. Specially, the IDE achieves 83.14% and 71.99% rank-1 accuracy on Market-1501 and DukeMTMC-reID with using ResNet-50, respectively. SVDNet gives rank-1 accuracy of 84.41% and 76.82% on Market-1501 and DukeMTMC-reID with ResNet-50, respectively. This is 1.81% higher for Market-1501 and 4.38% higher for DukeMTMC-reID than the TriNet with ResNet-50. However, on CUHK03, the performance of TriNet is higher than IDE and SVDNet, since the lack of training samples compromises the Softmax loss. TriNet obtains 49.86% rank-1 accuracy and 46.74% mAP on CUHK03 for the labeled setting with ResNet-50.</figDesc><table><row><cell></cell><cell>.14</cell><cell>63.41</cell></row><row><cell>TriNet [10]</cell><cell>84.92</cell><cell>69.14</cell></row><row><cell>DJL [17]</cell><cell>85.1</cell><cell>65.5</cell></row><row><cell>SVDNet+Ours</cell><cell>87.08</cell><cell>71.31</cell></row><row><cell>SVDNet+Ours+re [41]</cell><cell>89.13</cell><cell>83.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>Comparison of our method with state-of-the-art methods on the DukeMTMC-reID dataset. We use ResNet-50 as backbone.</figDesc><table><row><cell>Method</cell><cell cols="2">Labeled Rank-1 mAP</cell><cell cols="2">Detected Rank-1 mAP</cell></row><row><cell>BOW+XQDA [38]</cell><cell>7.93</cell><cell>7.29</cell><cell>6.36</cell><cell>6.39</cell></row><row><cell>LOMO+XQDA [18]</cell><cell>14.8</cell><cell>13.6</cell><cell>12.8</cell><cell>11.5</cell></row><row><cell>IDE [39]</cell><cell>22.2</cell><cell>21.0</cell><cell>21.3</cell><cell>19.7</cell></row><row><cell>IDE+DaF [32]</cell><cell>27.5</cell><cell>31.5</cell><cell>26.4</cell><cell>30.0</cell></row><row><cell>SVDNet [24]</cell><cell>40.9</cell><cell>37.8</cell><cell>41.5</cell><cell>37.2</cell></row><row><cell>DPFL [4]</cell><cell>43.0</cell><cell>40.5</cell><cell>40.7</cell><cell>37.0</cell></row><row><cell>TriNet [10]</cell><cell>49.86</cell><cell>46.74</cell><cell>50.50</cell><cell>46.47</cell></row><row><cell>TriNet+Ours</cell><cell>58.14</cell><cell>53.83</cell><cell>55.50</cell><cell>50.74</cell></row><row><cell>TriNet+Ours+re [41]</cell><cell>63.93</cell><cell>65.05</cell><cell>64.43</cell><cell>64.75</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">In image classification, an image is classified according to its visual content. In general, training data does not provide the location of the object, so we could not know where the object is. In this case, we perform Random Erasing on the whole image according to Alg. 1.Recently, the person re-ID model is usually trained in a classification network for embedding learning<ref type="bibr" target="#b38">[39]</ref>. In this task, since pedestrians are confined with detected bounding boxes, persons are roughly in the same position and take up the most area of the image. In this scenario, we adopt the same strategy as image classification, as in practice, the pedestrian can be occluded in any position. We randomly select rectangle regions on the whole pedestrian image and erase it. Examples of Random Erasing for image classification and person re-ID are shown inFig. 1.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive dropout for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse contextual activation for efficient visual re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07103</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Patchshuffle regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multicamera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by deep learning attribute-complementary information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Posedriven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SVDNet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Disturblabel: Regularizing cnn on the loss layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Divide and fuse: A re-ranking approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
