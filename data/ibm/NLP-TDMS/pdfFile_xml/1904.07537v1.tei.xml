<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Valeo Schalter und Sensoren GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Amende</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Valeo Schalter und Sensoren GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Kraus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Valeo Schalter und Sensoren GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Honer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Valeo Schalter und Sensoren GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Sämann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Valeo Schalter und Sensoren GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hauke</forename><surname>Kaulbersch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Valeo Schalter und Sensoren GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Milz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Valeo Schalter und Sensoren GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><forename type="middle">Michael</forename><surname>Gross</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ilmenau University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20% and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last few years self-driving cars got more and more into the focus of the automotive industry as well as new mobility players. Today, commercial vehicles already offer manifold automation like assisted or automated parking, adaptive cruise control and even highway pilots. To reach the full level of automation, they require a very precise system of environmental perception, working for every conceivable scenario. Additionally, real world scenarios strictly require real-time performance.</p><p>Recent vehicles are equipped with multiple different kind of sensors like ultrasonics, radar, cameras and Lidar (light detection and ranging) as well. With the help of redundancy and sensor fusion, relevant reliability and safety can be achieved. These circumstances significantly boosted the rapid development of sensor technology and the growth of artificial intelligence algorithms for fundamental tasks like object detection and semantic segmentation.</p><p>Many modern approaches for these tasks use camera, Lidar or combine both. Compared to camera images, there are some difficulties dealing with Lidar point cloud data. Such point clouds are unordered, sparse and have a highly varying density due to the non-uniform sampling of the 3D space, occlusion and reflection. On the other hand, they offer way higher spatial accuracy and reliable depth information. Therefore, Lidar is more common in the context of autonomous driving. In this paper, we propose Complexer-YOLO, a real-time 3D object detection and tracking on semantic point clouds (see <ref type="figure" target="#fig_0">Fig. 1, 2)</ref>. The main contributions are:</p><p>• Visual Class Features: Incorporation of visual pointwise Class-Features generated by fast camera-based Semantic Segmentation <ref type="bibr" target="#b38">[39]</ref>.</p><p>• Voxelized Input: Extension of Complex-YOLO <ref type="bibr" target="#b41">[42]</ref> processing voxelized input features with a variable depth of dimension instead of fixed RGB-maps.</p><p>• Real 3D prediction: Extension of the regression network to predict 3D box heights and z-offsets to treat targets in three dimensions.</p><p>• Scale-Rotation-Translation score (SRTs): We introduce SRTs, a new validation metric for 3D boxes, notably faster than intersection over union (IoU), considering the 3DoF pose of the detected object including the yaw angle such as width, height and length.</p><p>• Multitarget-Tracking: Application of an Online feature tracker decoupled from the detection network, enabling time depending tracking and target instantiation based on realistic, physical assumptions.</p><p>• Realtime capability: We present a complete novel tracking pipeline with an outstanding overall real-time </p><formula xml:id="formula_0">Complex Yolo (V3) Real 3D Multi-Class-Predictions b) c) b.5) Tracked Instances a) b) c) d) b.1) b.2) b.3) b.4)</formula><p>Tracking Pipeline capability, despite state-of-the-art results on semantic segmentation, 3D object detection such as Multitarget-Tracking. The pipeline can be directly brought into every self-driving cars percepting urban scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Detection Pipeline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we provide an overview of convolutional neural network (CNN) based object detection, semantic segmentation and multi target tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">2D Object Detection</head><p>Over the last few years many methods for robust and accurate object detection using CNN have been developed. Starting in 2D space on single images, two-stage detectors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b11">12]</ref> and one-stage detectors <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15]</ref> achieved state-of-the-art results, targeting the output of located 2D bounding boxes. Typically, two-stage detectors exploit object proposals and utilize region of interests (RoI) with the help of region proposal networks (RPN) in a first step. Afterwards, they generate the final object predictions using calculated features over the proposed RoIs. As a trade-off for runtime, one-stage detectors skip the proposal generation step and directly output the final object detections. They are usually capable of real-time performance, but mainly outperformed by two-stage detectors in terms of accuracy. YOLOv3 <ref type="bibr" target="#b33">[34]</ref>, one of the one-stage detectors, combines findings from <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. It divides the image into a sparse grid, performs multi-scale feature extraction and directly outputs object predictions per grid cell, using dimension clusters as anchor boxes <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D Object Detection</head><p>Although CNNs were originally designed for image processing, they became a key component for 3D object detection as well. First ideas were to use stereo images as input <ref type="bibr" target="#b2">[3]</ref>. Followed by <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b5">[6]</ref>, where 3D convolutions were applied to a voxelized representation of point cloud data and features extracted using 3D CNNs <ref type="bibr" target="#b42">[43]</ref>, respectively. In <ref type="bibr" target="#b48">[49]</ref>, voxel feature encoding was introduced and again processed by CNN to predict objects. Furthermore, VeloFCN <ref type="bibr" target="#b19">[20]</ref> created depth maps with the help of front-view projections of 3D point clouds and applied CNN. In contrast, MV3D <ref type="bibr" target="#b3">[4]</ref> merged image input with a multi-view representation of point cloud data projected into 2D space. Alternatively, <ref type="bibr" target="#b15">[16]</ref> aggregated features from image and birdseyeview representation of point clouds. Another method to fuse camera and Lidar inputs was explored in <ref type="bibr" target="#b27">[28]</ref>. In a first step, sub point clouds were extracted in viewing frustums detected by a 2D CNN. Afterwards, a PointNet <ref type="bibr" target="#b28">[29]</ref> predicts 3D objects within the frustum point clouds. Recently, PointNet was also used in <ref type="bibr" target="#b44">[45]</ref> in combination with 2D CNN and a fusion network. Further similar approaches using birdseye-view representations of point clouds were <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b0">1</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Semantic Segmentation</head><p>The goal of semantic segmentation is to classify each pixel of an image into a predefined class. This task is typically achieved by CNNs. Several widely used network architectures have been introduced, e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21]</ref>. Similar to the object detection task, there is a trade-off between accuracy and runtime. Therefore, approaches like convolutional factorization, e.g. applied in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>, quantization <ref type="bibr" target="#b29">[30]</ref>, pruning <ref type="bibr" target="#b12">[13]</ref> and dilated convolutions, e.g. applied in <ref type="bibr" target="#b47">[48]</ref>, came up. ENet <ref type="bibr" target="#b26">[27]</ref>, one of the most efficient models used a special encoder-decoder structure to highly reduce computational effort. Recently, <ref type="bibr" target="#b38">[39]</ref> applied the Channel Pruning method <ref type="bibr" target="#b12">[13]</ref> to the ENet to make it more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Multi Target Tracking</head><p>The task of multi-object tracking (MOT) is usually solved in two phases. First, an algorithm detects objects of interests and second, identical objects in different frames are associated. A widespread approach is using global information about the detections <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7]</ref>. In contrast to this, online approaches don't have any knowledge of future frames. With this characteristic they have one significant advantage: they are usable in real-world scenarios. Recent work focused especially on tracking of 2D objects from camera input <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. Online multi-target 3D object tracking based on detections from algorithms with point cloud inputs aren't popular until now. The basics of the Labeled Multi-Bernoulli Filter, which we use for multi target tracking, are explained in the following.</p><p>The state x i t of the ith target at discrete time t is a random variable. The set of all targets at time step t is a subset of the state space X and then denoted by</p><formula xml:id="formula_1">X t = x i t N x t i=1 ⊂ X.<label>(1)</label></formula><p>In turn, the set cardinality N x t = |X t | at time t is a discrete random variable.</p><p>The set of all measurements at time t is again modeled as a random set with set cardinality N z t = |Z t | and denoted by</p><formula xml:id="formula_2">Z t = z i t N z t i=1 ⊂ Z.</formula><p>(2)</p><p>Each individual measurement z i t is either targetgenerated or clutter. Yet the true origin is assumed unknown. Further, the set of all measurements up to and including the time step t is denoted by</p><formula xml:id="formula_3">Z t = t τ =1 Z τ .<label>(3)</label></formula><p>Both above sets are without order, i.e. the particular choice of indices is arbitrary. Targets and measurements are modeled as Labeled Multi-Bernoulli Random Finite Sets (LMB RFS) as proposed in <ref type="bibr" target="#b1">[2]</ref>. A Bernoulli RFS is a set that is either empty with probability 1 − r or contains a single element. As in <ref type="bibr" target="#b35">[36]</ref>, the probability density of a Bernoulli RFS may be written as</p><formula xml:id="formula_4">π(X) = 1 − r, if X = ∅, r p(x), if X = {x}<label>(4)</label></formula><p>with p(·) a spatial distribution on X. A Multi-Bernoulli RFS is then the union of independent Bernoulli RFSs, i.e.</p><formula xml:id="formula_5">X MB = i X (i) B .</formula><p>In turn a Multi-Bernoulli RFS is welldefined by the parameters {r (i) , p (i) } i .</p><p>Labeled RFS allow the estimation of both the targets' state and their individual trajectories. For this reason the target state is extended by a label l ∈ L, i.e. each single target state is given by x = (x, l) and in turn the multitarget state X lives on the product space X × L with L a discrete space. Note that this definition does not enforce the labels l to be distinct. <ref type="bibr" target="#b43">[44]</ref> introduced the so called distinct label indicator</p><formula xml:id="formula_6">∆(X) := δ |X| (|L(X)|)<label>(5)</label></formula><p>that enforces the cardinality of X to be identical to the cardinality of the projection</p><formula xml:id="formula_7">L(X) = {L(x) : x ∈ X}, L(x) = l.</formula><p>Together with Eq. 4, it follows that the probability density of a LMB RFS is well-defined by the parameter set {r (l) , p (l) } l∈L and the cardinality distribution yields</p><formula xml:id="formula_8">ρ(n) = i∈L (1 − r (i) ) L∈Fn(L) l∈L r (l) 1 − r (l)<label>(6)</label></formula><p>with F n (L) the set of all subsets of L containing n elements.</p><p>The core objective of the multi-target tracking is to approximate the multi-target distribution f t|t (X t |Z t ) in each time step t. This is achieved with the multi-target Bayes filter,</p><formula xml:id="formula_9">f t|t (X t |Z t ) = f t (Z t |X t )f t|t−1 (X t |Z t−1 ) f t (Z t |X t )f t|t−1 (X t |Z t−1 )δX t<label>(7)</label></formula><p>and the Chapman-Kolmogorov prediction</p><formula xml:id="formula_10">f t+1|t (X t+1 |Z t ) = f t+1|t (X t+1 |X t )f t|t (X t |Z t )δX t<label>(8)</label></formula><p>with f t (Z t |X t ) the multi-target measurement set density and f t+1|t (X t+1 |X t ) the multi-target transition density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint Detection and Extended Target Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Point Cloud Preprocessing</head><p>First, we generate a semantic segmentation map of the front camera images using the efficient model from <ref type="bibr" target="#b38">[39]</ref>, pre-trained on <ref type="bibr" target="#b4">[5]</ref> and fine tuned on KITTI <ref type="bibr" target="#b7">[8]</ref>. Second, we quantize the point cloud to a 3D voxel representation, which is able to contain certain features of the points that lie within such a voxel. Our region of interest of the point cloud is set to [0, 60]m × [−40, 40]m × [−2.73, 1.27]m in sensor coordinates, according to the KITTI <ref type="bibr" target="#b7">[8]</ref> dataset. We chose a resolution of 768 × 1024 × 21 resulting in approximately 0.08m×0.08m×0.19m per cell. Each voxel, where at least one point exists inside its 3D space and is visible to the front camera, is filled with a normalized class value extracted from the semantic map in range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Therefore, we project all relevant points into the image using calibrations from <ref type="bibr" target="#b7">[8]</ref> and argmax over the frequency of all resolved classes. In this way, contextual information with visual features are passed into our voxel map, which is especially helpful for higher ranges with low density of points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Voxel based Complex-YOLO</head><p>We use the full detection pipeline introduced in <ref type="bibr" target="#b41">[42]</ref>, but exchange the input map for our voxel representation. Inspired by <ref type="bibr" target="#b33">[34]</ref>, we exchange max-pooling layers by convolutions with stride 2 and add residual connections. Altogether, we have 49 convolutional layers. Additionally, we add object height h and ground offset z as target regression parameters and incorporate both into the multi-part loss function.</p><formula xml:id="formula_11">L =L Euler + λ coord S 2 i=0 B j=0 1 obj ij (h i −ĥ i ) 2 + (z i −ẑ i ) 2 (9)</formula><p>Usually IoU is used to compare detection and ground truth during the training process. However, it has drawbacks when comparing rotated bounding boxes. If two boxes are compared with the same size and position and an angle difference of π the IoU between these two boxes is 1, which means they match perfectly. This is obviously not the case since the angle between the two boxes has the maximum difference it can have. So while training a network it is not penalized and even encouraged by predicting boxes like these. This leads to wrong predictions for the object orientation. Also calculating an exact IoU for rotated bounding boxes in 3D space is a time consuming task.</p><p>To overcome these two problems we introduce a new highly parameterizable simple evaluation metric called Scaling-Rotation-Translation score (SRTs). The SRTs is based on the fact, that given two arbitrary 3D objects of the same shape, one can be transformed into the other using a transformation. Therefore, we can define a score S srt as composite of independent scores for scaling S s , rotation S r and translation S t with</p><formula xml:id="formula_12">S s = 1 − min |1 − s x | + |1 − s y | + |1 − s z | w s , 1 (10) S r = max 0, 1 − θ w r π , w r ∈ (0, 1]<label>(11)</label></formula><formula xml:id="formula_13">r i = d i · w t 2 , i ∈ {1, 2}<label>(12)</label></formula><p>S t = max 0,</p><formula xml:id="formula_14">r 1 + r 2 − t r 1 + r 2<label>(13)</label></formula><formula xml:id="formula_15">p t = 0, if r 1 + r 2 &lt; t 1, otherwise<label>(14)</label></formula><p>where s x,y,z denotes size ratios in x, y, z directions, θ denotes the difference of the yaw angles, t the Euclidean distance between the two object centers and p t is a penalty if the objects do not intersect. S t is calculated in respect to the size of the two objects, because for small objects a small translation can already have a big impact and vice versa for large objects. So the length of the diagonals d i of both objects are used to calculate two radii r i .</p><p>To adjust the score w s , w t and w r can be used. They control how strict the individual scores are. We used w s = 0.3, w t = 1 and w r = 0.5</p><p>All the previous scores are in the interval [0, 1] and can be combined into the final score (S srt ) using a simple weighted average and the penalty p t .</p><formula xml:id="formula_16">S srt = p t · (α S s + β S t + γ S r ) (15) α + β + γ = 1</formula><p>Using α, β, γ the weight of the three sub scores can be defined. We used γ = 0.4 and α = β = 0.3 to give more ... weight to the angle, because translation and scaling are easier to learn for the network.</p><p>SRTs perfectly lines up with the three subtasks (rotation, position, size) a network has to do in order to predict 3D boxes with a yaw angle. It is designed so it can be parametrized to approximate the IoU but considers object orientations. Using all the parameters the score can be adjusted to suit the needs of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extended target model in the LMB RFS</head><p>To apply the RFS approach to the output of the YOLO network, which consists of boxes in the three dimensional space, we interpret the output as Gaussian noise corrupted measurements z i t , i ∈ {1, . . . , N z t } of the positional parameters (see Eq. 2) and extend those as extended targets <ref type="bibr" target="#b8">[9]</ref> x i t , i ∈ {1, . . . , N x t } (see Eq. 1) with the measurement noise covariance matrix R = diag(0.5 2 , 0.5 2 , 0.5 2 , 0.5 2 , 0.5 2 , 0.1 2 ). The target is assumed to move according to a coordinated turn motion model <ref type="bibr" target="#b37">[38]</ref> with the process noise covariance Q = diag(σ 2 a , σ 2 α ) consisting of the standard deviation of the acceleration σ a = 17.89 and the yaw rates derivative σ α = 1.49.</p><p>The individual measurements z consist of the box center position c = [x, y, z] in the three dimensional space, the box dimension (length, width, height) s = [l, w, h] and the box orientation along the first dimensions φ i t (yaw), such that</p><formula xml:id="formula_17">z = [c, s, φ] .<label>(16)</label></formula><p>The extended target state meanx i t used for tracking contains the same parameters as the measurements as well as motion parameters of the coordinated turn model consisting of the velocity v and yaw rateφ. The state mean of the ith target at time t can be described as</p><formula xml:id="formula_18">x i t = [c i t , s i t , φ i t , v i t ,φ i t ]<label>(17)</label></formula><p>with the according state covariance matrixP i t . We can state the measurement equation</p><formula xml:id="formula_19">z = H ·x<label>(18)</label></formula><p>for an individual measurement z and the target meanx, with the measurement matrix</p><formula xml:id="formula_20">H = I 7 0 ∈ R 7×9 ,<label>(19)</label></formula><p>where I 7 is the identity matrix of dimension 7. Based on this measurement equation, a Bayesian filter can be defined where the innovation is calculated using a Kalman filter update according to the stated measurement model. The prediction is performed using an Unscented Kalman filter according to the assumed nonlinear coordinated turn model <ref type="bibr" target="#b37">[38]</ref>.</p><p>In the LMB update step each predicted target is associated with each measurement of the time step and an update according to the defined measurement model is performed. A heatmap is generated from the update likelihood, modeling the association probabilities based on which targets will be kept or discarded. Be p a (x i , z j ) the association probability of the measurement z j and the state x i . If the nonassignment probability</p><formula xml:id="formula_21">p na (z j ) = 1 − x i ∈X p a (x i , z j )<label>(20)</label></formula><p>is higher than a threshold P na , we assume that a new target is born from an unexplained measurement. The number N e of targets to be extracted is derived from the mean of the cardinality distribution presented in Eq. 6. All N e targets with the highest existence probability r (l) are extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate Complexer-YOLO on the KITTI benchmarks for 3D object detection and bird's eye view (BEV) detection. Furthermore, we evaluate the capabilities of our multi target tracking with the help of the object tracking benchmark. Our ablation studies investigate the importance of different input features encoded in our voxel representation and show further findings. Finally, some qualitative results visualize the outcome of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Details</head><p>The KITTI dataset <ref type="bibr" target="#b7">[8]</ref> consists of 7481 training images and 7518 testing images. First, we follow <ref type="bibr" target="#b3">[4]</ref> and use the training/validation split to optimize our settings. Afterwards, we use the full training set for the official evaluation. We augment the training dataset with rotation and increase the size by a factor of 4. Therefore, we randomly pick 3 angles between [−20, 20] deg with a minimum difference of 8 deg to each other. Similar to <ref type="bibr" target="#b46">[47]</ref>, we use random flipping along the x axis during training.</p><p>For training, we use an extended version of the darknet framework <ref type="bibr" target="#b30">[31]</ref>. We train the model from scratch for 140k iterations with learning rate scaled at 20k, 80k and 120k iterations respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Detection Results</head><p>We submitted our results to the KITTI vision benchmark suite <ref type="bibr" target="#b7">[8]</ref> for Orientation Similarity, BEV, 3D Object Detection and Object Tracking benchmarks on the official test set. To achieve a fair comparison, we only selected some of the leading 3D object detectors that are able to detect at least classes Car, Pedestrian and Cyclist. For tracking, only online methods are listed.</p><p>We show evaluation results for Orientation Similarity, BEV and 3D object detection in <ref type="table">Table 1</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows our results in MOT accurancy and precision (MOTA and MOTP), mostly tracked (MT) and mostly lost (ML).</p><p>Unfortunately, the whole evaluation process is based on 2D bounding boxes in camera space due to the handling of Dontcare labels and ignored objects, e.g. truncated or occluded (see <ref type="bibr" target="#b7">[8]</ref>). Following the 2D Object Detection Benchmark, which is accompanied with BEV and 3D, we achieve 79.31% for class Car in moderate difficulty. Also, Orientation for these settings is ranked at 79.08%. However, our algorithm detects and tracks bounding boxes in 3D space. Therefore, all detections are projected to the image plane. Although we do not track in image space, we achieve stateof-the-art results while running in real-time using our tracking (visualization <ref type="figure" target="#fig_5">Fig. 6</ref>). Moreover we are the first one with 3D tracking based on point cloud detections on the KITTI tracking benchmark. But there is an inconsistency compared to BEV and 3D results, where we achieve only 66.07% and 49.44% respectively. Based on less than 50% AP in 3D space, tracking is not able to reach actual results, which we think mainly comes from wrongly counted Dontcare objects. In opposition to the KITTI guidelines, we found that their current object detection evaluation scripts fully ignore Dontcare labels for BEV and 3D Object detec-tion benchmarks. All such detections count as false positives, which is crucial in our case. Furthermore, most 2D ground truth bounding boxes for class Pedestrian are manually refined and do not match a reprojected bounding box from 3D space anymore, which leads to additional wrongly counted false positives, when ignored objects are assigned. <ref type="figure" target="#fig_2">Fig. 3</ref> shows outstanding results on several sequences with different use cases. Our model is able to detect accurate rotated bounding boxes in 3D space for multiple classes even though the strongly unbalanced dataset. With the help of voxelized semantic features, the network is able to detect even small objects like pedestrians or cyclist, as long as they have a minimum spatial distance to other appearing objects.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We conducted ablation experiments with fixed training setup to investigate the influence of our hyper parameters and several input features. The use of 21 height channels for our voxel map results in similar mAP at IoU threshold 0.7 as using 51 channels (cuboidal voxels). It seems that our network is not able to fully utilize fine grained height information. Furthermore, it is the best trade-off for runtime and accuracy, because runtime was slightly increasing for more than 21 height channels with our hardware setup. <ref type="table" target="#tab_4">Table 3</ref> shows results using different voxel maps with intensity values from Lidar sensor normalized in range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, binary occupancy similar to <ref type="bibr" target="#b24">[25]</ref> and our novel semantic map. Additionally, the approach from <ref type="bibr" target="#b41">[42]</ref> using extracted features encoded as an RGB image is listed.</p><p>In order to reduce wrongly counted false positives due to ignored Dontcare labels, we tried to filter our detections in a post processing step. Therefore, we counted the number of 3D points falling into each 3D bounding box. All detections with less then 13 points and less than 52m radial distance to the Lidar sensor were removed, because Dontcare is often used for objects at higher distances or occluded objects. This improved all object detection results by 1.3% on average, but decreased e.g. BEV for Car on moderate difficulty by 4.8%. Consequently, our filter removed a few Dontcare or ignored detections, but removed correct ones as well. Also, it seemed to have stronger impact on moderate settings since valid easy detections are all in near range, which explains AP drops from easy.</p><p>Finally, using SRTs for training instead of IoU gives 1.3% improvement on mAP at IoU 0.7 as it directly penalizes orientation. It also halved our training time and resulted in a 10-20% runtime improvement for inference. Additionally, we tried to limit object rotations into subsections using anchors instead of complex angles for the full 360 deg, but this decreased accuracy. Further investigation is required here, because we see a potential reduction  . Results for Semantic Segmentation are taken from the KITTI leader board. We point out, that our overall Detection and Tracking Pipeline is faster than many single task algorithms.</p><p>in complexity for the learning task of the network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we propose Complexer-YOLO, a tracked real-time 3D object detector that operates on point clouds fused with visual semantic segmentation. Our architecture takes advantage of both spatial Lidar data and explored scene understanding from 2D. Detection results obtained from 3D space show competitive performance on KITTI benchmarks <ref type="bibr" target="#b7">[8]</ref> compared to state-of-the-art. At the same time, we introduce SRTs, a powerful, more flexible and simplified evaluation metric for object comparison that overcomes the limits of IoU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Complexer-YOLO processing pipeline: We present a novel and complete 3D Detection (b.1-5) and Tracking pipeline (a,b,c,d,e) on Point Clouds in Real-Time. The Tracking-Pipeline is composed by: (a) Lidar + RGB frame grabbing from stream, (b) Frame-wise Complex-YOLO 3D Multiclass predictions, (c) Joint Object and extended Target Model for feature Tracking and (d) 3D object instance tracking within the environmental model. In detail (b) is composed by: (1) The Voxelization of the Lidar frame, (2) the Semantic Segmentation of the RGB image with the aid of ENet, (3) the Point-wise classification by Lidar to Semantic-Image backprojection, (4) the generation of the Semantic Voxel Grid and finally (5): The real 3D Complex YOLO for 3D Multi-class predictions. (see Fig. 2 for more details)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Figure 2 :</head><label>12</label><figDesc>Overview of our architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results. For visualization, our detections are projected into camera space with overlayed pixel wise semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Results for the orientation benchmark compared to SECOND [46], BirdNet [1], AVOD-FPN [16] on official KITTI test set. Det. (BEV) (%) MOTP -Object Tracking (%) IoU Class -Semantic S. (%)FPS Complexer-YOLO (Overall Runtime 11.5 FPS) CY-3D(15.6 FPS) CY-MOT(100 FPS) CY-ENet(90 FPS) CY-All(11.5 FPS) Tracking Semantic Segmentation Complexer-YOLO (CY) -Runtime Evaluation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The Complexer-YOLO runtime evaluation (reference hardware: NVIDIA GTX1080i/Titan) shows state-ofthe-art results of all single tasks (Semantic Segmentation, 3D object detection (BEV, cars → hard, Tab. 1), Tracking Tab. 2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Tracked objects trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with non-anonymous pure online submissions on KITTI MOT benchmark<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of different input features. mAP values (in %) for the 3D benchmark on KITTI validation set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">BirdNet: a 3D Object Detection Framework from LiDAR information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cruzado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De La Escalera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Generalized Labeled Multi-Bernoulli Filter with Object Spawning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">3D Object Proposals using Stereo Imagery for Accurate Object Class Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3D object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. -30th IEEE Conf. Comput. Vis. Pattern Recognition</title>
		<meeting>-30th IEEE Conf. Comput. Vis. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end Learning of Multisensor 3D Tracking by Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Extended object tracking: Introduction, overview and applications. ISIF Journal of Advances in Information Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Granstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-Dimensional Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. Proc. IEEE Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Channel Pruning for Accelerating Very Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Joint 3D Proposal Generation and Object Detection from View Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-class multiobject tracking using changing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rhee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">3D Fully Convolutional Network for Vehicle Detection in Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Vehicle Detection from 3D Lidar Using Fully Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">ESPNet : Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Enet : A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">74</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">XNOR-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Darknet: Open source neural networks in c</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<ptr target="http://pjreddie.com/darknet/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Pharm. Contract</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">YOLOv3: An Incremental Improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The labeled multi-bernoulli filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ekf/ukf maneuvering target tracking using coordinated turn models with polar/cartesian velocity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hendeby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gustafsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Fusion (FUSION), 2014 17th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Efficient Semantic Segmentation for Visual Bird &apos; s-eye View Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sämann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petzold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mono-Camera 3D Multi-Object Tracking Using Deep Learning Detections and PMBM Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjaminsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Granstrom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Complex-YOLO: Real-time 3D Object Detection on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis., 2015 International Conference on Computer Vision</title>
		<meeting>IEEE Int. Conf. Comput. Vis., 2015 International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A Random Finite Set Conjugate Prior and Application to Multi-Target Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-N</forename><surname>Vo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely Embedded Convolutional Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">PIXOR: Real-time 3D Object Detection from Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
