<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepIM: Deep Iterative Matching for 6D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">·</forename><surname>Xiangyang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">·</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
							<email>dieterf@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
							<email>xyji@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Xiang</forename><surname>Nvidia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Tsinghua University and BNRist</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University and BNRist</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University and BNRist</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepIM: Deep Iterative Matching for 6D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D Object Recognition</term>
					<term>6D Object Pose Estimation</term>
					<term>Object Tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 6D poses of objects from images is an important problem in various applications such as robot manipulation and virtual reality. While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the input image can produce accurate results. In this work, we propose a novel deep neural network for 6D pose matching named DeepIM. Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image. The network is trained to predict a relative pose transformation using a disentangled representation of 3D location and 3D orientation and an iterative training process. Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over stateof-the-art methods. We furthermore show that DeepIM is able to match previously unseen objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Localizing objects in 3D from images is important in many real world applications. For instance, in a robot manipulation task, the ability to recognize the 6D pose of objects, i.e., 3D location and 3D orientation of objects, provides useful information for grasp and motion planning. In a virtual reality application, 6D object pose estimation enables virtual interactions between human and objects. While several recent techniques have used depth cameras for object pose estimation, such cameras have limitations with respect to frame rate, field of view, resolution, and depth range, making it very difficult to detect small, thin, transparent, or fast moving objects. Unfortunately, RGB-only 6D object pose estimation is still a challenging problem, since the appearance of objects in the images changes according to a number of factors, such as lighting, pose variations, and occlusions between objects. Furthermore, a robust 6D pose estimation method needs to handle both textured and textureless objects.</p><p>Traditionally, the 6D pose estimation problem has been tackled by matching local features extracted from an image to features in a 3D model of the object <ref type="bibr" target="#b29">(Lowe, 1999;</ref><ref type="bibr" target="#b41">Rothganger et al., 2006;</ref><ref type="bibr" target="#b6">Collet et al., 2011)</ref>. By using the 2D-3D correspondences, the 6D pose of the object can be recovered. Unfortunately, such methods cannot handle textureless objects well since only few local features can be extracted for them. To handle textureless objects, two classes of approaches were proposed in the literature. Methods in the first class learn to estimate the 3D model coordinates of pixels or keypoints of the object in the input image. In this way, the 2D-3D correspondences are established for 6D pose estimation <ref type="bibr" target="#b2">(Brachmann et al., 2014;</ref><ref type="bibr" target="#b38">Rad and Lepetit, 2017;</ref><ref type="bibr" target="#b50">Tekin et al., 2017)</ref>. Methods in the second class convert the 6D pose estimation problem into a pose classifi-pose <ref type="bibr">(0)</ref> Δpose <ref type="formula">(</ref> We propose DeepIM, a deep iterative matching network for 6D object pose estimation. The network is trained to predict a relative SE(3) transformation that can be applied to an initial pose estimation for iterative pose refinement. Given a 6D pose estimation of an object, which can be the output of other pose estimation methods like PoseCNN ) (pose (0) in the figure) or the refined pose from previous iteration (pose <ref type="bibr">(1)</ref> in the figure), along with the 3D model of the object, we generate the rendered image showing the appearance of the target object under this rough pose estimation. With the image pairs of rendered image and observed image, the network predicts a relative transformation (∆pose in the figure) which can be applied to refine the input pose. The refined pose can be used as the input pose of next iteration and therefore the process can be repeated until the refined pose converges or the number of iterations reaches a pre-determined number.</p><p>cation problem by discretizing the pose space <ref type="bibr" target="#b17">(Hinterstoisser et al., 2012b)</ref> or into a pose regression problem . These methods can deal with textureless objects, but they are not able to achieve highly accurate pose estimation, since small errors in the classification or regression stage directly lead to pose mismatches. A common way to improve the pose accuracy is pose refinement: Given an initial pose estimation, a synthetic RGB image can be rendered and used to match against the target input image. Then a new pose is computed to increase the matching score.</p><p>Existing methods for pose refinement use either handcrafted image features <ref type="bibr" target="#b52">(Tjaden et al., 2017)</ref> or matching score functions <ref type="bibr" target="#b38">(Rad and Lepetit, 2017)</ref>. In this work, we propose DeepIM, a new refinement technique based on a deep neural network for iterative 6D pose matching. Given an initial 6D pose estimation of an object in a test image, DeepIM predicts a relative SE(3) transformation that matches a rendered view of the object against the observed image, or in other words, it predicts the relative rotation and translation that can refine the initial 6D pose estimation. By iteratively re-rendering the object based on the improved pose estimates, the two input images to the network become more and more similar, thereby enabling the network to generate more and more accurate pose estimates. <ref type="figure">Fig. 1</ref> illustrates the iterative matching procedure of our network for pose refinement.</p><p>This work makes the following main contributions. i) We introduce a deep network for iterative, imagebased pose refinement that does not require any handcrafted image features and automatically learns an internal refinement mechanism. ii) We propose a disentangled representation of the SE(3) transformation between object poses to achieve accurate pose estimates. This representation also enables our approach to refine pose estimates of unseen objects. iii) We have conducted extensive experiments on the LINEMOD <ref type="bibr" target="#b17">(Hinterstoisser et al., 2012b)</ref> and the Occlusion LINEMOD <ref type="bibr" target="#b2">(Brachmann et al., 2014)</ref> datasets to evaluate the accuracy and various properties of DeepIM. These experiments show that our approach achieves large improvements over state-of-the-art RGB-only methods on both datasets. Furthermore, initial experiments demonstrate that DeepIM is able to accurately match poses for textureless objects (T-LESS <ref type="bibr" target="#b19">(Hodan et al., 2017)</ref>) and for unseen objects <ref type="bibr" target="#b57">(Wu et al., 2015)</ref>. The rest of the paper is organized as follows. After reviewing related works in Section 2, we describe our approach for pose matching in Section 3. Experiments are presented in Section 4, and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We review representative works on 6D pose estimation in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RGB based 6D Pose Estimation</head><p>Traditionally, object pose estimation using RGB images is tackled by matching local features <ref type="bibr" target="#b29">(Lowe, 1999;</ref><ref type="bibr" target="#b41">Rothganger et al., 2006;</ref><ref type="bibr" target="#b6">Collet et al., 2011)</ref>. In this paradigm, a 3D model of an object is first reconstructed and local features of the object are attached to the 3D model. Keypoint-based features such as SIFT <ref type="bibr" target="#b29">(Lowe, 1999)</ref> or SURF <ref type="bibr" target="#b0">(Bay et al., 2008)</ref> are widely used. Given an input image, local features extracted from the image are matched against features on the 3D model. By filtering out incorrect matches using robust estimation techniques such as RANSAC <ref type="bibr" target="#b35">(Nistér, 2005)</ref>, the 6D pose of the object can be recovered using the 2D-to-3D correspondences between the local features. Local-feature matching based methods can handle partial occlusions between objects as long as the features on the visual part of the object are sufficient to determine the 6D pose. However, these methods cannot handle textureless objects well, since rich texture on the object is required in order to detect these features robustly.</p><p>In contrast, template-matching based methods are capable of handling textureless objects <ref type="bibr" target="#b21">(Jurie and Dhome, 2001;</ref><ref type="bibr" target="#b26">Liu et al., 2010;</ref><ref type="bibr" target="#b15">Gu and Ren, 2010;</ref><ref type="bibr" target="#b16">Hinterstoisser et al., 2012a)</ref>. In this paradigm, templates of an object are first constructed, where examples of templates are renderings of the object from the 3D object model or Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b8">(Dalal and Triggs, 2005)</ref> templates from different viewpoints. Then these templates are matched against the input image to determine the location and orientation of the target object in the input image. The drawback of templatematching based methods is that they are not robust to occlusions between objects. When the target object is heavily occluded, the matching score is usually low which may result in incorrect pose estimation.</p><p>Recent approaches apply machine learning, especially deep learning, for 6D pose estimation using RGB images <ref type="bibr" target="#b2">(Brachmann et al., 2014;</ref><ref type="bibr" target="#b24">Krull et al., 2015)</ref>. Learning techniques are employed to detect object keypoints for matching or learn better feature representations for pose estimation. The state-of-the-art methods <ref type="bibr" target="#b38">(Rad and Lepetit, 2017;</ref><ref type="bibr" target="#b22">Kehl et al., 2017;</ref><ref type="bibr" target="#b50">Tekin et al., 2017;</ref><ref type="bibr" target="#b58">Xiang et al., 2018;</ref><ref type="bibr" target="#b54">Tremblay et al., 2018)</ref> augment deep learning based object detection or segmentation methods <ref type="bibr" target="#b14">(Girshick, 2015;</ref><ref type="bibr" target="#b28">Long et al., 2015;</ref><ref type="bibr" target="#b27">Liu et al., 2016;</ref><ref type="bibr" target="#b39">Redmon et al., 2016)</ref> for 6D pose estimation. For example, <ref type="bibr" target="#b38">(Rad and Lepetit, 2017;</ref><ref type="bibr" target="#b52">Tjaden et al., 2017;</ref><ref type="bibr" target="#b54">Tremblay et al., 2018)</ref> utilize deep neural networks to detect keypoints on the objects, and then compute the 6D pose by solving the PnP problem. <ref type="bibr" target="#b22">(Kehl et al., 2017;</ref><ref type="bibr" target="#b58">Xiang et al., 2018)</ref> employ deep neural networks to detect objects in the input image, and then classify or regress the detected object to its pose. A recent work <ref type="bibr" target="#b48">(Sundermeyer et al., 2018)</ref> uses an autoencoder to map the object in the image to a vector and search for the most similar vector in a pre-generated codebook for pose estimation. Overall, learning-based methods achieve better performance than traditional methods, largely due to the ability of learning a powerful feature representation for pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Depth based 6D Pose Estimation</head><p>From another point of view, the 6D pose estimation problem can be tackled using depth images. Given a 3D model of an object and an input depth image, the problem is formulated as aligning the two point clouds computed from the 3D model and the depth image, respectively, which is also known as the geometric registration problem. Roughly speaking, geometric registration methods can be classified as local refinement methods and global registration methods. The most well-known local refinement algorithm is the Iterative Closest Point (ICP) algorithm <ref type="bibr" target="#b1">(Besl and McKay, 1992)</ref> and its variants <ref type="bibr" target="#b42">(Rusinkiewicz and Levoy, 2001;</ref><ref type="bibr" target="#b44">Salvi et al., 2007;</ref><ref type="bibr" target="#b49">Tam et al., 2013)</ref>. Given an initial pose estimation, the ICP algorithm iterates between finding the correspondences between points and refining the pose estimation using the new correspondences. In general, local refinement algorithms are sensitive to the initial pose. If the initial pose estimation is not close enough, the algorithm may converge to a local mimimum.</p><p>Global registration methods <ref type="bibr" target="#b31">(Mellado et al., 2014;</ref><ref type="bibr" target="#b51">Theiler et al., 2015;</ref><ref type="bibr" target="#b61">Zhou et al., 2016;</ref><ref type="bibr" target="#b59">Yang et al., 2016)</ref> solve a more challenging problem by not assuming an initial pose estimate. A common strategy is to utilize iterative model fitting frameworks such as RANSAC. In each iteration, a set of point correspondences are sampled, and an alignment is computed and evaluated using the sampled correspondences. The limitation of most global registration methods is that they are computationally expensive. Also, the registration quality heavily depends on the quality of the 3D model and the scanned point cloud. In order to improve the registration performance, features on point clouds are also introduced for matching. These include point pairs <ref type="bibr" target="#b32">(Mian et al., 2006;</ref><ref type="bibr" target="#b18">Hinterstoisser et al., 2016)</ref>, spin-images <ref type="bibr" target="#b20">(Johnson and Hebert, 1999)</ref>, and point-pair histograms <ref type="bibr" target="#b43">(Rusu et al., 2009;</ref><ref type="bibr" target="#b53">Tombari et al., 2010)</ref>. Similar to the trend in image-based matching, recent approaches <ref type="bibr" target="#b55">(Wang et al., 2019)</ref> propose to learn point features for registration, such as applying deep neural networks to point clouds <ref type="bibr" target="#b37">(Qi et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">RGB-D based 6D Pose Estimation</head><p>When both RGB images and depth images are available, they can be combined to improve 6D pose estimation. A common strategy is to estimate an initial pose of an object based on the color image, and then refine the pose using depth-based local refinement algorithms such as ICP <ref type="bibr" target="#b17">(Hinterstoisser et al., 2012b;</ref><ref type="bibr" target="#b33">Michel et al., 2017;</ref><ref type="bibr" target="#b60">Zeng et al., 2017)</ref>.</p><p>For example, <ref type="bibr" target="#b17">Hinterstoisser et al. (2012b)</ref> renders the 3D model of an object into templates of color images, and then matches these templates against the input image to estimate an initial pose. The final pose estimation is obtained via ICP refinement on the initial pose. <ref type="bibr" target="#b2">Brachmann et al. (2014</ref><ref type="bibr" target="#b3">), Brachmann et al. (2016</ref>, <ref type="bibr" target="#b33">Michel et al. (2017)</ref> regress each pixel on the object in the input image to the 3D coordinate of that pixel on the 3D model. When depth images are available, the 3D coordinate regression establishes correspondences between 3D scene points and 3D model points, from which the 6D pose can be computed by solving a least-squares problem. PoseCNN  introduces an end-to-end neural network for 6D object pose estimation using RGB images only. Given an initial pose from the network, a customized ICP method is applied to refine the pose. A recent work <ref type="bibr" target="#b55">(Wang et al., 2019)</ref> introduces a neural network that combines RGB images and depth images for 6D pose estimation, and an iterative pose refinement network using point clouds as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">RGB vs. RGB-D</head><p>Overall, the performance of RGB-based methods is still not comparable to that of the RGB-D based methods. We believe that this performance gap is largely due to the lack of an effective pose refinement procedure using RGB images only. <ref type="bibr" target="#b30">Manhardt et al. (2018)</ref> which is published at the same time as ours introduces a method to refine 6D object poses with only RGB images, but there is still a large performance gap between <ref type="bibr" target="#b30">Manhardt et al. (2018)</ref> and depth-based methods. Our work is complementary to existing 6D pose estimation methods by providing a novel iterative pose matching network for pose refinement on RGB images.</p><p>The approaches most related to ours are the object pose refinement network in <ref type="bibr" target="#b38">Rad and Lepetit (2017)</ref> and the iterative hand pose estimation approaches in <ref type="bibr" target="#b5">Carreira et al. (2016)</ref>; <ref type="bibr" target="#b36">Oberweger et al. (2015)</ref>. Compared to these techniques, our network is designed to directly regress to relative SE(3) transformations. We are able to do this due to our disentangled representation of rotation and translation and the reference frame we used for rotation, which also allows our approach to match unseen objects. As shown in <ref type="bibr" target="#b34">Mousavian et al. (2017)</ref>, the choice of reference frame is important to achieve good pose estimation results. Our work is also related to recent visual servoing methods based on deep neural networks <ref type="bibr" target="#b45">(Saxena et al., 2017;</ref><ref type="bibr" target="#b7">Costante and Ciarfuglia, 2018</ref>) that estimate the relative camera pose between two image frames, while we focus on 6D pose refinement of objects. Recent works <ref type="bibr" target="#b13">(Garon et al., 2016;</ref><ref type="bibr" target="#b12">Garon and Lalonde, 2017</ref>) that focus on tracking could predict the transformation of the object pose between previous frame and current frame and have the potential to be used for pose refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DeepIM Framework</head><p>In this section, we describe our deep iterative matching network for 6D pose estimation. Given an observed image and an initial pose estimate of an object in the image, we design the network to directly output a relative SE(3) transformation that can be applied to the initial pose to improve the estimate. We first present our strategy of zooming in the observed image and the rendered image that are used as inputs of the network. Then we describe our network architecture for pose matching. After that, we introduce a disentangled representation of the relative SE(3) transformation and a new loss function for pose regression. Finally, we describe our procedure for training and testing the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">High-resolution Zoom In</head><p>It can be difficult to extract useful features for matching if objects in the input image are very small. To obtain enough details for pose matching, we zoom in the observed image and the rendered image before feeding them into the network, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Specifically, in the i-th stage of the iterative matching, given a 6D pose estimate p (i−1) from the previous step, we render a synthetic image using the 3D object model viewed according to p (i−1) .</p><p>We additionally generate one foreground mask for the observed image and rendered image. The four images are cropped using an enlarged bounding box according to the observed mask and the rendered mask, where we make sure the enlarged bounding box has the Zoom in observed/rendered image observed/rendered image observed/rendered mask observed/rendered mask same aspect ratio as the input image and is centered at the 2D projection of the origin of the 3D object model. In more detail, given the rendered mask m rend and the observed mask m obs , the cropping patch is computed as</p><formula xml:id="formula_0">x dist = max(|l obs − x c |, |l rend − x c |, |r obs − x c |, |r rend − x c |), y dist = max(|u obs − y c |, |u rend − y c |, |d obs − y c |, |d rend − y c |), width = max(x dist , y dist · r) · 2λ, height = max(x dist /r, y dist ) · 2λ,<label>(1)</label></formula><p>where u * , d * , l * , r * denotes the upper, lower, left, right bound of foreground mask of observed or rendered images, x c , y c represent the 2D projection of the center of the object in img rend , r represent the aspect ratio of the origin image (width/height), λ denotes the expand ratio, which is fixed to 1.4 in the experiment in order to make the expanded patch is roughly twice than the nested one. Then this patch is bilinearly sampled to the size of the original image, which is 480 × 640 in this paper. By doing so, not only the object is zoomed in without being distorted, but also the network is provided with the information about where the center of the object lies. The observed image, the rendered image, and the two masks, are concatenated into an eight-channel tensor input to the network (3 channels for observed/rendered image, 1 channel for each mask). We use the FlowNet-Simple architecture from <ref type="bibr" target="#b10">Dosovitskiy et al. (2015)</ref> as the backbone network, which is trained to predict optical flow between two images. We tried using the VGG16 image classification network <ref type="bibr" target="#b47">(Simonyan and Zisserman, 2014)</ref> as the backbone network, but the results were very poor, confirming the intuition that a representation related to optical flow is very useful for pose matching <ref type="bibr" target="#b56">(Wang et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Structure</head><p>The pose estimation branch takes the feature map after 10 convolution layers from FlowNetSimple as input. It contains two fully-connected layers each with dimension 256, followed by two additional fully-connected layers for predicting the quaternion of the 3D rotation and the 3D translation, respectively.</p><p>During training, we also add two auxiliary branches to regularize the feature representation of the network and increase training stability and performance, see Sec. 4.4 and <ref type="table">Table.</ref> 2 for more details. One branch is trained for predicting optical flow between the rendered image and the observed image, and the other branch for predicting the foreground mask of the object in the observed image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Disentangled Transformation Representation</head><p>The representation of the coordinate frames and the relative SE(3) transformation ∆p between the current pose estimate and the target pose has important ramifications for the performance of the network. Ideally, we would like (1) the individual components of these trans- Taking observed image and rendered image and their corresponding masks as input, the convolution layers output a feature map which then be forwarded through several fully connected layers to predict the translation and rotation. The same feature map, combined with feature maps in the previous layers, will also be used to predict flow and foreground mask during training.</p><p>formations to be maximally dis-entangled, thereby not requiring the network to learn unnecessarily complex geometric relationships between translations and rotations, and (2) the transformations to be independent of the intrinsic camera parameters and the actual size and coordinate system of an object, thereby enabling the network to reason about changes in object appearance rather than accurate distance estimates. The most obvious choice are camera coordinates to represent object poses and transformations. Denote the relative rotation and translation as [R ∆ |t ∆ ] (We denote R * as rotation and and t * as translation in this paper). Given a source object pose [R src |t src ], the transformed target pose would be as follows:</p><formula xml:id="formula_1">R tgt = R ∆ R src , t tgt = R ∆ t src + t ∆ ,<label>(2)</label></formula><p>where [R tgt |t tgt ] denotes the target pose resulting from the transformation. The R ∆ t src term indicates that a rotation will cause the object not only to rotate, but also translate in the image even if the translation vector t ∆ equals to zero. Column (b) in <ref type="figure" target="#fig_4">Fig. 4</ref> illustrates this connection for an object rotating in the image plane. In standard camera coordinates, the translation t ∆ of an object is in the 3D metric space (meter, for instance), which couples object size with distance in the metric space. This would require the network to memorize the actual size of each object in order to transform mismatches in images to distance offsets. It is obvious that such a representation is not appropriate, particularly for matching unknown objects.</p><p>To eliminate these problems, we propose to decouple the estimation of R ∆ and t ∆ . First, we move the center of rotation from the origin of the camera to the center of the object in the camera frame, given by the current pose estimate. In this representation, a rotation does not change the translation of the object in the camera frame. The remaining question is how to choose the directions of the rotational axes of the coordinate frame. One way is to use the axes as specified in the 3D object model. However, as illustrated in column (c) of <ref type="figure" target="#fig_4">Fig. 4</ref>, such a representation would require the network to learn and memorize the coordinate frames of each object, which makes training more difficult and cannot be generalized to pose matching of unseen objects. Thus,  In the camera coordinate system, the center of rotation is in the center of the image, thereby causing an undesired translation in addition to the object rotation. In the model coordinate frame, as the frame of the object model can be defined arbitrarily, an object might rotate along any axis given the same rotation vector. Shown here is a CCW rotation, but the same axis might also result in an out of plane rotation for a differently defined object coordinate frame. In our disentangled representation, the center of rotation is in the center of the object and the axes are defined parallel to the camera axes. As a result, a rotation around a specific axis always results in the same object rotation, independent of the object. (Lower row) Rotation vectors a network would have to predict in order to achieve an inplace rotation using the different coordinate systems. Notice the extra translations required to compensate for the translation caused by the rotation using camera coordinates (column b). In model coordinates, the network would have to learn the frame specified for the object model in order to determine the correct rotation axis and angle. In our disentangled representation, rotation axis and angle are independent of the object.  are represented by vectors in 3D space. As a result, the same translation in the 2D image corresponds to different translation vectors depending on whether an object is close or far from the camera. In our disentangled representation, the value of x and y is only related to the 2D vector in the image-plane. Additionally, as shown in column (c), in the camera representation, a translation along the z-axis is not only difficult to infer from the image, but also causes a move relative to the center of the image. In our disentangled translation representation (column (d)), only the change of scale needs to be estimated, making it independent of other translations and the metric size and distance of the object.</p><p>we propose to use axes parallel to the axes of the camera frame when computing the relative rotation. By doing so, the network can be trained to estimate the relative rotation independently of the coordinate frame of the 3D object model, as illustrated in column (d) in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>In order to estimate the relative translation, let t tgt = (x tgt , y tgt , z tgt ) and t src = (x src , y src , z src ) be the target translation and the source translation. A straightforward way to represent translation is t ∆ = (∆ x , ∆ y , ∆ z ) = t tgt − t src . However, it is not easy for the network to estimate the relative translation in the 3D metric space given only 2D images without depth information. The network has to recognize the size of the object, and map the translation in 2D space to 3D according to the object size. Such a representation is not only difficult for the network to learn, but also has problems when dealing with unknown objects or objects with similar appearance but different sizes. Instead of training the network to directly regress to the vector in the 3D space, we propose to regress to object changes in the 2D image space. Specifically, we train the network to regress to the relative translation</p><formula xml:id="formula_2">t ∆ = (v x , v y , v z ),</formula><p>where v x and v y denote the number of pixels the object should move along the image x-axis and y-axis and v z is the scale change of the object:</p><formula xml:id="formula_3">v x = f x (x tgt /z tgt − x src /z src ), v y = f y (y tgt /z tgt − y src /z src ), v z = log(z src /z tgt ),<label>(3)</label></formula><p>where f x and f y denote the focal lengths of the camera. The scale change v z is defined to be independent of the absolute object size or distance by using the ratio between the distances of the rendered and observed object. We use logarithm for v z to make sure that a value of zero corresponds to no change in scale or distance.</p><p>Considering the fact that f x and f y are constant for a specific dataset, we simply fix it to 1 in training and testing the network.</p><p>Our representation of the relative transformation has several advantages. First, rotation does not influence the estimation of translation, so that the translation no longer needs to offset the movement caused by rotation around the camera center. Second, the intermediate variables v x , v y , v z represent simple translations and scale change in the image space. Third, this representation does not require any prior knowledge of the object. Using such a representation, the DeepIM network can operate independently of the actual size of the object, its internal model coordinate framework, and the camera intrinsics. It only has to learn to transform the rendered image such that it becomes more similar to the observed image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Matching Loss</head><p>A straightforward way to train the pose estimation network is to use separate loss functions for rotation and translation. For example, we can use the angular distance between two rotations to measure the rotation error and use the 2 distance to measure the translation error. However, using two different loss functions for rotation and translation suffers from the difficulty of balancing the two losses. <ref type="bibr" target="#b23">(Kendall and Cipolla, 2017)</ref> proposed a geometric reprojection error as the loss function for pose regression that computes the average distance between the 2D projections of 3D points in the scene using the ground truth pose and the estimated pose. Considering the fact that we want to accurately predict the object pose in 3D, we introduce a modified version of the geometric reprojection loss in <ref type="bibr" target="#b23">(Kendall and Cipolla, 2017)</ref>, and we call it the Point Matching Loss. Given the ground truth pose p = [R|t] and the estimated posep = [R|t], the point matching loss is computed as:</p><formula xml:id="formula_4">L pose (p,p) = 1 n n i=1 (Rx i + t) − (Rx i +t) 1 ,<label>(4)</label></formula><p>where x i denotes a randomly selected 3D point on the object model and n is the total number of points (we choose 3,000 points in our experiments). The formulation of point matching loss is similar to the one used to compute average distance (ADD) metric in Eq. 5.</p><p>The main difference is that other than using 2 norm, point matching loss computes the average 1 distance between 3D points transformed by the ground truth pose and the estimated pose in order to avoid the large graident caused by outliers and maintain the stability of loss during training. In this way, it measures how the transformed 3D models match against each other for pose estimation.  also uses a variant of the point matching loss for rotation regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Testing</head><p>In training, we assume that we have 3D object models and images annotated with ground truth 6D object poses. By adding noises to the ground truth poses as the initial poses, we can generate the required observed and rendered inputs to the network along with the pose target output that is the pose difference between the ground truth pose and the noisy pose. Then we can train the network to predict the relative transformation between the initial pose and the target pose. During testing, we find that the iterative pose refinement can significantly improve the accuracy. To see, let p (i) be the pose estimate after the i-th iteration of the network. If the initial pose estimate p (0) is relatively far from the correct pose, the rendered image img rend (p (0) ) may have only little viewpoint overlap with the observed image img obs . In such cases, it is very difficult to accurately estimate the relative pose transformation ∆p (0) directly. This task is even harder if the network has no priori knowledge about the object to be matched. In general, it is reasonable to assume that if the network improves the pose estimate p (i+1) by updating p (i) with ∆p (i) in the i-th iteration, then the image rendered according to this new estimate, img rend (p (i+1) ) is also more similar to the observed image img obs than img rend (p (i) ) was in the previous iteration, thereby providing input that can be matched more accurately.</p><p>However, we found that, if we train the network to regress the relative pose in a single step, the estimates of the trained network do not improve over multiple iterations in testing. To generate a more realistic data distribution for training similar to testing, we perform multiple iterations during training as well. Specifically, for each training image and pose, we apply the transformation predicted from the network to the pose and use the transformed pose estimate as another training example for the network in the next iteration. By repeating this process multiple times, the training data better represents the test distribution and the trained network also achieves significantly better results during iterative testing (such an approach has also proven useful for iterative hand pose matching <ref type="bibr" target="#b36">(Oberweger et al., 2015)</ref> and image alignment <ref type="bibr" target="#b25">(Lin and Lucey, 2017)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments on the LINEMOD dataset <ref type="bibr" target="#b17">(Hinterstoisser et al., 2012b)</ref> and the Occlusion LINEMOD dataset <ref type="bibr" target="#b2">(Brachmann et al., 2014)</ref> to evaluate our DeepIM framework for 6D object pose estimation. We test different properties of DeepIM and show that it surpasses other RGB-only methods by a large margin. We also show that our network can be applied to pose matching of unseen objects during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Implementation Details</head><p>Training Parameters: We use the pre-trained FlowNet-Simple <ref type="bibr" target="#b10">(Dosovitskiy et al., 2015)</ref> to initialize the weights in our network. Weights of the new layers are randomly initialized, except for the additional weights in the first conv layer that deals with the input masks and the fully-connected layer that predicts the translation, which are initialized with zeros. Other than predicting the pose transformation, the network also predicts the optical flow and the foreground mask. Including the two additional losses could slightly increase the pose estimation performance and make the training more stable. Specifically, we use the optical flow loss L flow as in FlowNet <ref type="bibr" target="#b10">(Dosovitskiy et al., 2015)</ref> and the sigmoid cross-entropy loss as the mask loss L mask . Two deconvolutional blocks in FlowNet are inherited to produce the feature map used for the mask and the optical flow prediction, whose spatial scale is 0.0625. Two 1 × 1 convolutional layers with output channel 1 (mask prediction) and 2 (flow prediction) are appended after this feature map. The predictions are then bilinearly up-sampled to the original image size (480 × 640) to compute losses.</p><p>The overall loss is L = αL pose + βL flow + γL mask , where we use α = 0.1, β = 0.25, γ = 0.03 throughout the experiments (except some of our ablation studies). Each training batch contains 16 images. We train the network with 4 GPUs where each GPU processes 4 images. We generate 4 items for each image as described in Sec. 3.1: two images and two masks. The observed mask is randomly dilated with no more than 10 pixels to avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Distribution of Rendered Pose during Training:</head><p>The rendered image img rend and mask m rend are randomly generated during training without using prior knowledge of the initial poses in the test set. Specifically, given a ground truth posep, we add noises top to generate the rendered poses. For rotation, we independently add a Gaussian noise N (0, 15 2 ) to each of the three Euler angles of the rotation. If the angular distance between the new pose and the ground truth pose is more than 45 • , we discard the new pose and generate another one in order to make sure the initial pose for refinement is within 45 • of the ground truth pose during training. For translation, considering the fact that RGB-based pose estimation methods usually have larger standard deviation on depth estimation, the following Gaussian noises are added to the three components of the translation: ∆x ∼ N (0, 0.01 2 ), ∆y ∼ N (0, 0.01 2 ), ∆z ∼ N (0, 0.05 2 ), where the standard deviations are 1 cm, 1 cm and 5 cm, respectively.</p><p>Synthetic Training Data: Real training images provided in existing datasets may be highly correlated or lack images in certain situations such as occlusions between objects. Therefore, generating synthetic training data is essential to enable the network to deal with different scenarios in testing. In generating synthetic training data for the LINEMOD dataset, considering the fact that the elevation variation is limited in this dataset, we calculate the elevation range of the objects in the provided training data. Then we rotate the object model with a randomly generated quaternion and repeat it until the elevation is within this range. The translation is randomly generated using the mean and the standard deviation computed from the training set. During training, the background of the synthetic image is replaced by a randomly chosen indoor image from the PASCAL VOC dataset as shown in <ref type="figure" target="#fig_6">Fig. 6</ref>.</p><p>For the Occlusion LINEMOD dataset, multiple objects are rendered into one image in order to introduce occlusions among objects. The number of objects ranges from 3 to 8 in these synthetic images. As in the LINEMOD dataset, the quaternion of each object is also randomly generated to ensure that the elevation range is within that of training data in the Occlusion LINEMOD dataset. The translations of the objects in the same image are drawn according to the distributions of the objects in the YCB-Video dataset  by adding a small Gaussian noise.</p><p>For the YCB-Video dataset, synthetic images are generated on the fly. Other than the target object, we also render another object close to it to introduce partial occlusion. The real training images may also lack variations in light conditions exhibited in the real world or in the testing set. Therefore, we add a random light condition to each synthetic image in both the LINEMOD dataset and the Occlusion LINEMOD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Testing Implementation Details</head><p>Testing Parameters: The mask prediction branch and the optical flow branch are removed during testing. Since there is no ground truth segmentation of the object in testing, we use the tightest bounding box of the rendered mask m rend instead, so the network searches the neighborhood near the estimated pose to find the target object to match. Unless specified, we use the pose estimates from PoseCNN  as the initial poses. Our DeepIM network runs at 12 fps per object using an NVIDIA 1080 Ti GPU with 2 iterations during testing.</p><p>Pose Initialization during inference: Our framework takes an input image and an initial pose estimation of an object in the image as inputs, and then refine the initial pose iteratively. In our experiments, we have tested two pose initialization methods.</p><p>The first one is PoseCNN ), a convolutional neural network designed for 6D object pose estimation. PoseCNN performs three tasks for 6D pose estimation, i.e., semantic labeling to classify image pixels into object classes, localizing the center of the object on the image to estimate the 3D translation of the object, and 3D rotation regression. In our experiments, we use the 6D poses from PoseCNN as initial poses for pose refinement.</p><p>To demonstrate the robustness of our framework on pose initialization, we have implemented a simple 6D pose estimation method for pose initialization, where we extend the Faster R-CNN framework designed for 2D object detection <ref type="bibr" target="#b40">(Ren et al., 2015)</ref> to 6D pose estimation. Specifically, we use the bounding box of the object from Faster R-CNN to estimate the 3D translation of the object. The center of the bounding box is treated as the center of the object. The distance of the object is estimated by maximizing the overlap of the projection of the 3D object model with the bounding box. To estimate the 3D rotation of the object, we add a rotation regression branch to Faster R-CNN as in PoseCNN. In this way, we can obtain a 6D pose estimation for each detected object from Faster R-CNN.</p><p>In our experiments on the LINEMOD dataset described in Sec. 4.4, we have shown that, although the initial poses from Faster R-CNN are much worse than the poses from PoseCNN, our framework is still able to refine these poses using the same weights. The performance gap between using the two different pose initialization methods is quite small, which demonstrates the ability of our framework in using different methods for pose initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We use the following three evaluation metrics for 6D object pose estimation. i) The 5 • , 5cm metric considers an estimated pose to be correct if its rotation error is within 5 • and the translation error is below 5cm. ii) The 6D Pose metric <ref type="bibr" target="#b17">(Hinterstoisser et al., 2012b)</ref> computes the average distance between the 3D model points transformed using the estimated pose and the ground truth pose. For symmetric objects, we use the closest point distance in computing the average distance. An estimated pose is correct if the average distance is within 10% of the 3D model diameter. iii) The 2D Projection metric computes the average distance of the 3D model points projected onto the image using the estimated pose and the ground truth pose. An estimated pose is correct if the average distance is smaller than 5 pixels. k • , k cm: Proposed in <ref type="bibr" target="#b46">Shotton et al. (2013)</ref>. The 5 • , 5cm metric considers an estimated pose to be correct if its rotation error is within 5 • and the translation error is below 5cm. We also provided the results with 2 • , 2cm and 10 • , 10cm in <ref type="table" target="#tab_7">Table 6</ref> to give a comprehensive view about the performance.</p><p>For symmetric objects such as eggbox and glue in the LINEMOD dataset, we compute the rotation error and the translation error against all possible ground truth poses with respect to symmetry and accept the result when it matches one of these ground truth poses. </p><p>where m is the number of points on the 3D object model, M is the set of all 3D points of this model, p = [R|t] is the ground truth pose andp = [R|t] is the estimated pose. Here the number of points m can be different from the number of points n used in Eq. 4 as the point clouds used for training is a subset randomly sampled from the original point clouds to reduce the time to compute the loss during training. Rx + t indicates transforming the point with the given SE(3) transformation (pose) p. Following <ref type="bibr" target="#b3">(Brachmann et al., 2016)</ref>, we compute the distance between all pairs of points from the model and regard the maximum distance as the diameter d of this model. Then a pose estimation is considered to be correct if the computed average distance is within 10% of the model diameter.</p><p>In addition to using 0.1d as the threshold, we also provided pose estimation accuracy using thresholds 0.02d and 0.05d in <ref type="table" target="#tab_7">Table 6</ref>. We use 0.1d as the threshold of 6D Pose metric in the following paper if not specified. For symmetric objects, we use the closest point distance in computing the average distance for 6D pose evaluation as in <ref type="bibr" target="#b17">Hinterstoisser et al. (2012b)</ref>:</p><formula xml:id="formula_6">ADD-S = 1 m x1∈M min x2∈M (Rx 1 +t)−(Rx 2 +t) 2 . (6)</formula><p>In the YCB-Video Dataset, we use the metric ADD and ADD-S described in <ref type="bibr" target="#b58">Xiang et al. (2018)</ref>. After getting the ADD and ADD-S distance described in Eq. 5 and Eq. 6, we vary the threshold from 0 to 10 cm and accumulate the area under the accuracy curves.</p><p>2D Projection: focuses on the matching of pose estimation on 2D images. This metric is considered to be important for applications such as augmented reality. We compute the error using Eq. 7 and accept a pose estimation when the 2D projection error is smaller than a predefined threshold:</p><formula xml:id="formula_7">Proj. 2D = 1 m x∈M K(Rx + t) − K(Rx +t) 2 ,<label>(7)</label></formula><p>where K denotes the intrinsic parameter matrix of the camera and K(Rx + t) indicates transforming a 3D point according to the SE(3) transformation and then projecting the transformed 3D point onto the image. In addition to using 5 pixels as the threshold, we also show our results with the thresholds 2 pixels and 10 pixels. We use 5 pixels as the threshold of Proj. 2D metric in the following paper if not specified. For symmetric objects such as eggbox and glue in the LINEMOD dataset, we compute the 2D projection error against all possible ground truth poses and accept the result when it matches one of these ground truth poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments on the LINEMOD Dataset</head><p>The LINEMOD dataset contains 15 objects. We train and test our method on 13 of them as other methods in the literature. We follow the procedure in <ref type="bibr" target="#b3">(Brachmann et al., 2016)</ref> to split the dataset into the training and test sets, with around 200 images for each object in the training set and 1,000 images in the test set. <ref type="figure">Fig. 9</ref> shows a subset of objects used in LINEMOD dataset. These objects are textureless and thus difficult for pose estimation methods using only local features.</p><p>Training strategy: For every image, we generate 10 random poses near the ground truth pose, resulting in 2,000 training samples for each object in the training set. Furthermore, we generate 10,000 synthetic images for each object where the pose distribution is similar to the real training set. For each synthetic image, we generate 1 random pose near its ground truth pose. Thus, we have a total of 12,000 training samples for each object in training. The background of a synthetic image is replaced with a randomly chosen indoor image from PASCAL VOC <ref type="bibr" target="#b11">(Everingham et al., 2010)</ref>. We train the networks for 8 epochs with initial learning rate 0.0001. The learning rate is divided by 10 after the 4th and 6th epoch, respectively.</p><p>Ablation study on iterative training and testing: <ref type="table" target="#tab_2">Table  1</ref> shows the results that use different numbers of iterations during training and testing. The networks with train iter = 1 and train iter = 2 are trained with 32 and 16 epochs respectively to keep the total number of updates the same as train iter = 4. The table shows that without iterative training (train iter = 1), multiple iteration testing does not improve, potentially even making the results worse (test iter = 4). We believe that the reason is due to the fact that the network is not trained with enough rendered poses close to their ground truth poses. The table also shows that one more iteration during training and testing already improves the results by a large margin. The network trained with 2 iterations and tested with 2 iterations is slightly better than the one trained with 4 iterations and tested with 4 iterations. This may be because the LINEMOD dataset is not sufficiently difficult to generate further improvements by using 3 or 4 iterations. Since it is not straightforward to determine how many iterations to use in each dataset, we use 4 iterations during training and testing in all other experiments.</p><p>Ablation study on the zoom in strategy, network structures, transformation representations, and loss functions: <ref type="table" target="#tab_4">Table 3</ref> summarizes the ablation studies on various aspects of DeepIM. The "zoom" column indicates whether the network uses full images as its input or zoomed in bounding boxes up-sampled to the original image size. Comparing rows 5 and 7 shows that the higher resolution achieved via zooming in provides very significant improvements. "Regressor": We train the DeepIM network jointly over all objects, generating a pose transformation independent of the specific input object (labeled "shared" in "regressor" column). Alternatively, we could train a different 6D pose regressor for each individual object by using a separate fully connected layer for each object after the final FC256 layer shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. This setting is labeled as "sep." in <ref type="table" target="#tab_4">Table 3</ref>. Comparing rows 3 and 7 shows that both approaches provide nearly indistinguishable results. But the shared network provides some efficiency gains.</p><p>"Network": Similarly, instead of training a single network over all objects, we could train separate networks, one for each object as in <ref type="bibr" target="#b38">Rad and Lepetit (2017)</ref>.</p><p>Comparing row 1 to 7 shows that a single, shared network provides better results than individual ones, which indicates that training on multiple objects can help the network learn a more general representation for matching. We also present an ablation study of mask prediction and flow prediction in <ref type="table">Table 2</ref>. It shows that when trained with these two auxiliary branches, the network could achieve the highest performance.</p><p>"Coordinate": This column investigates the impact of our choice of coordinate frame to reason about object transformations, as described in <ref type="figure" target="#fig_4">Fig. 4</ref>. The row labeled "camera" provides results when choosing the camera frame of reference as the representation for the object pose, rows labeled "model" move the center of rotation to the object model and choose the object model coordinate frame to reason about rotations, and the "disentangled" rows provide our disentangled approach of moving the center into the object model while keeping the camera coordinate frame for rotations. Comparing rows 2 and 3 shows that reasoning in the camera rotation frame provides slight improvements. Furthermore, it should be noted that only our "disentangled" approach is able to operate on unseen objects. Comparing rows 4 and 5 shows the large improvements our representation achieves over the common approach of reasoning fully in the camera frame of reference.</p><p>"Loss": The traditional loss for pose estimation is specified by the distance ("Dist") between the estimated and ground truth 6D pose coordinates, i.e., angular distance for rotation and euclidean distance for translation. Comparing rows 6 and 7 indicates that our point matching loss ("PM") provides significantly better results especially on the 6D pose metric, which is the most important measure for reasoning in 3D space.</p><p>Application to different initial pose estimation networks: <ref type="table" target="#tab_5">Table 4</ref> provides results when we initialize DeepIM with two different pose estimation networks. The first one is PoseCNN , and the second one is a simple 6D pose estimation method based on Faster R-CNN <ref type="bibr" target="#b40">(Ren et al., 2015)</ref>. Specifically, we use the bounding box of the object from Faster R-CNN to estimate the 3D translation of the object. The center of the bounding box is treated as the center of the object. The distance of the object is estimated by maximizing the overlap of the projection of the 3D object model with the bounding box. To estimate the 3D rotation of the object, we add a rotation regression branch to Faster R-CNN as in PoseCNN. As we can see in <ref type="table" target="#tab_5">Table 4</ref>, our network achieves very similar pose estimation accuracy even when initialized with the estimates from the extension of Faster R-CNN, which are not as accurate as those provided by PoseCNN .  <ref type="table">Table 2</ref>: Ablation study on the role of mask prediction and flow prediction branch. The networks are trained 5 times for each setting on the object ape of the LINEMOD dataset. The numbers denote mean ± standard deviation.   Comparison with the state-of-the-art 6D pose estimation methods: <ref type="table" target="#tab_6">Table 5</ref> shows the comparison with the best color-only techniques on the LINEMOD dataset. DeepIM achieves very significant improvements over all prior methods, even those that also deploy refinement steps (BB8 <ref type="bibr" target="#b38">(Rad and Lepetit, 2017)</ref> and SSD-6D <ref type="bibr" target="#b22">(Kehl et al., 2017)</ref>).</p><p>Detailed Results on the LINEMOD Dataset: <ref type="table" target="#tab_7">Table 6</ref> shows our detailed results on all the 13 objects in the LINEMOD dataset. The network is trained and tested with 4 iterations and 8 epochs. Initial poses are estimated by PoseCNN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments on the Occlusion LINEMOD Dataset</head><p>The Occlusion LINEMOD dataset proposed in <ref type="bibr" target="#b2">Brachmann et al. (2014)</ref> shares the same images used in the LINEMOD dataset <ref type="bibr" target="#b17">(Hinterstoisser et al., 2012b)</ref>, but annotated 8 objects in one video that are heavily blocked by other objects.</p><p>Training: For every real image, we generate 10 random poses as described in Sec. 4.4. Considering the fact that most of the training data lacks occlusions, we generated about 20,000 synthetic images with multiple objects in each image. By doing so, every object has around 12,000 images which are partially occluded, and a total of 22,000 images for each object in training. We perform the same background replacement and training procedure as in the LINEMOD dataset.</p><p>Comparison with the state-of-the-art methods: The comparison between our method and other RGB-only methods is shown in <ref type="figure">Fig. 8</ref>. We only show the plots with accuracies on the 2D Projection metric because these are the only results reported in <ref type="bibr" target="#b38">Rad and Lepetit (2017)</ref> and <ref type="bibr" target="#b50">(Tekin et al., 2017)</ref> (results for eggbox and glue use a symmetric version of this accuracy). It can be seen  that our method greatly improves the pose accuracy generated by PoseCNN and surpasses all other RGBonly methods by a large margin. It should be noted that BB8 <ref type="bibr" target="#b38">(Rad and Lepetit, 2017)</ref> achieves the reported results only when using ground truth bounding boxes during testing. Our method is even competitive with the results that use depth information and ICP to refine the estimates of PoseCNN. <ref type="figure">Fig. 9</ref> shows some pose refinement results from our method on the Occlusion LINEMOD dataset.</p><p>Detailed Results on the Occlusion LINEMOD Dataset: <ref type="table" target="#tab_8">Table 7</ref> shows our results on the Occlusion LINEMOD dataset. We can see that DeepIM can significantly improve the initial poses from PoseCNN. Notice that the diameter here is computed using the extents of the 3D model following the setting of  and other RGB-D based methods. Some qualitative results are shown in <ref type="figure" target="#fig_8">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Experiments on the YCB-Video Dataset</head><p>The YCB-Video Dataset, which is proposed in , annotates 21 YCB objects <ref type="bibr" target="#b4">(Calli et al., 2015)</ref> in 92 video sequences (133,827 frames). It is a challenging dataset as the objects have varied sizes (diameter from 10 cm to 40 cm), different types of symmetries, and a large variety of occlusions and lighting conditions. We split the dataset as , with 80 video sequences for training and 2,949 keyframes in the remaining 12 videos for testing.</p><p>Training Strategy: As images in one video are similar to those in nearby frames, we use 1 image out of every 10 images in the training set for training. Training batches consist of captured real images from the dataset (1/8) and synthetic images which are partially occluded and generated on the fly (7/8). The network is trained with 8 epochs and we decrease the learning rate after 4 and 6 epochs. We found that with large training sets and enough epochs it was not necessary to include the flow prediction and the masks in the input, so we removed those branches and the corresponding loss from this experiment. For different categories, they share the same network but use separate regressors to achieve the best performance.</p><p>Evaluation Metric: We follow the PoseCNN  paper when evaluating the results which uses accuracy under curve of ADD (Eq. 5) and ADD-S (Eq. 6 for each object. We also report the results of ADD(-S) and AUC ADD(-S) metric which is similar to the one we used in LINEMOD <ref type="bibr" target="#b2">(Brachmann et al., 2014)</ref>. More specifically, we use ADD when the object is not symmetric and use ADD-S when the object is symmetric. Then we compute the averaged accuracy as the final result.</p><p>Symmetric Objects: As described in Sec. 4.1, we only keep rendered poses that have an angular distance less than 45 degrees from ground truth poses during training, which means we don't need to take special care of objects which have a symmetry angle of more than 90 degrees. However, object 024 bowl in the YCB-Video dataset is rotational symmetric. To deal with this kind of symmetry, rather than using the ground truth posep provided by the dataset to compute the loss, we choose the distance to the closest pose p * among all poses that look the same as the ground truth pose:</p><formula xml:id="formula_8">p * = arg min p∈Q Θ(p, p src )<label>(8)</label></formula><p>Here, Q denotes the set of poses whose corresponding rendered images are the same as the one rendered using the ground truth pose. We assume that the rotation axis goes through the origin of the model frame so that no translation needs to be considered. In the experiment, we calibrate the rotation axis manually and use bisection search to locate the closest ground truth pose. <ref type="table">Table.</ref> 8 compares networks trained with and without this strategy, showing that this training loss is useful.</p><p>Comparison with state-of-the-art methods: <ref type="table" target="#tab_2">Table 10</ref> compares our results with two state-of-the-art methods: PoseCNN  and DenseFusion <ref type="bibr" target="#b55">(Wang et al., 2019)</ref>. As can be seen, DeepIM greatly refines <ref type="figure">Fig. 8</ref>: Comparison with state-of-the-art methods on the Occlusion LINEMOD dataset <ref type="bibr" target="#b2">(Brachmann et al., 2014)</ref>. Accuracies are measured via the Projection 2D metric. <ref type="figure">Fig. 9</ref>: Examples of refined poses on the Occlusion LILNEMOD dataset using the results from PoseCNN  as initial poses. The red and green lines represent the silhouettes of the initial estimates and our refined poses, respectively.  <ref type="table">Table 8</ref>: Ablation study about using closest ground truth pose to handle rotational symmetric objects. These three columns show the evaluation results of initial poses, poses refined by a DeepIM network that treats 024 bowl as a regular object, and poses refined by a network trained with closest ground truth pose. Initial poses are generated as rendered pose during training described in Sec. 4.1 the initial pose provided by PoseCNN and is on par with those refined with ICP on many objects despite not using any depth or point cloud data. Notice that DeepIM produces low numbers on symmetric objects, like 024 bowl, under ADD metric. This is because the ADD metric cannot well represent the performance on symmetric objects as such objects have multiple correct poses but only one of these poses are labeled as the ground truth in the dataset. <ref type="table" target="#tab_10">Table 9</ref> shows the result compared with PoseCNN  and PoseRBPF <ref type="bibr" target="#b9">(Deng et al., 2019)</ref> using the ADD(-S) metrci which can avoid such problems. <ref type="figure" target="#fig_9">Fig. 10</ref> visualizes some pose refinement results from our method on the YCB-Video dataset.</p><p>Tracking in the YCB-Video Dataset: Considering the similarity between pose refinement and object tracking, it is natural to use DeepIM to track objects in videos. Therefore, we conducted an experiment testing DeepIM's ability to track objects in the YCB-Video dataset. Provided with the ground truth pose of an ob-    and Dense-Fusion <ref type="bibr" target="#b55">(Wang et al., 2019)</ref>. The network is trained and tested with 4 iterations. The ADD and ADD-S is short for AUC of ADD and AUC of ADD-S. ject in the first frame of each video, DeepIM can perform tracking by using the refined pose estimate from the previous frame as the initial pose of the next frame. Rather than doing inference only on key frames, we applied DeepIM to all images in the test video so that the object poses were close between successive frames.</p><p>In order to determine when DeepIM loses track of an object due to heavy occlusion, we follow a simple strategy: we count the tracking as "lost" if the last iteration of the last 10 frames has an average rotation greater than 10 degrees or an average translation greater than 1 cm. Once the tracking is marked as lost, the network will be re-initialzed with PoseCNN's prediction. This strategy is designed with the intuition that successful tracking should have a small offset at the last iteration. Re-initialization happens every 340 frames on average. <ref type="table" target="#tab_2">Table 9 and Table 10</ref> shows our numerical results. Notice that the results of tracking are better than PoseCNN+DeepIM in most cases and are comparable to the results refined with ICP which uses depth information. Also note that the performance on object 036 wood block is bad because the model of the wooden block is different from the object used in the actual dataset video, which makes it nearly impossible to match the model with the image.</p><p>Tracking YCB objects in real scenes: To demonstrate our framework's generalization, we use our network to   as initial poses. The green and red lines represent the silhouettes of the initial estimates and our refined poses, respectively. <ref type="figure">Fig. 11</ref>: Examples on tracking in the real world, using the same network as in <ref type="table">Table.</ref> 10 and no prior knowledge about focal length. The first row shows the images captured with a webcam and the second row renders the object onto the image based on the estimated pose. track objects in real scenes. This means we don't have any prior knowledge about the lighting conditions, background, or camera parameters. Similar to tracking on the YCB-Video dataset, we use DeepIM to refine poses predicted from the previous frame. Thanks to the disentangled representation, we did not have to calibrate the camera to get its intrinsic matrix. <ref type="figure">Fig. 11</ref> shows some tracking results using our method in the real world environment in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using Depth information:</head><p>Other than using RGB images to do pose refinement, DeepIMcan be easily extended to utilize depth information to improve its performance. Here we append the depth images of the ob- served image and the rendered image with the two zeroinitialized additional channels in the first convolution (one for the rendered depth and the other for the observed depth). To provide the network with information of the center of the object, we normalize the depth images by subtract them from the depth of the object's center. The results are shown in <ref type="table">Table.</ref> 10.</p><p>Failure cases: In <ref type="figure" target="#fig_1">Fig. 12</ref> we show 10 instances that the network fails to refine to a correct pose. They can be grouped into 5 categories: 1) discrepancy between object models and images. This can be caused by bad light conditions or an inaccurate object model; 2) few patterns to match. This usually happens when only certain featureless side-views are visible or the object is heavily occluded; 3) objects' shapes are unusu al and difficult to learn; 4) the initial pose is too far away from the correct pose; 5) objects with tiny key components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Application to Unseen Objects and Unseen Categories</head><p>As stated in Sec. 3.3, we designed the disentangled pose representation such that it is independent of the coordinate frame and the size of a specific 3D object model. In other words, the transformation predicted from the network does not need to have prior knowledge about the model itself. Therefore, the pose transformations correspond to operations in the image space. This opens the question whether DeepIM can refine the poses of objects that are not included in the training set. From the experiment results we found that our network can perform accurate refinement on these unseen models. See <ref type="figure" target="#fig_2">Fig. 13</ref> for example results. We also tested our framework on refining the poses of unseen object categories, where the training categories and the test categories are completely different.</p><p>Test on Unseen Objects: In this experiment, we explore the ability of the network in refining poses of objects that has never been seen in training. ModelNet <ref type="bibr" target="#b57">(Wu et al., 2015)</ref> contains a large number of 3D models in different object categories. Here, we tested our network on three of them: airplane, car and chair. For each of these categories, we train a network on no more than 200 3D models and test its performance on 70 unseen 3D models from the same category. Similar to the way that we generate synthetic data as described in Sec 4.1, we generate 50 poses for each model as the target poses and train the network for 4 epochs. We use uniform gray texture for each model and add a light source which has a fixed relative position to the object to reflect the norms of the object. The initial pose used in training and testing is generated in the same way as we did in previous experiments as described in Sec. 4.1. The results are show in <ref type="table" target="#tab_2">Table 11</ref>.</p><p>Test on Unseen Categories: We also tested our framework on refining the poses of unseen object categories, where the training categories and the test categories are completely different. We train the network on 8 categories from ModelNet <ref type="bibr" target="#b57">(Wu et al., 2015)</ref>: airplane, bed, bench, car, chair, piano, sink, toilet with 30 models in each category and 50 image pairs for each model. The network was trained with 4 iterations and 4 epochs. Then we tested the network on 7 other categories: bathtub, bookshelf, guitar, range hood, sofa, wardrobe, and tv stand. The results are shown in <ref type="table">Table.</ref> 12. It shows that the network indeed has learned some general features for pose refinement across different object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work we introduce DeepIM, a novel framework for iterative pose matching using color images only. Given an initial 6D pose estimation of an object, we have designed a new deep neural network to directly output a relative pose transformation that improves the pose estimate. The network automatically learns to match object poses during training. We introduce an disentangled pose representation that is also independent of the object size and the coordinate frame of <ref type="figure" target="#fig_1">Fig. 12</ref>: Failure cases in YCB-Video dataset. These images illustrate 5 different reasons we concluded that leads to fail cases. <ref type="figure" target="#fig_2">Fig. 13</ref>: Results on pose refinement of 3D models from the ModelNet dataset. These instances were not seen in training. The red and green lines represent the edges of the initial estimates and our refined poses. the 3D object model. In this way, the network can even match poses of unseen objects, as shown in our experiments. Our method significantly outperforms state-ofthe-art 6D pose estimation methods using color images only and provides performance close to methods that use depth images for pose refinement, such as using the iterative closest point algorithm. Example visualizations of our results on LINEMOD, ModelNet, T-LESS can be found here: https://rse-lab.cs.washington. edu/projects/deepim. This work opens up various directions for future research. For instance, we expect that a stereo version of DeepIM could further improve pose accuracy. Furthermore, DeepIM indicates that it is possible to produce accurate 6D pose estimates using color images only, enabling the use of cameras that capture high resolution images at high frame rates with a large field of view, providing estimates useful for applications such as robot manipulation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1: We propose DeepIM, a deep iterative matching network for 6D object pose estimation. The network is trained to predict a relative SE(3) transformation that can be applied to an initial pose estimation for iterative pose refinement. Given a 6D pose estimation of an object, which can be the output of other pose estimation methods like PoseCNN (Xiang et al., 2018) (pose (0) in the figure) or the refined pose from previous iteration (pose (1) in the figure), along with the 3D model of the object, we generate the rendered image showing the appearance of the target object under this rough pose estimation. With the image pairs of rendered image and observed image, the network predicts a relative transformation (∆pose in the figure) which can be applied to refine the input pose. The refined pose can be used as the input pose of next iteration and therefore the process can be repeated until the refined pose converges or the number of iterations reaches a pre-determined number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>DeepIM operates on a zoomed in, up-sampled input image, the rendered image, and the two object masks (480 × 640 in our case after zooming in). More specifically, we enlarge the bounding box of the object in the rendered image, crop the corresponding patch using the enlarged bounding box in both image pairs and mask pairs and then up-sample them to high resolution. Notice that the aspect ratio is kept during this process to avoid image distortion. See Sec. 3.1 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>illustrates the network architecture of DeepIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>DeepIM uses a FlowNetSimple backbone to predict a relative SE(3) transformation to match the observed and rendered image of an object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Rotations using different coordinate systems. (Upper row) The panels show how a 90 degree rotation in the image plane axis changes the position of the object shown in column (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Translations using camera and our disentangled representations. In camera coordinates, translations in the image plane</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Synthetic Data for the LINEMOD, Occlusion LINEMOD and YCB-Video separately. 6a shows the synthetic training data used when training on the LINEMOD dataset, only one object is presented in the image so there is no occlusion. 6b shows the synthetic training data used when training on the Occlusion LINEMOD dataset, multiple objects are presented in one image so one object may be occluded by other objects. 6c shows the synthetic training data used when training on the YCB-Video dataset. These images are rendered on the fly, so we only render two objects to maintain efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>6D Pose:</head><label></label><figDesc><ref type="bibr" target="#b17">Hinterstoisser et al. (2012b)</ref> use the average distance (ADD) metric to compute the averaged distance between points transformed using the estimated pose and the ground truth pose as in Eq. 5: t) − (Rx +t) 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Some pose refinement results on the Occlusion LINEMOD dataset. The red and green lines represent the edges of 3D model projected from the initial poses and our refined poses respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Examples of refined poses on the YCB-Video dataset which use results from PoseCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of the number of iterations during training and testing.</figDesc><table><row><cell cols="2">train iter init test iter</cell><cell>1</cell><cell>1 2</cell><cell>4</cell><cell>1</cell><cell>2 2</cell><cell>4</cell><cell>1</cell><cell>4 2</cell><cell>4</cell></row><row><cell>5cm 5 •</cell><cell cols="10">19.4 57.4 58.8 54.6 76.3 86.2 86.7 70.2 83.7 85.2</cell></row><row><cell cols="11">6D Pose 62.7 77.9 79.0 76.1 83.1 88.7 89.1 80.9 87.6 88.6</cell></row><row><cell cols="11">Proj. 2D 70.2 92.4 92.6 89.7 96.1 97.8 97.6 94.6 97.4 97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on different design choices of the DeepIM network on the LINEMOD dataset.</figDesc><table><row><cell>Row</cell><cell cols="3">methods zoom regressor network coordinate</cell><cell>loss</cell><cell cols="3">5cm 5 • 6D Pose Proj. 2D</cell></row><row><cell>1</cell><cell>-</cell><cell>sep.</cell><cell cols="2">disentangled PM</cell><cell>83.3</cell><cell>87.6</cell><cell>96.2</cell></row><row><cell>2</cell><cell>sep.</cell><cell>shared</cell><cell>model</cell><cell>PM</cell><cell>79.2</cell><cell>87.5</cell><cell>95.4</cell></row><row><cell>3</cell><cell>sep.</cell><cell cols="3">shared disentangled PM</cell><cell>86.6</cell><cell>89.5</cell><cell>96.7</cell></row><row><cell>4</cell><cell>shared</cell><cell>shared</cell><cell>camera</cell><cell>PM</cell><cell>16.6</cell><cell>44.3</cell><cell>62.5</cell></row><row><cell>5</cell><cell>shared</cell><cell cols="3">shared disentangled PM</cell><cell>38.3</cell><cell>65.2</cell><cell>80.8</cell></row><row><cell>6</cell><cell>shared</cell><cell cols="3">shared disentangled Dist</cell><cell>86.5</cell><cell>79.2</cell><cell>96.2</cell></row><row><cell>7</cell><cell>shared</cell><cell cols="3">shared disentangled PM</cell><cell>85.2</cell><cell>88.6</cell><cell>97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on two different methods for generating initial poses on the LINEMOD dataset.</figDesc><table><row><cell cols="2">method PoseCNN</cell><cell>PoseCNN +OURS</cell><cell>Faster R-CNN</cell><cell>Faster R-CNN +OURS</cell></row><row><cell>5cm 5 •</cell><cell>19.4</cell><cell>85.2</cell><cell>11.9</cell><cell>83.4</cell></row><row><cell>6D Pose</cell><cell>62.7</cell><cell>88.6</cell><cell>33.1</cell><cell>86.9</cell></row><row><cell>Proj. 2D</cell><cell>70.2</cell><cell>97.5</cell><cell>20.9</cell><cell>95.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art methods on the LINEMOD dataset</figDesc><table><row><cell>methods</cell><cell>Brachmann et al. (2016)</cell><cell>BB8 w/ ref. (Rad and Lepetit 2017)</cell><cell>SSD-6D w ref. (Kehl et al., 2017)</cell><cell>Tekin et al. (2017)</cell><cell>PoseCNN (Xiang et al., 2018)</cell><cell>PoseCNN (Xiang et al., 2018) +OURS</cell></row><row><cell>5cm 5 •</cell><cell>40.6</cell><cell>69.0</cell><cell>-</cell><cell>-</cell><cell>19.4</cell><cell>85.2</cell></row><row><cell>6D Pose</cell><cell>50.2</cell><cell>62.7</cell><cell>79</cell><cell>55.95</cell><cell>62.7</cell><cell>88.6</cell></row><row><cell>Proj. 2D</cell><cell>73.7</cell><cell>89.3</cell><cell>-</cell><cell>90.37</cell><cell>70.2</cell><cell>97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results of using more detailed thresholds on the LINEMOD dataset</figDesc><table><row><cell>metric</cell><cell></cell><cell>(n • , n cm)</cell><cell></cell><cell></cell><cell>6D Pose</cell><cell cols="2">Projection 2D</cell></row><row><cell>threshold</cell><cell cols="7">(2, 2) (5, 5) (10,10) 0.02d 0.05d 0.10d 2 px. 5 px. 10 px.</cell></row><row><cell>ape</cell><cell>37.7</cell><cell>90.4</cell><cell>98.0</cell><cell>14.3</cell><cell>48.6</cell><cell>77.0 92.2 98.4</cell><cell>99.6</cell></row><row><cell>benchvise</cell><cell>37.6</cell><cell>88.7</cell><cell>98.2</cell><cell>37.5</cell><cell>80.5</cell><cell>97.5 67.7 97.0</cell><cell>99.6</cell></row><row><cell>camera</cell><cell>56.1</cell><cell>95.8</cell><cell>99.2</cell><cell>30.9</cell><cell>74.0</cell><cell>93.5 86.3 98.9</cell><cell>99.7</cell></row><row><cell>can</cell><cell>58.0</cell><cell>92.8</cell><cell>99.0</cell><cell>41.4</cell><cell>84.3</cell><cell>96.5 98.6 99.7</cell><cell>99.8</cell></row><row><cell>cat</cell><cell>33.5</cell><cell>87.6</cell><cell>97.8</cell><cell>17.6</cell><cell>50.4</cell><cell cols="2">82.1 88.4 98.7 100.0</cell></row><row><cell>driller</cell><cell>49.4</cell><cell>92.9</cell><cell>99.1</cell><cell>35.7</cell><cell>79.2</cell><cell>95.0 64.2 96.1</cell><cell>99.4</cell></row><row><cell>duck</cell><cell>30.8</cell><cell>85.2</cell><cell>98.5</cell><cell>10.5</cell><cell>48.3</cell><cell>77.7 88.1 98.5</cell><cell>99.8</cell></row><row><cell>eggbox</cell><cell>32.1</cell><cell>63.9</cell><cell>94.5</cell><cell>34.7</cell><cell>77.8</cell><cell>97.1 53.4 96.2</cell><cell>99.6</cell></row><row><cell>glue</cell><cell>32.8</cell><cell>83.0</cell><cell>98.0</cell><cell>57.3</cell><cell>95.4</cell><cell>99.4 81.5 98.9</cell><cell>99.7</cell></row><row><cell>holepuncher</cell><cell>8.7</cell><cell>54.5</cell><cell>93.8</cell><cell>5.3</cell><cell>27.3</cell><cell>52.8 59.1 96.3</cell><cell>99.5</cell></row><row><cell>iron</cell><cell>47.5</cell><cell>92.7</cell><cell>99.3</cell><cell>47.9</cell><cell>86.3</cell><cell>98.3 67.4 97.2</cell><cell>99.9</cell></row><row><cell>lamp</cell><cell>47.5</cell><cell>90.9</cell><cell>98.4</cell><cell>45.3</cell><cell>86.8</cell><cell>97.5 60.0 94.2</cell><cell>99.0</cell></row><row><cell>phone</cell><cell>34.8</cell><cell>89.6</cell><cell>98.6</cell><cell>22.7</cell><cell>60.5</cell><cell>87.7 75.9 97.7</cell><cell>99.8</cell></row><row><cell>MEAN</cell><cell>39.0</cell><cell>85.2</cell><cell>97.9</cell><cell>30.9</cell><cell>69.2</cell><cell>88.6 75.6 97.5</cell><cell>99.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Results on the Occlusion LINEMOD dataset.The network is trained and tested with 4 iterations.</figDesc><table><row><cell>metric</cell><cell cols="2">(5 • , 5cm)</cell><cell cols="2">6D Pose</cell><cell cols="2">Projection 2D</cell></row><row><cell cols="7">method Init. Refined Init. Refined Init. Refined</cell></row><row><cell>ape</cell><cell>2.3</cell><cell>51.8</cell><cell>9.9</cell><cell>59.2</cell><cell>34.6</cell><cell>69.0</cell></row><row><cell>can</cell><cell>4.1</cell><cell>35.8</cell><cell>45.5</cell><cell>63.5</cell><cell>15.1</cell><cell>56.1</cell></row><row><cell>cat</cell><cell>0.3</cell><cell>12.8</cell><cell>0.8</cell><cell>26.2</cell><cell>10.4</cell><cell>50.9</cell></row><row><cell>driller</cell><cell>2.5</cell><cell>45.2</cell><cell>41.6</cell><cell>55.6</cell><cell>7.4</cell><cell>52.9</cell></row><row><cell>duck</cell><cell>1.8</cell><cell>22.5</cell><cell>19.5</cell><cell>52.4</cell><cell>31.8</cell><cell>60.5</cell></row><row><cell>eggbox</cell><cell>0.0</cell><cell>17.8</cell><cell>24.5</cell><cell>63.0</cell><cell>1.9</cell><cell>49.2</cell></row><row><cell>glue</cell><cell>0.9</cell><cell>42.7</cell><cell>46.2</cell><cell>71.7</cell><cell>13.8</cell><cell>52.9</cell></row><row><cell>hole.</cell><cell>1.7</cell><cell>18.8</cell><cell>27.0</cell><cell>52.5</cell><cell>23.1</cell><cell>61.2</cell></row><row><cell cols="2">MEAN 1.7</cell><cell>30.9</cell><cell>26.9</cell><cell>55.5</cell><cell>17.2</cell><cell>56.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Overall results on YCB video results compared with PoseCNN and PoseRBPF<ref type="bibr" target="#b9">(Deng et al., 2019)</ref>. The ADD(-S) metric and AUC ADD(-S) metric is introduced in Sec. 4.6</figDesc><table><row><cell></cell><cell></cell><cell>RGB</cell><cell></cell><cell></cell><cell></cell><cell>RGB-D</cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">PoseCNN PoseRBPF++</cell><cell>PoseCNN +DeepIM</cell><cell>DeepIM +Tracking</cell><cell>PoseCNN +ICP</cell><cell>PoseRBPF</cell><cell>PoseCNN +DeepIM</cell></row><row><cell>ADD(-S)&lt;2cm</cell><cell>27.55</cell><cell>-</cell><cell>71.5</cell><cell>79.0</cell><cell>78.9</cell><cell>-</cell><cell>90.3</cell></row><row><cell>AUC of ADD(-S)</cell><cell>61.31</cell><cell>64.4</cell><cell>81.9</cell><cell>85.9</cell><cell>86.6</cell><cell>88.5</cell><cell>90.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Detailed Results on the YCB-Video dataset compared with PoseCNN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Results on unseen objects. These models are not included in the training set.</figDesc><table><row><cell>category</cell><cell cols="2">airplane</cell><cell></cell><cell>car</cell><cell></cell><cell>chair</cell></row><row><cell cols="7">method Init. Refined Init. Refined Init. Refined</cell></row><row><cell>5cm 5 •</cell><cell>0.8</cell><cell>68.9</cell><cell>1.0</cell><cell>81.5</cell><cell>1.0</cell><cell>87.6</cell></row><row><cell cols="2">6D Pose 25.7</cell><cell>94.7</cell><cell>10.8</cell><cell>90.7</cell><cell>14.6</cell><cell>97.4</cell></row><row><cell cols="2">Proj. 2D 0.4</cell><cell>87.3</cell><cell>0.2</cell><cell>83.9</cell><cell>1.5</cell><cell>88.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Results on unseen categories. These categories has never been seen by the network during training.</figDesc><table><row><cell>metric</cell><cell cols="2">(5 • , 5cm)</cell><cell cols="2">6D Pose</cell><cell cols="2">Projection 2D</cell></row><row><cell>method</cell><cell cols="6">Init. Refined Init. Refined Init. Refined</cell></row><row><cell>bathtub</cell><cell>0.9</cell><cell>71.6</cell><cell>11.9</cell><cell>88.6</cell><cell>0.2</cell><cell>73.4</cell></row><row><cell>bookshelf</cell><cell>1.2</cell><cell>39.2</cell><cell>9.2</cell><cell>76.4</cell><cell>0.1</cell><cell>51.3</cell></row><row><cell>guitar</cell><cell>1.2</cell><cell>50.4</cell><cell>9.6</cell><cell>69.6</cell><cell>0.2</cell><cell>77.1</cell></row><row><cell cols="2">range hood 1.0</cell><cell>69.8</cell><cell>11.2</cell><cell>89.6</cell><cell>0.0</cell><cell>70.6</cell></row><row><cell>sofa</cell><cell>1.2</cell><cell>82.7</cell><cell>9.0</cell><cell>89.5</cell><cell>0.1</cell><cell>94.2</cell></row><row><cell>wardrobe</cell><cell>1.4</cell><cell>62.7</cell><cell>12.5</cell><cell>79.4</cell><cell>0.2</cell><cell>70.0</cell></row><row><cell>tv stand</cell><cell>1.2</cell><cell>73.6</cell><cell>8.8</cell><cell>92.1</cell><cell>0.2</cell><cell>76.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Lirui Wang at University of Washington for his contribution in this probject. This work was funded in part by a Siemens grant. We would also like to thank NVIDIA for generously providing the DGX station used for this research via the NVIDIA Robotics Lab and the UW NVIDIA AI Lab (NVAIL </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speeded-up robust features (surf)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor Fusion IV: Control Paradigms and Data Structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page" from="586" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6D object pose estimation using 3D object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6D pose estimation of objects and scenes from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3364" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Robotics (ICAR), 2015 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The MOPED framework: Object recognition and pose estimation for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LS-VO: Learning dense optical subspace for robust visual odometry estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Costante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Ciarfuglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Poserbpf: A rao-blackwellized particle filter for 6d object pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bretl</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Journal of Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep 6-dof tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2410" to="2418" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time high resolution 3d data on the hololens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Boulet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Doironz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative mixture-oftemplates for viewpoint classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="408" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient response maps for real-time detection of textureless objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="876" to="888" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going further with point pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">T-less: An rgb-d dataset for 6d pose estimation of texture-less objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obdržálekš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="880" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="433" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real time 3d template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="pp I" to="I" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SSD-6D: Making rgb-based 3D detection and 6D pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geometric loss functions for camera pose regression with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning analysis-bysynthesis for 6D pose estimation in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="954" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inverse compositional spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2568" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast directional chamfer matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1696" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object recognition from local scaleinvariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="800" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Super 4pcs fast global pointcloud registration via smart indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="205" to="215" />
			<date type="published" when="2014" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Threedimensional model-based object recognition and segmentation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1584" to="1601" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global hypothesis generation for 6D object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5632" to="5640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Preemptive ransac for live structure and motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="321" to="329" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rothganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="259" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient variants of the icp algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Third International Conference on</title>
		<meeting>Third International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
	<note>3-D Digital Imaging and Modeling</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA), Citeseer</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A review of recent range image registration methods with accuracy evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matabosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fofi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="578" to="596" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring convolutional networks for end-toend visual servoing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3817" to="3823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv:14091556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Registration of 3d point clouds and meshes: a survey from rigid to nonrigid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Langbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1199" to="1217" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Real-time seamless single shot 6D object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno>arXiv:171108848</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Globally consistent registration of terrestrial laser scans via graph optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Theiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="126" to="138" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Real-time monocular pose estimation of 3D objects using temporally consistent local color histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tjaden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schwanecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schömer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="124" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="356" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>arXiv:190104780</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2043" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
	<note>3D shapenets: A deep representation for volumetric shapes</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">PoseCNN: A convolutional neural network for 6D object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno>arXiv:160503344</idno>
		<title level="m">Go-icp: a globally optimal solution to 3d icp point-set registration</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-view self-supervised deep learning for 6D pose estimation in the amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1386" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
