<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple and Effective Text Matching with Richer Alignment Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runqi</forename><surname>Yang</surname></persName>
							<email>runqiyang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Gao</surname></persName>
							<email>gaoxing.gx@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
							<email>haiqing.chenhq@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simple and Effective Text Matching with Richer Alignment Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a fast and strong neural approach for general purpose text matching applications. We explore what is sufficient to build a fast and well-performed text matching model and propose to keep three key features available for inter-sequence alignment: original point-wise features, previous aligned features, and contextual features while simplifying all the remaining components. We conduct experiments on four well-studied benchmark datasets across tasks of natural language inference, paraphrase identification and answer selection. The performance of our model is on par with the state-of-the-art on all datasets with much fewer parameters and the inference speed is at least 6 times faster compared with similarly performed ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text matching is a core research area in natural language processing with a long history. In text matching tasks, a model takes two text sequences as input and predicts a category or a scala value indicating their relationship. A wide range of tasks, including natural language inference (also known as recognizing textual entailment) <ref type="bibr" target="#b2">(Bowman et al., 2015;</ref><ref type="bibr" target="#b9">Khot et al., 2018)</ref>, paraphrase identification , answer selection <ref type="bibr" target="#b29">(Yang et al., 2015)</ref>, and so on, can be seen as specific forms of text matching problems. Research on general purpose text matching algorithm is beneficial to a large number of relevant applications.</p><p>Deep neural networks are the most popular choices for text matching nowadays. Semantic alignment and comparison of two text sequences are the keys in neural text matching. Many previous deep neural networks contain a single intersequence alignment layer. To make full use of this only alignment process, the model has to take rich external syntactic features or hand-designed align-ment features as additional inputs of the alignment layer <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr" target="#b6">Gong et al., 2018)</ref>, adopt a complicated alignment mechanism <ref type="bibr" target="#b21">Tan et al., 2018)</ref>, or build a vast amount of post-processing layers to analyze the alignment result <ref type="bibr" target="#b24">(Tay et al., 2018b;</ref><ref type="bibr" target="#b6">Gong et al., 2018)</ref>.</p><p>More powerful models can be built with multiple inter-sequence alignment layers. Instead of making a prediction based on the comparison result of a single alignment process, a stacked model with multiple alignment layers maintains its intermediate states and gradually refines its predictions. However, suffering from inefficient propagation of lower-level features and vanishing gradients, these deeper architectures are harder to train. Recent works have come up with ways of connecting stacked building blocks including dense connection <ref type="bibr" target="#b23">(Tay et al., 2018a;</ref><ref type="bibr" target="#b10">Kim et al., 2018)</ref> and recurrent neural networks <ref type="bibr" target="#b12">(Liu et al., 2018)</ref>, which strengthen the propagation of lower-level features and yield better results than those with a single alignment process. This paper presents RE2, a fast and strong neural architecture with multiple alignment processes for general purpose text matching. We question the necessity of many slow components in text matching approaches presented in previous literature, including complicated multi-way alignment mechanisms, heavy distillations of alignment results, external syntactic features, or dense connections to connect stacked blocks when the model is going deep. These design choices slow down the model by a large amount and can be replaced by much more lightweight and equally effective ones. Meanwhile, we highlight three key components for an efficient text matching model. These components, which the name RE2 stands for, are previous aligned features (Residual vectors), original point-wise features (Embedding vectors), and contextual features (Encoded vectors). The re-maining components can be as simple as possible to keep the model fast while still yielding strong performance.</p><p>The general architecture of RE2 is illustrated in <ref type="figure">Figure 1</ref>. An embedding layer first embeds discrete tokens. Several same-structured blocks consisting of encoding, alignment and fusion layers then process the sequences consecutively. These blocks are connected by an augmented version of residual connections (see section 2.1). A pooling layer aggregates sequential representations into vectors which are finally processed by a prediction layer to give the final prediction. The implementation of each layer is kept as simple as possible, and the whole model, as a well-organized combination, is quite powerful and lightweight at the same time.</p><p>Our proposed method achieves the performance on par with the state-of-the-art on four benchmark datasets across three different tasks, namely SNLI and SciTail for natural language inference, Quora Question Pairs for paraphrase identification, and WikiQA for answer selection. Furthermore, our model has the least number of parameters and the fastest inference speed in all similarlyperformed models. We also conduct an ablation study to compare with alternative implementations of most components, perform robustness checks to see whether the model is robust to changes of structural hyperparameters, explore what roles the three key features in RE2 play by comparing their occlusion sensitivity and show the evolution of alignment results by a case study. We release the source code 1 of our experiments for reproducibility and hope to facilitate future researches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>In this section, we introduce our proposed approach RE2 for text matching. <ref type="figure">Figure 1</ref> gives an illustration of the overall architecture. Two text sequences are processed symmetrically before the prediction layer, and all parameters except those in the prediction layer are shared between the two sequences. For conciseness, we omit the part for the other sequence in the figure.</p><p>In RE2, tokens in each sequence are first embedded by the embedding layer and then processed consecutively by N same-structured blocks with independent parameters (dashed boxes in <ref type="figure">Figure  Figure 1</ref>: An overview of RE2. There are three parts in the input of alignment and fusion layers: original pointwise features (Embedding vectors, denoted by blank rectangles), previous aligned features (Residual vectors, denoted by rectangles with diagonal stripes), and contextual features (Encoded vectors, denoted by solid rectangles). The architecture on the right is the same as the one on the left so it's omitted for conciseness. 1) connected by augmented residual connections. Inside each block, a sequence encoder first computes contextual features of the sequence (solid rectangles in <ref type="figure">Figure 1</ref>). The input and output of the encoder are concatenated and then fed into an alignment layer to model the alignment and interaction between the two sequences. A fusion layer fuses the input and output of the alignment layer. The output of the fusion layer is considered as the output of this block. The output of the last block is sent to the pooling layer and transformed into a fixed-length vector. The prediction layer takes the two vectors as input and predicts the final target. The cross entropy loss is optimized to train the model in classification tasks.</p><p>The implementation of each layer is kept as simple as possible. We use only word embeddings in the embedding layer, without character embeddings or syntactic features. Vanilla multi-layer convolutional networks with same padding <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>) are adopted as the encoder. Recurrent networks are slower and do not lead to further improvements, so they are not adopted here. A max-over-time pooling operation <ref type="bibr" target="#b4">(Collobert et al., 2011)</ref> is used in the pooling layer.</p><p>The details of augmented residual connections and other layers are introduced as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Augmented Residual Connections</head><p>To provide richer features for alignment processes, RE2 adopts an augmented version of residual connections to connect consecutive blocks. For a sequence of length l, We denote the input and output of the n-th block as</p><formula xml:id="formula_0">x (n) = (x (n) 1 , x (n) 2 , . . . , x (n) l ) and o (n) = (o (n) 1 , o (n) 2 , . . . , o (n)</formula><p>l ), respectively. Let o (0) be a sequence of zero vectors. The input of the first block x (1) , as mentioned before, is the output of the embedding layer (denoted by blank rectangles in <ref type="figure">Figure 1</ref>). The input of the n-th block x (n) (n ≥ 2), is the concatenation of the input of the first block x (1) and the summation of the output of previous two blocks (denoted by rectangles with diagonal stripes in <ref type="figure">Figure 1</ref>):</p><formula xml:id="formula_1">x (n) i = [x (1) i ; o (n−1) i + o (n−2) i ],<label>(1)</label></formula><p>where [; ] denotes the concatenation operation. With augmented residual connections, there are three parts in the input of alignment and fusion layers, namely original point-wise features kept untouched along the way (Embedding vectors), previous aligned features processed and refined by previous blocks (Residual vectors), and contextual features from the encoder layer (Encoded vectors). Each of these three parts plays a complementing role in the text matching process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Alignment Layer</head><p>A simple form of alignment based on the attention mechanism is used following <ref type="bibr" target="#b15">Parikh et al. (2016)</ref> with minor modifications. The alignment layer, as shown in <ref type="figure">Figure 1</ref>, takes features from the two sequences as input and computes the aligned representations as output. Input from the first sequence of length l a is denoted as a = (a 1 , a 2 , . . . , a la ) and input from the second sequence of length l b is denoted as</p><formula xml:id="formula_2">b = (b 1 , b 2 , . . . , b l b ).</formula><p>The similarity score e ij between a i and b j is computed as the dot product of the projected vectors:</p><formula xml:id="formula_3">e ij = F (a i ) T F (b j ).</formula><p>(2)</p><p>F is an identity function or a single-layer feedforward network. The choice is treated as a hyperparameter. The output vectors a and b are computed by weighted summation of representations of the other sequence. The summation is weighted by similarity scores between the current position and the corresponding positions in the other sequence:</p><formula xml:id="formula_4">a i = l b j=1 exp(e ij ) l b k=1 exp(e ik ) b j , b j = la i=1 exp(e ij ) la k=1 exp(e kj ) a i .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fusion Layer</head><p>The fusion layer compares local and aligned representations in three perspectives and then fuse them together. The output of the fusion layer for the first sequenceā is computed bȳ</p><formula xml:id="formula_5">a 1 i = G 1 ([a i ; a i ]), a 2 i = G 2 ([a i ; a i − a i ]), a 3 i = G 3 ([a i ; a i • a i ]), a i = G([ā 1 i ;ā 2 i ;ā 3 i ]),<label>(4)</label></formula><p>where G 1 , G 2 , G 3 , and G are single-layer feedforward networks with independent parameters and • denotes element-wise multiplication. The subtraction operator highlights the difference between the two vectors while the multiplication highlights similarity. Formulations forb are similar and omitted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Prediction Layer</head><p>The prediction layer takes the vector representations of the two sequences v 1 and v 2 from the pooling layers as input and predicts the final target following <ref type="bibr" target="#b14">Mou et al. (2016)</ref>:</p><formula xml:id="formula_6">y = H([v 1 ; v 2 ; v 1 − v 2 ; v 1 • v 2 ]).<label>(5)</label></formula><p>H is a multi-layer feed-forward neural network. In a classification task,ŷ ∈ R C represents the unnormalized predicted scores for all classes where C is the number of classes. The predicted class isŷ = argmax iŷi . In a regression task,ŷ is the predicted scala value. In symmetric tasks like paraphrase identification, a symmetric version of the prediction layer is used for better generalization:</p><formula xml:id="formula_7">y = H([v 1 ; v 2 ; |v 1 − v 2 |; v 1 • v 2 ]).<label>(6)</label></formula><p>We also provide a simplified version of the prediction layer. Which version to use is treated as a hyperparameter. The simplified prediction layer can be expressed as:</p><formula xml:id="formula_8">y = H([v 1 ; v 2 ]).<label>(7)</label></formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>In this section, we briefly introduce datasets used in the experiments and their evaluation metrics. SNLI (Bowman et al., 2015) (Stanford Natural Language Inference) is a benchmark dataset for natural language inference. In natural language inference tasks, the two input sentences are asymmetrical. The first one is called "premise" and the second is called "hypothesis". The dataset contains 570k human annotated sentence pairs from an image captioning corpus, with labels "entailment", "neutral", "contradiction" and "-". The "-" label indicates that the annotators cannot reach an agreement, so we ignore text pairs with this kind of labels in training and testing following Bowman et al. <ref type="bibr">(2015)</ref>. We use the same dataset split as in the original paper. Accuracy is used as the evaluation metric for this dataset.</p><p>SciTail <ref type="bibr" target="#b9">(Khot et al., 2018</ref>) (Science Entailment) is an entailment classification dataset constructed from science questions and answers. Since scientific facts cannot contradict with each other, this dataset contains only two types of labels, entailment and neutral. We use the original dataset partition. This dataset contains 27k examples in total. 10k examples are with entailment labels and the remaining 17k are labeled as neutral. Accuracy is used as the evaluation metric for this dataset.</p><p>Quora Question Pairs 2 is a dataset for paraphrase identification with two classes indicating whether one question is a paraphrase of the other. The dataset contains more than 400k real question pairs collected from Quora.com. We use the same dataset partition as mentioned in . Accuracy is used as the evaluation metric for this dataset.</p><p>WikiQA <ref type="bibr" target="#b29">(Yang et al., 2015)</ref> is a retrieval-based question answering dataset based on Wikipedia. It contains questions and their candidate answers, with binary labels indicating whether a candidate sentence is a correct answer to the question it belongs to. This dataset has 20.4k training pairs, 2.7k development pairs, and 6.2k testing pairs. Mean average precision (MAP) and mean reciprocal rank (MRR) are used as the evaluation metrics for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implement our model with TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and train on Nvidia P100 GPUs. We tokenize sentences with the NLTK toolkit <ref type="bibr" target="#b1">(Bird et al., 2009)</ref>, convert them to lower cases and remove all punctuations. We do not limit the maximum sequence length, and all sequences in a batch are padded to the batch-wise maximum. Word embeddings are initialized with 840B-300d GloVe word vectors <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref> and fixed during training. Embeddings of out-ofvocabulary words are initialized to zeros and fixed as well. All other parameters are initialized with He initialization <ref type="bibr" target="#b7">(He et al., 2015)</ref> and normalized by weight normalization <ref type="bibr" target="#b17">(Salimans and Kingma, 2016)</ref>. Dropout with a keep probability of 0.8 is applied before every fully-connected or convolutional layer. The kernel size of the convolutional encoder is set to 3. The prediction layer is a two-layer feed-forward network. The hidden size is set to 150 in all experiments. Activations in all feed-forward networks are GeLU activations <ref type="bibr" target="#b8">(Hendrycks and Gimpel, 2016)</ref>, and we use √ 2 as an approximation of the variance balancing parameter for GeLU activations in He initialization. We scale the summation in augmented residual connections by 1/ √ 2 when n ≥ 3 to preserve the variance under the assumption that the two addends have the same variance.</p><p>The number of blocks is tuned in a range from 1 to 3. The number of layers of the convolutional encoder is tuned from 1 to 3. Although in robustness checks <ref type="table" target="#tab_9">(Table 7)</ref> we validate with up to 5 blocks and layers, in all other experiments we deliberately limit the maximum number of blocks and number of layers to 3 to control the size of the model. We use the Adam optimizer (Kingma and Ba, 2015) and an exponentially decaying learning rate with a linear warmup. The initial learning rate is tuned from 0.0001 to 0.003. The batch size is tuned from 64 to 512. The threshold for gradient clipping is set to 5. For all the experiments except for the comparison of ensemble models, we report the average score and the standard deviation of 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on Natural Language Inference</head><p>Results on SNLI dataset are listed in <ref type="table">Table 1</ref>. We compare single models and ensemble models. For a fair comparison, we only compare with results obtained without external contextualized embed-  <ref type="bibr" target="#b3">(Chen et al., 2017)</ref> 4.3M 88.0 DIIN <ref type="bibr" target="#b6">(Gong et al., 2018)</ref> 4.4M 88.0 MwAN <ref type="bibr" target="#b21">(Tan et al., 2018)</ref> 14M 88.3 CAFE <ref type="bibr" target="#b24">(Tay et al., 2018b)</ref> 4.7M 88.5 HIM <ref type="bibr" target="#b3">(Chen et al., 2017)</ref> 7.7M 88.6 SAN <ref type="bibr" target="#b12">(Liu et al., 2018)</ref> 3.5M 88.6 CSRAN <ref type="bibr" target="#b23">(Tay et al., 2018a)</ref> 13.9M 88.7 DRCN <ref type="bibr" target="#b10">(Kim et al., 2018)</ref> 6.7M 88.9 RE2 <ref type="formula">(</ref> Model Acc(%) ESIM <ref type="bibr" target="#b3">(Chen et al., 2017)</ref> 70.6 DecompAtt <ref type="bibr" target="#b15">(Parikh et al., 2016)</ref> 72.3 DGEM <ref type="bibr" target="#b9">(Khot et al., 2018)</ref> 77.3 HCRN <ref type="bibr" target="#b25">(Tay et al., 2018c)</ref> 80.0 CAFE <ref type="bibr" target="#b24">(Tay et al., 2018b)</ref> 83.3 CSRAN <ref type="bibr" target="#b23">(Tay et al., 2018a)</ref> 86.7 RE2 (ours) 86.0±0.6 dings. In the ensemble experiment, we train 8 models with different random seeds and ensemble the results by a voting strategy.</p><p>Our method obtains a result on par with the state-of-the-art among single models and a highly competitive result among ensemble models, with only a few parameters. Compared to SAN, our model reduces 20% parameters while improves the performance by 0.3% in accuracy, which indicates that our proposed architecture is highly efficient.</p><p>Results on Scitail dataset are listed in <ref type="table" target="#tab_2">Table 2</ref>. The performance of our method is very close to state-of-the-art. This dataset is considered much more difficult with fewer training data available and generally low accuracy as a binary classification problem. The variance of the results is larger since the size of training and test set is only 4% and 20% compared to those of SNLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on Paraphrase Identification</head><p>Results on Quora dataset are listed in <ref type="table" target="#tab_3">Table 3</ref>. Since paraphrase identification is a symmetric task where two input sequences can be swapped with no effect to the label of the text pair, in hyperparameter tuning we validate between two symmet-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc.(%) BiMPM  88.2 pt-DecAttn-word (Tomar et al., 2017) 87.5 pt-DecAttn-char <ref type="bibr" target="#b26">(Tomar et al., 2017)</ref> 88.4 DIIN <ref type="bibr" target="#b6">(Gong et al., 2018)</ref> 89.1 MwAN <ref type="bibr" target="#b21">(Tan et al., 2018)</ref> 89.1 CSRAN <ref type="bibr" target="#b23">(Tay et al., 2018a)</ref> 89.2 SAN <ref type="bibr" target="#b12">(Liu et al., 2018)</ref> 89.4 RE2 (ours) 89.2±0.2  ) and use no additional data augmentation. The performance of RE2 is on par with the state-of-the-art on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results on Answer Selection</head><p>Results on WikiQA dataset are listed in <ref type="table" target="#tab_4">Table 4</ref>. Note that some of the previous methods round their reported results to three decimal points, but we choose to align with the original paper <ref type="bibr" target="#b29">(Yang et al., 2015)</ref> and round our results to four decimal points. In hyperparameter tuning, we choose the best hyperparameters including early stopping according to MRR on WikiQA development set. We obtain a result on par with the state-of-the-art reported on this dataset. It's worth mentioning that we still train our model by point-wise binary classification loss, unlike some of the previous methods (including HCRN) which are trained by the pairwise ranking loss. Our method can perform well in the answer selection task without any taskspecific modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Inference Time</head><p>To show the efficiency of our proposed model, we compare the inference time with some other models whose code is open-source. <ref type="table" target="#tab_5">Table 5</ref> shows the comparison results. All the compared models are implemented in TensorFlow in the original implementations. The † mark indicates that the model uses POS tags as external syntactic features and the computation time of POS tagging is not included. In our RE2 model, the number of en-Model time(s/batch) BiMPM  0.05 ± 0.00 CAFE † <ref type="bibr" target="#b24">(Tay et al., 2018b)</ref> 0.07 ± 0.01 DIIN † <ref type="bibr" target="#b6">(Gong et al., 2018)</ref> 0.85 ± 0.11 DIIN with EM feature † 1.79 ± 0.22 CSRAN † (Tay et al., 2018a) 0.28 ± 0.02 RE2 <ref type="table">(1 block)</ref> 0.03 ± 0.00 RE2 <ref type="table" target="#tab_2">(2 blocks)</ref> 0.04 ± 0.00 RE2 <ref type="table" target="#tab_3">(3 blocks)</ref> 0.05 ± 0.00 coder layers is set to 3, the largest possible number in all previously reported experiments. Besides, since all the reported results of our proposed method are obtained with no more than 3 blocks, we only measure the inference time of RE2 with 1-3 blocks. We train all the compared models using the official training code and commands released by the authors on Nvidia P100 GPUs and save model checkpoints to disk. After training, all the models are required to make predictions for a batch of 8 pairs of sentences on a MacBook Pro with Intel Core i7 CPUs. The lengths of these sentences are 20 and the maximum number of characters in a word is 12. The reported statistics are the average and the standard deviation of processing 100 batches. The comparison results in <ref type="table" target="#tab_5">Table 5</ref> show that our method has very high CPU inference speed, even with multiple stacked blocks. Compared with similarly performed methods, ours is 6 times faster than CSRAN and at least 17 times faster than DIIN. With the highly efficient design, our method can perform well without any strong but slow building blocks like recurrent neural networks, dense connections or any syntactic features. Compared with models of similar inference speed, BiMPM and CAFE, ours obtains much higher prediction scores according to <ref type="table" target="#tab_2">Table 1, Table 2, Table 3 and Table 4</ref>.</p><p>In summary, our proposed method achieves performance on par with the state-of-the-art on all four well-studied datasets across three different tasks with only a few parameters and fast inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Analysis</head><p>Ablation study. We present an ablation study of our model, comparing the original model with 6 ablation baselines: (1) "w/o enc-in": use directly  the output of the encoder as the input of the alignment and fusion layers like in most previous approaches without concatenating the encoder input;</p><p>(2) "residual conn.": use vanilla residual connections (x</p><formula xml:id="formula_9">(n) i = o (n−1) i + o (n−2) i</formula><p>) in place of the augmented version; (3) "simple fusion": use simplȳ</p><formula xml:id="formula_10">a i = G 1 ([a i ; a i ]) andb i = G 1 ([b i ; b i ])</formula><p>as the fusion layer; (4) "alignment alt.": use the alternative version of the alignment layer where F in Equation 2 is a single-layer feed-forward network or an identity function; (5) "prediction alt.": use the alternative version (Equation 5/6 or Equation 7) of the prediction layer; (6) parallel blocks: feed the embeddings directly to all the blocks and sum up their outputs as the input of the pooling layer instead of processing input sequences consecutively by each block. The last setting is designed to study whether the improvement is due to deeper architecture or just a larger amount of parameters.</p><p>The ablation study is conducted on the development set of SNLI, Quora, Scitail, and WikiQA. In WikiQA we choose MRR as the evaluation metric. Note that on SciTail, F in Equation 2 in alignment layers is an identity function while on all other datasets F is a single-layer feed-forward network. On WikiQA, the simplified version <ref type="formula" target="#formula_8">(Equation 7)</ref> is used as the prediction layer while on all other datasets the full version (Equation 5 or 6) is used. The reported results are the average of 10 runs and the standard deviations are omitted for clarity.</p><p>The result is shown in <ref type="table" target="#tab_7">Table 6</ref>. The first ablation baseline shows that without richer features as the alignment input, the performance on all datasets degrades significantly. This is the key component in the whole model. The results of the second baseline show that vanilla residual connections without direct access to the original pointwise features are not enough to model the relations in many text matching tasks. The simpler implementation of the fusion layer leads to evidently worse performance, indicating that the fu-   sion layer cannot be further simplified. On the other hand, the alignment layer and the prediction layer can be simplified on some of the datasets. In the last ablation study, we can see that parallel blocks perform worse than stacked blocks, which supports the preference for deeper models over wider ones.</p><p>Robustness checks. To check whether our proposed method is robust to different variants of structural hyperparameters, we experiment with (1) the number of blocks varying from 1 to 5 with the number of encoder layers set to 2; (2) the number of encoder layers varying from 1 to 5 with the number of blocks set to 2. Robustness checks are performed on the development set of SNLI, Quora and Scitail. The result is presented in <ref type="table" target="#tab_9">Table 7</ref>. We can see in the table that fewer blocks or layers may not be sufficient but adding more blocks or layers than necessary hardly harms the performance. On WikiQA dataset, our method does not seem to be robust to structural hyperparameter changes. <ref type="bibr" target="#b5">Crane (2018)</ref> mentions that on WikiQA dataset a neural matching model <ref type="bibr" target="#b18">(Severyn and Moschitti, 2015)</ref> trained with different random seeds can result in differences up to 0.08 in MAP and MRR. We leave the further investigation of the high variance on the WikiQA dataset for further work.</p><p>Occlusion sensitivity. To better understand what roles the three alignment features play, we perform an analysis of occlusion sensitivity similar to those in computer vision <ref type="bibr" target="#b32">(Zeiler and Fergus, 2014)</ref>. We use a three-block RE2 model to predict on SNLI dev set, mask one feature in one block to zeros at a time and report changes in accuracy of the three categories: entailment, neutral and contradiction. Occlusion sensitivity can help to reveal how much the model depends on each part when deciding on a specific category and we can make some speculations about how the model works based on the observations. <ref type="figure" target="#fig_0">Figure 2</ref> shows the result of occlusion sensitivity. Previous aligned features are absent in the first block and thus left blank.</p><p>The text matching process can be abstracted, with moderate simplifications, to three stages: aligning tokens between the two sequences, focusing on a subset of the aligned pairs, discerning the semantic relations between the attended pairs. Each of the three key features in RE2 has a closer connection with one of the stages.</p><p>As we can see in <ref type="figure" target="#fig_0">Figure 2a</ref>, contextual features, represented by the output of the encoder, are indispensable when predicting entailment. These features connect with the first stage of text matching. The sequence encoder, implemented by convolutional networks, models local and phrase-level semantics, which helps to build correct alignment for each position. For example, consider the pair "A red car is next to a green house" and "A red car is parked near a house". If the noun phrases in the two sentences are not correctly modeled by the contextual encoding and "green" is incorrectly aligned with another color word "red", the pair looks much less like entailment.</p><p>In <ref type="figure" target="#fig_0">Figure 2b</ref> and <ref type="figure" target="#fig_0">Figure 2c</ref>, we can see that lacking direct access of previous aligned features (residual vectors), especially in the final block, results in significant degradation when predicting neutral and contradiction. Previous aligned features are related to the second stage of focusing on a subset of the aligned pairs. Without correct focus, the model may ignore non-entailing pairs and attend to other trivially aligned and semantically matched pairs, which results in failure in predicting neutral and contradiction. The importance of each position can be distilled and stored in previous aligned features and helps the model to focus in latter blocks.</p><p>We can conclude from <ref type="figure" target="#fig_0">Figure 2b</ref>  Case study. We present a case study of our model to show how inter-sequence alignment results evolve in our stacked architecture. An example pair of sentences are chosen from the development set of the SNLI dataset. The premise is "A green bike is parked next to a door", and the hypothesis is "The bike is chained to the door". In the first block, the alignment results are almost word-or phrase-level. "parked next to" is associated mostly with "bike" and "door" since there is a weaker direct connection between "parked" and "chained". In the final block, the alignment results take consideration of the semantics and structures of the whole sentences. The word "parked" is strongly associated with "chained" and "next to" is aligned with "to the" following "chained". With correct alignment, the model is able to tell that although most parts in the premise entail the aligned parts in the hypothesis, "parked" does not entail "chained", so it correctly predicts that the relation between the two sentences is neutral. Our model keeps the lower-level alignment results as intermediate states and gradually refines them to higherlevel ones.  <ref type="figure">Figure 3</ref>: A case study of the natural language inference task. The premise is "A green bike is parked next to a door", and the hypothesis is "The bike is chained to the door".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Deep neural networks are dominant in the text matching area. Semantic alignment and comparison between two text sequences lie in the core of text matching. Early works explore encoding each sequence individually into a vector and then building a neural network classifier upon the two vectors. In this paradigm, recurrent <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>, recursive <ref type="bibr" target="#b20">(Tai et al., 2015)</ref> and convolutional <ref type="bibr" target="#b31">(Yu et al., 2014;</ref><ref type="bibr" target="#b22">Tan et al., 2016)</ref> networks are used as the sequence encoder. The encoding of one sequence is independent of the other in these models, making the final classifier hard to model complex relations.</p><p>Later works, therefore, adopt the matching aggregation framework to match two sequences at lower levels and aggregate the results based on the attention mechanism. DecompAtt <ref type="bibr" target="#b15">(Parikh et al., 2016)</ref> uses a simple form of attention for alignment and aggregate aligned representations with feed-forward networks. ESIM <ref type="bibr" target="#b3">(Chen et al., 2017)</ref> uses a similar attention mechanism but employs bidirectional LSTMs as encoders and aggregators.</p><p>Three major paradigms are adopted to further improve performance. First is to use richer syntactic or hand-designed features. HIM <ref type="bibr" target="#b3">(Chen et al., 2017)</ref> uses syntactic parse trees. POS tags are found in many previous works including <ref type="bibr" target="#b24">Tay et al. (2018b)</ref> and <ref type="bibr" target="#b6">Gong et al. (2018)</ref>. The exact match of lemmatized tokens is reported as a powerful binary feature in <ref type="bibr" target="#b6">Gong et al. (2018)</ref> and <ref type="bibr" target="#b10">Kim et al. (2018)</ref>. The second way is adding complexity to the alignment computation. BiMPM  utilizes an advanced multiperspective matching operation, and MwAN <ref type="bibr" target="#b21">(Tan et al., 2018)</ref> applies multiple heterogeneous attention functions to compute the alignment results.</p><p>The third way to enhance the model is building heavy post-processing layers for the alignment results. CAFE <ref type="bibr" target="#b24">(Tay et al., 2018b</ref>) extracts additional indicators from the alignment process using alignment factorization layers. DIIN <ref type="bibr" target="#b6">(Gong et al., 2018)</ref> adopts DenseNet as a deep convolutional feature extractor to distill information from the alignment results. More effective models can be built if intersequence matching is allowed to be performed more than once. CSRAN <ref type="bibr" target="#b23">(Tay et al., 2018a)</ref> performs multi-level attention refinement with dense connections among multiple levels. DRCN <ref type="bibr" target="#b10">(Kim et al., 2018)</ref> stacks encoding and alignment layers. It concatenates all previously aligned results and has to use an autoencoder to deal with exploding feature spaces. SAN <ref type="bibr" target="#b12">(Liu et al., 2018)</ref> utilizes recurrent networks to combine multiple alignment results. This paper also proposes a deep architecture based on a new way to connect consecutive blocks named augmented residual connections, to distill previous aligned information which serves as an important feature for text matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a highly efficient approach, RE2, for general purpose text matching. It achieves the performance on par with the state-of-the-art on four well-studied datasets across three different text matching tasks with only a small number of parameters and very high inference speed. It highlights three key features, namely previous aligned features, original point-wise features, and contextual features for inter-sequence alignment and simplifies most of the other components. Due to its fast speed and strong performance, the model is quite suitable for a wide range of related applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Occlusion sensitivity of different parts in the input of the alignment layers on SNLI dev set: original point-wise features (embed), aligned features (residual), and contextual features (enc-out).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and Figure 2c that when original point-wise features represented by embedding vectors are not directly accessible by alignment layers and fusion layers, the model is struggling to predict neutral and contradiction correctly. Original point-wise features connect with the final stage where semantic differences between aligned pairs are compared. Intact point-wise representations of the aligned pairs facilitate the model in the comparison of their semantic differences, which plays a vital role in predicting neutral and contradiction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3 shows the visualization of the attention distribution (normalized e ij in Equation 3) in alignment layers of the first and the last blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Alignment results in the first block (b) Alignment results in the third block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Experimental results on SciTail test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on Quora test set.</figDesc><table><row><cell>Model</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell>ABCNN (Yin et al., 2016)</cell><cell>0.6921</cell><cell>0.7108</cell></row><row><cell cols="2">KVMN (Miller et al., 2016) 0.7069</cell><cell>0.7265</cell></row><row><cell cols="2">BiMPM (Wang et al., 2017) 0.718</cell><cell>0.731</cell></row><row><cell>IWAN (Shen et al., 2017)</cell><cell>0.733</cell><cell>0.750</cell></row><row><cell cols="2">CA (Wang and Jiang, 2017) 0.7433</cell><cell>0.7545</cell></row><row><cell>HCRN (Tay et al., 2018c)</cell><cell>0.743</cell><cell>0.756</cell></row><row><cell>RE2 (ours)</cell><cell>0.7452</cell><cell>0.7618</cell></row><row><cell></cell><cell cols="2">±0.0044 ±0.0040</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on WikiQA test set.</figDesc><table><row><cell>ric versions of the prediction layer (Equation 6 and</cell></row><row><cell>Equation 7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Inference time when batch size = 8 on Intel Core i7 CPUs. Models with † marks use POS tags as external syntactic features and the computation time of POS tagging is not included.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on dev sets of the corresponding datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Robustness checks on dev sets of the corresponding datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hitvoice/RE2, under the Apache License 2.0.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ten-sorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<meeting><address><addrLine>Savannah, GA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural language processing with Python: Analyzing text with the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Questionable answers in question answering research: Reproducibility and variability of published results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Crane</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00018</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="241" to="252" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bridging nonlinearities and stochastic regularizers with Gaussian error linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SciTail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic sentence matching with densely-connected recurrent and co-attentive information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hyuk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11360</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Version 2</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic answer networks for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07888</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
	<note>Amir-Hossein Karimi, Antoine Bordes, and Jason Weston</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2022</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany. As</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1244</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767738</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inter-weighted alignment network for sentence pair modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gehui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1179" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiway attention networks for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/613</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4411" to="4417" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved representation learning for question answer matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Co-stack residual affinity networks with multi-level attention refinement for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4492" to="4502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compare, compress and propagate: Enhancing neural architectures with alignment factorization for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1565" to="1575" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hermitian co-attention networks for text matching in asymmetrical domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/615</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4425" to="4431" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural paraphrase identification of questions with noisy pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thyago</forename><surname>Gaurav Singh Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Duque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4121</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Subword and Character Level Models in NLP</title>
		<meeting>the First Workshop on Subword and Character Level Models in NLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/579</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">WikiQA: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1237</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ABCNN: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00097</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<meeting><address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
