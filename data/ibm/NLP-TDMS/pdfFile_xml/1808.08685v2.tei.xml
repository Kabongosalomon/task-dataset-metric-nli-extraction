<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HMS-Net: Hierarchical Multi-scale Sparsity-invariant Network for Sparse Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015">AUGUST 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">HMS-Net: Hierarchical Multi-scale Sparsity-invariant Network for Sparse Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="2015">AUGUST 2015</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense depth cues are important and have wide applications in various computer vision tasks. In autonomous driving, LIDAR sensors are adopted to acquire depth measurements around the vehicle to perceive the surrounding environments. However, depth maps obtained by LIDAR are generally sparse because of its hardware limitation. The task of depth completion attracts increasing attention, which aims at generating a dense depth map from an input sparse depth map. To effectively utilize multi-scale features, we propose three novel sparsity-invariant operations, based on which, a sparsity-invariant multi-scale encoder-decoder network (HMS-Net) for handling sparse inputs and sparse feature maps is also proposed. Additional RGB features could be incorporated to further improve the depth completion performance. Our extensive experiments and component analysis on two public benchmarks, KITTI depth completion benchmark and NYU-depth-v2 dataset, demonstrate the effectiveness of the proposed approach. As of Aug. 12th, 2018, on KITTI depth completion leaderboard, our proposed model without RGB guidance ranks 1st among all peer-reviewed methods without using RGB information, and our model with RGB guidance ranks 2nd among all RGB-guided methods.</p><p>Index Terms-depth completion, convolutional neural network, sparsity-invariant operations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D PETH completion, aiming at generating a dense depth map from the input sparse depth map, is an important task for computer vision and robotics. In <ref type="figure" target="#fig_3">Fig. 2 (a)</ref>, (b), (e), we show one example input sparse depth map, its corresponding RGB image, and the depth completion result by our proposed method. Because of the limitation of current LIDAR sensors, the inputs of depth completion are generally sparse. For instance, the $100,000 Velodyne HDL-64E has only a vertical resolution of ∼ 0.4 • and an azimuth angular resolution of 0.08 • . It generates sparse depth maps, which might be insufficient for many real-world applications. Depth completion algorithms could estimate dense depth maps from sparse inputs and has great pontential in practice. With an accurate depth completion algorithm, many high-level vision tasks, such as semantic segmentation, 3D object detection, visual odometry and SLAM with 3D point clouds, could be solved more effectively. Therefore, it becomes a hot research topic for self-driving cars and UAVs, and is listed as one of the ranked tasks in the KITTI benchmark <ref type="bibr" target="#b0">[1]</ref>.</p><p>Many different methods have been proposed for depth completion, which could be generally categorized into learning-based <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and non-learning-based methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Non-learning-based approaches generate Engineering at Chinese University of Hong Kong, Hong Kong, China. • The first two authors contribute equally to the paper. They finished the work while they were at SenseTime Research. • E-mail: {hsli, xgwang}ee.cuhk.edu.hk dense depth maps from sparse inputs based on handcrafted rules. Therefore, the outputs of these algorithms are generated based on assumed prior by humans. As a result, they are not robust enough to sensor noises and are usually specifically designed for certain datasets. In addition, most non-learning-based methods ignore the correlations among sparse input depth points and might result in inaccurate object boundaries. An example of errors by a non-learningbased method <ref type="bibr" target="#b4">[5]</ref> is shown in <ref type="figure" target="#fig_3">Fig. 2(e)</ref>. The noises in the white box are not removed at all, and boundaries of the cars and trees in the yellow box are inaccurate.</p><p>For learning-based approaches, state-of-the-art methods are mainly based on deep neural networks. Previous methods mainly utilized deep convolutional neural networks (CNN) for generating dense depth maps from sparse inputs. Ma and Karaman <ref type="bibr" target="#b2">[3]</ref> simply filled 0s to locations without depth inputs to create dense input maps, which might introduce ambiguity to very small depth values. Chodosh et al. <ref type="bibr" target="#b3">[4]</ref> proposed to extract multi-level sparse codes from the inputs and used a 3-layer CNN for depth completion. However, those two methods used conventional convolution operations designed for dense inputs (see <ref type="figure" target="#fig_3">Fig. 2</ref>(c) for an example). Uhrig et al. <ref type="bibr" target="#b0">[1]</ref> proposed sparsity-invariant convolution, which is specifically designed to process sparse maps and enables processing sparse inputs more effectively with CNN.</p><p>However, the sparsity-invariant convolution in <ref type="bibr" target="#b0">[1]</ref> only mimics the behavior of convolution operations in conventional dense CNNs. Its feature maps of later stages lose much spatial information and therefore cannot effectively integrate both low-level and high-level features for accurate depth completion (see <ref type="figure" target="#fig_2">Fig. 1(a)</ref>   work structures for dense pixel-wise classification tasks (see <ref type="figure" target="#fig_2">Fig. 1(b)</ref>), such as U-Net <ref type="bibr" target="#b7">[8]</ref>, Feature Pyramid Network <ref type="bibr" target="#b8">[9]</ref>, Full Resolution Residual Network <ref type="bibr" target="#b9">[10]</ref>. Direct integration of the sparsity-invariant convolution in <ref type="bibr" target="#b0">[1]</ref> into the multi-scale structures is infeasible, as those structures also require other operations for multi-scale feature fusion, such as sparsityinvariant feature upsampling, average, and concatenation.</p><p>To overcome such limitation, we propose three novel sparsity-invariant operations to enable using encoderdecoder networks for depth completion. The three novel operations include sparsity-invariant upsampling, sparsityinvariant average, and joint sparsity-invariant concatenation and convolution. To effectively and efficiently handle sparse feature maps, sparsity masks are utilized at all locations of the feature maps. They record the locations of the sparse features at the output of each processing stage and guide the calculation of the forward and backward propagation. Each sparsity-invariant operation is designed to properly maintain and modify the sparisity masks across the network. The design of those operations are non-trivial and are the keys to using encoder-decoder structures with sparse features. Based on such operations, we propose a multiscale encoder-decoder network, HMS-Net, which adopts a series of sparsity-invariant convlutions with downsampling and upsampling to generate multi-scale feature maps and shortcut paths for effectively fusing multi-scale features. Extensive experiments on KITTI <ref type="bibr" target="#b0">[1]</ref> and NYU-depth-v2 <ref type="bibr" target="#b10">[11]</ref> datasets show that our algorithm achieves state-of-the-art depth completion accuracy.</p><p>The main contributions of our work can be summarized to threefold. 1) We design three sparsity-invariant operations for handling sparse inputs and feature maps, which are important to handle sparse feature maps. 2) Based on the proposed sparsity-invariant operations, a novel hierarchical multi-scale network structure fusing information from different scales is designed to solve the depth completion task. 3) Our method outperforms state-of-the-art methods in depth completion. On KITTI depth completion benchmark, our method without RGB information ranks 1st among all peer-reviewed methods with RGB inputs, while our method with RGB guidance ranks 2nd among all RGB-guided methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Depth completion</head><p>Depth completion is an active research area with a large number of applications. According to the sparsity of the inputs, current methods could be divided into two categories: sparse depth completion and depth enhancement. The former methods aim at recovering dense depth maps from spatially sparse inputs, while the later methods work on conventional RGB-D depth data and focus on filling irregular and relatively small holes in input dense depth maps. Besides, if the input depth maps are regularly sampled, the depth completion task could be regarded as a depth upsampling (also known as depth super-resolution) task. In other words, depth upsampling algorithms handle a special subset of the depth completion task. The inputs of depth upsampling are depth maps of lower resolution. According to whether RGB information is utilized, depth upsampling methods could be divided into two categories: guided depth upsampling and depth upsampling without guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Sparse depth completion</head><p>This type of methods take sparse depth maps as inputs. To handle sparse inputs and sparse intermediate feature maps, Uhrig et al. <ref type="bibr" target="#b0">[1]</ref> proposed sparsity-invariant convolution to replace the conventional convolution in convolution neural networks (CNN). The converted sparsity-invariant CNN keeps track of sparsity masks at each layer and is able to estimate dense depth maps from sparse inputs. Ku et al. <ref type="bibr" target="#b4">[5]</ref> proposed to use a series of hand-crafted image processing algorithms to transform the sparse depth inputs into dense depth maps. The proposed framework first utilized conventional morphological operations, such as dilation and closure, to make the input depth maps denser. It then filled holes in the intermediate denser depth maps to obtain the final outputs. Eldesokey et al. <ref type="bibr" target="#b11">[12]</ref> proposed an algebraically-constrained normalized convolution operation for handling sparse data and propagate depth confidence across layers. The regression loss and depth confidence are jointly optimized. Ren et al. <ref type="bibr" target="#b12">[13]</ref> focused on another task, the efficiency of convolution, but also have the design of sparsity masks. However, their algorithm could not be used in the scenario where the sparsity of the input varies in a large range. The above mentioned methods did not consider image information captured by the calibrated RGB cameras.</p><p>There also exist works utilizing RGB images as additional information to achieve better depth completion. Schneider et al. <ref type="bibr" target="#b13">[14]</ref> combined both intensity cues and object boundary cues to complete sparse depth maps. Liao et al. <ref type="bibr" target="#b1">[2]</ref> utilized a residual neural network and combine the classification and regression losses for depth estimation with both RGB and sparse depth maps as inputs. Ma and Karaman <ref type="bibr" target="#b2">[3]</ref> proposed to use a single deep regression network to learn directly from the RGB-D raw data, where the depth channel only has sparse depth values. However, the proposed algorithm mainly focused on the indoor scenes and was only tested on the indoor NYUv2 dataset. Instead, Van Gansbeke et al. <ref type="bibr" target="#b14">[15]</ref> combine RGB and depth information through summation according to two predicted confidence maps. Chodosh et al. <ref type="bibr" target="#b3">[4]</ref> utilized compressed sensing techniques and Alternating Direction Neural Networks to create a deep recurrent auto-encoder for depth completion. Sparse codes were extracted at the outputs of multiple levels of the CNN and were used to generate dense depth prediction. Zhang and Funkhouser <ref type="bibr" target="#b15">[16]</ref> adopted a neural network to predict dense surface normals and occlusion boundaries from RGB images. These predictions were then used as auxiliary information to accomplish the depth completion task from sparse depth data. Qiu et al. <ref type="bibr" target="#b16">[17]</ref> further extended this idea on outdoor datasets by generating surface normals as intermediate representation. Jaritz et al. <ref type="bibr" target="#b17">[18]</ref> argued that by using networks with large receptive field, the networks were not required to have special treatment for sparse data. They instead trained networks with depth maps of different sparsities. Cheng et al. <ref type="bibr" target="#b18">[19]</ref> proposed to learn robust affinities between pixels to spatially propagate depth values via convolution operations to fulfill the entire depth map, while Eldesokey el al. <ref type="bibr" target="#b19">[20]</ref> used continuous confidence instead of validity maps and algebraically constrained filters to tackle the sparsity-invariance problem and also guidance from both RGB images and the output confidence produced by their unguided network. Yang et al. <ref type="bibr" target="#b20">[21]</ref> proposed to yield the full posterior over depth maps with a Conditional Prior Network from their previous work <ref type="bibr" target="#b21">[22]</ref>. And a selfsupervised training framework was designed by Ma et al. <ref type="bibr" target="#b22">[23]</ref>, which explores temporal relations of video sequences to provide additional photometric supervisions for depth completion networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Depth enhancement</head><p>The inputs of depth enhancement or depth hole-filling methods are usually dense depth maps with irregular and rare small holes. The input depth maps are usually captured with RGB images. Matyunin et al. <ref type="bibr" target="#b23">[24]</ref> used the depth from the neighborhoods of the hole regions to fill the holes, according to the similarity of RGB pixels. In <ref type="bibr" target="#b24">[25]</ref>, the missing depth values are obtained by iteratively applying a joint-bilateral filter to the hole regions' neighboring pixels. Yang et al. <ref type="bibr" target="#b25">[26]</ref> propose an efficient depth image recovery algorithm based on auto-regressive correlations and recover high-quality multi-view depth frames with it. They also utilized color image, depth maps from neighboring views and temporal adjacency to help the recovery. Chen et al. <ref type="bibr" target="#b26">[27]</ref> firstly created smooth regions around the pixels without depth, and then adopted a bilateral filter without smooth region constraints to fill the depth values. Yang et al. <ref type="bibr" target="#b27">[28]</ref> proposed an adaptive color-guided autoregressive model for depth enhancement, which utilized local correlation in the initial depth map and non-local similarity in the corresponding RGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Guided depth upsampling</head><p>Depth upsampling methods take low-resolution depth maps as inputs and output high-resolution ones. As the guidance signals, the provided RGB images bring valuable information (e.g., edges) for upsampling. Li et al. <ref type="bibr" target="#b28">[29]</ref> proposed a CNN to extract features from the low-resolution depth map and the guidance image to merge their information for estimating the upsampled depth map. In <ref type="bibr" target="#b29">[30]</ref>, an anisotropic diffusion tensor is calculated from a highresolution intensity image to serve as the guidance. An energy function is designed and solved to solve the depth upsampling problem. Hui et al. <ref type="bibr" target="#b30">[31]</ref> proposed a convolution neural network, which fused the RGB guidance signals at different stages. They trained the neural network in the high-frequency domain. Xie et al. <ref type="bibr" target="#b31">[32]</ref> upsampled the lowresolution depth maps with the guidance of image edge maps and a Markov Random Field model. Jiang et al. <ref type="bibr" target="#b32">[33]</ref> proposed a depth super-resolution algorithm utilizing transform domain regularization with an auto-regressive model, as well as a spatial domain regularization by injecting a multi-directional total variation prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Depth upsampling without guidance</head><p>Depth upsampling could also be achieved without the assistance of corresponding RGB images. As an MRF based method, the approach in <ref type="bibr" target="#b33">[34]</ref> matched against the height field of each input low-resolution depth patch, and searched the database for a list of most appropriate high-resolution candidate patches. Selecting the correct candidates was then posed as a Markov Random Field labeling problem. Ferstl et al. <ref type="bibr" target="#b34">[35]</ref> learned a dictionary of edge priors from an external database of high-and low-resolution depth samples, and utilized a novel variational sparse coding approach for upsampling. Xie et al. <ref type="bibr" target="#b35">[36]</ref> used a coupled dictionary learning method with locality coordinate constraints to transform the original depth maps into high-resolution depth maps. Riegler et al. <ref type="bibr" target="#b36">[37]</ref> integrated a variational method that modeled the piecewise affine structures in depth maps on top of a deep network for depth upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-scale networks for pixelwise prediction</head><p>Neural networks that utilize multi-scale feature maps for pixelwise prediction (e.g., semantic segmentation) were widely investigated. Combining both low-level and highlevel features was proven to be crucial for making accurate pixelwise prediction. Ronneberger et al. <ref type="bibr" target="#b7">[8]</ref> proposed a U-shaped network (U-Net). It consists of an iterative downsampling image encoder that gradually summarized image information into smaller but deeper feature maps, and an iterative upsampling decoder that gradually combined low-level and highlevel features to output the pixelwise prediction maps. The low-level information from the encoder was passed to the high-level information of the decoder by direct concatenation of the feature maps of the same spatial sizes. Such a network has shown its great usefulness in many 2D and 3D segmentation tasks. Similar network structures, which include Hourglass <ref type="bibr" target="#b37">[38]</ref> and Feature Pyramid Network <ref type="bibr" target="#b8">[9]</ref>, have also been investigated to tackle pixelwise prediction tasks. Recently, Pohlen et al. <ref type="bibr" target="#b9">[10]</ref> proposed the full-resolution residual network (FRRN), which treated fullresolution information as a residual flow to pass valuable information across different scales for semantic segmentation. He et al. <ref type="bibr" target="#b38">[39]</ref> designed a fully fused network to utilize both images and focal length to learn the depth map.</p><p>However, even with the sparsity-invariant convolution operation proposed in <ref type="bibr" target="#b0">[1]</ref>, the multi-scale encoder-decoder networks cannot be directly converted to handle sparse inputs. This is because there exist many operations that do not support sparse feature maps. Our proposed sparsityinvariant operations solve the problem and allow encoderdecoder networks to be used for sparse data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We introduce our proposed framework for depth completion in this section. In Section 3.1, we first review sparsityinvariant convolution proposed in <ref type="bibr" target="#b0">[1]</ref>. In Section 3.2, we then introduce three novel sparsity-invariant operations, which are crucial for adopting multi-scale encoder-decoder networks to process sparse inputs. In Section 3.3, based on such sparsity-invariant operators, the hierarchical multiscale encoder-decoder network (HMS-Net) for effectively combining multi-scale features is proposed to tackle the depth completion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sparsity-invariant Convolution</head><p>In this subsection, we first review sparsity-invariant convolution in <ref type="bibr" target="#b0">[1]</ref>, which modifies conventional convolution to handle sparse input feature maps. The sparsity-invariant convolution is formulated as</p><formula xml:id="formula_0">z(u, v) = k i,j=−k m x (u + i, v + j)w(i, j)x(u + i, v + j) k i,j=−k m x (u + i, v + j) + + b.<label>(1)</label></formula><p>The sparisity-invariant convolution takes a sparse feature map x and a binary single-channel sparsity mask m as inputs, which both has the same spatial size H × W . The convolution generates output features z(u, v) for each location (u, v). At each spatial location (u, v), the binary sparsity mask m x (u, v) records whether there are input features at this location, i.e., 1 for features existence and 0 otherwise. The convolution kernel w is of size (2k + 1) × (2k + 1), and b represents a learnable bias vector. Note that the kernel weights w and bias vector b are learned via backpropagation, while the sparsity mask m x is specified by the previous layer and is trained.</p><p>The key difference with conventional convolution is the use of binary sparsity mask m x for convolution calculation. The mask values on numerator of Eq. (1) denote that when conducting convolution, only input features at the valid or visible locations specified by the sparsity mask m x are considered. The mask values in denominator denote that, since only a subset of input features are involved, the output features should be normalized according to the number of valid input locations. represents a very small number and is used to avoid division by 0.</p><p>Note that the sparsity mask should always indicate the validity or sparsity of each location of the feature maps. Since the convolution layers in a neural network is generally stacked for multiple times, the output sparsity mask m z at each stage should be modified to match the output of each stage z. For each output feature location (u, v), if there is at least one valid input location in its receptive field of the previous input, its sparsity mask m z (u, v) should be updated to 1. In practice, the output sparsity mask is obtained by conducting max pooling on the input sparsity mask with the same kernel size of convolution (2k + 1) × (2k + 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparsity-invariant operations</head><p>The sparsity-invariant convolution successfully converts conventional convolution to handle sparse input features and is able to stack multiple stages for learning highly non-linear functions. However, only modifying convolution operations is not enough if one tries to utilize stateof-the-art multi-scale encoder-decoder structure for pixelwise prediction. As shown in <ref type="figure" target="#fig_2">Fig. 1(b)</ref>, average, upsampling, and concatenation are also common operations in a multi-scale encoder-decoder networks. Therefore, we propose the sparsity-invariant version of these operations: sparsity-invariant average, sparsity-invariant upsampling, and joint sparsity-invariant concatenation and convolution. The three sparsity-invariant operations allow effectively handling sparse feature maps across the entire encoderdecoder network. They are the foundations of complex building blocks in our overall framework.</p><p>Designing the sparsity-invariant operations is nontrivial. Our proposed sparsity-invariant operations share the similar spirit of sparsity-invariant convolution: using singlechannel sparsity masks to track the validity of feature map locations. The sparsity masks could be used to guide and regularize the calculation of the operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Sparsity-invariant bilinear upsampling</head><p>One of the most important operations in encoder-decoder networks is the upsampling operation in the decoder part. We first propose the sparsity-invariant bilinear upsampling operation. Let x and m x denote the input sparse feature map and the corresponding input sparsity mask of size H × W . The operation generates the output feature map z and its corresponding sparsity mask m z of size 2H × 2W . Let F represents the conventional bilinear upsampling operator, which bilinearly upsamples the input feature map or mask by two times. The proposed sparsity-invariant bilinear upsampling can be formulated as</p><formula xml:id="formula_1">z = F (m x x) F (m x ) + ,<label>(2)</label></formula><formula xml:id="formula_2">m z = 1 [F (m x ) = 0] ,<label>(3)</label></formula><p>where denotes the spatial elementwise multiplication, is a very small number to avoid division by zero, and 1[·] denotes the indicator function, i.e., 1[true] = 1 and 1[false] = 0. The proposed sparsity-invariant bilinear upsampling operation is illustrated in <ref type="figure" target="#fig_4">Fig. 3</ref>.</p><p>As shown by Eq. <ref type="formula" target="#formula_1">(2)</ref>, the proposed operation first uses the input sparsity mask m x to mask out the invalid features from the input feature maps x as m x x. The traditional bilinear upsampling F operator is then applied to upsample both the masked feature maps m x x and the input sparsity mask m x . The upsampled sparse features F (m x x) are Note that for sparsity-invariant max-pooling or downsampling, it could be calculated the same as Eqs. <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> by replacing the upsampling function F with max-pooling or downsampling operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sparsity-invariant average</head><p>Pixelwise average of two feature maps of the same spatial sizes is needed for fusing features from different levels without increasing the output channels. For average of sparse input feature maps, however, specifically designed average operation is needed. We propose sparsity-invariant average, which takes two input sparse feature maps, x and y, with their corresponding sparsity masks, m x and m y , as inputs. It generates the fused sparse feature maps z with its corresponding sparsity mask m z . Unlike the sparsityinvariant upsampling or downsampling, the key difference is that the average operation takes two sparse features as inputs.</p><p>We formulate the sparsity-invariant average as</p><formula xml:id="formula_3">z = m x x + m y y m x + m y + ,<label>(4)</label></formula><formula xml:id="formula_4">m z = m x ∨ m y ,<label>(5)</label></formula><p>where ∨ denotes elementwise alternation (i.e., logical "or") function, represents elementwise multiplication, and is a very small number to avoid division by zero. The sparsity-invariant average is illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>. The two input sparse features are first masked out by their corresponding masks to obtain m x x and m y y. Both the masked features and the masks are then pixelwisely added. The added features are then normalized by added sparsity masks to calculate the output sparse features z. For the output sparsity mask, at each location, if the location is valid for either of the input feature maps, the mask is set to 1 for this location. At each location of the output feature map, the output feature vector is the mean of two input feature vectors if both input maps are valid at this location. If only one input feature is valid at a location, the valid single input feature vector is directly copied to the same location of the output feature maps. The two types of output feature vectors would have similar magnitude  because the added features are properly normalized by the added sparsity mask.</p><p>Note feature addition was also explored in <ref type="bibr" target="#b0">[1]</ref>. However, an output sparsity mask was not generated. Without such a mask, following convolution operations cannot handle sparse output features. Therefore, their operation could only be used as the last layer of a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Joint sparsity-invariant concatenation and convolution</head><p>Another commonly used approach of fusing two feature maps with the same spatial size is feature concatenation, i.e., concatenation along the channel dimension. However, different from aforementioned other operations, concatenation would introduce sparsity in both the spatial dimension (H × W) and the feature dimension (C), and the latter actually prevent us from simply extending the idea of sparsityinvariant average in the previous subsection. Let's consider the scenario where two feature maps with shape C 1 ×H ×W and C 2 × H × W are now being concatenated into one with shape (C 1 + C 2 ) × H × W as illustrated in <ref type="figure" target="#fig_6">Fig. 5</ref>. The concatenation is further followed by a convolution layer (1×1 for simplicity), which is a common operation in CNNs. However, we know that convolution performs filtering on a location by extracting one local feature vector with length C and summing all entries up into a number with learnable weights. Then, the convolution kernels iterate over the whole feature map, treating every location equally.</p><p>Recall the situation in sparsity-invariant convolution where the feature vector for a certain location only have two possible sparsity patterns-the whole vector of length C is valid, or all the entries of this vector are zeros. Note that the latter situation would not influence the training as its contribution to the output as well as the gradient of kernels is zero. Therefore, it is enough for us to use one set of convolution kernels, equally for every valid location.</p><p>However, when we are convolving on the feature maps after concatenation, we have four different types of vectors or sparsity patterns for each location: the first C 1 feature channels of the vector is valid while the latter C 2 feature channels are not; or C 2 is valid while C 1 is not, or both of them are valid/invalid. Therefore, we need three different sets of kernels to tackle these four different sparsity patterns. In other words, to effectively handle different scenarios at different locations of the concatenated feature maps, we propose to use an adaptive-kernel version convolution to solve the difficulty and combine it with concatenation together.</p><p>Another advantage of combining them is that all convolution kernels would generate outputs with the same spatial sparsity patterns. Therefore the output mask is still of single channel, which is computationally efficient and reduces the model complexity significantly. Specifically, our joint sparsity-invariant concatenation and convolution is described and explained formally as following:</p><p>Given the two input sparse feature maps x and y with their sparsity masks m x and m x , the proposed joint concatenation and convolution operation is formulated as</p><formula xml:id="formula_5">z = [x; y] * k a ,<label>(6)</label></formula><formula xml:id="formula_6">m z = m x ∨ m y ,<label>(7)</label></formula><p>where [; ] denotes the concatenation of two feature maps along the channel dimension, and * denotes the conventional convolution operation. Note that the output sparsity mask is calculated exactly the same as that in sparsityinvariant average. The key of the proposed operation is a 1 × 1 convolution with an adaptive convolution kernel k a that handles three different scenarios of concatenating sparse feature maps, which is formulated as</p><formula xml:id="formula_7">k a (u, v) =      k (1) a m x (u, v) = 1, m y (u, v) = 0; k (2) a m x (u, v) = 0, m y (u, v) = 1; k (3) a m x (u, v) = 1, m y (u, v) = 1,<label>(8)</label></formula><p>where k a (u, v) are the 1 × 1 adaptive convolution kernel at location (u, v) of the concatenated feature maps [x; y]. k (1) a k <ref type="bibr" target="#b1">(2)</ref> a , k (3) a are the three sets of learnable convolution weights for the three different feature concatenation scenarios: at each location (u, v), either both input feature vectors are valid (i.e., m x (u, v) = 1 and m y (u, v) = 1), or only one of the input feature vectors is valid (i.e., either m x (u, v) = 1 or m y (u, v) = 1). The key reason for using different sets of kernel weights instead of the same set of convolution weights, as we briefly introduced before, is to avoid involving invalid input features in the concatenated feature maps into feature learning process. Illustrating with our notation above, if the current 1 × 1 convolution kernel is on the location (u, v) and find that the first mask here m x (u, v) is one and the second mask m y (u, v) is zero, it would choose the first set of kernel weights k <ref type="bibr" target="#b0">(1)</ref> a to use during forward pass. So is the back propagation. In this case, we know the second chunk of the feature vector, of which the length is fixed, is always zero. And because it's consistently processed by the first   kernel, this kernel would naturally learn how to adapt to this pattern. In other words, by adopting the proposed adaptive convolution kernel k a , the three sets of kernel weights k <ref type="bibr" target="#b0">(1)</ref> a , k (2) a , k (3) a are able to handle different sparse feature concatenation scenarios. With joint training, the different kernels are learned to best adapt each other to generate appropriate feature representations for further processing. In this way, the sparse feature maps could be effectively fused with proposed concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Multi-scale Network (HMS-Net) for depth completion</head><p>Multi-scale encoder-decoder neural networks for dense inputs are widely investigated for pixelwise prediction tasks. Those networks have the advantages of fusing both lowlevel and high-level features for accurate pixel prediction. However, with only the sparsity-invariant convolution in <ref type="bibr" target="#b0">[1]</ref>, encoder-decoder networks cannot be converted to handle sparse inputs. On the other hand, for frequently studied pixelwise prediction tasks, such as semantic segmentation, global high-level information usually shows greater importance to the final performance, while the full-resolution low-level features are less informative and generally go through fewer non-linearity layers compared with highlevel features. However, we argue that depth completion is a low-level vision task. The low-level features in this task should be non-linearly transformed and fused with mid-level and high-level features for more times to achieve satisfactory depth completion accuracy.</p><p>Based on this motivation, we propose the Hierarchical Multi-scale encoder-decoder Network (HMS-Net) with our proposed sparsity-invariant operations for sparse depth completion. The network structure without RGB information is illustrated in <ref type="figure" target="#fig_9">Fig. 7(a)</ref>. We propose two basic building blocks, a two-scale block and a three-scale block, consisting proposed sparsity-invariant operations. The two-scale block has an upper path that non-linearly transforms the fullresolution low-level features by a k × k sparsity-invariant convolution. The lower path takes downsampled low-level features as inputs for learning higher-level features with another k × k convolution. We empirically set k=5 in all our experiments according to our hyperparameter study. The resulting higher-level features are then upsampled and added back to the full-resolution low-level features. Compared with the two-scale block, the three-scale block fuses features from two higher levels into the upper low-level feature path to utilize more auxiliary global information. In this way, the full-resolution low-level features are effectively fused with higher-level information and are non-linearly transformed multiple times to learn more complex prediction functions. All feature maps in our network are of 16 channels regardless of scales.</p><p>Our final network utilizes a 5 × 5 sparsity-invariant convolution at the first layer. The resulting features then go through three of the proposed multi-scale blocks followed by sparsity-invariant max-pooling, and are then upsampled three times to generate the full-resolution feature maps. The final feature maps are then transformed by one 1 × 1 convolution layers to generate the final per-pixel prediction. The output depth predictions are supervised by the Meansquare Error (MSE) loss function with ground-truth annotations.</p><p>Another way to understand the network structure we proposed is that our structure could be considered as a backbone encoder-decoder CNN shown in <ref type="figure" target="#fig_2">Fig. 1(b)</ref> but with special design for the depth completion task. By looking at the details in <ref type="figure" target="#fig_9">Fig. 7(a)</ref> and compare it with commonly used encoder-decoder networks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b37">[38]</ref>, it's easy to find that there are several key shortcuts (importance demonstrated later in the ablation study) between different flows uniquely in our network for a better fusion as we described before. Except for this aspect, the low-level features in the mentioned encoder-decoder networks go through very few nonlinear transformations, while our proposed network emphasizes much more on the low-level features. Furthermore, the total depth of our network is also much shallower. Compared with Full-Resolution Residual Network <ref type="bibr" target="#b9">[10]</ref> which also has multiple shortcuts, the latter's full-resolution lowlevel features only serve as residual signals. In addition, it does not consider the fusion of multiple-scale features at the same time as our three-scale block does. We further compared the proposed network with the commonly used encoder-decoder network structures in our experimental studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">RGB-guided multi-scale depth completion</head><p>The LIDAR sensors are usually paired with RGB cameras to obtain the aligned sparse depth maps and RGB images. RGB images could therefore act as auxiliary guidance for depth completion.</p><p>To integrate RGB features into our multi-scale encoderdecoder network, we added an RGB feature path to our proposed network. The network structure is illustrated in <ref type="figure" target="#fig_9">Fig. 7(b)</ref>. The input image is first processed by an RGB subnetwork to obtain mid-level RGB features. The structure of the sub-network follows the first six blocks of the ERFNet <ref type="bibr" target="#b39">[40]</ref>. It consists of two downsampling blocks and four residual blocks. The downsampling block has a 2×2 convolution layer with stride 2 and a 2 × 2 max-pooling layer. The input features are input into the two layers simultaneously, and their results are concatenated along the channel dimension to obtain the 1/2 size feature maps. The main path of the residual block has two sets of 1 × 3 conv → BN → ReLU </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training scheme</head><p>We adopt the mean squared error (MSE) loss function to train our proposed encoder-decoder networks. Since some datasets could only provide sparse ground-truth depth maps, the loss function is only evaluated at locations with ground-truth annotations, which could be formulated as</p><formula xml:id="formula_8">L(x, y) = 1 |V | u,v∈V |o(u, v) − t(u, v)| 2<label>(9)</label></formula><p>where V is the set containing coordinates with ground-truth depth values, |V | calculates the total number of valid points in V , and o and t are the predicted and ground-truth depth maps.</p><p>For network training, all network parameters except those of the RGB sub-network are randomly initilized. We adopt the ADAM optimizer <ref type="bibr" target="#b40">[41]</ref> with an initial learning rate of 0.01. The network is trained for 50 epochs. To gradually decrease the learning rate, it is decayed according to the following equation, learning rate = 0.01 × 1 − iter epoch 50</p><formula xml:id="formula_9">0.9 ,<label>(10)</label></formula><p>where iter epoch denotes the current epoch iteration. Also note that sparsity masks are generated for every input directly depending on the network structure, without any learnable parameters. The values of input masks are set to 1 for all valid spatial locations and 0 for invalid locations. The mask in one layer is propagated to the following layer. In other words, they purely depend on the network structure and the current input. During training, they filter out both invalid feature points and the gradient for invalid spatial locations.</p><p>For our RGB sub-network, we use the first six blocks of the ERFNet <ref type="bibr" target="#b39">[40]</ref>. Its initial network parameters are copied from the network pretrained on the CityScapes dataset <ref type="bibr" target="#b41">[42]</ref>. Both paths are then end-to-end trained until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conduct experiments on the KITTI depth completion dataset <ref type="bibr" target="#b0">[1]</ref> and NYU-depth-v2 dataset <ref type="bibr" target="#b10">[11]</ref> for evaluating the performance of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KITTI depth completion benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Data and evaluation metrics</head><p>We first evaluate our proposed approach on the KITTI depth completion benchmark <ref type="bibr" target="#b0">[1]</ref>. Following the experimental setup in <ref type="bibr" target="#b0">[1]</ref>, 85,898 depth maps are used for training, 1,000 for validation and 1,000 for test. The LIDAR depth maps are aligned with RGB images by projecting the depth map into the image coordinates according to the two sensors' essential matrix. The input depth maps generally contains &lt; 10% sparse points with depth values and the top 1/3 of the input maps do not contain any depth measurements. One example is shown in <ref type="figure" target="#fig_3">Fig. 2(a)</ref>.</p><p>According to the benchmark, all algorithms are evaluated according to the following metrics, root mean square error (RMSE in mm), mean absolute error (MAE in mm), root mean squared error of the inverse depth (iRMSE in 1/km), and mean absolute error of the inverse depth (iMAE in 1/km), i.e.,</p><formula xml:id="formula_10">RMSE =   1 |V | u,v∈V |o(u, v) − t(u, v)| 2   0.5 ,<label>(11)</label></formula><formula xml:id="formula_11">MAE = 1 |V | u,v∈V |o(u, v) − t(u, v)|,<label>(12)</label></formula><formula xml:id="formula_12">iRMSE =   1 |V | u,v∈V 1 o(u, v) − 1 t(u, v) 2   0.5 ,<label>(13)</label></formula><formula xml:id="formula_13">iMAE = 1 |V | u,v∈V 1 o(u, v) − 1 t(u, v) ,<label>(14)</label></formula><p>where o and t represent the output of our approach and ground-truth depth values. For RMSE and MAE, RMSE is more sensitive to large errors compared. This is because even a small number of large errors would be magnified by the square operation and dominate the overall loss value. RMSE is therefore chosen as the main metric for ranking different algorithms in the KITTI leaderboard. Since large depth values usually have greater errors and might dominate the calculation of RMSE and MAE, iRMSE and iMAE are also evaluated, which calculate the mean of inverse of depth errors. In this way, large depth values' errors would have much lower weights on the two metrics. The two metrics focus more on depth points near the LIDAR sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison with state-of-the-arts</head><p>The performance of our proposed approaches and state-ofthe-art depth completion methods are recorded in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>SparseConvs represents the 6-layer convolution neural network with only sparsity-invariant convolution proposed in <ref type="bibr" target="#b0">[1]</ref>. It only supports convolution and max-pooling operations and therefore loses much high-resolution information. IP-Basic represents the method in <ref type="bibr" target="#b4">[5]</ref>, a well-designed algorithm hand-crafted rules based on several traditional image processing algorithms. NConv-CNN <ref type="bibr" target="#b11">[12]</ref> proposed a constrained convolution layer and propagating confidence across layers for depth completion. Bilateral NN <ref type="bibr" target="#b6">[7]</ref> uses RGB images as guidance and integrate bilaterial filters into deep neural networks. It was modified to handle sparse depth completion following <ref type="bibr" target="#b0">[1]</ref>. Spade-sD and Spade-RGBsD <ref type="bibr" target="#b17">[18]</ref> do not have special treatment for sparse data. They utilize conventional dense CNN but adopt different loss function and training strategy. CSPN <ref type="bibr" target="#b18">[19]</ref> iteratively learns inter-pixel affinities with RGB guidance via recurrent convolution operations. The affinities could then be used to spatially propagate depth values between different locations. Sparse-to-dense(d) and Sparse-to-dense(gd) <ref type="bibr" target="#b22">[23]</ref> explore additional temporal information from sequential data to apply additional supervisions based on the photometric loss between neighboring frames.</p><p>For methods without RGB guidance, our proposed network without RGB guidance outperforms all other peerreviewed methods in terms of RMSE (the main ranking metric in KITTI leaderboard). Spade-sD has better MAE, iRMSE and iMAE, which mean that this method performs better on nearby objects but is more likely to generate large errors than our proposed method. Note that we utilize the L2 loss function to deliberately minimize RMSE. If other metrics are considered to be more important, different loss functions could be adopted for training. For methods with RGB guidance, our method ranks 2nd behind Sparse-to-dense(gd) <ref type="bibr" target="#b22">[23]</ref> in terms of RMSE. However, Sparse-to-dense(gd) utilized additional supervisions from temporal information, while our proposed method only uses supervisions from individual frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Ablation study</head><p>We investigate individual components in our framework to see whether they contribute to the final performance. The investigated components include, multi-scale structure, sparsity-invariant operations, and RGB guidance. We choose our full-resolution low-level feature path without the mid-level or high-level flow in our network (i.e., the upper path in <ref type="figure" target="#fig_9">Fig. 7(a)</ref>) as the baseline model for this section. The baseline model does not include RGB guidance. The analysis results on KITTI validation set are shown in <ref type="table" target="#tab_2">Table 2</ref>. And the baseline model without multi-scale feature fusion generates large depth estimation errors.</p><p>Multi-scale structure. Using our multi-scale encoderdecoder structure (denoted as Baseline+MS (Full)) in addition to the baseline model enables fusing low-level and high-level information from different scales. The multi-scale structure provides much larger receptive fields for neurons in the last convolution layer. Therefore, even if some regions in the input depth map are very sparse, the model could still predict depth values for every grid points. Using multi-scale features generally results in clearer boundaries and shows higher robustness to noises. Baseline+MS (Full) significantly decreases RMSE from 1819.81 to 1137.42. Also, our fusing skip connections connect different scales also enable a better fusion for the information in our network. By removing either the up fusing skip connection (shown by up arrows within the blocks in <ref type="figure" target="#fig_7">Fig. 6(a)</ref>) or the down fusing skip connection, our network performs worse as shown in <ref type="table" target="#tab_2">Table  2</ref> (Up only and Down only entries). Also, the mid-level flow is making the performance better (Mid-level flow removed entry). An example showing the differences between Base-line+MS (Full) and Baseline is in <ref type="figure">Fig. 8</ref>.</p><p>Sparsity-invariant operations. The Baseline+MS utilizes dense convolution. It has difficulty in handling sparse input data and sparse feature maps, especially for regions where there are very sparse points. The Baseline+MS+SO uses our proposed sparsity-invariant operations to maintain a correct mask flow and then converts conventional operations in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Sparse Depth Map Baseline</head><p>Corresponding Image Baseline + MS   <ref type="figure" target="#fig_2">Fig. 10</ref>, where the resulting depth maps with RGB guidance are much sharper at image boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Comparison with other encoder-decoder structures</head><p>To evaluate the effectiveness of our proposed HMS-Net structure, we conduct experiments to test other commonly used encoder-decoder network structures. We modify those structures with the sparsity-invariant convolution and our proposed sparsity-invariant operations to handle sparse inputs. The experimental results on the KITTI validation set are reported in <ref type="table" target="#tab_3">Table 3</ref>. We compared our network structure without RGB guidance with modified U-Net <ref type="bibr" target="#b7">[8]</ref>, FRRN <ref type="bibr" target="#b9">[10]</ref>, PSPNet <ref type="bibr" target="#b42">[43]</ref>, FPN <ref type="bibr" target="#b43">[44]</ref> and He et al. <ref type="bibr" target="#b38">[39]</ref> without focal length. Our proposed network structure achieves the lowest errors in terms of RMSE and MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Robustness testing on KITTI benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Robustness to depth noises</head><p>Since the sparse depth maps are obtained by LIDAR scans, inevitably, there would be noises in acquired depth values. As a result, the robustness of depth completion algorithms with regard to different noise levels is important in practice. We conduct experiments to test the robustness of our proposed model without RGB guidance and compare with SparseConvs <ref type="bibr" target="#b0">[1]</ref> and IP-Basic <ref type="bibr" target="#b4">[5]</ref>. Note that all models in this section are trained on original data and directly tested on noisy data.</p><p>Scene-level Gaussian noises. For this experiment, we add Gaussian noises on randomly selected 10% depth points among all points. Once the 10% points are selected, for a specific noise standard deviation level from 5-50 meters, we sample additive noise values from the zero-mean Gaussian distribution. The negative additive noises could simulate occlusion from raindrops, snowflakes or fog, while the positive additive noises mimic the laser going through glasses that mistakenly measures objects behind glasses. Noisy points whose depth values are smaller than 1 meter are set to 1 meter to simulate the minimum range of the LIDAR sensor. The RMSEs by three methods on the KITTI validation set with Gaussian noises are shown in <ref type="figure" target="#fig_2">Fig. 11(a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Sparse Depth Map Baseline + MS + SO</head><p>Corresponding Image Baseline + MS + SO + RGB  Our method outperforms both SparseConvs <ref type="bibr" target="#b0">[1]</ref> and IP-Basic <ref type="bibr" target="#b4">[5]</ref> on different noisy depth values.</p><p>Region-level Gaussian noises. We randomly select eight regions of size 25 × 25 pixels in every input depth map. In each region, 50% of depth points are randomly selected to add Gaussian noises of zero mean and different standard deviation values. Noisy points whose depth values are smaller than 1 meter are set to 1 meter to simulate the minimum range of the LIDAR sensor. The region-level noises are used to simulate the cases where large glasses or mirrors exist. Those regions would reflect most laser and leave large holes in the obtained depth map. The RMSE by different methods on the KITTI validation set are shown in <ref type="figure" target="#fig_2">Fig. 11(b)</ref>. Our method again outperforms the two compared methods, because of its capability of fusing low-level and high-level information with the proposed multi-scale encoder-decoder structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Robustness to sparsity</head><p>Robustness to sparsity is also essential to depth completion algorithms. We conduct experiments on testing different levels of sparsity of the input depth maps. For each input depth map, we randomly abondon 10%-90% of valid depth points. Note again that all methods are trained on original training data and are not finetuned to adapt the sparser inputs. The results on the KITTI validation set by the our model without RGB guidance and two compared methods are shown in <ref type="figure" target="#fig_2">Fig. 11(c)</ref>. Our proposed method shows the highest tolerance against different sparsity levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NYU-depth-v2 dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Data, experimental setup, and evaluation metrics</head><p>We also evaluate our proposed method on the NYU-depth-v2 dataset <ref type="bibr" target="#b10">[11]</ref> with its official train/test split. Each RGB image in the dataset is paired with a spatially aligned dense depth map. The original depth maps are dense and the dataset is not originally proposed for sparse depth completion. Following the experimental setup in <ref type="bibr" target="#b2">[3]</ref>, synthetic sparse depth maps could be created to test the performance of depth completion algorithms. Only N depth points in each depth map are randomly kept as input depth maps for depth completion. The training set consists of depth and image pairs from 249 scenes, and 654 images are selected for evaluating the final performance according to the setup in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. RMSE (Eq. (11)) and mean absolute relative error (REL in meters) are adopted as the evaluation metrics. REL is calculated as</p><formula xml:id="formula_14">REL = 1 |V | u,v∈V o(u, v) − t(u, v) t(u, v) ,<label>(15)</label></formula><p>where o and t are the outputs of our network and the ground truth dense depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with state-of-the-art</head><p>We compare our method to methods proposed in Ma et al. <ref type="bibr" target="#b2">[3]</ref>, [23] 1 , Jaritz et al. <ref type="bibr" target="#b17">[18]</ref> and He et al. <ref type="bibr" target="#b38">[39]</ref>. Since the input depth maps are much sparser than the depth maps in KITTI dataset <ref type="bibr" target="#b0">[1]</ref>, we added a 2 × 2 max-pooling layer following the first 5 × 5 convolution layer, and added a Batch Normalization <ref type="bibr" target="#b46">[47]</ref> layer after each convolution layer. Each input depth map has N = 20, 50, 200 randomly kept depth points. RMSE and REL of different N values are reported in <ref type="table" target="#tab_4">Table 4</ref>, which demonstrates that our proposed method outperforms all other methods without using RGB 1. The code released by the authors were utilized.</p><p>information. We also show the result with RGB guidance in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we proposed several novel sparsity-invariant operations for handling sparse feature maps. The novel operations enable us to design a novel sparsity-invariant encoder-decoder network, which effectively fuses multiscale features from different CNN layers for accurate depth completion. RGB features for better guiding depth completion is also integrated into the proposed framework. Extensive experiment results and component analysis show advantages of the proposed sparsity-invariant operations and the encoder-decoder network structure. The proposed method outperforms state-of-the-arts and demonstrate great robustness against different levels of data corruption.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Zixuan Huang is with Department of Computer Sciences, University of WisconsinMadison, United States • Junming Fan, Shenggan Cheng and Shuai Yi are with SenseTime Research, Beijing, China. • Xiaogang Wang and Hongsheng Li are with the Department of Electronic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>for illustration). On the other hand, there exist effective multi-scale encoder-decoder net-arXiv:1808.08685v2 [cs.CV] 20 Feb 2020 CNN with sparsity-invariant convolution only (b) Proposed Sparsity-invariant Encoder-decoder Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>(a) CNN with only sparsity-invariant convolution could only gradually downsample feature maps, which loses much resolution at later stages. (b) Our proposed encoder-decoder network with novel sparsity-invariant operations could effectively fuse multi-scale features from different layers for depth completion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Result by IP-Basic<ref type="bibr" target="#b4">[5]</ref> (f) Result by Proposed HMS-Net Illustration of depth completion results by previous methods and our proposed HMS-Net. (a) An example input sparse depth map. (b) Corresponding RGB image. (c) Result by ADNN [4]. (d) Result by sparse convolution [1]. (e) Result by hand-crafted classical image processing method [5]. (f) Result by the proposed HMS-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the proposed sparsity-invariant upsampling operation. F stands for Bilinear Upsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the proposed sparsity-invariant average. then normalized at each location according to the upsampled sparsity mask values F (m x ). The final sparsity mask m z is obtained by identifying the non-zero locations of the upsampled sparsity mask F (m x ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Sparsity patterns vary from regions, thus we need several different kernels to deal with out feature maps after concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of the proposed joint sparsity-invariant concatenation and convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Illustration of our multi-scale encoder-decoder network structure for depth completion based on the proposed sparsity-invariant operations. (a) Proposed network with RGB guidance. (b) Proposed network without RGB guidance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>→ 3 ×</head><label>3</label><figDesc>1 conv → BN → ReLU. Because the obtained midlevel RGB features are downsampled to 1/4 of its original size, they are upsamepled back to the input image's original size. The upsampled RGB features are then transformed by a series of convolutions. They act as additional guidance signals and are concatenated to the low-level sparse depth feature maps of different multi-scale blocks. Our experimental results show including the additional RGB mid-level features as guidance further improves the depth completion accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>An example in the KITTI validation set to show the results by the baseline model (Baseline) vs. baseline model with mutli-scale encoderdecoder structure (Baseline + MS). See rectangles for better boundary regions by Baseline + MS. Input Sparse Depth Map Baseline + MS Baseline + MS (magnified) Corresponding Image Baseline + MS + SO Baseline + MS + SO (magnified) An example in the KITTI validation set to show the effectiveness with (Baseline + MS) and without proposed sparsity-invariant operations (Baseline + MS + SO). Rectangles show Baseline + MS + SO better handles input depth maps with very sparse depth points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>An example in the KITTI validation set to show the effectiveness withour RGB guidance (Baseline + MS + SO) and with RGB guidance (Baseline + MS + SO + RGB). Rectangles show Baseline + MS + SO + RGB generates clearer boundaries of the cyclist and the far-away vehicle.(a) Scene-level Gaussian noises (b) Region-level Gaussian noise (c) Randomly abandon input depth points Testing robustness on the validation set of KITTI depth completion benchmark with manually corrupted data. (a) Scene-level Gaussian noises on 10% depth points. (b) Region-level Gaussian noise on eight 25× 25 regions. (c) Randomly abandon input depth points with varying probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Depth completion examples in NYU-depth-v2 dataset [11] by our proposed method with vary N values. (First row) Input sparse depth maps. (Second row) Corresponding RGB images (not used as algorithm inputs). (Third row) Predicted dense depth maps by our proposed method. (Fourth row) Ground truth dense depth maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Depth completion errors by different methods on the test set of KITTI depth completion benchmark.</figDesc><table><row><cell>Methods</cell><cell cols="2">RGB info?</cell><cell>RMSE</cell><cell>MAE</cell><cell>iRMSE</cell><cell>iMAE</cell></row><row><cell>SparseConvs [1]</cell><cell></cell><cell>×</cell><cell>1601.33</cell><cell>481.27</cell><cell>4.94</cell><cell>1.78</cell></row><row><cell>IP-Basic [5]</cell><cell></cell><cell>×</cell><cell>1288.46</cell><cell>302.60</cell><cell>3.78</cell><cell>1.29</cell></row><row><cell>NConv-CNN [12]</cell><cell></cell><cell>×</cell><cell>1268.22</cell><cell>360.28</cell><cell>4.67</cell><cell>1.52</cell></row><row><cell>Spade-sD [18]</cell><cell></cell><cell>×</cell><cell>1035.29</cell><cell>248.32</cell><cell>2.60</cell><cell>0.98</cell></row><row><cell>Sparse-to-Dense(d) [23]</cell><cell></cell><cell>×</cell><cell>954.36</cell><cell>288.64</cell><cell>3.21</cell><cell>1.35</cell></row><row><cell>Ours w/o RGB</cell><cell></cell><cell>×</cell><cell>937.48</cell><cell>258.48</cell><cell>2.93</cell><cell>1.14</cell></row><row><cell cols="2">Bilateral NN [7] ADNN [4] CSPN [19] Spade-RGBsD [18] Sparse-to-Dense(gd) [23] Ours w/ RGB</cell><cell>√ √ √ √ √ √</cell><cell>1750.00 1325.37 1019.64 917.64 814.73 841.78</cell><cell>520.00 439.48 279.46 234.81 249.95 253.47</cell><cell>-59.39 2.93 2.17 2.80 2.73</cell><cell>-3.19 1.15 0.95 1.21 1.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Component analysis of our proposed method on the validation set of KITTI depth completion benchmark.</figDesc><table><row><cell>Method</cell><cell>RMSE</cell><cell>MAE</cell></row><row><cell>Baseline w/o sparseconv</cell><cell>1819.81</cell><cell>426.84</cell></row><row><cell>Baseline w/ sparseconv</cell><cell>1683.22</cell><cell>447.93</cell></row><row><cell>Baseline + MS (Up only)</cell><cell>1185.02</cell><cell>323.41</cell></row><row><cell>Baseline + MS (Down only)</cell><cell>1192.43</cell><cell>322.95</cell></row><row><cell>Baseline + MS (Mid-level flow removed)</cell><cell>1166.87</cell><cell>317.74</cell></row><row><cell>Baseline + MS (Full)</cell><cell>1137.42</cell><cell>315.32</cell></row><row><cell>Baseline + MS + SO</cell><cell>994.14</cell><cell>262.41</cell></row><row><cell>Baseline + MS + SO + RGB</cell><cell>883.74</cell><cell>257.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Comparison with commonly used encoder-decoder networks without RGB guidance on KITTI depth completion validation set.Baseline+MS into sparsity-invariant ones to handle very sparse inputs. RMSE by Baseline+MS+SO improves from 1137.42 to 994.14. An example is shown inFig. 9, which shows Baseline+MS+SO better handles regions with very sparse inputs. By incorporating RGB features into Baseline+MS+SO+RGB, the network utilizes useful additional guidance from RGB images for further improving the depth completion accuracy. An example comparing Baseline+MS+SO+RGB and Baseline+MS+SO is shown in</figDesc><table><row><cell>Method</cell><cell>RMSE</cell><cell>MAE</cell></row><row><cell>U-Net [8]</cell><cell>1387.35</cell><cell>445.73</cell></row><row><cell>FRRN [10]</cell><cell>1148.27</cell><cell>338.56</cell></row><row><cell>PSPNet [43]</cell><cell>1185.39</cell><cell>354.21</cell></row><row><cell>FPN [44]</cell><cell>1441.82</cell><cell>473.65</cell></row><row><cell>He et al. [39]</cell><cell>1056.39</cell><cell>293.86</cell></row><row><cell>Ours w/o RGB</cell><cell>994.14</cell><cell>262.41</cell></row></table><note>Deep fusion of RGB features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Comparison with other methods on NYU-depth-v2 depth dataset with varying N values.</figDesc><table><row><cell>Method</cell><cell cols="2">20 samples RMSE REL</cell><cell cols="2">50 samples RMSE REL</cell><cell cols="2">200 samples RMSE REL</cell></row><row><cell>Ma et al. [3] w/o RGB</cell><cell>0.461</cell><cell>0.110</cell><cell>0.347</cell><cell>0.076</cell><cell>0.259</cell><cell>0.054</cell></row><row><cell>Jaritz et al. [18] w/o RGB</cell><cell>0.476</cell><cell>0.114</cell><cell>0.358</cell><cell>0.074</cell><cell>0.246</cell><cell>0.051</cell></row><row><cell>Ma et al. [23] w/o RGB</cell><cell>0.481</cell><cell>0.113</cell><cell>0.352</cell><cell>0.073</cell><cell>0.245</cell><cell>0.049</cell></row><row><cell>He et al. [39] w/o RGB</cell><cell>0.478</cell><cell>0.113</cell><cell>0.355</cell><cell>0.072</cell><cell>0.238</cell><cell>0.045</cell></row><row><cell>Ours w/o RGB</cell><cell>0.449</cell><cell>0.110</cell><cell>0.344</cell><cell>0.073</cell><cell>0.233</cell><cell>0.044</cell></row><row><cell>Ma et al. [3] w/ RGB</cell><cell>0.351</cell><cell>0.078</cell><cell>0.281</cell><cell>0.059</cell><cell>0.230</cell><cell>0.044</cell></row><row><cell>Ours w/ RGB</cell><cell>0.350</cell><cell>0.079</cell><cell>0.274</cell><cell>0.059</cell><cell>0.212</cell><cell>0.041</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>as well as examples of different N values and the depth completion results in Fig. 12.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparsity invariant CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parse geometry from a line: Monocular depth estimation with partial laser observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="page" from="5059" to="5066" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep convolutional compressed sensing for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00036</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On estimating regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Nadaraya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="141" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The fast bilateral solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="617" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4151" to="4160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sbnet: Sparse blocks network for fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8711" to="8720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05356</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3313" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Confidence propagation through cnns for guided sparse depth regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3353" to="3362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional prior networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="271" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-supervised sparseto-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00275</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal filtering for depth maps generated by kinect depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matyunin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vatolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Berdnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smirnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DTV Conference: The True Vision-Capture, Transmission and Display of 3D Video</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient spatio-temporal hole filling strategy for kinect depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Three-dimensional image processing and applications</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Global auto-regressive depth recovery via iterative non-local filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<ptr target="http://infoscience.epfl.ch/record/253660" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Broadcasting</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth image enhancement for kinect using region growing and bilateral filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3070" to="3073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Color-guided depth recovery from rgb-d data using an adaptive autoregressive model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3443" to="3458" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep joint image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image guided depth upsampling using anisotropic total generalized variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Depth map super-resolution by deep multi-scale guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Edge-guided single depth image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="428" to="438" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical features driven residual learning for depth map super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2545" to="2557" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Patch based synthesis for single depth image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Variational depth superresolution using example-based edge representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="513" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint super resolution and denoising from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1525" to="1537" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Atgv-net: Accurate depth super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning depth from single images with deep neural network embedding focal length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4676" to="4689" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nternational Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
