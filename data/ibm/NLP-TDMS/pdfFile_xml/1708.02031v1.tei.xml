<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Uncertain Convolutional Features for Accurate Saliency Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Uncertain Convolutional Features for Accurate Saliency Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks (CNNs) have delivered superior performance in many computer vision tasks. In this paper, we propose a novel deep fully convolutional network model for accurate salient object detection. The key contribution of this work is to learn deep uncertain convolutional features (UCF), which encourage the robustness and accuracy of saliency detection. We achieve this via introducing a reformulated dropout (R-dropout) after specific convolutional layers to construct an uncertain ensemble of internal feature units. In addition, we propose an effective hybrid upsampling method to reduce the checkerboard artifacts of deconvolution operators in our decoder network. The proposed methods can also be applied to other deep convolutional networks. Compared with existing saliency detection methods, the proposed UCF model is able to incorporate uncertainties for more accurate object boundary inference. Extensive experiments demonstrate that our proposed saliency model performs favorably against state-ofthe-art approaches. The uncertain feature learning mechanism as well as the upsampling method can significantly improve performance on other pixel-wise vision tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Saliency detection targets to identify the most important and conspicuous objects or regions in an image. As a preprocessing procedure in computer vision, saliency detection has greatly benefited many practical applications such as object retargeting <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref>, scene classification <ref type="bibr" target="#b36">[37]</ref>, semantic segmentation <ref type="bibr" target="#b33">[34]</ref> and visual tracking <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14]</ref>. Although significant progress has been made <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref>, saliency detection remains very challenging due to complex factors in real world scenarios. In this work we focus on the task of improving robustness of saliency detection models, which has been ignored in the literature.</p><p>Previous saliency detection methods utilize several handcrafted visual features and heuristic priors. Recently, deep * Prof.Lu is the corresponding author.</p><p>(a) Feature Visualization <ref type="bibr" target="#b6">[7]</ref> (b) Generative Adversarial Example <ref type="bibr" target="#b34">[35]</ref> (c) Saliency Detection <ref type="bibr" target="#b44">[45]</ref> (d) Semantic Segmentation <ref type="bibr" target="#b30">[31]</ref>  <ref type="figure">Figure 1</ref>. Examples of the checkerboard artifacts in pixel-wise vision tasks using deep CNNs. High resolution to see better.</p><p>learning based methods become more and more popular, and have set the benchmark on many datasets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3]</ref>. Their superior performance is partly attributed to the strong representation power in modeling object appearances and varied scenarios. However, existing methods fail to provide a probabilistic interpretability of the "black-box" learning in deep neural networks, and mainly enjoy the models' exceptional performance. A reasonable probabilistic interpretation can provide relational confidences alongside predictions and make the prediction system into a more robust one <ref type="bibr" target="#b9">[10]</ref>. In addition, since the uncertainty is a natural part of any predictive system, modeling the uncertainty is of crucial importance. For instance, the object boundary strongly affects the prediction accuracy of a saliency model, it is desirable that the model can provide meaningful uncertainties on where the boundary of distinct objects is. As far as we know, there is no work to model and analyze the uncertainty of saliency detection methods based on deep learning.</p><p>Another important issue is the checkerboard artifact in pixel-wise vision tasks, which target to generate images or feature maps from low to high resolution. Several typical examples are shown in <ref type="figure">Fig. 1</ref> (ref. <ref type="bibr" target="#b31">[32]</ref> for more details). The odd artifacts sometimes are very fatal for deep CNNs based approaches. For example, when the artifacts appear in the output of a fully convolutional network (FCN), the network training may fail and the prediction can be completely wrong <ref type="bibr" target="#b35">[36]</ref>. We find that the actual cause of these artifacts is the upsampling mechanism, which generally utilizes the deconvolution operation. Thus, it is of great interest to explore new upsampling methods to better reduce the artifacts for pixel-wise vision tasks. Meanwhile, the artifacts are also closely related to the uncertainty learning of deep CNNs.</p><p>All of the issues discussed above motivate us to learn uncertain features (probabilistic learning) through deep networks to achieve accurate saliency detection. Our model has several unique features, as outlined below.</p><p>• Different from existing saliency detection methods, our model is extremely simplified. It consists of an encoder FCN, a corresponding decoder FCN followed by a pixel-wise classification layer. The encoder FCN hierarchically learns visual features from raw images while the decoder FCN progressively upsamples the encoded feature maps to the input size for the pixelwise classification. • Our model can learn deep uncertain convolutional features (UCF) for more accurate saliency detection. The key ingredient is inspired by dropout <ref type="bibr" target="#b12">[13]</ref>. We propose a reformulated dropout (R-dropout), leading to an adaptive ensemble of the internal feature units in specific convolutional layers. Uncertain features are achieved with no additional parameterization. • We propose a new upsampling method to reduce the checkerboard artifacts of deconvolution operations. The new upsampling method has two obvious advantages. On the one hand it separates out upsampling (to generate higher resolution feature maps) from convolution (to extract convolutional features), on the other hand it is compatible with the regular deconvolution. • The uncertain feature extraction and saliency detection are unified in an encoder-decoder network architecture. The parameters of the proposed model (i.e., weights and biases in all the layers) are jointly trained by end to end gradient learning. • Our methods show good generalization on saliency detection and other pixel-wise vision tasks. Without any post-processing steps, our model yields comparable even better performance on public saliency detection, semantic segmentation and eye fixation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, deep learning has delivered superior performance in saliency detection. For instance, Wang et al. <ref type="bibr" target="#b43">[44]</ref> propose two deep neural networks to integrate local estimation and global search for saliency detection. Li et al. <ref type="bibr" target="#b22">[23]</ref> train fully connected layers of mutiple CNNs to predict the saliency degree of each superpixel. To deal with the problem that salient objects may appear in a low-contrast background, Zhao et al. <ref type="bibr" target="#b49">[50]</ref> take global and local context into account and model the saliency prediction in a multi-context deep CNN framework. These methods have excellent performances, however, all of them include fully connected layers, which are very computationally expensive. What's more, fully connected layers drop spatial information of input images. To address these issues, Li et al. <ref type="bibr" target="#b25">[26]</ref> propose a FCN trained under the multi-task learning framework for saliency detection. Wang et al. <ref type="bibr" target="#b44">[45]</ref> design a recurrent FCN to leverage saliency priors and refine the coarse predictions.</p><p>Although motivated by the similar spirit, our method significantly differs from <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref> in three aspects. First, the network architecture is very different. The FCN we used is in the encoder-decoder style, which is in the view of main information reconstruction. In <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref>, the FCN originates from the FCN-8s <ref type="bibr" target="#b27">[28]</ref> designed with both long and short skip connections for the segmentation task. Second, instead of simply using FCNs as predictors in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref>, our model can learn uncertain convolutional features by using multiple reformulated dropouts, which improve the robustness and accuracy of saliency detection. Third, our model is equipped with a new upsampling method, that naturally handles the checkerboard artifacts of deconvolution operations. The checkerboard artifacts can be reduced through training the entire neural network. In contrast, the artifacts is handled by hand-crafted methods in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref>. Specifically, <ref type="bibr" target="#b25">[26]</ref> uses superpixel segmentation to smooth the prediction. In <ref type="bibr" target="#b44">[45]</ref>, an edge-aware erosion procedure is used.</p><p>Our work is also related to the model uncertainty in deep learning. Gal et al. <ref type="bibr" target="#b9">[10]</ref> mathematically prove that a multilayer perceptron models (MLPs) with dropout applied before every weight layer, is equivalent to an approximation to the probabilistic deep Gaussian process. Though the provided theory is solid, a full verification on deep CNNs is underexplored. Base on this fact, we make a further step in this direction and show that a reformulated dropout can be used in convolutional layers for learning uncertain feature ensembles. Another representative work on the model uncertainty is the Bayesian SegNet <ref type="bibr" target="#b19">[20]</ref>. The Bayesian Seg-Net is able to predict pixel-wise scene segmentation with a measure of the model uncertainty. They achieve the model uncertainty by Monte Carlo sampling. Dropout is activated at test time to generate a posterior distribution of pixel class labels. Different from <ref type="bibr" target="#b19">[20]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture Overview</head><p>Our architecture is partly inspired by the stacked denoising auto-encoder <ref type="bibr" target="#b42">[43]</ref>. We generalize the auto-encoder to a deep fully convolutional encoder-decoder architecture. The resulting network forms a novel hybrid FCN which consists of an encoder FCN for high-level feature extraction, a corresponding decoder FCN for low-level information reconstruction and a pixel-wise classifier for saliency prediction. The overall architecture is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. More specifically, the encoder FCN consists of multiple convolutional layers with batch normalizations (BN) <ref type="bibr" target="#b15">[16]</ref> and rectified linear units (ReLU), followed by non-overlapping max pooling. The corresponding decoder FCN additionally introduces upsampling operations to build feature maps up from low to high resolution. We use the softmax classifier for the pixel-wise saliency prediction. In order to achieve the uncertainty of learned convolutional features, we utilize the reformulated dropout (dubbed R-Dropout) after several convolutional layers. The detailed network configuration is included in supplementary materials. We will fully elaborate the R-Dropout, our new upsampling method and the training strategy in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">R-Dropout for Deep Uncertain Convolutional Feature Ensemble</head><p>Dropout is typically interpreted as bagging a large number of individual models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>. Although plenty of experiments show that dropout for fully connected layers improves the generalization ability of deep networks, there is a lack of research about using dropout for other type layers, such as convolutional layers. In this subsection, we show that using modified dropout after convolutional layers can be interpreted as a kind of probabilistic feature ensembles.</p><p>In light of this fact, we provide a strategy on learning uncertain convolutional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-Dropout in Convolution:</head><p>Assume X ∈ R W ×H×C is a 3D tensor, and f (X) is a convolution operation in CNNs, projecting X to the R W ×H space by parameters W and b:</p><formula xml:id="formula_0">f (X) = WX + b.<label>(1)</label></formula><p>Let g(·) be a non-linear activation function. When the original dropout <ref type="bibr" target="#b12">[13]</ref> is applied to the outputs of g(f ), we can get its disturbed versionĝ(f ) by</p><formula xml:id="formula_1">g(f ) = g(WX + b), (2) g(f ) = M g(f ) = M g(WX + b),<label>(3)</label></formula><p>where denotes element-wise product and M is a binary mask matrix of size W ×H with each element M i,j drawn independently from M i,j ∼ Bernoulli(p). Eq.(3) denotes the activation with dropout during training, and Eq.(2) denotes the activation at test time. In addition, Srivastava et al. <ref type="bibr" target="#b38">[39]</ref> suggest to scale the activations g(f ) with p at test time to obtain an approximate average of the unit activation. Many commonly used activation functions such as Tanh, ReLU and LReLU <ref type="bibr" target="#b11">[12]</ref>, have the property that g(0) = 0. Thus, Eq.(3) can be re-written as the R-Dropout formula,</p><formula xml:id="formula_2">g(f ) = g(M (WX + b)) (4) = g(M (WX) + M b) (5) = g((M ⊗ W)X + M b) (6) = g(SX + M b),<label>(7)</label></formula><p>where ⊗ denotes the cross-channel element-wise product. From above equations, we can derive that when S = M⊗W is still binary, Eq. <ref type="bibr" target="#b6">(7)</ref> implies that a kind of stochastic properties 1 is applied at the inputs to the activation function. Let S i,j,k ∈ [0, 1] and j k S i,j,k = 1, the above equations will strictly construct an ensemble of internal feature units of X. However, in practice there is certainly no evidence to hold above constraints. Even though, we note that: (1) the stochastic mask matrix S mainly depends on the mask generator 2 ; (2) when training deep convolutional networks, the R-Dropout after convolutional layers acts as an uncertain ensemble of convolutional features; (3) this kind of feature ensemble is element-wisely probabilistic, thus it can bring forth robustness in the prediction of dense labeling vision tasks such as saliency detection and semantic segmentation.</p><p>Uncertain Convolutional Feature Extraction: Motivated by above insights, we employ the R-Dropout into convolutional layers of our model, thereby generating deep uncertain convolutional feature maps. Since our model consists of alternating convolutional and pooling layers, there exist two typical cases in our model. For notational simplicity, we subsequently drop the batch normalization (BN). 1) Conv+R-Dropout+Conv: If the proposed R-Dropout is followed by a convolutional layer, the forward propagation of input is formulated aŝ</p><formula xml:id="formula_3">g(f l ) = g(S l X l−1 + M l b l ),<label>(8)</label></formula><formula xml:id="formula_4">f l+1 = Conv(W l+1 ,ĝ(f l )),<label>(9)</label></formula><formula xml:id="formula_5">g l+1 = g(f l+1 ),<label>(10)</label></formula><p>where l is the layer number and Conv is the convolution operation. As we can see from Eq.(9), the disturbed activation g(f l ) is convolved with filter W l+1 to produce convolved features f l+1 . In this way, the network will focus on learning the weight and bias parameters, i.e., W and b, and the uncertainty of using the R-Dropout will be dissipated during training deep networks.</p><p>2) Conv+R-Dropout+Pooling: In this case, the forward propagation of input becomeŝ</p><formula xml:id="formula_6">g(f l ) = g(S l X l−1 + M l b l ),<label>(11)</label></formula><formula xml:id="formula_7">g l+1 j = Pooling(ĝ(f l ) 1 , ...,ĝ(f l ) n ), i ∈ R l j .<label>(12)</label></formula><p>Here Pooling(·) denotes the max-pooling function. R l j is the pooling region j at layer l andĝ(f l ) i is the activition of each neuron within R l j . n = |R l j | is the number of units in R l j . To formulate the uncertainty, without loss of generality, we suppose the activationsĝ(f l ) in each pooling region j are ordered in non-decreasing order, i.e. g(f l ) 1 ≤ĝ(f l ) 2 ≤ ... ≤ĝ(f l ) n . As a result,ĝ(f l ) i will be selected as the pooled activation on conditions that</p><formula xml:id="formula_8">(1)ĝ(f l ) i+1 ,ĝ(f l ) i+2 , .</formula><p>..,ĝ(f l ) n are dropped out, and (2) g(f l ) i is retained. This event occurs with probability of P i according to the probability theory,</p><formula xml:id="formula_9">P (g l+1 j =ĝ(f l ) i ) = P i = pq n−i , p = 1 − q.<label>(13)</label></formula><p>Therefore, performing R-dropout before the max-pooling operation is exactly sampling from the following multinomial distribution to select an index i, then the pooled activation is simplyĝ(f l ) i ,</p><formula xml:id="formula_10">g l+1 j =ĝ(f l ) i , i ∼ M ultinomial(P 0 , P 1 , ..., P n ),<label>(14)</label></formula><p>where P 0 (= q n ) is the special event that all the units in a pooling region is dropped out. The latter strategy exhibits the effectiveness of building the uncertainty by employing the R-Dropout into convolutional layers. We adopt it to build up our network architecture (see <ref type="figure" target="#fig_0">Fig. 2</ref>). We will experimentally demonstrate that the R-Dropout based FCN yields marvelous results on the saliency detection datasets in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hybrid Upsampling for Prediction Smoothing</head><p>In this subsection, we first explicate the cause of checkerboard artifacts by the deconvolution arithmetic <ref type="bibr" target="#b41">[42]</ref>. Then we derive a new upsampling method to reduce the artifacts as much as possible for the network training and inference.</p><p>Without loss of generality, we focus on the square input (n × n), square kernel size (k × k), same stride (s × s) and same zero padding (p × p) (if used) along both axes. Since we aim to implement upsampling, we set s ≥ 2. In general, the convolution operation C can be described by</p><formula xml:id="formula_11">O = C(I, F) = I * F,<label>(15)</label></formula><p>where I (n+p)×(n+p) is the input, F k×k is the filter with stride s, * is the discrete convolution and O is the output whose dimension is (n + 2p − k)/s + 1. The convolution C has an associated deconvolution D described byn , k = k, s = 1 and p = k − p − 1, wheren is the size of the stretched input obtained by adding s − 1 zeros between each input unit, and the output size of the deconvolution is o = s(n − 1) + k − 2p 3 . This indicates that the regular deconvolution operator is equivalent to performing convolution on a new input with inserted zeros (n ×n ). A toy example is shown in <ref type="figure">Fig. 3</ref>. In addition, when the filter size k can not be divided by the stride s, the deconvolution will cause the overlapping issue. If the stretched input is high-frequency or near periodic, i.e., the value is extremely undulating when zeros are inserted, the output results of deconvolution operations naturally have numerical artifacts like a checkerboard. <ref type="figure">Figure 3</ref>. The detailed explanation of deconvolution. The deconvolution of a 3 × 3 kernel using 2 × 2 strides over a 5 × 5 input padded with a 1 × 1 border of zeros (i.e., n = 5, k = 3, s = 2 and p = 1). It is equivalent to convolving a 3 × 3 kernel over a 3 × 3 input (with 1 zero inserted between inputs) padded with a 1 × 1 border of zeros using unit strides (i.e., n = 3,n = 5, k = 3, s = 1 and p = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ Deconvolution</head><p>Inter-convolution Input Output <ref type="figure">Figure 4</ref>. The hybrid upsampling. Two strategies are jointly used: 1) deconvolution with restricted filter sizes (left branch); 2) linear interpolation with 1x1 convolution (right branch). The final output is the summed result of two strategies.</p><p>Base on the above observations, we propose two strategies to avoid the artifacts produced by the regular deconvolution. The first one is restricting the filter size. We can simply ensure the filter size is a multiple of the stride size, avoiding the overlapping issue, i.e., k = λs, λ ∈ N + .</p><p>Then the deconvolution will dispose the zero-inserted input with the equivalent convolution, deriving a smooth output. However, because this method only focuses on changing the receptive fields of the output, and can not change the frequency distribution of the zero-inserted input, the artifacts can still leak through in several extreme cases. We propose another alternative strategy which separates out upsampling from equivalent convolution. We first resize the original input into the desired size by interpolations, and then perform some equivalent convolutions. Although this strategy may destroy the learned features in deep CNNs, we find that high resolution maps built by iteratively stacking this kind of upsampling can reduce artifacts amazingly. In order to take the strength of both strategies, we introduce the hybrid upsampling method by summing up the outputs of the two strategies. <ref type="figure">Fig. 4</ref> illustrates the proposed upsampling method. In our proposed model, we use bilinear (or nearest-neighbor) operations for the interpolation. These interpolation methods are linear operations, and can be embedded into the deep CNNs as efficient matrix multiplications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training the Entire Network</head><p>Since there is a lack of enough saliency detection data for training our model from scratch, we utilize the front-end of the VGG-16 model <ref type="bibr" target="#b37">[38]</ref> as our encoder FCN (13 convolutional layers and 5 pooling layers pre-trained on ILSVRC 2014 for the image classification task). Our decoder FCN is a mirrored version of the encoder FCN, and has multiple series of upsampling, convolution and rectification layers. Batch normalization (BN) is added to the output of every convolutional layer. We add the R-dropout with an equal sampling rate p = 0.5 after specific convolutional layers, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. For saliency detection, we randomly initialize the weights of the decoder FCN and fine-tune the entire network on the MSRA10K dataset <ref type="bibr" target="#b3">[4]</ref>, which is widely used in salient object detection community (More details will be described in Section 4). We convert the ground-truth saliency map of each image in that dataset to be a 0-1 binary map. This kind of transform perfectly matches the channel output of the FCN when we use the softmax cross-entropy loss function given by the following equation <ref type="formula" target="#formula_0">(17)</ref> for separating saliency foreground from general background. <ref type="bibr" target="#b16">(17)</ref> where l m (= 0, 1) is the label of a pixel m in the image and q m is the probability that the pixel is the saliency foreground. The value of q m is obtained from the output of the network. Before putting the training images into our proposed model, each image is subtracted with the ImageNet mean <ref type="bibr" target="#b4">[5]</ref> and rescaled into the same size (448 × 448). For the correspondence, we also rescale the 0-1 binary maps to the same size. The model is trained end to end using the mini-batch stochastic gradient descent (SGD) with a momentum, learning rate decay schedule. The detailed settings of parameters are included in the supplementary material.</p><formula xml:id="formula_13">L = − m l m log(q m ) + (1 − l m ) log(1 − q m ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Saliency Inference</head><p>Because our model is a fully convolutional network, it can take images with arbitrary size as inputs when testing. After the feed-forward process, the output of the network is composed of a foreground excitation map (M f e ) and a background excitation map (M be ). We use the difference between M f e and M be , and clip the negative values to obtain the resulting saliency map, i.e.,</p><formula xml:id="formula_14">Sal = max(M f e − M be , 0).<label>(18)</label></formula><p>This subtraction strategy not only increases the pixel-level discrimination but also captures context contrast information. Optionally, we can take the ensemble of multi-scale predicted maps to further improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we start by describing the experimental setup for saliency detection. Then, we thoroughly evaluate and analyze our proposed model on public saliency detection datasets. Finally, we provide additional experiments to verify the generalization of our methods on other pixel-wise vision tasks, i.e., semantic segmentation and eye fixation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Saliency Datasets: For training the proposed network, we simply augment the MSRA10K dataset <ref type="bibr" target="#b3">[4]</ref> by the mirror reflection and rotation techniques (0 • , 90 • , 180 • , 270 • ), producing 80,000 training images totally.</p><p>For the detection performance evaluation, we adopt six widely used saliency detection datasets as follows, DUT-OMRON <ref type="bibr" target="#b48">[49]</ref>. This dataset consists of 5,168 high quality images. Images in this dataset have one or more salient objects and relatively complex background. Thus, this dataset is difficult and challenging in saliency detection.</p><p>ECSSD <ref type="bibr" target="#b47">[48]</ref>. This dataset contains 1,000 natural images, including many semantically meaningful and complex structures in the ground truth segmentations.</p><p>HKU-IS <ref type="bibr" target="#b49">[50]</ref>. This dataset contains 4,447 images with high quality pixel-wise annotations. Images in this dataset are well chosen to include multiple disconnected objects or objects touching the image boundary.</p><p>PASCAL-S <ref type="bibr" target="#b26">[27]</ref>. This dataset is carefully selected from the PASCAL VOC dataset <ref type="bibr" target="#b7">[8]</ref> and contains 850 images.</p><p>SED <ref type="bibr" target="#b1">[2]</ref>. This dataset contains two different subsets: SED1 and SED2. The SED1 has 100 images each containing only one salient object, while the SED2 has 100 images each containing two salient objects.</p><p>SOD <ref type="bibr" target="#b47">[48]</ref>. This dataset has 300 images, and it was originally designed for image segmentation. Pixel-wise annotation of salient objects was generated by <ref type="bibr" target="#b17">[18]</ref>.</p><p>Implementation Details: We implement our approach based on the MATLAB R2014b platform with the modified Caffe toolbox <ref type="bibr" target="#b19">[20]</ref>. We run our approach in a quad-core PC machine with an i7-4790 CPU (with 16G memory) and one NVIDIA Titan X GPU (with 12G memory). The training process of our model takes almost 23 hours and converges after 200k iterations of the min-batch SGD. The proposed saliency detection algorithm runs at about 7 fps with 448 × 448 resolution (23 fps with 224 × 224 resolution). The source code can be found at http://ice.dlut.edu.cn/lu/.</p><p>Saliency Evaluation Metrics: We adopt three widely used metrics to measure the performance of all algorithms, i.e., the Precision-Recall (PR) curves, F-measure and Mean Absolute Error (MAE) <ref type="bibr" target="#b2">[3]</ref>. The precision and recall are computed by thresholding the predicted saliency map, and comparing the binary map with the ground truth. The PR curve of a dataset indicates the mean precision and recall of saliency maps at different thresholds. The F-measure is a balanced mean of average precision and average recall, and can be calculated by</p><formula xml:id="formula_15">F β = (1 + β 2 ) × P recision × Recall β 2 × P recision × Recall .<label>(19)</label></formula><p>Following existing works <ref type="bibr" target="#b47">[48]</ref> [44] <ref type="bibr" target="#b2">[3]</ref> [49], we set β 2 to be 0.3 to weigh precision more than recall. We report the performance when each saliency map is adaptively binarized with an image-dependent threshold. The threshold is determined to be twice the mean saliency of the image:</p><formula xml:id="formula_16">T = 2 W × H W x=1 H y=1 S(x, y),<label>(20)</label></formula><p>where W and H are width and height of an image, S(x, y) is the saliency value of the pixel at (x, y).</p><p>We also calculate the mean absolute error (MAE) for fair comparisons as suggested by <ref type="bibr" target="#b2">[3]</ref>. The MAE evaluates the saliency detection accuracy by</p><formula xml:id="formula_17">M AE = 1 W × H W x=1 H y=1 |S(x, y) − G(x, y)|,<label>(21)</label></formula><p>where G is the binary ground truth mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison with State-of-the-art</head><p>We compare the proposed UCF algorithm with other 10 state-of-the-art ones including 6 deep learning based algorithms (DCL <ref type="bibr" target="#b23">[24]</ref>, DS <ref type="bibr" target="#b25">[26]</ref>, ELD <ref type="bibr" target="#b21">[22]</ref>, LEGS <ref type="bibr" target="#b43">[44]</ref>, MDF <ref type="bibr" target="#b49">[50]</ref>, RFCN <ref type="bibr" target="#b44">[45]</ref>) and 4 conventional counterparts (BL <ref type="bibr" target="#b40">[41]</ref>, BSCA <ref type="bibr" target="#b32">[33]</ref>, DRFI <ref type="bibr" target="#b17">[18]</ref>, DSR <ref type="bibr" target="#b24">[25]</ref>). The source codes with recommended parameters or the saliency maps of the competing methods are adopted for fair comparison.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 5</ref> and Tab. 1, our proposed UCF model can consistently outperform existing methods across almost all the datasets in terms of all evaluation metrics, which convincingly indicates the effectiveness of the proposed methods. Refer to the supplemental material for more results on DUT-OMRON, HKU-IS, PASCAL-S and SOD datasets.</p><p>From these results, we have several fundamental observations: (1) Our UCF model outperforms other algorithms on ECSSD and SED datasets with a large margin in terms of F-measure and MAE. More specifically, our model improves the F-measure achieved by the best-performing existing algorithm by 3.9% and 6.15% on ECSSD and SED datasets, respectively. The MAE is consistently improved.</p><p>(2) Although our proposed UCF is not the best on HKU-IS and PASCAL-S datasets, it is still very competitive (our model ranks the second on these datasets). It is necessary to note that only the augmented MSRA10K dataset is used for training our model. The RFCN, DS and DCL methods   <ref type="table">Table 1</ref>. The F-measure and MAE of different saliency detection methods on five frequently used datasets. The best three results are shown in red, green and blue, respectively. The proposed methods rank first and second on these datasets.</p><p>are pre-trained on the additional PASCAL VOC segmentation dataset <ref type="bibr" target="#b8">[9]</ref>, which is overlaped with the PASCAL-S and HKU-IS datasets. This fact may interpret their success on the two datasets. However, their performance on other datasets is obviously inferior. (3) Compared with other methods, our proposed UCF achieves lower MAE on most of datasets. It means that our model is more convinced of the predicted regions by the uncertain feature learning. The visual comparison of different methods on the typical images is shown in <ref type="figure" target="#fig_3">Fig. 7</ref>. Our saliency maps can reliably highlight the salient objects in various challenging scenarios, e.g., low contrast between objects and backgrounds (the first two rows), multiple disconnected salient objects (the 3-4 rows) and objects near the image boundary (the 5-6 rows). In addition, our saliency maps provide more accurate boundaries of salient objects (the 1, 3, 6-8 rows).</p><p>Ablation Studies: To verify the contributions of each component, we also evaluate several variants of the proposed UCF model with different settings as illustrated in Tab. 2. The corresponding performance are reported in Tab. 1. The V-A model is an approximation of the DeconvNet <ref type="bibr" target="#b30">[31]</ref>. The  comparison between V-A and V-B demonstrates that our uncertain learning mechanism can indeed benefit to learn more robust features for accurate saliency inference. The comparison between V-B and V-C shows the effects with two upsampling strategies. Results imply that the interpolation strategy performs much better in saliency detection. The joint comparison of V-B, V-C and UCF confirms that our hybrid upsampling method is capable of better refining the output saliency maps. An example on the visual effects is illustrated in <ref type="figure" target="#fig_2">Fig. 6</ref>. In addition, the V-D model and V-E model verify the usefulness of deconvolution and interpolation upsampling, respectively. The V-B and V-C models achieve competitive even better results than other saliency methods. This further confirms the strength of our methods.</p><formula xml:id="formula_18">Settings V-A V-B V-C V-D V-E UCF +Dropout √ +R-Dropout √ √ √ +Rest Deconv √ √ √ √ +Inter √ √ √</formula><formula xml:id="formula_19">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generalization Evaluation</head><p>To verify the generalization of our methods, we perform additional experiments on other pixel-wise vision tasks.</p><p>Semantic Segmentation: Following existing works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20]</ref>, we simply change the classifier into 21 classes and perform the semantic segmentation task on the PASCAL VOC 2012 dataset <ref type="bibr" target="#b8">[9]</ref>. Our UCF model is trained with the PASCAL VOC 2011 training and validation data, using the Berekely's extended annotations <ref type="bibr" target="#b10">[11]</ref>. We achieve expressive results (mean IOU: 68.25, mean pix.accuracy: 92.19, pix.accuracy: 77.28), which are very comparable with other state-of-the-art segmentation methods. In addition, though the segmentation performance gaps are not as large as in saliency detection, our new upsampling method indeed performs better than regular deconvolution (mean IOU: 67.45 vs 65.173, mean pix.accuracy: 91.21 vs 90.84, pix.accuracy: 76.18 vs 75.73).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUC-J sAUC</head><p>CC NSS IG MIT300 <ref type="bibr" target="#b18">[19]</ref> 0.8584 0.7109 0.7423 2.14 -iSUN <ref type="bibr" target="#b46">[47]</ref> 0.8615 0.5498 0.8142 -0.1725 SALICON <ref type="bibr" target="#b16">[17]</ref> 0.7621 0.6326 0.8453 -0.3167 <ref type="table">Table 3</ref>. Results on eye fixation datasets. Metrics in first row can be found in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref> Eye Fixation: The task of eye fixation prediction is essentially different from our classification task. We use the Euclidean loss for the gaze prediction. We submit our results to servers of MIT300 <ref type="bibr" target="#b18">[19]</ref>, iSUN <ref type="bibr" target="#b46">[47]</ref> and SALICON <ref type="bibr" target="#b16">[17]</ref> benchmarks with standard setups. Our model also achieves comparable results shown in Tab. 3. All above results on semantic segmentation and eye fixation tasks indicate that our model has a strong generalization in other pixel-wise tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel fully convolutional network for saliency detection. A reformulated dropout is utilized to facilitate probabilistic training and inference. This uncertain learning mechanism enables our method to learn uncertain convolutional features and yield more accurate saliency prediction. A new upsampling method is also proposed to reduce the artifacts of deconvolution operations, and explicitly enforce the network to learn accurate boundary for saliency detection. Extensive evaluations demonstrate that our methods can significantly improve performance of saliency detection and show good generalization on other pixel-wise vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of the proposed UCF model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Performance of the proposed algorithm compared with other state-of-the-art methods. AE F β M AE F β M AE F β M AE F β M AE F β M AE UCF 0.6283 0.1203 0.8517 0.0689 0.8232 0.0620 0.7413 0.1160 0.8647 0.0631 0.8102 0.0680 V-E 0.6135 0.1224 0.7857 0.0795 0.7716 0.0785 0.6303 0.1284 0.8128 0.0732 0.7576 0.0851 V-D 0.5072 0.1345 0.6942 0.1195 0.6851 0.0967 0.5695 0.1624 0.7754 0.0844 0.6930 0.0954 V-C 0.6165 0.1210 0.8426 0.0711 0.8156 0.0670 0.7201 0.1203 0.8665 0.0653 0.8014 0.0795 V-B 0.6168 0.1305 0.8356 0.0781 0.8060 0.0651 0.6845 0.1254 0.8547 0.0685 0.7905 0.0709 V-A 0.6128 0.1409 0.8166 0.0811 0.7346 0.0988 0.6172 0.1367 0.7641 0.1023 0.6536 0.1044 DCL [24] 0.6842 0.1573 0.8293 0.1495 0.8533 0.1359 0.7141 0.1807 0.8546 0.1513 0.7946 0.1565 DS [26] 0.6028 0.1204 0.8255 0.1216 0.7851 0.0780 0.6590 0.1760 0.8445 0.0931 0.7541 0.1233 ELD [22] 0.6109 0.0924 0.8102 0.0796 0.7694 0.0741 0.7180 0.1232 0.8715 0.0670 0.7591 0.1028 LEGS [44] 0.5915 0.1334 0.7853 0.1180 0.7228 0.1193 --0.8542 0.1034 0.7358 0.1236 MDF [50] 0.6442 0.0916 0.8070 0.1049 0.8006 0.0957 0.7087 0.1458 0.8419 0.0989 0.8003 0.1014 RFCN [45] 0.6265 0.1105 0.8340 0.1069 0.8349 0.0889 0.7512 0.1324 0.8502 0.1166 0.7667 0.1131 BL [41] 0.4988 0.2388 0.6841 0.2159 0.6597 0.2071 0.5742 0.2487 0.7675 0.1849 0.7047 0.1856 BSCA [33] 0.5091 0.1902 0.7048 0.1821 0.6544 0.1748 0.6006 0.2229 0.8048 0.1535 0.7062 0.1578 DRFI [18] 0.5504 0.1378 0.7331 0.1642 0.7218 0.1445 0.6182 0.2065 0.8068 0.1480 0.7341 0.1334 DSR [25] 0.5242 0.1389 0.6621 0.1784 0.6772 0.1422 0.5575 0.2149 0.7909 0.1579 0.7116 0.1406</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of different upsampling algorithms. (a) Input image; (b) Deconvolution; (c) Interpolation; (d) Our method; (e) Ground truth. More examples in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Comparison of saliency maps. (a) Input images; (b) Ground truth; (c) Our method; (d) RFCN; (e) DCL; (f) DS; (g) LEGS; (h) MDF; (i) ELD; (j) DRFI. More examples in each dataset can be found in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Variants of our UCF model. Note that Rest Deconv and Inter indicate the hybrid upsampling method.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Stochastic property means that one can use a specific probability distribution to generate the learnable tensor S during each training iteration. The update of S forms a stochastic process not a certain decision.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In R-Dropout, the generator can be any probability distribution. The original dropout is a special case of the R-Dropout, when the generator is the Bernoulli distribution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The constraint on the size of the input n can be relaxed by introducing another parameter t ∈ 0, ..., s − 1 that allows to distinguish between the s different cases that all lead to the same n .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We thank to Alex Kendall for sharing the Seg-Net code. This paper is supported by the Natural Science Foundation of China #61472060, #61502070, #61528101 and #61632006.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is an object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is a salient object? a dataset and a baseline model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="742" to="756" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet:a large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Importance filtering for image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02753</idno>
		<title level="m">Inverting visual representations with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Insights and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06796</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Salicon: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Salient region detection by ufo: Uniqueness, focusness and objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIT Technical Report</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Center-surround divergence of feature statistics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2214" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Saliency-based discriminant tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1007" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A framework for visual saliency detection with applications to image thumbnailing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cifarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2232" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://distill.pub/2016/deconv-checkerboard/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Saliency detection via cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rapid biologically-inspired scene classification using features shared with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Siagian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="312" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scale and object aware image retargeting for thumbnail browsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1511" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Salient object detection via bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Francesco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07285</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kernelized subspace ranking for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="450" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06755</idno>
		<title level="m">Turkergaze: Crowdsourcing saliency with webcam based eye tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
