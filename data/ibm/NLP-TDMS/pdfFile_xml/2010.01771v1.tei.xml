<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving AMR Parsing with Sequence-to-Sequence Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent News</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
							<email>gdzhou@suda.edu.cnmuhuazhu@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving AMR Parsing with Sequence-to-Sequence Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes <ref type="bibr" target="#b0">(Banarescu et al., 2013)</ref>. Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches <ref type="bibr" target="#b39">(Wang et al., 2015b;</ref><ref type="bibr" target="#b13">Groschwitz *</ref> Corresponding Author: Junhui Li. (c / consider-01 :ARG0 (c2 / country :wiki "China" :name (n / name :op1 "China")) :ARG1 (p / partner-01 :ARG1 (c4 / country :wiki "Germany" :name (n3 / name :op1 "Germany")) :mod (i / important :degree (m / most)) :mod (t / trade-01) :location (c3 / continent :wiki "Europe"</p><p>:name (n2 / name :op1 "Europe")))) ( consider-01 : ARG0 ( country : name ( name : op1 " China " ) ) : ARG1 ( partner-01 : ARG1 ( country : name ( name : op1 " Germany " ) ) : mod ( important : degree ( most ) ) : mod ( trade-01 ) : location ( continent : name ( name : op1 " Europe " ) ) ) ) (b) AMR (c) AMR Linearization <ref type="figure">Figure 1</ref>: An example of seq2seq-based AMR parsing. <ref type="bibr">et al., 2018)</ref>, graph-based approaches <ref type="bibr" target="#b9">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b41">Werling et al., 2015;</ref><ref type="bibr" target="#b1">Cai and Lam, 2019)</ref>, transition-based approaches <ref type="bibr" target="#b6">(Damonte et al., 2017;</ref><ref type="bibr" target="#b14">Guo and Lu, 2018)</ref>, sequence-to-sequence (seq2seq) approaches <ref type="bibr" target="#b29">(Peng et al., 2017;</ref><ref type="bibr" target="#b28">van Noord and Bos, 2017;</ref><ref type="bibr" target="#b19">Konstas et al., 2017;</ref><ref type="bibr" target="#b11">Ge et al., 2019)</ref>, and sequence-to-graph (seq2graph) approaches <ref type="bibr">(Zhang et al., 2019a,b;</ref><ref type="bibr" target="#b2">Cai and Lam, 2020)</ref>. Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains only 36.5K training AMRs. To alleviate the effect of such restriction, a previous attempt is to utilize large-scale unlabeled sentences with self-training <ref type="bibr" target="#b19">(Konstas et al., 2017)</ref>. Alternatively, a more recent feasible solution is to resort to pre-training which builds pre-trained models on large-scale (unlabeled) data. Linguistic knowledge captured in pre-trained models can then be properly  <ref type="table">Table 1</ref>: Three seq2seq learning tasks explored in this paper to obtain pre-trained models. Here silver dataset indicates that the sequences in the target-side are generated automatically .</p><p>incorporated into the training of an AMR parser. However, the widely used pre-trained models such as ELMO  and BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> may not work as expected for building a state-of-the-art seq2seq AMR parser. The reasons are two-fold. On the one hand, previous studies on both seq2seq-based AMR parsing and AMR-to-text generation demonstrate the necessity of a shared vocabulary for the source and target sides <ref type="bibr" target="#b11">(Ge et al., 2019;</ref>. Using pretrained models like BERT as pre-trained encoders for AMR parsing, however, will violate the rule of sharing a vocabulary. On the other hand, pretrained models such as BERT are basically tuned for the purpose of representing sentences instead of generating target sequences. According to <ref type="bibr" target="#b46">Zhu et al. (2020)</ref>, by contrast to using BERT directly as the encoder, a more reasonable approach is to utilize BERT as an extra feature or view BERT as an extra encoder. See Section 5.1 for more detailed discussions on the effect of BERT on AMR parsing. In this paper, we propose to pre-train seq2seq models that aim to capture different linguistic knowledge from input sentences. To build such pre-trained models, we explore three different yet relevant seq2seq tasks, as listed in <ref type="table">Table 1</ref>. Here, machine translation acts as the most representative seq2seq task which takes a bilingual dataset as the training data. According to <ref type="bibr" target="#b35">Shi et al. (2016)</ref> and <ref type="bibr" target="#b21">Li et al. (2017)</ref>, a machine translation system with good performance requires the model to well derive linguistic information from input sentences. The other two tasks require auto-parsed syntactic parse trees and AMR graphs as the training data, respectively. It is worth noting that the pre-training task of AMR parsing is in the similar spirit of selftraining <ref type="bibr" target="#b19">(Konstas et al., 2017)</ref>.</p><p>In order to investigate whether various seq2seq pre-trained models are complementary to each other in the sense that they can be learned jointly to achieve better performance, we further explore joint learning of several pre-training tasks and eval-uate its effect on AMR parsing. In addition, motivated by <ref type="bibr" target="#b22">Li and Hoiem (2018)</ref>, we extend the vanilla fine-tuning method to optimize for both the performance of AMR parsing and response preservation of the pre-trained models. Detailed experimentation on two widely used English benchmarks shows that our approach substantially improves the performance, which greatly advances the state-ofthe-art. This is very encouraging since we achieve the state-of-the-art by simply making use of the generic seq2seq framework rather than designing sophisticated AMR parsing models.</p><p>2 Baseline: AMR Parsing as Seq2Seq Learning Seq2Seq Modeling. The encoder in the Transformer <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> consists of a stack of multiple identical layers, each of which has two sub-layers: one implements the multi-head selfattention mechanism and the other is a positionwise fully connected feed-forward network. The decoder is also composed of a stack of multiple identical layers. Each layer in the decoder consists of the same sub-layers as in the encoder layers plus an additional sub-layer that performs multihead attention to the output of the encoder stack. See <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref> for more details.</p><p>Pre-Processing: Linearize AMR Graph to Target Sequence. As in van Noord and Bos <ref type="formula">(2017)</ref>, we obtain simplified AMRs by removing variables and wiki links. Variables in AMR graphs are only necessary to indicate co-referring nodes and they do not carry any semantic information by themselves. Therefore, AMR graphs are first converted into AMR trees by removing variables and duplicating the co-referring nodes. Then newlines present in an AMR tree are replaced by spaces to get a sequence. <ref type="figure">Figure 1</ref>(c) illustrates the linearization result of the AMR graph in <ref type="figure">Figure 1</ref>(b). Based on the data of sentences paired with linearized AMR graphs, we train a seq2seq model whose outputs are also linearized AMRs.</p><p>Post-Processing: Recover AMR Graph from Target Sequence. The output from Transformer is an AMR sequence without variables, wiki-links, and co-occurrent variables. Moreover, the output may contain brackets that do not match, resulting incomplete concepts. To recover its full graph, the post-processing should restore information removed in pre-processing by assigning a unique variable to each concept, pruning duplicated and redundant material, performing Wikification, and restoring co-referring nodes. Meanwhile, it should fix incomplete concepts. We use the pre-processing and post-processing scripts provided by van Noord and Bos (2017). 1 3 Seq2Seq Pre-training for AMR Parsing</p><p>In this section, we first present our single pretraining approach, followed by the joint pretraining approach on two or more pre-training tasks. Then we present our fine-tuning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single Pre-training</head><p>To be consistent with the seq2seq model for AMR parsing, the pre-trained models in this paper are all built on the Transformer. That is, for each pretraining task listed in <ref type="table">Table 1</ref>, we learn a seq2seq model which will be used to initialize seq2seq model for AMR parsing in the fine-tuning phase. When building the pre-trained models, we merge all the source and target sides of the three pretraining tasks, and construct a shared vocabulary. Moreover, in all the models we share vocabulary embeddings for both the source and target sides.</p><p>PTM-MT is a seq2seq neural machine translation (NMT) model which is trained on a publicly available bilingual dataset. According to findings in <ref type="bibr" target="#b12">Goldberg (2019)</ref> and <ref type="bibr" target="#b15">Jawahar et al. (2019)</ref>, the Transformer encoder is strong in capturing syntax and semantics from source sentences, which is helpful to AMR parsing.</p><p>PTM-SynPar is a seq2seq constituent parsing model. Building such a model requires a training dataset which consists of sentences paired with constituency parse trees. To construct a silver treebank, we parse the English sentences in the bilingual data for MT by using an off-the-shelf parser. Then we linearize the automatic parse trees to get syntax sequences, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Note that in the linearization, we let the output contain the words from the source sentence. The motivation here is to regard parsing as a language generation problem, similar to the idea in Choe and Charniak (2016).</p><p>PTM-SemPar is a seq2seq AMR parsing model trained on a silver corpus of auto-parsed AMR graphs. To construct such a corpus, we apply the 1 https://github.com/RikVN/AMR Task3: baseline system of AMR parsing to process the English sentences in the bilingual MT corpus. Then we adopt the linearization process illustrated in <ref type="figure">Figure</ref> 1 to obtain source-target pairs. Finally, we train a seq2seq-based AMR parsing model on the silver corpus that will be used as a pre-trained model.</p><formula xml:id="formula_0">s 1 (2) s n 2 (2) Task2: s 1 (1) s n 1 (1) Task1: Transformer Encoder Transformer Decoder t 1 (1) t m 1 (1) … T1 t 1 (2) t m 2 (2) … T2 … … … t 1 (3) t m 3 (3) … T3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Pre-training</head><p>Intuitively, the above described single pre-trained models can capture linguistic features from different perspectives. One question is whether these models are complementary when they are properly used to initialize a seq2seq-based AMR parser. To empirically answer this question, we propose to build pre-trained models through jointly learning multiple pre-training tasks. Inspired by the zeroshot approach proposed for multi-lingual neural machine translation <ref type="bibr" target="#b16">(Johnson et al., 2017)</ref>, we add a unique preceding tag to the target side of training data to distinguish the task of each training instance, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>With such tagged training instances, multi-task learning is actually quite straightforward. We simply combine the training data of all the pre-training tasks that we are focusing on and then feed the combined training data to the Transformer model. The training process interleaves training data from each task. For example, we update parameters on a batch of training instances from task1 and then update parameters on a batch of training instances from task2, and the process iterates. With such a joint training strategy, we obtain four joint pre-trained models, i.e., PTM-MT-SynPar, PTM-MT-SemPar, PTM-SynPar-SemPar, and PTM-MT-SynPar-SemPar. Names of the models can tell what pre-training tasks are learned jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-tuning Methods</head><p>Given a pre-trained model, we can directly finetune it on a gold AMR corpus to train an AMR parser. For this purpose we use two different finetuning methods. In the following we first present the vanilla fine-tuning method, and then extend it under the framework of multi-task learning. For simplicity, we refer to the latter method as Multi-Task Learning (MTL) fine-tuning hereafter.</p><p>Vanilla Fine-Tuning optimizes the parameters of an existing pre-trained seq2seq models to train AMR parsing on a gold AMR corpus. Fine-tuning adapts the shared parameters to make them more discriminative for AMR parsing, and the low learning rate is an indirect mechanism to preserve some of the representational structure captured in the pre-training models.</p><p>MTL Fine-Tuning is designed to attack the potential drawback of the vanilla fine-tuning method. In vanilla fine-tuning, optimizing model parameters to train AMR parsing presents a potential risk of overfitting. Inspired by <ref type="bibr" target="#b22">Li and Hoiem (2018)</ref>, we propose to optimize for high accuracy of AMR parsing while preserving the performance on the pre-training tasks. Preservation of the performance on the pre-training tasks can be regarded as a regularizer for the training of AMR parsing. To implement such MTL fine-tuning, we once again adopt the generic multi-task learning framework depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. Now the question left behind is how to obtain fine-tuning instances for pre-training tasks. To this end, we use the pre-trained model focused and input sentences of gold AMR corpus to generate finetuning instances for pre-training tasks. Formally speaking, given an instance {s, t (0) } of the finetuning task , and a pre-trained model learned from k pre-training tasks, we first feed the pre-trained model with input s and obtain its k outputs, i.e. t 1 , · · · , t k for the k pre-training tasks, respectively. Therefore, each input s in the fine-tuning task is now equipped with k + 1 outputs, one for the finetuning task while the other k for the k pre-training tasks. Meanwhile, each output is associated with a unique preceding tag which indicates the corresponding task.</p><p>Please also note that we do not apply MTL finetuning to the pre-training task of AMR parsing. This is because the fine-tuning task is the same as the pre-training task. For example, for the pretrained model PTM-MT-SynPar-SemPar, in MTL fine-tuning we only keep the pre-training tasks of MT and syntactic parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimentation</head><p>In this section, we report the performance of our seq2seq pre-training approach to AMR parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Pre-training Dataset and Pre-trained Models For pre-trained models, we use the WMT14 English-to-German dataset 2 which consists of about 3.9M training sentence pairs after filtering out long and imbalanced pairs. To obtain syntactic parse trees for the source sentences, we utilize toolkit AllenNLP  which is trained on Penn Treebank <ref type="bibr" target="#b24">(Marcus et al., 1993)</ref>. To obtain AMR graphs for the source sentences, we utilize our baseline AMR parsing system. Then we merge English/German sentences and linearized parse trees, and AMR graphs together and segment all the tokens into subwords by byte pair encoding (BPE) <ref type="bibr" target="#b34">(Sennrich et al., 2016)</ref> with 20K operations.</p><p>We implement above pre-trained models based on OpenNMT-py <ref type="bibr" target="#b18">(Klein et al., 2017)</ref>. 3 For simplicity, we unify parameters of these models as the Transformer-base model in <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref>. The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the hidden size are 512 while the size of feedforward network is 2048. Moreover, we use Adam optimizer (Kingma and Ba, 2015) with β 1 of 0.9 and β 2 of 0.998. Warm up step, learning rate, dropout rate and label smoothing epsilon are 16000, 2.0, 0.1 and 0.1 respectively. In addition, we set the batch token-size to 8,192. We train the models for 300K steps and choose the model with the best performance on WMT2014 Englishto-German development set as the final pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMR Parsing Benchmarks</head><p>We evaluate AMR performance on AMR 1.0 (LDC2015E86) and AMR 2.0 (LDC2017T10). The two datasets contain 16,833 and 36,521 training AMRs, respectively, and share 1,368 development AMRs and 1,371 testing AMRs. All the source sentences and linearized AMRs are segmented into subwords by using the BPE trained for the pre-trained models.</p><p>To fine-tune the pre-trained models for AMR parsing, we follow the settings of hyper-parameters used for training pre-trained models.</p><p>Evaluation Metrics For evaluation purpose, we use the AMR-evaluation toolkit to evaluate parsing performance in Smatch and other fine-grained metrics <ref type="bibr" target="#b6">Damonte et al., 2017)</ref>. We report results of single models that are tuned on the development set. <ref type="table" target="#tab_2">Table 2</ref> presents the comparison of our approach and related studies on the test sets of AMR 1.0 and AMR 2.0. From the results, we have the following observations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>• Pre-trained models on a single task (i.e., from #2 to #6) significantly improve the performance of AMR parsing, indicating seq2seq pre-training is helpful for seq2seqbased AMR parsing. We also note that the pre-trained model of NMT achieves the best performance, followed by the pre-trained models on AMR parsing and on syntactic parsing. This indicates that seq2seq AMR parsing benefits more from pre-training tasks that require the encoder be able to capture the semantics from source sentences.</p><p>• Joint pre-trained models on two or more pre-training tasks further improve the performance of AMR parsing. However, in the presence of NMT pre-training task, the benefits from joint pre-training with either AMR parsing, syntactic parsing or both are shrunk.</p><p>• MTL fine-tuning consistently outperforms the vanilla fine-tuning method. For example, on single pre-training tasks, MTL outperforms vanilla fine-tuning by 1.5 ∼ 2.0 Smatch F1 scores while on joint pre-training tasks, the improvements of MTL over vanilla fine-tuning instead decrease.</p><p>• With twice training sentences in AMR 2.0, overall the performance on AMR 2.0 is higher than that on AMR 1.0. However, the gap between the performance on AMR 2.0 and AMR 1.0 gets smaller when we move from single pre-training models to joint pre-training models. For example, based on PTM-MT-SynPar-SemPar, the performance gap is 1.1 in Smatch F1 scores, much less than the performance gap 6.9 between their corresponding baselines.</p><p>• Finally, our approach achieves the best reported performance on AMR 1.0 and the performance on AMR 2.0 is higher than or close to that achieved by previous studies which use BERT. This is very encouraging taking into consideration the fact that our seq2seq model is much simper than the graph-based models proposed in related studies <ref type="bibr">(Zhang et al., 2019a,b;</ref><ref type="bibr" target="#b27">Naseem et al., 2019;</ref><ref type="bibr" target="#b2">Cai and Lam, 2020)</ref>. <ref type="table" target="#tab_3">Table 3</ref> compares the performance of our best system and the systems reported recently with finegrained metrics. We obtain the best performance for Reentrancies, NER, and SRL. Compared to the systems of Z'19a, Z'19b, and C'20, we achieve lower performance for Wiki and Negations. One possible reason for our relatively lower performance on Wiki and Negations is that unlike above three systems, in this paper we do not anonymize named entities and do not use an extra algorithm to add polarity attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Discussion</head><p>In this section, we conduct more analysis on AMR parsing with pre-trained models. In the following all the results are obtained on AMR 2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of BERT on Seq2Seq AMR Parsing</head><p>To explore the effect of BERT on seq2seq AMR parsing, motivated by <ref type="bibr" target="#b46">Zhu et al. (2020)</ref>, we use BERT in various ways to boost the performance of AMR parsing. Given an input sentence x = (x 1 , · · · , x n ) with n words, the BERT tokenizer segments it into a subword sequence x = (x 1 , · · · , x m ) with m    <ref type="bibr" target="#b44">Zhang et al. (2019b)</ref>, C'20 for <ref type="bibr" target="#b2">Cai and Lam (2020)</ref> subwords. Then BERT returns a hidden state se-</p><formula xml:id="formula_1">quence b = (b 1 , · · · , b m ) in shape R m×d BERT ,</formula><p>where d BERT is the size of BERT hidden states (e.g., d BERT =768 in our experiment). <ref type="figure">Figure 4</ref> illustrates the process of obtaining BERT hidden states for an input sentence. Next we use the following methods to properly incorporate BERT hidden states b into Transformer-based AMR parsing.</p><p>• BERT as embedding, which uses f bW B as input of the the Transformer encoder, where W B ∈ R d BERT ×d are model parameters to be learned, d is the model size for seq2seq AMR parsing, and f is the activation function ReLu.</p><p>• BERT as encoder, which uses f bW B as the output of the Transformer encoder. That is to say, we replace the Transformer encoder with BERT.</p><p>• BERT as extra feature, which views b as extra features for an input sentence x . The input of the Transformer encoder is defined as f [b, (Emb (x ) + P os (x ))]W E , where [·, ·] represents the operation of concatenation, Emb (x ) and P os (x ) return the word embeddings and position embeddings of x respectively, and W E ∈ R (d+d BERT )×d are model parameters to be learned.</p><p>• BERT as extra encoder, which adds a sublayer, i.e, BERT-context-attention sub-layer, in the Transformer decoder after the maskedself-attention sub-layer and the contextattention sub-layer. The BERT-contextattention sub-layer works in a similar way as the context-attention sub-layer by attending to BERT hidden states f bW B .  Meanwhile, we also provide another Transformer-based baseline in which we segment input sentences into subwords with the BERT tokenizer. For all above experiments, the source-side vocabulary is the set of subwords in training sentences segmented by the BERT tokenizer while the target-side vocabulary is the set of subwords in training AMRs segmented by BPE mentioned in Section 4.1. <ref type="table" target="#tab_5">Table 4</ref> compares the performance of AMR parsing when incorporating BERT in various methods. By comparing the performance of #1 in <ref type="table" target="#tab_5">Table 4</ref> against the baseline #1 in <ref type="table" target="#tab_2">Table 2</ref>, we observe a drop of Smatch F1 score from 71.5 to 70.0, indicating that it is important to share vocabulary for seq2seq AMR parsing. Based on the baseline of not sharing vocabulary, the four different methods of incorporating BERT result in very different performance ranging from 71.5 to 75.2 in Smatch F1 score. Among them, incorporating BERT as embedding or extra feature achieves similar performance, which is much higher than the performance of incorporating BERT as either encoder or extra encoder. This suggests that rather than straightly feeding BERT hidden states into a decoder, it is important to feed them into an encoder first. However, our pre-trained seq2seq models, even on a single pre-training task (i.e., #3, #5, #6) outperform using BERT, indicating the effectiveness of pre-trained seq2seq models for AMR parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Training Data Sizes on</head><p>Pre-training Models</p><p>In this section we investigate the impact of the size of pre-training data to check whether AMR parsing benefits more from pre-trained models that are trained on larger datasets. To this end, we randomly use 20%, 40%, 60%, and 80% of the full pre-training instances to train the pre-trained models, respectively. As shown in <ref type="figure">Figure 5</ref>, except syntactic pars-  ing (i.e., PTM-SynPar), the pre-training models on the other three kinds of pre-training tasks achieve higher AMR parsing performance with the increasing of training data sizes. Based on the learning curve, we suspect there still exists much room for further improvements if we enlarge the training data of pre-training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Different Pre-Training Components on Seq2Seq AMR Parsing</head><p>When adapt a pre-trained model to AMR parsing, we initialize the whole seq2seq Transformer model of AMR parsing with the counterpart of the pretrained model. However, it is unveiled what part of initialization contributes most. To this end, we decompose the whole seq2seq model into three components, i.e., (shared) word embedding, encoder and decoder. The three components take account of 31.1%, 29.5% and 39.4% of parameters, respectively. Then we do ablation study by accumulating the initialization using the pre-trained model while the other components will be randomly initialized.</p><p>We use the PTM-MT-SynPar-SemPar pretrained model as representative (i.e., #14 in Table 2).  bedding, we substantially boost the performance from 71.5 in Smatch F1 score to 78.4 while initializing the other two components with the pre-trained model leads to another 1.8 improvement in Smatch F1 score (i.e., from 78.4 to 80.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of Pre-trained Models Trained on Different Datasets</head><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, the pre-trained model of PTM-SynPar (or PTM-SemPar) significantly improves the performance AMR parsing from 71.5 to 75.3 (or 77.9) in Smatch F1 score. However, in the presence of PTM-MT, joint pre-training with either PTM-SynPar, PTM-SemPar, or both gives another up to 1.0 improvement, suggesting that complementarity among the pre-trained models does exist but is relatively limited. We suspect that the overlapping is mainly due to the fact that we pre-train these models on the same source-side dataset. We conjecture that more improvement is potentially reachable if the pre-training tasks are trained on different datasets. To test the conjecture, we construct another silver dataset for both syntactic parsing and AMR parsing that is in the same size (i.e., 3.9M) as before. This is done by randomly selecting 3.9M English sentences from WMT14 English monolingual language model training data. 4 <ref type="table" target="#tab_9">Table 6</ref> compares the Smatch F1 scores. From it, we observe consistent improvement if the pre-trained models are jointly trained on different datasets. For example, by replacing the pre-training dataset of AMR parsing with the new constructed dataset, we improve AMR parsing from 80.1 in Smatch F1 score to 81.4. This suggests that assigning different pre-training tasks with different datasets improves the performance of AMR parsing.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of Different Bilingual Datasets</head><p>For the pre-training task of machine translation, we have chosen English-to-German (EN-DE) with 3.9M sentence pairs. However, it is still unclear whether it is critical to choose the right language pair. To this end, we move to WMT14 Englilsh-to-French (EN-FR) translation and randomly select 3.9M sentence pairs from its training dataset, as the same size of EN-DE translation. <ref type="table" target="#tab_10">Table 7</ref> compares the Smatch F1 scores when the pre-trained models are trained on different bilingual datasets. From it, we observe that pre-training on EN-FR dataset achieves even slight higher performance than that on EN-DE dataset. This further confirms our finding that AMR parsing can greatly benefit from machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>We describe related work from two perspectives: pre-training and AMR parsing.</p><p>Pre-training. Pre-training a universal model and then fine-tuning the model on a downstream task have recently become a popular strategy in the field of natural language processing. Previous works on pre-training can be roughly grouped into two categories. One category of approaches is to learn static word embeddings such as word2vec <ref type="bibr" target="#b26">(Mikolov et al., 2013)</ref> and GloVe <ref type="bibr" target="#b30">(Pennington et al., 2014)</ref> while the other group builds dynamic pre-trained models that would also be used in downstream tasks. Representative examples in the latter group in-clude Dai and Le <ref type="formula">(2015)</ref>, <ref type="bibr">CoVe (McCann et al., 2017)</ref>, ELMo <ref type="bibr" target="#b8">Edunov et al., 2019)</ref>, OpenAI GPT <ref type="bibr" target="#b32">(Radford et al., 2018)</ref>, and BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>. Besides the aforementioned encoder-only (e.g., BERT) or decoderonly (e.g., GPT) pre-training approaches, recent studies also propose approaches to pre-training seq2seq models, such as MASS <ref type="bibr" target="#b36">(Song et al., 2019)</ref>, PoDA <ref type="bibr" target="#b40">(Wang et al., 2019)</ref>, PEGASUS <ref type="bibr" target="#b42">(Zhang et al., 2020)</ref>, BART <ref type="bibr" target="#b20">(Lewis et al., 2020)</ref>, and T5 <ref type="bibr" target="#b33">(Raffel et al., 2020)</ref>.</p><p>AMR Parsing. As a semantic parsing task that translates texts into AMR graphs, AMR parsing has received much attention in recent years. Diverse approaches have been applied to the task. <ref type="bibr" target="#b9">Flanigan et al. (2014)</ref> pioneer the research work on AMR parsing by using a a two-stage approach: node identification followed by relation recognition. <ref type="bibr" target="#b41">Werling et al. (2015)</ref> improve the first stage in the parser of <ref type="bibr" target="#b9">Flanigan et al. (2014)</ref> by generating subgraph aligned to lexical items. To avoid conducting AMR parsing from scratch, <ref type="bibr" target="#b39">Wang et al. (2015b)</ref> propose to obtain AMR graphs from dependency trees by using a transition-based method. <ref type="bibr" target="#b38">Wang et al. (2015a)</ref> extend their previous work by introducing a new transition action to get better performance. <ref type="bibr" target="#b6">Damonte et al. (2017)</ref> propose a complete transition-based approach that parses sentences left-to-right in linear time. The recent neural AMR parsing could be roughly grouped into two categories. On the one hand, the generic seq2seq-based approaches have been widely used for AMR parsing which show competitive performance <ref type="bibr" target="#b29">(Peng et al., 2017;</ref><ref type="bibr" target="#b28">van Noord and Bos, 2017;</ref><ref type="bibr" target="#b19">Konstas et al., 2017;</ref><ref type="bibr" target="#b11">Ge et al., 2019)</ref>. On the other hand, to better model the graph structure on the target side, graph-based models are well studies for AMR parsing which achieve the state-of-theart-performance <ref type="bibr" target="#b23">(Lyu and Titov, 2018;</ref><ref type="bibr" target="#b14">Guo and Lu, 2018;</ref><ref type="bibr" target="#b13">Groschwitz et al., 2018;</ref><ref type="bibr">Zhang et al., 2019a,b;</ref><ref type="bibr" target="#b2">Cai and Lam, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we proposed a seq2seq-based pretraining approach to improving the performance of seq2seq-based AMR parsing. To this end, we designed three relevant seq2seq learning tasks, including machine translation, syntactic parsing, and AMR parsing itself. Then we built seq2seq pre-trained models through either single or joint pre-training tasks. Detail experimentation shows that both the single and joint pre-trained models substantially improve our baseline and the performance reaches the state of the art. The accomplishment is encouraging since we achieve this simply by using the generic seq2seq framework rather than complex models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the most important trade partner of Europe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(Figure 2 :</head><label>2</label><figDesc>S ( NP ( NNS Children ) ) ( VP ( NN flock ) ( PP ( IN to ) ( NP ( JJ social ) ( NNS networks ) ) A linearization example of the parse tree for the sentence of Children flock to social networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the joint pre-training approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Illustration of obtaining BERT hidden states for an given sentence. Learning curve over the number of training sentences in pre-training datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Smatch scores on the test sets of AMR 1.0 and AMR 2.0. † is for using BERT as extra resource while * for using other resources.</figDesc><table><row><cell>Metric</cell><cell cols="2">C'19 G'19 N'19 Z'19a Z'19b C'20 Our</cell></row><row><cell>Smatch</cell><cell>73.2 74.3 75.5 76.3</cell><cell>77 80.2 80.2</cell></row><row><cell cols="2">Unlabeled 77.0 77.3 80 79.0</cell><cell>80 82.8 83.7</cell></row><row><cell cols="2">No WSD 74.2 74.8 76 76.8</cell><cell>78 80.8 80.8</cell></row><row><cell cols="2">Reentrancy 55.3 58.3 56 60.0</cell><cell>61 64.6 66.5</cell></row><row><cell cols="2">Concepts 84.4 84.2 86 84.8</cell><cell>86 88.1 87.4</cell></row><row><cell>NER</cell><cell>82.0 82.4 83 77.9</cell><cell>79 81.1 85.4</cell></row><row><cell>Wiki</cell><cell>73.2 71.3 80 85.8</cell><cell>86 86.3 75.1</cell></row><row><cell cols="2">Negations 62.9 64.0 67 75.2</cell><cell>77 78.9 71.5</cell></row><row><cell>SRL</cell><cell>66.7 70.4 72 69.7</cell><cell>71 74.2 78.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Detailed F1 scores on AMR 2.0 test set. Here,</cell></row><row><cell>C'19 is for Cai and Lam (2019), G'19 for Ge et al.</cell></row><row><cell>(2019), N'19 for Naseem et al. (2019), Z'19 for Zhang</cell></row><row><cell>et al. (2019a), Z'19b for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Smatch scores on AMR 2.0 when incorporate BERT in various methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>presents the performance. From the table, we observe that with well-learned word em-</figDesc><table><row><cell>Pre-trained Initialization</cell><cell>P.</cell><cell>R.</cell><cell>F1</cell></row><row><cell>None</cell><cell cols="3">75.8 67.7 71.5</cell></row><row><cell>Embedding</cell><cell cols="3">80.7 76.3 78.4</cell></row><row><cell>Embedding + Encoder</cell><cell cols="3">81.3 77.2 79.2</cell></row><row><cell>Embedding + Decoder</cell><cell cols="3">80.7 76.5 78.5</cell></row><row><cell>All</cell><cell cols="3">82.3 78.3 80.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Smatch F1 scores on the test sets of AMR2.0 when initialize different components of seq2seq model with a pre-trained model. Here we use MTL as finetuning method.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">: Smatch F1 scores on the test set of AMR</cell></row><row><cell cols="2">2.0 when the pre-training tasks are trained on differ-</cell></row><row><cell cols="2">ent datasets. Here WMT14B is for WMT14 English-</cell></row><row><cell cols="2">to-German dataset while WMT14M is for WMT14 En-</cell></row><row><cell>glish monolingual dataset.</cell><cell></cell></row><row><cell># Pre-trained Model</cell><cell>F1</cell></row><row><cell>1 PTM-MT on EN-DE 2</cell><cell>Vanilla 77.1 MTL 79.1</cell></row><row><cell>3 PTM-MT on EN-FR 4</cell><cell>Vanilla 77.5 MTL 79.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Smatch F1 scores on the test set of AMR 2.0 when the pre-training tasks are trained on different bilingual dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.statmt.org/wmt14/ translation-task.html 3 https://github.com/OpenNMT/OpenNMT-py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://statmt.org/wmt14/ training-monolingual-news-crawl/news. 2008.en.shuffled.gz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments and suggestions. This work is supported by the National Natural Science Foundation of China (Grant No. 61876120).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Core semantic first: A top-down approach for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3799" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AMR parsing via graph sequence iterative inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1290" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An incremental parser for abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="536" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pre-trained language model representations for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4052" to="4059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AllenNLP: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop for Natural Language Processing Open Source Software</title>
		<meeting>ACL Workshop for Natural Language Processing Open Source Software</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling source syntax and semantics for neural AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4975" to="4981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Assessing BERT&apos;s syntactic abilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05287</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AMR dependency parsing with a typed semantic algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meaghan</forename><surname>Fowlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1831" to="1841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Better transitionbased AMR parsing with a refined search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1712" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zeroshot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Nikhil Thorat, and Fernanda Viégas</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">OpenNMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL, System Demonstrations</title>
		<meeting>ACL, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural AMR: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="688" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AMR parsing as graph prediction with latent alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rewarding smatch: Transition-based AMR parsing with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4586" to="4592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural semantic parsing by character-based translation: Experiments with abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics in the Netherlands Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Addressing the data sparsity issue in neural AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Does string-based neural MT learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Boosting transition-based AMR parsing with refined actions and auxiliary analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A transition-based algorithm for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Denoising based sequence-tosequence pre-training for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4003" to="4015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust subgraph generation improves abstract meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenon</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoerpher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">AMR parsing as sequence-to-graph transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="80" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Broad-coverage semantic parsing as transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3786" to="3798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling graph structure in transformer for better amr-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5459" to="5468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Incorporating BERT into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
