<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Calibrated neighborhood aware confidence measure for deep metric learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-08">8 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryna</forename><forename type="middle">Karpusha</forename><surname>Amazon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghee</forename><surname>Yun</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istvan</forename><forename type="middle">Fehervari</forename><surname>Amazon</surname></persName>
							<email>istvanfe@amazon.com</email>
						</author>
						<title level="a" type="main">Calibrated neighborhood aware confidence measure for deep metric learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-08">8 Jun 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep metric learning has gained promising improvement in recent years following the success of deep learning. It has been successfully applied to problems in fewshot learning, image retrieval, and open-set classifications. However, measuring the confidence of a deep metric learning model and identifying unreliable predictions is still an open challenge. This paper focuses on defining a calibrated and interpretable confidence metric that closely reflects its classification accuracy. While performing similarity comparison directly in the latent space using the learned distance metric, our approach approximates the distribution of data points for each class using a Gaussian kernel smoothing function. The post-processing calibration algorithm with proposed confidence metric on the held-out validation dataset improves generalization and robustness of state-of-the-art deep metric learning models while provides an interpretable estimation of the confidence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep distance metric learning (DDML) aims to learn a deep learning model that maps arbitrary groups of data to a high-dimensional vector embedding space such that the representations of semantically similar items of the same class are closer than the representations of dissimilar items. Such models are applied for tasks such as near-duplicate detection <ref type="bibr" target="#b66">[67]</ref>, feature-based retrieval <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56]</ref>, clustering <ref type="bibr" target="#b21">[22]</ref>, visual search <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b65">66]</ref>, etc. The advantage of DDML is that it can be used for challenging extreme multi-label classification problems where the data between classes are unbalanced, scarce, or the number of classes is very large <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b63">64]</ref>. In contrast to the traditional classification approach, the goal of DDML is to learn the general concept of similarity between data items opposite to class-specific features. As a result, the trained model can generalize to new classes without retraining. When the DDML model is trained, the class of a query example is predicted based on the distances to samples with known labels. A set of examples with known labels used for prediction is known as support or gallery set. However, such approach does not provide a notion of model confidence in the prediction, and the model lacks of easily computable confidence measure that correlates well with the accuracy of the model. Moreover, DDML inherits the disadvantages from its underlying deep neural architecture. Deep neural networks are shown to be sensitive to small input perturbations. For example, natural changes in the data distribution like noise, blurring, and JPEG compression can lead to a significant decrease in the model performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref>. Consequently, one can even engineer such adversarial samples that are imperceptible to human observers, yet completely fool the model <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>To address these problems, different confidence scores for deep learning models have been proposed <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b26">27]</ref>. However, they are either known to have the significant computational overhead or not applicable in metric learning settings. To overcome these limitations, we propose an approach to approximate the true correctness likelihood that can leverage learned spatial relations of similar and dissimilar items in the embedding space. In particular, we propose a novel confidence score called NED that calculates the Normalized sum of Exponential of the Distances to the nearest neighbors in the embedding space. We provide theoretical evidence derived from the Bayes' theorem that NED approximates the distribution of data points for each class by a Gaussian kernel smoothing function and calculates the conditional probability of a requested point belonging to a specific class. Similar to temperature scaling used to calibrate confidence in classification networks with softmax loss <ref type="bibr" target="#b16">[17]</ref>, our approach uses normalization to improve probability estimates of the true correctness likelihood. Only a single parameter is fitted to the held-out validation data. Therefore, unlike fitting the original neural network, the algorithm comes with generalization guarantees based on traditional statistical learning theory.</p><p>Even though in this paper we focus on the task of visual object classification as it is easier to analyze, our approach can be extended to other modalities of deep metric learning such as speech recognition <ref type="bibr" target="#b51">[52]</ref> or natural language processing <ref type="bibr" target="#b58">[59]</ref>.</p><p>The main contributions of this paper are the following:</p><p>• We propose a novel confidence score called NED that enables the computation of reliable and interpretable confidence scores in the deep metric learning setting. We are mathematically motivated to use the proposed NED algorithm as an approximation of the data distribution for each class by a Gaussian kernel smoothing function. • We present state-of-the-art results with our approach on four popular benchmark datasets Caltech-UCSD Birds, Stanford Online Product, Stanford Car-196, and In-shop Clothes Retrieval using different state-of-the-art DDML models. We show that the robustness of DDML models with the proposed approach can be improved. The proposed NED confidence score improves model generalization to adversarial examples and more natural distortions in the data.</p><p>1 Background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Deep Distance Metric Learning</head><p>Suppose a supervised training on given N independent and identically distributed (i.i.d.) instances {(x i , y i )} N i=1 with x i ∈ R n and y i ∈ C from unknown joint distribution of x and y where n is the dimension of the input space and C = {c 1 , . . . , c M } is the set of the labels in the training set. The goal of DDML training is to learn a mapping function f : R n → R m such that distances between sample pairs belonging to the same class in the embedding space are smaller than those between sample pairs belonging to the different classes. To quantify the distance we define a function d : R m × R m → R + representing the distance in the embedding space where m is the dimension of the embedding space. Then the mapping z = f (x) with deep neural network model should satisfy the following relation:</p><formula xml:id="formula_0">d(z i , z j ) d(z k , z l )<label>(1)</label></formula><p>for all 1 ≤ i, j, k, l ≤ N such that y i = y j and y k = y l . Euclidean distance or cosine distance is typically used for d in DDML models.</p><p>The performance of this mapping is measured by some loss function L : R m × R m → R + . Early approaches were based on contrastive loss <ref type="bibr" target="#b4">[5]</ref> and triplet loss <ref type="bibr" target="#b22">[23]</ref> where the loss function is defined on pairs or triplets of samples in order to minimize intra-class distances and maximize inter-class distances. Since computing the loss on every possible pair or triplet is intractable, recent approaches propose either better sampling strategies <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b12">13]</ref> or loss functions that consider the relationship of all samples within the training batch. Parametric models have also been proposed with the idea of storing some information about the global context <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b54">55]</ref>. Several model ensembles have also been explored, focusing on improving classification and retrieval performance using boosting <ref type="bibr" target="#b37">[38]</ref> or attending diverse spatial locations <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Confidence in Deep Learning</head><p>In optimization theory, a model is defined robust if it can perform well under a certain level of uncertainty <ref type="bibr" target="#b2">[3]</ref>. However, it has been shown that neural networks are susceptible to small intentional or unintentional shifts in the data distribution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15]</ref>. In real-world decision-making systems, it is important to indicate whether or not the prediction is reliable. To achieve this, given a requested point x ∈ R n , we associate the confidence estimatep(x) for every class predictionŷ(x) whereŷ : R n → C andp : R n → [0, 1]. The confidence estimatep(x) is said to be calibrated if it represents the true probability of the correctness p <ref type="bibr" target="#b16">[17]</ref>.</p><p>As deep neural networks are shown to provide overconfident predictions, multiple post-processing steps were proposed to produce calibrated confidence measures.</p><p>Calibration on the held-out validation data. Different parametric and non-parametric approaches where the logits are used as features to learn a calibration model from a held-out validation data have been proposed <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>. Simple a single-parameter variant of Platt scaling <ref type="bibr" target="#b42">[43]</ref>, known as temperature scaling, is often an effective method at obtaining calibrated probabilities <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>. The key insight is that in the temperature scaling approach, only a single parameter is fit to the validation data. Therefore, unlike fitting the original neural network, the temperature scaling algorithm comes with generalization guarantees based on traditional statistical learning theory.</p><p>Bayesian approximation. Bayesian deep neural networks evaluate distributions over the models or their parameters. This approach proposes more accurate and tractable approximation of the uncertainty, but at the cost of expensive computation during training and inference <ref type="bibr" target="#b10">[11]</ref>. Some recent alternatives have been proposed to approximate predictive uncertainty. For example, Gal and Ghahramani propose considering dropout as a way of ensembling, which approximates Bayesian inference in deep Gaussian processes <ref type="bibr" target="#b11">[12]</ref>. Unfortunately, empirical results show that one needs to run inference at least 100 times to achieve accurate approximation, which can be infeasible in practice.</p><p>Support set based uncertainty estimation. Papernot et al. <ref type="bibr" target="#b9">[10]</ref> introduce a trust score that measures the conformance between the classier and k-nearest neighbors on the support example. It enhances robustness to adversarial attacks and leads to better calibrated uncertainty estimates <ref type="bibr" target="#b23">[24]</ref>. Jiang et al.develop this idea further and compute the trust score on deeper layers of DNN than the input to avoid the high-dimensionality of inputs <ref type="bibr" target="#b27">[28]</ref>.</p><p>The proposed NED algorithm described in the next Section 2 can be considered as calibration on the support set. The algorithm is designed specifically for DDML models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NED Algorithm</head><p>Our goal is to find predictionŷ and confidencep for requested point x from test data given support</p><formula xml:id="formula_1">set {(x i , y i )} N support i=1</formula><p>and learned mapping function z = f (x). The classes between train and support sets can be disjoint if samples share same learned similarity concept. C support = {c 1 , . . . , c M support } is the set of labels in support and test sets.</p><p>Since metric learning enables a similarity comparison directly in the embedding space using distance metric, the simplest way to obtain a predictionŷ is to compare z = f (x) to a set of samples from support set in the embedding space and pick the class of the nearest example. Then intuitively, we can assume that the magnitude of this distance can represent the confidence in the prediction. However, such a distance is an unbounded positive value, and we need to calibrate it to obtain an interpretable probability estimate of the true correctness likelihood. Furthermore, outliers in the support set can easily lead to misclassification since multiple neighbors are not considered.</p><p>To overcome this difficulty, we propose the NED confidence score. To show that it very closely approximates the true correctness likelihood, we first propose the following theorem. The proof can be found in Appendix A.</p><formula xml:id="formula_2">Theorem 1. Given embedding f : R n → R m and support set {(x i , y i )} N support i=1</formula><p>where x i ∈ R n and y i ∈ C support the probability p of x ∈ R n belonging to c j ∈ C support , i.e., y = c j , can be approximate by:p</p><formula xml:id="formula_3">(y ∈ c j ) ∝ ∑ N support i=1 exp − z − z i 2 2 /T I[y i ∈ c j ] ∑ N support i=1 exp − z − z i 2 2 /T .<label>(2)</label></formula><p>where z i = f (x i ) for i = 1, . . . , N support and T &gt; 0 is a parameter to be tuned.</p><p>When T = 1, the equation in <ref type="formula" target="#formula_3">(2)</ref> is a simple softmax function of the negatives of squares of the Euclidean distances. However, similar to the bandwidth selection in kernel density estimation <ref type="bibr" target="#b1">[2]</ref>, we can control the degree of smoothing applied to the samples using T . For example, as T becomes larger, the relative influence of near samples becomes smaller. On the other hand, if T becomes smaller, the relative influence of near samples becomes larger. Therefore the optimizing of T is equivalent to finding T which provides the best relative distances in terms of probability estimate for the true correctness likelihood.</p><p>We optimize T with respect to the negative log likelihood on the support set instead of optimizing on held-out validation dataset. Because the parameter T does not change the relative magnitudes of the softmax function, we can preserve the spatial order of the nearest neighbors to the requested point x in the embedding space.</p><formula xml:id="formula_4">Suppose that we chose {x i } k i=1 nearest neighbors with labels {y i } k i=1</formula><p>for the confidence score calculation. Because the rest of the data points are far enough from x in the embedding space,</p><formula xml:id="formula_5">exp − z − z i 2 2 /T ≈ 0 for i &gt; k, thus the probability in (2) becomeŝ p(y ∈ c j ) = ∑ k i=1 exp − z − z i 2 2 /T I[y i ∈ c j ] ∑ k i=1 exp − z − z i 2 2 /T .<label>(3)</label></formula><p>The equation in <ref type="formula" target="#formula_5">(3)</ref> defines the NED confidence score. Below we show the precise NED algorithm description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: NED algorithm</head><formula xml:id="formula_6">Inputs :query point x, DDML model z = f (x), support set {(x i , y i )} N support i=1</formula><p>, optimal T . Output :predictionŷ with estimated confidencep.</p><formula xml:id="formula_7">1 {z i } N support i=1 = { f (x i )} N support i=1</formula><p>{Obtain embedding vectors for samples in support set};</p><formula xml:id="formula_8">2 z = f (x) {Obtain embedding vector for query sample x}; 3 find k nearest neighbors {z i } k i=1 and their labels {y i } k i=1 ; 4 for j ← 1 to M support do 5p(y = y j ) = ∑ k i=1 exp(− z−z i 2 2 )/T )I[yi=yj] ∑ k i=1 exp(− z−z i 2 2 )/T ) ; 6 end 7p = max 1≤ j≤M supportp (y = y j ); 8ŷ = y j * where j * = arg max 1≤ j≤M supportp (y = y j ); 9 returnŷ,p</formula><p>As we show in section 3, one of the advantages of using such a post-processing algorithm is that it can be used to improve generalization and robustness of already trained DDML models. Moreover, it can be combined with known defenses like adversarial training <ref type="bibr" target="#b14">[15]</ref> or adversarial logit pairing <ref type="bibr" target="#b50">[51]</ref> to improve further adversarial robustness. Mao et al. proposes adversarial training specially modified with deep metric learning settings <ref type="bibr" target="#b33">[34]</ref>. By carefully sampling examples for metric learning, the learned representation increases robustness to adversarial examples and help detect previously unseen adversarial samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Comparison with k-nearest neighbor (kNN) and weighted kNN algorithms</head><p>Non-parametric classifiers like k-nearest neighbors (kNN) and weighted k-nearest neighbors (WkNN) can also be used to improve the classification performance for already trained DMML models. However, as we show in section 3, they provide non-calibrated confidence estimation. We use kNN and WkNN as baselines to compare with the proposed NED algorithm.</p><p>The kNN classifier is a simple non-parametric classifier that predicts the label of an input based on a majority vote from labels of the k neighbors in the embedding space. Intuitively, the confidence score for every class can be selected as the percentage of nearest neighbors labels belonging to c j class:</p><formula xml:id="formula_9">p(y ∈ c j ) = 1 k k ∑ i=1 I[y i ∈ c j ]<label>(4)</label></formula><p>The robustness of kNN has already been shown from both theoretical perspectives and empirical analyses <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">44]</ref>. However, the main disadvantage of kNN is that its reliability depends critically on the value of k. Therefore many weighted k-nearest neighbor (WkNN) approaches have been proposed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref> where the closer neighbors are weighted more heavily than the farther ones. We have experimented with various weighted approaches and achieved the most reliable and calibrated estimation of the confidence score using an approach described in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b15">[16]</ref>.</p><p>In this case, if the distance-weighted function w i : R n → R + is defined, the confidence score can be selected as the weighted percentage of nearest neighbors belonging to c j class:</p><formula xml:id="formula_10">p(y ∈ c j ) = ∑ k i=1 w i I[y i ∈ c j ] ∑ k i=1 w i .<label>(5)</label></formula><p>For these methods, the weights w i are linear functions of the distance between z and z i , i.e., in the embedding space.</p><p>To compare the performance of NED algorithm with kNN and WkNN algorithm, we use equations <ref type="formula" target="#formula_9">(4)</ref> and <ref type="formula" target="#formula_10">(5)</ref> to calculatep in algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>We consider the following scenarios to evaluate the performance of the proposed NED algorithm.</p><p>First, we train the state-of-the-art DDML model with the normalized, temperature-weighted version of the cross-entropy loss following the protocol described in <ref type="bibr" target="#b62">[63]</ref>. Minimizing the cross-entropy can be seen as an approximate bound-optimization algorithm for minimizing many popular in DDML pairwise losses <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref>. Therefore, we use this approach as a baseline that gives state-of-the-art results with simpler hyperparameter and sampling strategy. Empirical experiments with other popular DDML approaches can be found in Appendix B. When DDML model is trained, we evaluate performance of the model complemented with Algorithm 1.</p><p>Second, we empirically evaluate the robustness of our approach in the presence of the distribution shift in test data. In particular, we repeat our experiments when test data contains small common distortions (image transformations related to JPEG compressions, different illumination conditions, camera quality) or when test images are modified using adversarial white-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metrics</head><p>For all experiments, we use the following metrics:</p><p>Accuracy. We use an accuracy metric to evaluate reliability for challenging extreme multi-class classification problems. Classification accuracy is equivalent to the Recall@1 metric in image retrieval <ref type="bibr" target="#b46">[47]</ref>.</p><p>Reliability Diagrams. A reliability diagram such as the one shown in <ref type="figure">Figure 1</ref> is a visual representation of confidence metric calibration <ref type="bibr" target="#b5">[6]</ref>. They show the relationship between expected accuracy and confidence estimation. To estimate the expected accuracy from finite samples, we split test data into M bins. For each bin, the mean predicted confidence score is plotted against the true fraction of positive cases. Both metrics should be near the diagonal line if the model is well calibrated.</p><p>Expected Calibration Error. While the reliability diagram is a useful visual representation method of confidence calibration, it does not show proportion of samples in each bin. Therefore, we use Expected Calibration Error (ECE) to evaluate calibration <ref type="bibr" target="#b16">[17]</ref> which is defined as:</p><formula xml:id="formula_11">ECE = 1 N M ∑ m=1 |B m ||acc(B m ) − conf(B m )|.<label>(6)</label></formula><p>where N is the number of samples in test dataset, M is the number of the bins, B m is the index set for the mth bin, and acc(B m ) and conf(B m ) are the accuracy and average confidence for the mth bin respectively which are defined as:</p><formula xml:id="formula_12">acc(B m ) = 1 |B m | ∑ i∈B m I[ŷ i ∈ c i ] and conf(B m ) = 1 |B m | ∑ i∈B mp (y i ∈ c i ).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>We conduct our experiments using DDML models on four benchmark datasets: Caltech-UCSD Birds (CUB-200) <ref type="bibr" target="#b49">[50]</ref>, Stanford Online Product (SOP) <ref type="bibr" target="#b46">[47]</ref>, Stanford Car-196 (CARS196) <ref type="bibr" target="#b29">[30]</ref> and In-shop Clothes Retrieval <ref type="bibr" target="#b31">[32]</ref>.</p><p>We follow the common evaluation protocol for these datasets <ref type="bibr" target="#b62">[63]</ref>. In particular, the object categories between train and test sets are disjoint. This split makes the problem more challenging since deep networks can overfit to the categories in the train set and generalization to unseen object categories could be poor. <ref type="table" target="#tab_0">Table 1</ref> shows the performance of DDML model complemented with NED algorithm, and the performance comparison with three baselines kNN, WkNN <ref type="bibr" target="#b8">[9]</ref>, and WkNN <ref type="bibr" target="#b15">[16]</ref>. We provide the accuracy reported in <ref type="bibr" target="#b62">[63]</ref> and the accuracy of our experiment for the case the label of the first nearest neighbor in the embedding space is used for prediction, i.e., 1NN. The difference in accuracy is caused by using different initialization parameters during training. The results presented in the  <ref type="bibr" target="#b8">[9]</ref>, and WkNN <ref type="bibr" target="#b15">[16]</ref>). The best results of each column are shown in boldface. The proposed approach consistently outperforms the baseline methods for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>While kNN and WkNN provide more reliable predictions, their confidences do not represent the interpretable confidence of predictions. The ECE is significantly higher than for the proposed NED approach (at least by 3% and at most by 31.2%).</p><p>The SOP dataset contains very scarce data (from 2 to 12 images per class). While the proposed approach does not improve classification accuracy compare to baselines, it provides better calibrated confidences. The ECE of the proposed approach is 2.2% whereas that of WkNN is 5.2%. This difference demonstrates that NED can be used for uncertainty estimation to detect cases when the model most probably misclassifies for even such scarce dataset like SOP. <ref type="figure">Figure 1</ref> shows reliability diagrams for CARS dataset. We see that the kNN, WkNN <ref type="bibr" target="#b8">[9]</ref>, and WkNN <ref type="bibr" target="#b8">[9]</ref> tend to be overconfident in its predictions. On the other hand, NED algorithm produces much better confidence estimation at the cost of tuning a single parameter T . Also, all the bins are well calibrated by NED algorithm.</p><p>We analyze the impact of value of k on the effectiveness of the proposed algorithm. <ref type="figure" target="#fig_1">Figure 2</ref> shows the accuracy of the DDML model with different approaches based on the number of neighbors, k, used to detect the predicted label. Since the kNN method weights data points far from z with the same importance as those close to z, its performance degrades with a larger k. Both variants of WkNN are better than kNN because they consider the distances between z and the neighbors {z j } k j=1 . However, the weights they impose on samples are linear functions of the distances whereas Theorem 1 shows that the correct weights should be exponential functions of the negative of the squared distance, i.e.,  <ref type="bibr" target="#b8">[9]</ref> (c) WkNN <ref type="bibr" target="#b15">[16]</ref> (d) NED <ref type="figure">Figure 1</ref>: Reliability diagrams for DDML model complemented with different approaches to estimate confidence of classification (CARS196 dataset). Our novel NED algorithm provides more accurate estimation of true correctness likelihood with the smallest expected calibration error (ECE). exp( z − z i 2 2 /T ), hence should rapidly decrease as the distance increases. This is why both kNN and WkNN show poor performance for large ks.</p><p>On the other hand, the performance of NED algorithm monotonically improves as k grows. This is due to Theorem 1, i.e., (2) is the accurate value for the correctness likelihood and (3) is a better approximation for (2) with a larger k. Therefore, the performance of the NED algorithm should yield better results with larger ks. <ref type="figure" target="#fig_1">Figure 2</ref> also shows a critical advantage of NED algorithm over the other methods. The accuracy of NED algorithm does not vary much after a certain point. Therefore NED is robust to the choice of k, which reduces the efforts of choosing k considerably whereas the other methods are sensitive to the choice of k requiring much effort to find the optimal value for k.</p><p>This can also be explained by Theorem 1. In (3), we can see exp(− z − z i 2 2 )/T ) become negligible for those points far from z in the embedding space, hence the additional accuracy gain obtained by increasing k rapidly diminishes after a certain point. In Appendix A we also provide the detailed interpretation of parameter tuning for T . Tuning T corresponds to tuning the smoothing factor in the Gaussian kernel function. Hence we can interpret the tuning of T as finding a better estimate for the true probability distribution function of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Distorted and Adversarial Images</head><p>To evaluate the robustness of the proposed algorithm, we modified images in the test set with fifteen different types of corruption and five severity levels following the protocol described in <ref type="bibr" target="#b19">[20]</ref>. Note that the support set was not altered and the value of T is the same as that used for the former experiments without distortions. <ref type="table" target="#tab_2">Table 2</ref> shows the average accuracy and average ECE for DDML model completed with proposed NED algorithm and the four baselines <ref type="figure">(1NN, kNN, WkNN [9]</ref>, and WkNN <ref type="bibr" target="#b15">[16]</ref>). The results show that, on average, the model with the NED algorithm is more robust than the baselines when considering the types of image distortions. In this case, the difference between the accuracy of model with NED algorithm and that of 1NN is even higher compared to the results obtained on clean images, i.e., the average accuracy for the model with NED algorithm is higher than that of 1NN by at least 1.6% and at most 12%.   <ref type="bibr" target="#b8">[9]</ref>, and WkNN <ref type="bibr" target="#b15">[16]</ref>) when images in test data are distorted with common corruption types <ref type="bibr" target="#b19">[20]</ref>. The proposed approach consistently outperforms the baseline methods for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB</head><p>To further examine the robustness of the models with the NED algorithm, we evaluate its performance on the adversarial examples. We craft adversarial samples using three popular white-box L ∞ bounded untargeted attacks: Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b14">[15]</ref>, Basic Iterative Method (BIM) <ref type="bibr" target="#b30">[31]</ref>, and Projected Gradient Descent (PGD) <ref type="bibr" target="#b32">[33]</ref>. We generate adversarial examples using PGD with 0.01 step size for 40 steps and using BIM with 0.02 step size for 10 steps during the training. <ref type="table" target="#tab_4">Table 3</ref> shows the performance of DDML model with NED algorithm and the four baselines (1NN, kNN, WkNN <ref type="bibr" target="#b8">[9]</ref>, and WkNN <ref type="bibr" target="#b15">[16]</ref>) for CARS dataset. The proposed approach consistently outperforms baseline methods for all experiments.  </p><formula xml:id="formula_13">FGSM BIM PGD ε = 0.1 ε = 0.3 ε = 0.1 ε = 0.3 ε = 0.1 ε = 0.3 A. E. A. E. A. E. A. E. A. E. A. E.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we described a novel algorithm called NED that can be used with pre-trained DDML models to improve the classification results while providing accurate approximation of true correctness likelihood. We demonstrate the consistent performance improvement over the other baseline methods for examples with normal data distribution, those with distribution shifts related to common image corruptions, and the adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The proof of Theorem 1</head><p>In this appendix, we show that with the NED algorithm we can provide a good approximation of the true correctness likelihood assuming that the empirical distribution function obtained using the Gaussian kernel smoothing function well estimates the true distribution of data in each class. Therefore we can not only show by experiments that NED score outperforms other methods, such as experiments with 1NN, kNN, WkNN <ref type="bibr" target="#b8">[9]</ref>, and WkNN <ref type="bibr" target="#b15">[16]</ref>, but also provide the theoretical ground to support the experimental results. Theorem 1. The probability p of y belonging to c j ∈ C given embedding z = f (x) and support set</p><formula xml:id="formula_14">{(x i , y i )} N i=1 can be approximate by: p(y ∈ c j ) ∝ ∑ N i=1 exp − z − z i 2 2 /T I[y i ∈ c j ] ∑ N i=1 exp − z − z i 2 2 /T .<label>(8)</label></formula><p>where T &gt; 0 is a parameter to be tuned. The true confidence score of a data point x ∈ R n for i-th class is the probability of y belongs to c j given z = f (x) ∈ R m in the embedding space, i.e.,</p><formula xml:id="formula_15">Prob(y ∈ c j |z = f (x)) = p Z|Y (z|y ∈ c j )p Y (y ∈ c j ) p Z (z)<label>(9)</label></formula><p>where p Y : C → R + and p Z : R m → R + are the marginal PDF of Y and Z respectively and p Z|Y : R m ×C → R + is the conditional PDF of Z given Y . Since we do not know the joint PDF p Z,Y , we cannot calculate any of these three quantities. Here we estimate these quantities using the data we are given.</p><p>Let us assume that we are given N data points, i.e., {(x i , y i )} N i=1 with x i ∈ R n and y i ∈ C where these are the samples from p X,Y and that N j is the number of data points belonging to the j-th class, i.e.,</p><formula xml:id="formula_16">N j = ∑ N i=1 I[y i ∈ c j ] for j = 1, . . . , M. Since Prob(y ∈ c j |z) ∝ p Z|Y (z|y ∈ c j )p Y (y ∈ c j )<label>(10)</label></formula><p>we need to estimate only two quantities p Y (y ∈ c j ) and p Z|Y (z|y ∈ c j ).</p><p>For the first quantity, we can estimate it by counting the data points belonging to each class assuming that the data set is balanced, i.e., the size of data for each class is proportional to the true portion of each class. Thus we havep</p><formula xml:id="formula_17">Y (y ∈ c j ) = N j /N<label>(11)</label></formula><p>wherep Y : C → R + denotes the estimate for the mass probability function (PMF) for Y , i.e., p Y .</p><p>For the second quantity, we use a probability density estimation method. If we knew the type of probability distribution the data of each class in the embedding space come from, we could use some parametric estimation method to estimate the conditional probability density, p Z|Y (z|y ∈ c j ). However, we do not know the details of the distribution in general. Therefore we use a non-parametric distribution estimation method which does not assume a specific type of data distribution. One such method uses the empirical probability density function (PDF), i.e., p Z|Y (z|y ∈ c j ) = 1</p><formula xml:id="formula_18">N j ∑ i:y j ∈c j δ (z − f (x i )) = 1 N j ∑ i:y j ∈c j δ (z − z i )<label>(12)</label></formula><p>where δ : R n → {0, ∞} is a Dirac delta function whose range is in the set of the extended real numbers andp Z|Y,· denotes an estimate for p Z|Y . This estimated PDF has infinite peaks, hence we cannot use it in practice. However, we can apply the convolution operation with some smoothing kernel to <ref type="bibr" target="#b11">(12)</ref> to obtain finite PDF estimate, i.e., p Z|Y,· (z|y ∈ c j ) =p Z|Y,δ (z|y ∈ c j ) g(z) = 1</p><formula xml:id="formula_19">N j ∑ i:y j ∈c j g(z − z i )<label>(13)</label></formula><p>where g : R m → R + is a kernel smoothing function and is the convolution operator. One typical choice for the kernel smoothing function is the (multi-dimensional) Gaussian PDF. In this case, the estimated PDF becomesp</p><formula xml:id="formula_20">Z|Y, f Σ j (z|y ∈ c j ) = 1 N j ∑ i:y j ∈c j f Σ j (z − z i )<label>(14)</label></formula><p>where f Σ j : R m → R ++ is the PDF of N(0, Σ j ) for some Σ j ∈ S m ++ , i.e., zero mean Gaussian with Σ j as its covariance matrix. We can interpret the convolution with the Gaussian kernel as applying m-dimensional low-pass filter. Here S m ++ denotes the set of all positive definite matrices in R m×m and the PDF is defined by</p><formula xml:id="formula_21">f Σ j = 1 (2π) m/2 det(Σ j ) 1/2 exp − 1 2 z T Σ −1 j z .<label>(15)</label></formula><p>Note that the PDF in <ref type="formula" target="#formula_0">(15)</ref> is not used for representing a random variable, but as a kernel smoothing function.</p><p>Now the right hand side of (10) can be approximated bŷ</p><formula xml:id="formula_22">p Y (y ∈ c j )p Z|Y, f Σ j (z|y ∈ c j ) = 1 N ∑ i:y i ∈c j f Σ j (z − z i )<label>(16)</label></formula><p>where <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(14)</ref> are used. Note that the approximation in (16) does not depend on N j because it is cancelled out. This is because the number of data in each class, N j , accounts for the probability for each class. Now <ref type="bibr" target="#b9">(10)</ref> implies that the true confidence score can be approximated by</p><formula xml:id="formula_23">Prob(y ∈ c j |z = f (x)) ≈p Y (y ∈ c j )p Z|Y (z|y ∈ c j ) ∑ M j =1p Y (y ∈ c j )p Z|Y (z|y ∈ c j ) = ∑ i:y i ∈c j f Σ j (z − z i ) ∑ M j =1 ∑ i:y i ∈c j f Σ j (z − z i ) = ∑ i:y i ∈c j exp −(z − z i ) T Σ −1 j (z − z i )/2 /det(Σ j ) 1/2 ∑ M j =1 ∑ i:y i ∈c j exp −(z − z i ) T Σ −1 j (z − z i )/2 /det(Σ j ) 1/2<label>(17)</label></formula><p>where we use the fact that ∑ M j=1 Prob(y ∈ c j | z = f (x)) = 1 for the derivation of (17) from <ref type="bibr" target="#b15">(16)</ref>. In our work, we use DDML models that assume during the training that all the coordinates of the embedding variable Z are independent and properly normalized for each class. Therefore, we can replace Σ j with α j I m where α j &gt; 0 and I m ∈ S m ++ denotes the identity matrix. Then <ref type="formula" target="#formula_0">(17)</ref> becomes</p><formula xml:id="formula_24">Prob(y ∈ c j |z = f (x)) ≈ ∑ i:y i ∈c j exp − z − z i 2 2 /2α j /α m/2 j ∑ M j =1 ∑ i:y i ∈c j exp − z − z i 2 2 /2α j /α m/2 j<label>(18)</label></formula><p>where · 2 denotes the 2 norm. If we further assume that the variance of the data in the embedding space is equal for all classes, i.e., α 1 = · · · = α M = α &gt; 0, we have</p><formula xml:id="formula_25">Prob(y ∈ c j |z = f (x)) ≈ ∑ i:y i ∈c j exp − z − z i 2 2 /2α ∑ M j =1 ∑ i:y i =c j exp − z − z i 2 2 /2α = ∑ i:y i ∈c j exp − z − z i 2 2 /2α ∑ N i=1 exp − z − z i 2 2 /2α = ∑ N i=1 exp − z − z i 2 2 /2α I[y i ∈ c j ] ∑ N i=1 exp − z − z i 2 2 /2α .<label>(19)</label></formula><p>If we replace α with T /2, we obtain the equation <ref type="bibr" target="#b7">(8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiments with different DDML approaches</head><p>We evaluate the performance of the proposed NED algorithm on the following three additional DDML models:</p><p>Triplet Semihard. We train a model with triplet loss and semi-hard sample mining <ref type="bibr" target="#b44">[45]</ref>. We use GoogleNet <ref type="bibr" target="#b48">[49]</ref> with batch normalization <ref type="bibr" target="#b25">[26]</ref> replacing the final dense layer to match the embedding dimension before triplet loss. Due to batch-size constraints, we omitted the experiments with this model on the SOP dataset.</p><p>N Pairs. Following the implementation details described in <ref type="bibr" target="#b45">[46]</ref>, we train a model with N pair loss, which is defined as a softmax cross-entropy loss function on the pairwise distances within each batch. The approach with N pair loss is known to perform better than the triplet variant. Batch composition strategy samples pairs of images from N unique classes. We add one dense layer and normalization using L2 norm to GoogleNet with batch normalization.</p><p>Proxy NCA. We train a model with proxy NCA loss <ref type="bibr" target="#b35">[36]</ref>. We replace positive and negative samples with points that represent the ideal cluster center of each class -proxies, which are initialized randomly and learned along with the embedding function. The model is not sensitive to the batch selection process and converges faster. We add one dense layer and normalization using the L2 norm to a pretrained Resnet50 <ref type="bibr" target="#b17">[18]</ref>. Since both proxies and embeddings are normalized and training can stall when relative distances become very small, we add a temperature parameter equal to 0.33 to the NCA loss.</p><p>The results presented in the <ref type="table" target="#tab_6">Table 4</ref> demonstrates the potential of using NED algorithm with different DDML models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-200</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy of DDML model based on the k value on CUB-200, CARS196, SOP and InShop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proof.</head><label></label><figDesc>Suppose that there are M classes for multi-class classification problem and the set of the classes is defined by C = {c 1 , . . . , c M }. Let X ∈ R n and Y ∈ C be the random variables representing the independent variables and the associated class respectively with the joint probability density function (PDF), p X,Y : R n ×C → R + . Let f : R n → R m represents the trained deep distance metric learning (DDML) model. Then this defines the joint PDF of Z = f (X) ∈ R m and Y , p Z,Y : R m ×C → R + . In general, we do not know these PDFs. Here n and m are the dimensions of the original and embedding spaces respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">CUB-200</cell><cell cols="2">CARS196</cell><cell>SOP</cell><cell></cell><cell>InShop</cell><cell></cell></row><row><cell></cell><cell cols="8">Accuracy ECE Accuracy ECE Accuracy ECE Accuracy ECE</cell></row><row><cell>1NN(reported)</cell><cell>67.6</cell><cell>-</cell><cell>89.1</cell><cell>-</cell><cell>80.8</cell><cell>-</cell><cell>90.6</cell><cell>-</cell></row><row><cell>1NN</cell><cell>67.2</cell><cell>-</cell><cell>89.1</cell><cell>-</cell><cell>81.2</cell><cell>-</cell><cell>90.9</cell><cell>-</cell></row><row><cell>kNN</cell><cell>73.8</cell><cell>9.2</cell><cell>90.1</cell><cell>10.0</cell><cell>81.2</cell><cell>24.4</cell><cell>90.9</cell><cell>32.5</cell></row><row><cell>WkNN [9]</cell><cell>74.3</cell><cell>20.5</cell><cell>91.3</cell><cell>12.2</cell><cell>81.2</cell><cell>5.2</cell><cell>91.0</cell><cell>17.8</cell></row><row><cell>WkNN [16]</cell><cell>74.3</cell><cell>20.7</cell><cell>91.3</cell><cell>12.3</cell><cell>81.2</cell><cell>5.2</cell><cell>91.1</cell><cell>17.9</cell></row><row><cell>NED(proposed)</cell><cell>74.9</cell><cell>2.4</cell><cell>91.5</cell><cell>1.5</cell><cell>81.2</cell><cell>2.2</cell><cell>91.3</cell><cell>0.3</cell></row></table><note>table demonstrate that the proposed approach using NED algorithm outperforms all the other approaches in both accuracy and ECE. For all experiments, the accuracy for the model with NED algorithm is higher at least by 0.3% and at most by 7.3% compared to 1NN. Similarly, using kNN and different versions of WkNN algorithm improves classification accuracy, which demonstrates that outliers to the training distribution can be identified more accurately at test time when more than the first neighbor is considered.The accuracy and ECE of proposed NED algorithm and the four baselines (1NN, kNN, WkNN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The average accuracy and average ECE for DDML model with proposed NED algorithm and the four baselines (1NN, kNN, WkNN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The accuracy (A.) and ECE (E.) for DDML model complemented with NED algorithm for adversarial images. The model with NED algorithm is more robust to adversarial images compared to the baselines.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The accuracy and ECE for DDML models trained with three different loss functions complemented with proposed NED algorithm and the four baselines (1NN, kNN, WkNN<ref type="bibr" target="#b8">[9]</ref>, and WkNN<ref type="bibr" target="#b15">[16]</ref>) on CUB-200, CARS196 and SOP datasets. The best results of each column are in bold. The proposed approach consistently outperforms baseline methods for all experiments.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification is a strong baseline for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The British Machine Vision Conference</title>
		<meeting>The British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bandwidth selection for kernel conditional density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Bashtannyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="279" to="298" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Robust optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparse local embeddings for extreme multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Neural Information Processing Systems</title>
		<meeting>the 29th Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The comparison and evaluation of forecasters. The statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Degroot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on computer communication and networks</title>
		<meeting>the 26th international conference on computer communication and networks</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 26th International Conference on Computer Communications and Networks</title>
		<meeting>26th International Conference on Computer Communications and Networks</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The distance-weighted k-nearest-neighbor rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Dudani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analyzing and improving representations with the soft nearest neighbor loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Uncertainty in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>eeding of the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision</title>
		<meeting>the 14th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Motivating the rules of the game for adversarial example research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1807.06732</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new distance-weighted k-nearest neighbor classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information and Computational Science</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On calibration of modern neural netwroks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision</title>
		<meeting>the 14th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weighted k-nearest-neighbor techniques and ordinal classification. Discussion Paper No. 399</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hechenbichler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schliep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Collaborative Research Center</title>
		<imprint>
			<biblScope unit="volume">386</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and surface variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01697</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07174</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 41st IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Similarity Based Pattern Analysis and Recognition</title>
		<meeting>the Third International Workshop on Similarity Based Pattern Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Web-scale responsive visual search at bing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Komlev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sacheti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attribution-based confidence metric for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jalaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">To trust or not to trust a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision</title>
		<meeting>the 14th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bangio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Metric learning for adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards neural networks that provably know when they don&apos;t know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth International Conference on Learning Representatives</title>
		<meeting>Eighth International Conference on Learning Representatives</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Naeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep metric learning with bier: Boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04765</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pac confidence sets for deep neural networks via calibrated prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Matni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth International Conference on Learning Representatives</title>
		<meeting>Eighth International Conference on Learning Representatives</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards the first adversarially robust neural network model on mnist</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Disentangling adversarial robustness and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Interpreting neural networks with nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Centroid-based deep metric learning for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brudho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the 44th International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Analysing the robustness of nearest neighbors to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kraehenbuehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Distance-based learning from errors for confidence calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth International Conference on Learning Representatives</title>
		<meeting>Eighth International Conference on Learning Representatives</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distance metric learning for aspect phrase grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>-H. Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Transforming classifier scores into accurate multiclass probability estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Classification is a strong baseline for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Extreme classification via adversarial softmax approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth International Conference on Learning Representatives</title>
		<meeting>Eighth International Conference on Learning Representatives</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep extreme multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visual search at alibaba</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Improving the robustness of deep neural networks via stability training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
