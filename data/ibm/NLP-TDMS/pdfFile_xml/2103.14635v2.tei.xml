<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runyu</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
							<email>hengshuang.zhao@eng.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<email>xjqi@eee.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Position Adaptive Convolution (PAConv), a generic convolution operation for 3D point cloud processing. The key of PAConv is to construct the convolution kernel by dynamically assembling basic weight matrices stored in Weight Bank, where the coefficients of these weight matrices are self-adaptively learned from point positions through ScoreNet. In this way, the kernel is built in a data-driven manner, endowing PAConv with more flexibility than 2D convolutions to better handle the irregular and unordered point cloud data. Besides, the complexity of the learning process is reduced by combining weight matrices instead of brutally predicting kernels from point positions.</p><p>Furthermore, different from the existing point convolution operators whose network architectures are often heavily engineered, we integrate our PAConv into classical MLP-based point cloud pipelines without changing network configurations. Even built on simple networks, our method still approaches or even surpasses the state-ofthe-art models, and significantly improves baseline performance on both classification and segmentation tasks, yet with decent efficiency. Thorough ablation studies and visualizations are provided to understand PAConv. Code is released on https://github.com/CVMI-Lab/PAConv.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the rise of 3D scanning technologies has been promoting numerous applications that rely on 3D point cloud data, e.g., autonomous driving, robotic manipulation and virtual reality <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b42">42]</ref>. Thus, the approaches to effectively and efficiently processing 3D point clouds are in critical needs. While remarkable advancements have been obtained in 3D point cloud processing with deep learning <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b25">26]</ref>, it is yet a challenging task in view of the sparse, irregular and unordered structure of point clouds.  <ref type="bibr" target="#b38">[38]</ref>, PointConv <ref type="bibr" target="#b54">[54]</ref>, KPConv <ref type="bibr" target="#b49">[49]</ref> and our PAConv. It illustrates the differences of these point-based convolutions. SOP denotes symmetric operations, like MAX.</p><p>To tackle these difficulties, previous research can be coarsely cast into two categories. The first line attempts to voxelize the 3D point clouds to form regular grids such that 3D grid convolutions can be adopted <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b41">41]</ref>. However, important geometric information might be lost due to quantization, and voxels typically bring extra memory and computational costs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Another stream is to directly process point cloud data. The pioneering work <ref type="bibr" target="#b38">[38]</ref> proposes to learn the spatial encodings of points by combing Multi-Layer Perceptron (MLP) <ref type="bibr" target="#b13">[14]</ref> and global aggregation as illustrated in <ref type="figure" target="#fig_0">Fig. 1  (a)</ref>. Follow-up works <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b53">53]</ref> exploit local aggregation schemes to improve the network. Nonetheless, all the points are processed by the same MLP, which limits the capabilities in representing spatial-variant relationships.</p><p>Beyond MLP, most recent works design convolutionlike operations on point clouds to exploit spatial correlations. To handle the irregularity of 3D point clouds, some works <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b29">30]</ref> propose to directly predict the kernel weights based on relative location information, which is further used to transform features just like 2D convolutions. One representative architecture <ref type="bibr" target="#b54">[54]</ref> in this line of research is shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>. Albeit conceptually effective, the methods severely suffer from heavy computation and memory costs caused by spatial-variant kernel prediction in practice. The efficient implementation also tradeoffs its design flexibility, leading to inferior performance. Another group of works relate kernel weights with fixed kernel points <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b32">33]</ref> and use a correlation (or interpolation) function to adjust the weight of kernels when they are applied to process point clouds. <ref type="figure" target="#fig_0">Fig. 1 (c)</ref> illustrates one representative architecture <ref type="bibr" target="#b49">[49]</ref>. However, the hand-crafted combination of kernels may not be optimal and sufficient to model the complicated 3D location variations.</p><p>In this paper, we present Position Adaptive Convolution, namely PAConv, which is a plug-and-play convolutional operation for deep representation learning on 3D point clouds. PAConv (shown in <ref type="figure" target="#fig_0">Fig. 1 (d)</ref>) constructs its convolutional kernels by dynamically assembling basic weight matrices in Weight Bank. The assembling coefficients are selfadaptively learned from relative point positions by MLPs (i.e. ScoreNet). Our PAConv is flexible to model the complicated spatial variations and geometric structures of 3D point clouds while being efficient. Specifically, instead of inferring kernels from point positions <ref type="bibr" target="#b54">[54]</ref> in a bruteforce way, PAConv bypasses the huge memory and computational burden via a dynamic kernel assembling strategy with ScoreNet. Besides, unlike kernel point methods <ref type="bibr" target="#b49">[49]</ref>, our PAConv gains flexibility to model spatial variations in a data-driven manner and is much simpler without requiring sophisticated designs for kernel points.</p><p>We conduct extensive experiments on three challenging benchmarks on top of three generic network backbones. Specifically, we adopt the simple MLP-based point networks PointNet <ref type="bibr" target="#b38">[38]</ref>, PointNet++ <ref type="bibr" target="#b39">[39]</ref> and DGCNN <ref type="bibr" target="#b53">[53]</ref> as the backbones, and replace their MLPs with PAConv without changing other network configurations. With these simple backbones, our method still achieves the state-ofthe-art performance on ModelNet40 <ref type="bibr" target="#b55">[55]</ref> and considerably improves the baseline by 2.3% on ShapeNet Part <ref type="bibr" target="#b63">[63]</ref> and 9.31% on S3DIS <ref type="bibr" target="#b0">[1]</ref> with decent model efficiency. It's also worth noting that recent point convolution methods often use complicated architectures and data augmentations tailored to their operators <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref> for evaluation, making it difficult to measure the progress made by the convolutional operator. Here, we adopt simple baselines and aim to minimize the influence of network architectures to better assess the performance gain from the operator -PAConv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Mapping point clouds into regular 2D or 3D grids (voxels). Since point cloud data has irregular structure in 3D space, early works <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7]</ref> project point clouds to multi-view images and then utilize conventional convolutions for feature learning. Yet, this 3D-to-2D projection is not robust to occluded surfaces or density variations. <ref type="bibr">Tatarchenko et al. [47]</ref> propose to map local surface points onto a tangent plane and further uses 2D convolutional operators, and FP-Conv <ref type="bibr" target="#b25">[26]</ref> flattens local patches onto regular 2D grids with soft weights. However, they heavily rely on the estimation of tangent planes, and the projection process will inevitably sacrifice the 3D geometry information. Another technique is to quantize the 3D space and map points into regular voxels <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref>, where 3D convolutions can be applied. However, the quantization will inevitably lose fine-grained geometric details, and the voxel representation is limited by the heavy computation and memory cost. Recently, to address the above issues, sparse representations <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8]</ref> are employed to obtain smaller grids with better performance. Nevertheless, they still suffer from the trade-off between the quantization rate and the computational efficiency. Point representation learning with MLPs. Many methods <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15]</ref> process unstructured point clouds directly with point-wise MLPs. PointNet <ref type="bibr" target="#b38">[38]</ref> is the pioneering work which encodes each point individually with shared MLPs and aggregates all point features with global pooling. However, it lacks the ability to capture local 3D structures. Several follow-up works address this issue by adopting hierarchical multi-scale or weighted feature aggregation schemes to incorporate local features <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58]</ref>. Other approaches use graphs to represent point clouds <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b59">59]</ref>, and the point features are aggregated through local graph operations, aiming to capture local point relationships. Nonetheless, they all adopt the shared MLPs to transform point features, which limits the model capabilities in capturing spatial-variant information. Point representation learning with point convolutions. More recently, lots of attempts <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref> focus on designing point convolutional kernels. PointCNN <ref type="bibr" target="#b24">[25]</ref> learns an X -transformation to relate points with kernels. However, this operation cannot satisfy permutation invariant, which is crucial for modeling un-ordered point cloud data. In addition, <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b29">30]</ref> propose to directly learn the kernel of local points based on point positions. Nevertheless, these methods directly predict kernels, which has much higher complexity (memory and computation) in the learning process.</p><p>Another type of point convolutions associate weight matrices with pre-defined kernel points in 3D space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref>. However, the positions of kernels have crucial influence on the final performance <ref type="bibr" target="#b49">[49]</ref> and need to be specifically optimized for different datasets or backbone architectures. Besides, the above approaches <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref> generate kernels through combining pre-defined kernels using hand-crafted rules which limit the model flexibility, lead-ing to inferior performance <ref type="bibr" target="#b22">[23]</ref>. Different from them, our method adaptively combines weight matrices in a learn-able manner, which improves the capability of the operator to fit irregular point cloud data. Dynamic and conditioned convolutions. Our work is also related to dynamic and conditional convolutions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b2">3]</ref>. Brabandere et al. <ref type="bibr" target="#b8">[9]</ref> propose to dynamically generate position-specific filters on pixel inputs. In <ref type="bibr" target="#b9">[10]</ref>, through learning the offsets on kernel coordinates, the kernel space is deformed to adapt to different scales of objects. Recently, Bello <ref type="bibr" target="#b2">[3]</ref> proposes to model the interactions between a query and context through a lambda function learned from both content and positional relations. CondConv <ref type="bibr" target="#b61">[61]</ref> generates the convolution kernel by combining several filters through a routing function that outputs the coefficients for filter combination, which is similar with our dynamic kernel assembly. Yet, the predicted kernels in CondConv <ref type="bibr" target="#b61">[61]</ref> are not position-adaptive, while the unstructured point clouds require the weights that adapt to different point locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first revisit the general formulation of point convolutions. Then we introduce PAConv. Finally, we compare PAConv with prior relevant works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given N points in a point cloud P = {p i |i = 1, ..., N } ∈ R N ×3 , the input and output feature map of P in a convolutional layer can be denoted as F = {f i |i = 1, ..., N } ∈ R N ×Cin and G = {g i |i = 1, ..., N } ∈ R N ×Cout respectively, where C in and C out are the channel numbers of the input and output. For each point p i , the generalized point convolution can be formulated as:</p><formula xml:id="formula_0">g i = Λ({K(p i , p j )f j |p j ∈ N i }),<label>(1)</label></formula><p>where K(p i , p j ) is a function which outputs convolutional weights according to the position relation between the center point p i and its neighboring point p j . N i denotes all the neighborhood points, and Λ refers to the aggregation function in terms of MAX, SUM or AVG. Under this definition, 2D convolution can be regarded as a special case of the point convolution. For instance, for a 3 × 3 2D convolution, the neighborhood N i lies in a 3 × 3 rectangular patch centered on pixel i , and K is a one-to-one mapping from a relative position (p i , p j ) to the corresponding weight matrix K(p i , p j ) ∈ R Cin×Cout in a fixed set of 3 × 3 ( <ref type="figure" target="#fig_1">Fig. 2. (a)</ref>). However, the simple one-to-one mapping kernel function defined on images is not applicable for 3D point clouds owing to the irregular and unordered characteristics of point clouds. Specifically, the spatial positions of 3D points are continuous and thus the number of possible relative offsets (p i , p j ) is infinite, which cannot be mapped into a finitesized set of kernel weights. Therefore, we redesign the kernel function K to learn a position-adaptive mapping by dynamic kernel assembly. First, we define a Weight Bank composed of several weight matrices. Then, a ScoreNet is designed to learn a vector of coefficients to combine the weight matrices according to point positions. Finally, the dynamic kernels are generated by combining the weight matrices and its associated position-adaptive coefficients. The details are shown in <ref type="figure" target="#fig_1">Fig. 2.</ref> (b) and elaborated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dynamic Kernel Assembling</head><p>Weight Bank. We first define a Weight Bank B = {B m |m = 1, ..., M }, where each B m ∈ R Cin×Cout is a weight matrix, and M controls the number of weight matrices stored in the Weight Bank B.</p><p>Intuitively, larger M contributes to more diversified weight matrices for kernel assembly. Yet, too many weight matrices may bring redundancies and cause heavy memory/computation overheads. We find that setting M to 8 or 16 is appropriate, which is discussed in Sec. 6.2. Equipped with Weight Bank, the next is to establish a mapping from discrete kernels to continuous 3D space. To this end, we propose ScoreNet to learn coefficients to combine weight matrices and produce dynamic kernels fitting to point cloud inputs, which is detailed as follows.</p><p>ScoreNet. The goal of ScoreNet is to associate relative positions with different weight matrices in Weight Bank B. Given the specific position relation between a center point p i and its neighbor point p j , ScoreNet predicts the positionadaptive coefficients S m ij for each weight matrix B m . The inputs of ScoreNet are based on position relations. We explore different input representations as illustrated in Sec. 6.1. For the sake of clarity, here we denote this input vector as (p i , p j ) ∈ R Din . The ScoreNet outputs a normalized score vector as:</p><formula xml:id="formula_1">S ij = α(θ(p i , p j )),<label>(2)</label></formula><p>where θ is a non-linear function implemented using Multilayer Perceptrons (MLPs) <ref type="bibr" target="#b13">[14]</ref> and α indicates Softmax normalization. The output vector S ij = {S m ij |m = 1, ..., M }, where S m ij represents the coefficient of B m in constructing the kernel K(p i , p j ). M is the number of weight matrices. Softmax ensures that the output scores are in range (0, 1). This normalization guarantees that each weight matrix will be chosen with a probability, with higher scores implying stronger relations between the position input and the weight matrix. Sec. 6.1 presents the comparison of different normalization schemes.</p><p>Kernel generation. The kernel of PAConv is derived by softly combining weight matrices in Weight Bank B with the corresponding coefficients predicted from ScoreNet: By doing this, our PAConv constructs the convolution kernel in a dynamic data-driven manner, where the score coefficients S m ij are self-adaptively learned from point positions. Our position-adaptive convolution gains flexibility in modeling irregular geometric structures of 3D point clouds with the kernel assembly strategy.</p><formula xml:id="formula_2">K(p i , p j ) = M m=1 (S m ij B m ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weight Regularization</head><p>While a large size of Weight Bank implies more weight matrices are available, the diversity of weight matrices is not ensured since they are randomly initialized and may converge to be similar with each other. To avoid this, we design a weight regularization to penalize the correlations between different weight matrices, which is defined as:</p><formula xml:id="formula_3">L corr = Bi,Bj ∈B,i =j | B i B j | ||B i || 2 ||B j || 2 .<label>(4)</label></formula><p>This enforces weight matrices to be diversely distributed, further promises the diversity in the generated kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relation to Prior Work</head><p>• Relation to PointCNN <ref type="bibr" target="#b24">[25]</ref>. PointCNN designs an MLPbased X -transformation to permute point features and associate them with corresponding kernels by weighted combination. However, the operator cannot preserve permutationinvariance which is important for point cloud processing. Our PAConv, nevertheless, learns kernels from position relations, naturally maintaining shape information, and utilize the symmetric function to ensure permutation-invariant.</p><p>• Relation to PointConv <ref type="bibr" target="#b54">[54]</ref>. PAConv differs from Point-Conv in the following folds: 1) PointConv treats convolutional kernels as nonlinear functions of point positions and densities. Instead, PAConv regards each weight matrix as a basis to capture certain spatial relations. These bases are further dynamically assembled via learnable ScoreNet to model continuous point position relations. 2) Our insight yields the following designs customized for PAConv, which is more flexible and effective: (a) Softmax normalization optimizes kernel scores as a whole, where higher scores imply stronger links between B m and spatial relations. We can also use other norms (e.g. Sigmoid, Tanh in <ref type="table" target="#tab_8">Table 5</ref>). (b) L corr encourages B m to be independent with each other; (c) More generic feature aggregation operation can be exploited: PAConv uses max-pooling, while Efficient PointConv can only realize sum-pooling.</p><p>• Relation to KPConv <ref type="bibr" target="#b49">[49]</ref>. PAConv and KPConv both strive to design the kernel function in a position adaptive way, yet there exists two key differences: 1) KPConv generates fixed kernel points with corresponding weights offline by optimization, where the kernel point space may need to be specifically tuned for different point cloud datasets, which is sensitive to hyper-parameters. However, our PA-Conv defines weight matrices without requiring the estimation of kernel point locations. 2) KPConv uses hand-crafted relation to combine weight matrices, which may be suboptimal and limited in flexibility. In contrast, PAConv defines a learnable ScoreNet to predict a vector coefficients adapted to point positions. PAConv is more flexible in both kernel design and weight learning, easily to be integrated with different architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Backbone Network Architectures</head><p>The network configurations largely vary across recent point cloud networks <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b25">26</ref>], yet most of them can be considered as different variants of the classical pointwise MLP-based networks <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b49">49]</ref>. To assess the effectiveness of PAConv and minimize the impact from complicated network architectures, we employ three classical and simple MLP-based network backbones for different 3D tasks, and integrate our PAConv without further modifications of network architectures. Networks for object-level tasks. The object-level tasks deal with individual 3D objects, which can be effectively solved using lightweight networks without down-sampling layers. Thus the scale/resolution of the point cloud is fixed through the whole network. PointNet <ref type="bibr" target="#b38">[38]</ref> and DGCNN <ref type="bibr" target="#b53">[53]</ref> are two representatives, which are chosen as the backbones for object classification and shape part segmentation. We directly replace the MLPs in the encoders of PointNet and EdgeConv <ref type="bibr" target="#b53">[53]</ref> of DGCNN with PAConv without changing the original network architectures.</p><p>DGCNN <ref type="bibr" target="#b53">[53]</ref> computes pairwise distance in feature space and takes the closest k points for each point, which brings huge computational cost and memory usage. Instead, we search the k-nearest neighbors in 3D coordinate space. Network for scene-level tasks. For large-scale scene-level segmentation tasks, it is necessary to employ the networks with encoder (downsampling) and decoder (upsampling). This effectively enlarges the receptive field of the network while achieving faster speed and less memory usage. Point-Net++ <ref type="bibr" target="#b39">[39]</ref> is such a pioneering architecture.</p><p>For the encoder, we follow PointNet++ which uses iterative farthest point sampling (FPS) to downsample point clouds. When building neighborhoods, PointNet++ finds all points within a ball centered at the query point. The ball radius is critical for performance and need to be tuned for different point cloud scales, thus we directly search k-nearest neighbor for flexibility. In addition, we adopt the simplest Single-scale grouping (SSG) approach instead of sophisticated MSG and MRG. The learned features are thus directly propagated to the next layer without feature fusion tricks.</p><p>Similar to object-level tasks, we directly replace the MLPs in the encoding layers of PointNet++ with PAConv. Our decoder is the same as PointNet++. The detailed network architectures are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We integrate PAConv into different point cloud networks mentioned in Sec. 4 and evaluate it on object classification, shape part segmentation and indoor scene segmentation. We implement a CUDA layer to efficiently realize PA-Conv, which is presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Object Classification</head><p>Dataset. First we evaluate our model on ModelNet40 <ref type="bibr" target="#b55">[55]</ref> for object classification. It consists 3D meshed models from 40 categories, with 9, 843 for training and 2, 468 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method (time order)</head><p>Input Accuracy MVCNN <ref type="bibr" target="#b46">[46]</ref> multi-view 90.1 OctNet <ref type="bibr" target="#b41">[41]</ref> hybrid grid octree 86.5 PointwiseCNN <ref type="bibr" target="#b15">[16]</ref> 1K points 86.  <ref type="bibr" target="#b55">[55]</ref>. *PN and *DGC respectively denote using PointNet <ref type="bibr" target="#b38">[38]</ref> and DGCNN <ref type="bibr" target="#b53">[53]</ref> as the backbones. "vot." indicates multi-scale inference following <ref type="bibr" target="#b29">[30]</ref>. PAConv obviously improves two baselines and surpasses other methods.</p><p>Implementation. As mentioned in Sec. 4, PAConv is utilized to replace the MLPs of the encoders in PointNet and EdgeConv of DGCNN. We sample 1, 024 points for training and testing following <ref type="bibr" target="#b38">[38]</ref>. Following <ref type="bibr" target="#b53">[53]</ref>, the training data are augmented by randomly translating objects and shuffling points. We do not add L corr (Sec. 3) while still achieving high performance due to the simplicity of the task.</p><p>Result. <ref type="table" target="#tab_0">Table 1</ref>   <ref type="bibr" target="#b63">[63]</ref>. *DGC indicates using DGCNN <ref type="bibr" target="#b53">[53]</ref> as the backbone. "vot." indicates multi-scale inference following <ref type="bibr" target="#b29">[30]</ref>. PAConv significantly improves both Class and Instance mIoU on DGCNN. 16 categories and is labeled in 50 parts where each shape has 2 − 5 parts. 2, 048 points are sampled from each shape and each point is annotated with a part label. Implementation. We displace EdgeConv in DGCNN <ref type="bibr" target="#b53">[53]</ref> with PAConv and follow the official train/validation/test split of <ref type="bibr" target="#b53">[53]</ref>. No data augmentations are used. Similar to the classification task, we do not employ L corr and the same voting strategy during test is applied following <ref type="bibr" target="#b29">[30]</ref>. Result. <ref type="table" target="#tab_1">Table 2</ref> lists the instance average and class average mean Inter-over-Union (mIoU), where PAConv notably lifts the performance of DGCNN on both class mIoU (2.3%↑) and instance mIoU (0.9%↑). PAConv also outperforms RS-CNN without voting (w/o vot.) Besides, our method surpasses or approaches other methods with much lower com-  <ref type="table" target="#tab_8">Table 3</ref>. Segmentation results (%) and #FLOPs/sample (M) on S3DIS Area-5 <ref type="bibr" target="#b0">[1]</ref>. BLK and Grid signify using block sampling and grid sampling in data pre-processing, respectively. PA denotes PAConv, *PN2 refers to applying PointNet++ <ref type="bibr" target="#b39">[39]</ref> as the backbone, and PA † symbolizes the CUDA implementation of PAConv. "vot." indicates multi-scale inference following <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Indoor Scene Segmentation</head><p>Dataset. Large-scale scene segmentation is a more challenging task. To further assess our method, we employ Stanford 3D Indoor Space (S3DIS) <ref type="bibr" target="#b0">[1]</ref> following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref>, which includes 271 rooms in 6 areas. 273 million points are scanned from 3 different buildings, and each point is annotated with one semantic label from 13 classes. Implementation. We employ PAConv to replace the MLPs in the encoder of PointNet++ <ref type="bibr" target="#b39">[39]</ref>. We follow <ref type="bibr" target="#b39">[39]</ref> to prepare the training data, where the points are uniformly sampled into blocks of area size 1m × 1m, and each point is represented by a 9-dimensional vector (XY Z, RGB and a normalized location in the room). We randomly sample 4,096 points from each block on-the-fly, and all the points are adopted for testing. Following <ref type="bibr" target="#b48">[48]</ref>, we utilize Area-5 as the test scene and all the other areas for training. The data augmentations consist of random scaling, rotating, and perturbing points. The same voting test scheme as in the classification task is employed following <ref type="bibr" target="#b29">[30]</ref>. NOTE: Different with our block sampling strategy, both KPConv <ref type="bibr" target="#b49">[49]</ref> and PosPool <ref type="bibr" target="#b30">[31]</ref> voxelize point clouds into grids. During training, the number of input points should be extremely large (≈ 10 × ours) in their actual implementa-tions. Although this brings more regular data structure and more context information for better performance, it suffers from high memory usage during training.</p><p>Result. For the evaluation metrics, we use mean of classwise intersection over union (mIoU). As shown in <ref type="table" target="#tab_8">Table 3</ref>, our PAConv with L corr (w/ L corr ) achieves the best mIoU among all methods which use block sampling to pre-process data. PAConv also considerably promotes PointNet++ by 9.31%↑. The result without voting (w/o vot.) is also listed. The visualization of segmentation results is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The result of 6-fold cross-validation and the mIoU of each category is provided in the supplementary material. Time complexity. Moreover, we take 4, 096 points as the input and test the time complexity (floating point operations/sample ‡ ) of KPConv deform <ref type="bibr" target="#b49">[49]</ref> and PosPool <ref type="bibr" target="#b30">[31]</ref> as shown in <ref type="table" target="#tab_8">Table 3</ref>. It demonstrates that our PAConv stands out with much less computational FLOPs (38.6%↓).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Studies</head><p>To better understand PAConv, ablation studies are conducted on S3DIS <ref type="bibr" target="#b0">[1]</ref> dataset. Unless specified, no correlation loss (Sec. 3.3) is added to PAConv in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">ScoreNet</head><p>ScoreNet input. We firstly explore different input representations of ScoreNet. As illustrated in <ref type="table" target="#tab_3">Table 4</ref>, when the ScoreNet input carries information from all three axes, PA-Conv can effectively utilize the rich relations to learn scores and achieve the best performance. Score normalization. We also investigate widely-used normalization functions in order to adjust the score distribution. <ref type="table" target="#tab_8">Table 5</ref> shows that Softmax normalization outperforms other schemes. It suggests that predicting scores for all weight matrices as a whole (Softmax) is superior than considering each score independently (Sigmoid and Tanh). Score distribution in 3D space. More importantly, <ref type="figure">Fig. 5</ref> shows the relationships between learned score distributions ‡ FLOPs from torch.nn.module is calculated by https : / / github . com / Lyken17 / pytorch -OpCounter. FLOPs from torch.nn.Parameter is also added manually. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">The Number of Weight Matrices</head><p>We further conduct experiments to figure out the influence of the number of weight matrices as shown in <ref type="table" target="#tab_8">Table 6</ref>. When the number of weight matrices is 2, the performance is 65.05%, only 0.58% apart from 16 weight matrices. This can be attributed to our kernel assembly strategy as diverse kernels will be generated even with only 2 weight matrices. This definitely demonstrates the power of our proposed approach. However, when the number becomes larger, the relative performance boost fluctuates due to optimization issues. Finally, we achieve the best and most stable performance when the number is 16.  <ref type="table" target="#tab_8">Table 6</ref>. Segmentation results (%) and #FLOPs/sample (M) of PA-Conv on S3DIS Area-5 using different numbers of weight matrices. Choosing more weight matrices ensures diversity of kernels selection and assembling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Weight Bank Regularization</head><p>As mentioned in Sec. 3.3, weight regularization encourages weight matrices to have low correlations with each other, thus promising more diversity of kernel assembling. We utilize Pearson's R <ref type="bibr" target="#b4">[5]</ref> to measure the correlation between different weight matrices and report the average Pearson's R (Lower Pearson's R value means lower correlations). As shown in <ref type="table" target="#tab_8">Table 7</ref>, PAConv with the correlation loss outperforms the baseline with 0.95 mIoU on the scene segmentation task, and the Pearson's R between weight matrices remarkably drops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Robustness Analysis</head><p>PAConv uses a symmetric function to aggregate neighbor features, making it invariant to permutation and im-  <ref type="figure">Figure 5</ref>. The spatial distribution of scores, where the input points are randomly initialized in x-y, x-z, y-z plane, and are sent to a trained ScoreNet. When the corresponding height of a point is higher (or the color is closer to yellow), the output score of this point is larger. It illustrates the relation between spatial positions and score distributions for each weight matrix Bi, Bj, B k . proving its robustness to rotation. Besides, the kernels are assembled by scores learned from diverse local spatial relations that may cover different transformations, which further enhances the robustness. We also evaluate our model in this respect. As shown in  <ref type="table" target="#tab_6">Table 8</ref>. Test mIoU (%) on S3DIS Area-5 of perturbing the trained model. PN2 refers to our backbone PointNet++ <ref type="bibr" target="#b39">[39]</ref>. We perform random permutation of points (Perm.), rotation around vertical axis (90 • , 180 • , 270 • ), translation in 3 directions (±0.2), scaling (×0.8, ×1.2) and Gaussian jittering (Jitter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented PAConv, a position adaptive convolution operator with dynamic kernel assembling for point cloud processing. PAConv constructs convolution kernels by combining basic weight matrices in Weight Bank, with the associated coefficients learned from point positions through ScoreNet. When embedded into simple MLP-based networks without modifications of network configurations, PAConv approaches or even surpasses the state-of-the-arts and significantly outperforms baselines with decent model efficiency. Extensive experiments and ablation studies illustrate the effectiveness of PAConv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for PAConv Outline</head><p>This supplementary document is arranged as follows: (1) Sec. A benchmarks the performance of the most recent point convolutional operators under the same backbone architecture and the same data augmentation strategies;</p><p>(2) Sec. B investigates the effects of ScoreNet on PAConv;</p><p>(3) Sec. C elaborates on network configurations and implementation strategies for different down-stream tasks; (4) Sec. D presents a CUDA implementation of PAConv; <ref type="bibr" target="#b4">(5)</ref> Sec. E lists detailed semantic segmentation results with per-category scores; (6) Sec. F visualizes the results of the baseline (i.e. Point-Net++) and ours (i.e. PointNet++ equipped with our PA-Conv) to facilitate the comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison of Point Convolutional Operators</head><p>In this section, we focus on comparing the capability of PAConv with other different point convolutional operators. We compare with the most recent convolutional operators -PointConv <ref type="bibr" target="#b54">[54]</ref> and KPConv <ref type="bibr" target="#b49">[49]</ref>. To minimize the influence of the network architectures, we choose the classical MLP-based network PointNet++ <ref type="bibr" target="#b39">[39]</ref> as the backbone and integrate other convolution operators by directly replacing the MLPs, following how we embed our PAConv into Point-Net++ as mentioned in Sec. C.2.</p><p>Specifically, we replace the MLPs in PointNet++ with the PointConv * <ref type="bibr" target="#b54">[54]</ref> and KPConv † <ref type="bibr" target="#b49">[49]</ref> operators, while ensure not making any changes on the original network architecture and feature-dims of the original PointNet++. All the experiments are conducted on the S3DIS <ref type="bibr" target="#b0">[1]</ref> dataset, and adopt the Area-5 evaluation protocol, under the same data augmentation strategies for fair comparisons.</p><p>As summarized in <ref type="table" target="#tab_8">Table.</ref> A.1, our PAConv improves the mIoU of PointNet++ by 9.31%↑ with decent efficiency. However, PointConv only promotes the mIoU of Point-Net++ with 2.7%↑ while the inference is time-consuming with tremendous amount of FLOPs.  Note: Since the radius to initialize kernel points in KP-Conv <ref type="bibr" target="#b49">[49]</ref> need to be specifically tuned for different point cloud scales, it is tricky to adjust this radius in our implementation. Specifically, following the official code † of KPConv, we have tried exhaustive search to set the radius ranging from 0.07 ∼ 0.6, which is multiplied by 2 at each downsampling layer. However, the mIoU can only reach 11% ∼ 26%. This result is not comparable with its original version, thus it is not reported here.</p><p>Compared with KPConv, our PAConv does not require either complicated design of network architecture or handcrafted adjustment of kernel point space, which is a more flexible and efficient point convolutional operator, adaptable to different applications. Here we aim to figure out how the depth of ScoreNet influences the performance of our PAConv. Same with the ablation studies in the main paper, this experiment is conducted on PAConv without adding correlation loss on the S3IDS dataset. <ref type="table" target="#tab_8">Table.</ref> B.1 shows the result, where the corresponding time complexity (floating point operations/sample) of each setting is also enumerated for developers to balance the performance and efficiency. We clearly see that a deeper ScoreNet brings better performance while has lower efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Explorations on ScoreNet</head><p>Score distribution in the network. Furthermore, we visualize the average score coefficients of each weight matrix B m for all points at different network layer depths on S3DIS Area-5 segmentation task, aiming to figure out how scores distribute in the network.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. B.2</ref>, the scores of different weight matrices are diversely distributed (i.e., non-uniform and not only focus on single weight matrix) at all layers, which means nearly all the weight matrices in the weight bank have the possibility to be chosen for assembling point convolution kernels. This proves that the weight matrices in our PAConv are fully utilized, bringing more flexibility in the dynamic kernel assembling.</p><p>Score distribution in 3D space. Following Sec. 6.1 of the main paper, we provide more visualizations to demonstrate the spatial distribution of scores. As demonstrated in <ref type="figure" target="#fig_0">Fig. B.1</ref>, different weight matrices capture different position relations in 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Configurations and Implementations</head><p>Our PAConv is implemented using Pytorch <ref type="bibr" target="#b35">[36]</ref>. A data-parallel training scheme is adopted on several Nvidia GeForce GTX 2080 Ti GPUs. The details of networks and training strategies for different tasks are illustrated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Object-Level Tasks</head><p>Network configurations. As mentioned in the main paper, our PAConv is purely embedded into simple classical MLP-based point cloud networks without any modifications on network architectures or parameters (i.e. feature dimensions).</p><p>We employ PointNet <ref type="bibr" target="#b38">[38]</ref> and DGCNN <ref type="bibr" target="#b53">[53]</ref> as the backbones for object-level tasks (i.e. object classification and part segmentation). Due to the similar architecture design of DGCNN and PointNet, we only provide the network architecture of DGCNN in <ref type="figure" target="#fig_2">Fig. B.3</ref>. It clearly shows that the scale/resolution of the point cloud is fixed across whole networks.</p><p>The feature dimensions are the same as the official code of DGCNN <ref type="bibr" target="#b53">[53]</ref> ( ‡ for classification and § for part segmentation). Several MLPs with a dropout probability of 0.5 are employed at the last feature layers of the network. The dimensions of the fully connected layers are (512, 256, C out ) for generating final classification scores, and (256, 256, 128, C out ) to obtain final per-point segmentation scores for part segmentation. All layers include ReLU and batch normalization except for the last score prediction layer. As for the part segmentation, the one-hot encoding <ref type="bibr">(16-d)</ref> of the object label is concatenated to the last feature layer.</p><p>We set the number of neighbors in KNN search to 20 for classification and 30 for part segmentation when building neighborhood in Euclidean space at each PAConv.</p><p>Training. We follow the official code of DGCNN <ref type="bibr" target="#b53">[53]</ref> to train the network.</p><p>For the classification task ‡ , we use SGD with learning rate 0.1 and reduce it to 0.001 with cosine annealing. The ‡ https://github.com/WangYueFt/dgcnn/blob/master/ pytorch § https://github.com/WangYueFt/dgcnn/blob/master/ tensorflow/part_seg momentum is 0.9 and the weight decay is 10 −4 . The batch size is set to 32.</p><p>For the segmentation task § , Adam with learning rate 0.003 is employed and is divided by 2 after every 40 epochs. The weight decay is 0 and the batch size is 32. Both classification and part segmentation networks converge in 350 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Scene-Level Task</head><p>Network configurations. For scene level tasks, we choose PointNet++ <ref type="bibr" target="#b39">[39]</ref> with encoding (downsampling) and decoding (upsampling) layers as the backbone. As shown in <ref type="figure" target="#fig_3">Fig. B.4</ref>, we directly replace the PointNet modules (i.e. MLPs) with our PAConv for local pattern learning in the encoding layers of PointNet++ without changing any other network configurations.</p><p>Each encoding layer takes an N × (d + C) matrix as input that is from N points with d-dim coordinates and Cdim point feature. It outputs an N × (d + C ) matrix of N subsampled points with d-dim coordinates and new C -dim feature vectors summarizing local context.</p><p>Our decoding layers are totally the same as PointNet++. Concretely, for each query point at each layer in the decoder, the point feature set is first upsampled through a nearest-neighbor interpolation based on the inverse distance weighted averagely among k nearest neighbors of the query point. Next, the upsampled feature maps are concatenated with the intermediate feature maps produced by encoding layers through skip connections, after which a shared MLP is applied to the concatenated feature maps.</p><p>Same with PointNet++ <ref type="bibr" target="#b39">[39]</ref>, we use the following notations to describe our network architecture. EN(N, [l 1 , ..., l d ]) is an encoding layer with N query points using d consecutive PAConv with feature dim l i (i = 1, ..., d). DE(l 1 , ..., l d ) is a decoding layer with d MLPs, which is used for updating features concatenated from interpolation and skip link. With these notations, the network can be represented as:   Additionally, in each PAConv, we choose the nearest 32 points (K = 32) in Euclidean space as the neighboring points for each query point. Each layer is followed by ReLU and batch normalization except for the last score prediction layer. At last, several MLPs (128, 128, C out ) with dropout ratio 0.5 is utilized to output the final point-wise scores for semantic segmentation.</p><p>Training. We employ SGD optimizer with initial learning rate 0.05 and divide it by 10 at the 60th and 80th epoch. The momentum is 0.9 and the weight decay is 10 −4 . The batch size is 16 and the total number of epochs is 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CUDA Implementation</head><p>By re-formulating Eq. (3) of the main paper to gi = Λj∈N i M m=1 ((Bmfj)S m ij ), PAConv can be realized equivalently by first transforming features using weight matrices, then assembling transformed features with scores. We implement a CUDA layer to assemble neighbor features by querying neighbor indices on-the-fly without storing large intermediate matrix <ref type="figure" target="#fig_0">(Fig. D.1</ref>). This reduces memory usage from 10G+ to 5600M with 65,536 points. The CUDA code is also released.</p><p>Note that the performance on S3DIS semantic segmentation is slightly different between CUDA version and the original version. In CUDA version, since we only maintain one feature for each point on-the-fly regardless of the local area it belongs to, it is necessary to apply neighboring feature aggregation after each PAConv layer. However, our original version exactly follows PointNet++ <ref type="bibr" target="#b39">[39]</ref>, where neighboring point features are firstly respectively refined in each local area by three continuous PAConv layers, and are aggregated then.  </p><formula xml:id="formula_4">( ! , + ! ) ( " , , + ! ) ( " , + " ) ( ! , + " + ! ) ( ! , + # ) ( , + # + ) ( , $</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUDA Implementation</head><p>Small matrix! Avoids expanding a dimension of K. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Segmentation Results</head><p>We provide more detailed segmentation results of our PAConv and all the other methods listed in the main paper.</p><p>First, we summarize the segmentation results of each category on S3DIS <ref type="bibr" target="#b0">[1]</ref>, plus mean of class-wise accuracy (mAcc). As shown in <ref type="table" target="#tab_8">Table.</ref> E.1, our PAConv with L corr achieves the best mAcc among all the approaches. Without adopting the computation and memory intensive grid sampling strategy, our approach compares on par with KPConv with deformable design.</p><p>Next, <ref type="table" target="#tab_8">Table.</ref> E.2 reports the results on S3DIS with 6fold cross validation (calculating the metrics with results from different folds merged), where our method achieves comparable performance with the state-of-the-arts. To be noted, we do not use grid sampling.</p><p>Last, <ref type="table" target="#tab_8">Table E</ref>.3 enumerates the mIoU of each class on ShapeNet Part [63] for shape part segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualization of Result Comparisons on S3DIS</head><p>Since we employ PointNet++ <ref type="bibr" target="#b39">[39]</ref> as the backbone for the indoor scene segmentation on S3DIS <ref type="bibr" target="#b0">[1]</ref>, we provide the visualization results to intuitively compare the performance between original PointNet++ and PointNet++ after integrating PAConv. As shown in <ref type="figure" target="#fig_0">Fig. F.1</ref>, PointNet++ equipped with PAConv achieves conspicuously stronger performance than original PointNet++ on various scenes or areas.  <ref type="bibr" target="#b12">[13]</ref> BLK 70.00 79.10 94.08 97.28 83. <ref type="bibr" target="#b42">42</ref>  BLK </p><formula xml:id="formula_5">- - - - - - - - - - - - GACNet [51] BLK - - - - - - - - - - - - - - - Point2Node</formula><formula xml:id="formula_6">- - - - - - - - - - - - - - - PosPool [31] Grid - - - - - - - - - - - - - - - PointNet++ [39] BLK - - - - - - - - - - - - - - - PAConv w/o</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth PAConv (*PN2) PointNet++ <ref type="figure">Figure F</ref>.1. Visualization of semantic segmentation results on S3DIS Area-5. The first column is original scene inputs, the second column is the ground truth of the segmentation, the third row is the scenes segmented by the backbone PointNet++ <ref type="bibr" target="#b39">[39]</ref>, and the last column shows the segmentation results of plugging our PAconv into PointNet++. Each row denotes a scene in S3DIS Area-5. The red bounding boxes indicate the specific areas, where our PAConv has significantly better performance than PointNet++.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview about convolutional sturctures of PointNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>PAConv. (a) shows the traditional 2D convolution operators where SOP means symmetric operations, like MAX. (b) illustrates how our PAConv designs the kernel function K(pi, pj), including defining Weight Bank B, learning ScoreNet and generating kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of shape part segmentation results on ShapeNet Parts. The first row is the ground truth, and the second row is the predictions of our PAConv. From left to right are motorbike, lamp, aeroplane, chair and pistol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of semantic segmentation results on S3DIS Area-5. The first row shows original scene inputs, the second row shows the ground truth annotations, and the last row shows the scenes segmented by our PAConv. Each column denotes a scene in S3DIS Area-5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ScoreNet depth. The ScoreNet in PAConv consists of several fully connected layers with feature dim [f 1 , ..., f d ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure B. 1 .</head><label>1</label><figDesc>The spatial distribution of scores, where the input points are randomly initialized and are sent to a trained ScoreNet. When the corresponding height of a point is higher (or the color is closer to yellow), the output score of this point is larger. It illustrates the relation between spatial positions and score distributions for each weight matrix Bm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure B. 2 .</head><label>2</label><figDesc>Average score coefficient of each weight matrix Bm at different layer depths. The corresponding score of each weight matrix is diversely distributed, indicating that all the weight matrices are fully utilized for assembling point convolution kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure B. 4 .</head><label>4</label><figDesc>% &amp; ) p e r-p o in t s c o re s Large scene segmentation network built on PointNet++ [39]. We directly replace the PointNet (i.e. MLPs) operations with our PAConv for local feature representation in the encoding layers of PointNet++ without changing any other network configurations. 2, =)×?/(200_ABC(0, 2), =, 34)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure D. 1 .</head><label>1</label><figDesc>The CUDA implementation of our PAConv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>1</cell></row></table><note>Classification accuracy (%) on ModelNet40</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>summarizes the quantitative comparisons. PAConv significantly improves the classification accuracy with 4.0%↑ on PointNet and 1.0%↑ on DGCNN. Especially, the accuracy achieved by DGCNN+PAConv is 93.9%, which is an excellent result compared with recent works. Following RS-CNN<ref type="bibr" target="#b29">[30]</ref>, we perform voting tests with random scaling and average the predictions during test. Without voting, the accuracy of the released RS-CNN model drops to 92.4%, while PAConv still gets 93.6%. By eliminating the post-processing factor, the results without voting better reflects the performance gained purely from model designs and show the effectiveness of our PAConv. Shape part segmentation results (%) on ShapeNet Parts</figDesc><table><row><cell>Method (time order)</cell><cell cols="2">Cls. mIoU Ins. mIoU</cell></row><row><cell>PointNet [38]</cell><cell>80.4</cell><cell>83.7</cell></row><row><cell>PointNet++ [39]</cell><cell>81.9</cell><cell>85.1</cell></row><row><cell>SynSpecCNN [64]</cell><cell>82.0</cell><cell>84.7</cell></row><row><cell>SPLATNet [45]</cell><cell>83.7</cell><cell>85.4</cell></row><row><cell>PCNN [2]</cell><cell>81.8</cell><cell>85.1</cell></row><row><cell>SpiderCNN [60]</cell><cell>82.4</cell><cell>85.3</cell></row><row><cell>SpecGCN [50]</cell><cell>-</cell><cell>85.4</cell></row><row><cell>PointCNN [25]</cell><cell>84.6</cell><cell>86.1</cell></row><row><cell>PointConv [54]</cell><cell>82.8</cell><cell>85.7</cell></row><row><cell>Point2Seq [28]</cell><cell>-</cell><cell>85.2</cell></row><row><cell>PVCNN [32]</cell><cell>-</cell><cell>86.2</cell></row><row><cell>RS-CNN [30] w/o vot.</cell><cell>84.2</cell><cell>85.8</cell></row><row><cell>RS-CNN [30] w/ vot.</cell><cell>84.0</cell><cell>86.2</cell></row><row><cell>KPConv [49]</cell><cell>85.1</cell><cell>86.4</cell></row><row><cell>InterpCNN [33]</cell><cell>84.0</cell><cell>86.3</cell></row><row><cell>DensePoint [29]</cell><cell>84.2</cell><cell>86.4</cell></row><row><cell>3D-GCN [27]</cell><cell>82.1</cell><cell>85.1</cell></row><row><cell>DGCNN [53]</cell><cell>82.3</cell><cell>85.2</cell></row><row><cell>PAConv (*DGC) w/o vot.</cell><cell>84.2</cell><cell>86.0</cell></row><row><cell cols="3">PAConv (*DGC) w/ vot. 84.6 (2.3↑) 86.1 (0.9↑)</cell></row></table><note>5.2. Shape Part Segmentation Dataset. PAConv is also evaluated on ShapeNet Parts [63] for shape part segmentation. It contains 16, 881 shapes with</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>xi, yj − yi, zj − zi, xi, yi, zi, eij) 65.63 Segmentation results (%) of PAConv with different ScoreNet input representations on S3DIS Area-5. While (xj, yj, zj) represents the 3D coordinates of neighbor point, (xi, yi, zi) indicates the center point position. eij refers to the Euclidean distance between neighbor point j and center point i.</figDesc><table><row><cell>Input</cell><cell>mIoU</cell></row><row><cell>(xj − xi, xj, xi, eij)</cell><cell>63.12</cell></row><row><cell>(yj − yi, yj, yi, eij)</cell><cell>63.31</cell></row><row><cell>(zj − zi, zj, zi, eij)</cell><cell>64.77</cell></row><row><cell>(xj −</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 ,</head><label>8</label><figDesc>PAConv performs stably ×0.8 ×1.2 jitter PN2 59.75 59.71 58.15 57.18 58.19 22.33 29.85 56.24 59.74 59.05 PAConv 65.63 65.64 61.66 63.48 61.8 55.81 57.42 64.20 63.94 65.12</figDesc><table><row><cell>Regularization</cell><cell>mIoU</cell><cell>Pearson's R [5]</cell></row><row><cell>w/o regularization</cell><cell>65.63</cell><cell>0.5393</cell></row><row><cell>w/ correlation loss</cell><cell>66.58</cell><cell>-0.0333</cell></row><row><cell cols="3">Table 7. Segmentation results (%) on S3DIS Area-5 and Pearson's</cell></row><row><cell cols="3">R of PAConv with /without Weight Regularization. Regularized by</cell></row><row><cell cols="3">correlation loss, weight matrices are low-correlated and diverse,</cell></row><row><cell>bringing performance gains.</cell><cell></cell><cell></cell></row><row><cell cols="2">well under various transformations.</cell><cell></cell></row></table><note>Method None Perm. 90 • 180 • 270 • +0.2 -0.2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell>ScoreNet Layer</cell><cell>mIoU</cell><cell>FLOPs/sample(M)</cell></row><row><cell>[16]</cell><cell>64.42</cell><cell>1178</cell></row><row><cell>[16, 16]</cell><cell>65.29</cell><cell>1215</cell></row><row><cell>[16, 16, 16]</cell><cell>65.63</cell><cell>1253</cell></row><row><cell cols="3">Table B.1. Segmentation results (%) and #FLOPs/sample (M) of</cell></row><row><cell cols="3">PAConv on the S3DIS dataset using different ScoreNet depth set-</cell></row><row><cell cols="3">tings. Area-5 evaluation is adaopted. Deeper ScoreNet brings bet-</cell></row><row><cell cols="2">ter performance but has lower efficiency.</cell><cell></cell></row><row><cell>.1. Segmentation results (%) of PointNet++ [39], Point-</cell><cell></cell><cell></cell></row><row><cell>Conv [54] and our PAConv on S3DIS Area-5. * indicates plugging</cell><cell></cell><cell></cell></row><row><cell>the corresponding convolution operator to the original PointNet++</cell><cell></cell><cell></cell></row><row><cell>[39] network without changing network configurations. KPConv</cell><cell></cell><cell></cell></row><row><cell>[49] is not reported due to the result reproduced by our implemen-</cell><cell></cell><cell></cell></row><row><cell>tation is not comparable with its original version.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Network based on DGCNN<ref type="bibr" target="#b53">[53]</ref> for object classification and shape part segmentation. The architecture is totally same with the official source code of DGCNN, where the EdgeConv of DGCNN is replaced by our PAConv. We observe that the scale/resolution of the point cloud is fixed across the whole network.</figDesc><table><row><cell>(</cell><cell>, 3 )</cell><cell>(</cell><cell>, 6 4 )</cell><cell>(</cell><cell>, 6 4 )</cell><cell>(</cell><cell>, 1 2 8 )</cell><cell>(</cell><cell>, 2 5 6 )</cell><cell></cell><cell></cell><cell>MLP Max pooling</cell><cell>1024</cell><cell>MLPs</cell><cell>!"#</cell><cell>classification</cell><cell>output scores</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Classification Network</cell><cell></cell></row><row><cell></cell><cell>PAConv</cell><cell></cell><cell>PAConv</cell><cell></cell><cell>PAConv</cell><cell></cell><cell>PAConv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(</cell><cell>, 6 4 )</cell><cell>(</cell><cell>, 6 4 )</cell><cell>(</cell><cell>, 1 2 8 )</cell><cell cols="2">MLPs Max pooling</cell><cell>1024</cell><cell cols="2">repeating categorical vector MLP</cell><cell cols="3">, 1088 Part Segmentation Network MLPs</cell><cell>, !"#</cell><cell>segmentation</cell><cell>output scores</cell></row><row><cell></cell><cell>PAConv</cell><cell></cell><cell>PAConv</cell><cell></cell><cell>PAConv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Concatenate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Operator</cell></row><row><cell cols="6">Figure B.3. .¸ ( , + ) ( ! , , + )</cell><cell></cell><cell></cell><cell></cell><cell>.¸</cell><cell></cell><cell></cell><cell cols="3">skip link concatenation</cell><cell></cell></row><row><cell cols="2">s ampling &amp;</cell><cell>PAConv</cell><cell>sampling &amp;</cell><cell></cell><cell>PAConv</cell><cell></cell><cell></cell><cell cols="2">i nterpolate</cell><cell></cell><cell>unit</cell><cell cols="2">interpolate</cell><cell>unit</cell><cell></cell></row><row><cell></cell><cell>grouping</cell><cell></cell><cell>grouping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PointNet</cell><cell></cell><cell></cell><cell>PointNet</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Encoding Layers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Decoding Layers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table E .</head><label>E</label><figDesc>1. Segmentation results (%) on S3DIS Area-5. BLK and Grid signify using block sampling and grid sampling in data preprocessing, respectively. *PN2 refers to applying PointNet++<ref type="bibr" target="#b39">[39]</ref> as the backbone. The best results under block and grid sampling are respectively highlighted.MethodPre. mIoU mAcc ceil. floor wall beam col. wind. door chair table book sofa board clut. PointNet [38] BLK 47.6 66.2 88.0 88.7 69.3 42.4 23.1 47.5 51.6 42.0 54.1 38.2 9.6 29.4 35.2 BLK 65.39 75.61 94.78 97.30 75.82 63.25 51.71 58.38 57.18 69.12 71.63 61.15 39.08 52.19 58.59 BLK 66.73 76.19 93.54 94.21 80.84 52.44 41.33 64.89 68.13 67.05 71.35 62.68 50.34 62.20 58.49 PointEdge [18] BLK 67.83 76.26 -</figDesc><table><row><cell>SegCloud [48]</cell><cell>BLK</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TangentConv [47]</cell><cell>BLK</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointCNN [25]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ParamConv [52]</cell><cell>BLK</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointWeb [19]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>62.68 52.28 72.31 64.30 70.78 75.77 49.83 65.73 60.26 60.90 KPConv rigid [49] Grid 69.6 78.1 93.7 92.0 82.5 62.5 49.5 65.7 77.3 57.8 64.0 68.8 71.7 60.1 59.6 KPConv deform[49] Grid 70.6 79.1 93.6 92.4 83.1 63.9 54.3 66.1 76.6 57.8 64.0 69.3 74.9 61.3 60.3 FPConv [26] BLK 68.7 -94.8 97.5 82.6 42.8 41.8 58.6 73.4 81.0 71.0 61.9 59.8 64.2 64.2 SegGCN [23]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Lcorr (*PN2) BLK 69.31 78.65 94.30 93.46 82.80 56.88 45.74 65.21 74.90 59.74 74.60 67.41 61.78 65.79 58.36 Table E.2. Segmentation results (%) on S3DIS with 6-fold cross validation. BLK and Grid signify using block sampling or grid sampling in data pre-processing, respectively. *PN2 refers to applying PointNet++ [39] as the backbone. Method Class Inst. aero bag cap car chair ear guitar knife lamp lap motor mug pistol rocket skate table (time order) mIoU mIoU phone top board PointNet [38] 80.4 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6 PointNet++ [39] 81.9 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6 SynSpecCNN [64] 82.0 84.7 81.6 81.7 81.9 75.2 90.2 74.9 93.0 86.1 84.7 95.6 66.7 92.7 81.6 60.6 82.9 82.1 SPLATNet [45] 83.7 85.4 83.2 84.3 89.1 80.3 90.7 75.5 92.1 87.1 83.9 96.3 75.6 95.8 83.8 64.0 75.5 81.8 PCNN [2] 81.8 85.1 82.4 80.1 85.5 79.5 90.8 73.2 91.3 86.0 85.0 95.7 73.2 94.8 83.3 51.0 75.0 81.8 SpiderCNN [60] 82.4 85.3 83.5 81.0 87.2 77.5 90.7 76.8 91.1 87.3 83.3 95.8 70.2 93.5 82.7 59.7 75.8 82.8 86.2 83.8 86.1 88.2 81.6 91.0 80.1 92.1 87.8 82.2 96.2 77.9 95.7 86.8 65.3 81.7 83.6 KPConv deform [49] 85.1 86.4 84.6 86.3 87.2 81.1 91.1 77.8 92.6 88.4 82.7 96.2 78.1 95.8 85.4 69.0 82.0 83.6 DensePoint [29] 84.2 86.4 84.0 85.4 90.0 79.2 91.1 81.6 91.5 87.5 84.7 95.9 74.3 94.6 82.9 64.6 76.8 83.7 3D-GCN [27] 82.1 85.1 83.1 84.0 86.6 77.5 90.3 74.1 90.0 86.4 83.8 95.6 66.8 94.8 81.3 59.6 75.7 82.8 DGCNN [53] 82.3 85.2 84.0 83.4 86.7 77.8 90.6 74.7 91.2 87.5 82.8 95.7 66.3 94.9 81.1 63.5 74.5 82.6 PAConv (*DGC) 84.6 86.1 84.3 85.0 90.4 79.7 90.6 80.8 92.0 88.7 82.2 95.9 73.9 94.7 84.7 65.9 81.4 84.0 Table E.3. Segmentation results (%) on ShapeNet Part dataset. *DGC indicates using DGCNN [53] as the backbone.</figDesc><table><row><cell>SpecGCN [50]</cell><cell>-</cell><cell>85.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointCNN [25]</cell><cell cols="18">84.6 86.1 84.1 86.5 86.0 80.8 90.6 79.7 92.3 88.4 85.3 96.1 77.2 95.3 84.2 64.2 80.0 83.0</cell></row><row><cell>PointConv [54]</cell><cell cols="2">82.8 85.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Point2Seq [28]</cell><cell cols="18">82.2 85.2 82.6 81.8 87.5 77.3 90.8 77.1 91.1 86.9 83.9 95.7 70.8 94.6 79.3 58.1 75.2 82.8</cell></row><row><cell>PVCNN [32]</cell><cell>-</cell><cell>86.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RS-CNN [30]</cell><cell cols="18">84.0 86.2 83.5 84.8 88.8 79.6 91.2 81.1 91.6 88.4 86.0 96.0 73.7 94.1 83.4 60.5 77.7 83.6</cell></row><row><cell>InterpCNN [33]</cell><cell cols="2">84.0 86.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KPConv rigid [49]</cell><cell>85.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">putational efficiency (analyzed in Sec. 5.3).Fig. 3visualizes segmentation results. The mIoU of each class is shown in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* https://github.com/DylanWusee/pointconv_pytorch † https:///github.com/HuguesTHOMAS/KPConv-PyTorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<title level="m">Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. In Noise reduction in speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convpoint: Continuous convolutions for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unstructured Point Cloud Semantic Labeling Using Deep Segmentation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deformable kernels: Adapting effective receptive fields for object deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flex-convolution (million-scale point-cloud learning beyond grid-worlds)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><forename type="middle">P A</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Point2node: Correlation learning of dynamic-node for point cloud feature modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020. 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A-cnn: Annularly convolutional neural networks on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Komarichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Felix Järemo Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Images and Patterns</title>
		<editor>Michael Felsberg, Anders Heyden, and Norbert Krüger</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Seggcn: Efficient 3d point cloud segmentation with fuzzy spherical kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">So-net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fpconv: Learning local flattening for point convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shiming Xiang, and Chunhong Pan</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointvoxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vv-net: Voxel vae net with group convolutions for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsien-Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alban Desmaison</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Towards 3d point cloud based object maps for household environments. Robotics and Autonomous Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dolha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning geometry-disentangled representation for complementary understanding of 3d object point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10921</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Geometry sharing network for 3d point cloud classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10339</idno>
		<title level="m">vestigate indistinguishable points in semantic segmentation of 3d point cloud</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cn: Channel normalization for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Method Pre. mIoU mAcc ceil. floor wall beam col. wind. door chair table book sofa board clut</title>
		<idno>PointNet [38] BLK 41.09 48.98 88.80 97.33 69.80 0.05 3.92 46.26 10.76 52.61 58.93 40.28 5.85 26.38 33.22</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segcloud</surname></persName>
		</author>
		<idno>48] BLK 48.92 57.35 90.06 96.05 69.86 0.00 18.37 38.35 23.12 75.89 70.40 58.42 40.88 12.96 41.60</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
		<idno>25] BLK 57.26 63.86 92.31 98.24 79.41 0.00 17.60 22.77 62.09 80.59 74.39 66.67 31.67 62.05 56.74</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointedge</surname></persName>
		</author>
		<idno>18] BLK 61.85 68.30 91.47 98.16 81.38 0.00 23.34 65.30 40.02 87.70 75.46 67.78 58.45 65.61 49.36</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gacnet</surname></persName>
		</author>
		<idno>51] BLK 62.85 - 92.28 98.27 81.90 0.00 20.35 59.07 40.85 85.80 78.54 70.75 61.70 74.66 52.82</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
