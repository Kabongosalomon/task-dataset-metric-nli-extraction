<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting city safety perception based on visual image content</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">F</forename><surname>Acosta</surname></persName>
							<email>sfacostale@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UnSecureLab Research Group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">E</forename><surname>Camargo</surname></persName>
							<email>jecamargom@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UnSecureLab Research Group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting city safety perception based on visual image content</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Safety perception measurement has been a subject of interest in many cities of the world. This is due to its social relevance, and to its effect on some local economic activities. Even though people safety perception is a subjective topic, sometimes it is possible to find out common patterns given a restricted geographical and sociocultural context. This paper presents an approach that makes use of image processing and machine learning techniques to detect with high accuracy urban environment patterns that could affect citizen's safety perception.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cities are spatial structures of a significant size. In the case of Bogotá-Colombia, the city has an urban area of 307, 36km 2 , and it is divided into 20 well defined zones that are named localities. This attribute makes it difficult to appreciate and experience them completely in just one round. This is the reason why the individual perception of a city is the result of a mixture of own experiences and experiences from others. Neighborhoods often differ in their demographics, such as income, and ethnicity of people that inhabits them, but also on how safe they are perceived <ref type="bibr" target="#b5">[6]</ref>. Some of the recent works about automatic urban perception prediction have been based on Convolutional Neural Networks (CNN) <ref type="bibr" target="#b6">[7]</ref>, sets of between 100,000 and 1,000,000 images and with a wider territory scale approach. This work presents an approach based on a restricted geographical and sociocultural context, a modest image set, and on a technique called transfer learning. According to <ref type="bibr" target="#b2">[3]</ref>, perception of security can be defined as:</p><p>Perception of security (PoS) refers to the subjective assessment of the risk and the magnitude of its consequences. The risk can be defined as the likelihood that an individual will experience the effect of danger, threats, or any adverse events. Projects such as <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b7">[8]</ref> have been focused on how to structure computational models that make it possible the automatic characterization of cities. These works have based their research on the visual appearance of city streets, and its association with the people perception. Misclassification is mainly caused by the huge variability in the set of images associated to the same group or tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1902.06871v1 [cs.CV] 19 Feb 2019</head><p>In <ref type="bibr" target="#b0">[1]</ref> it is proposed that the activation output of inner layer of a CNN trained for some other classification task can be used as a visual feature of an image in a different classification task. Based on this idea, the first urban perception model based CNN was proposed in <ref type="bibr" target="#b9">[10]</ref>. CNN approach is also implemented in <ref type="bibr" target="#b1">[2]</ref>, where a CNN architecture is implemented in order to build a worldwide urban perception model.</p><p>The methodology described in <ref type="bibr" target="#b10">[11]</ref> involves an image score estimation using the fraction of times this is selected over another image, then this is corrected by the "win" and "loss" ratios of all images with which it was compared during a visual survey. In <ref type="bibr" target="#b7">[8]</ref>, people perception obtained through visual surveys is converted to a ranked score for each image using the Microsoft Trueskill algorithm <ref type="bibr" target="#b3">[4]</ref>. In <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b9">[10]</ref> it is proposed to predict pairwise comparisons by training a CNN model directly from image pairs and their crowd-sourced comparisons, which is used to generate "synthetic" comparisons by taking random image pairs as input.</p><p>Our paper presents an alternative approach in the construction of a computational model whose purpose is to predict how safe may be a given Bogotá city zone. This has been carried out in a restricted sociocultural and geographical context. In order to restrict sociocultural context, just people living in Bogotá was invited to take the visual survey. Safety perception like humor can depend on a particular sociocultural context. This is why sometimes a joke that is fun in the USA may not be fun in Germany. Geographical context restriction means that just local street images were used. One motivation for not using foreign street images is that many of the urban environment found in other countries, e.g. Washington or Germany, does not exist in Bogotá. It is expected that these restrictions reduce the variability of the underling distribution as well as the noisy of data. The presented approach makes use of a technique called transfer learning. This takes a piece of a model that has already been trained on a related task and reusing it in a new model. In particular VGG19 <ref type="bibr" target="#b11">[12]</ref> model, loaded with weights trained on ImageNet, has been used for generic image feature extraction. A particular city zone of 40km 2 was chosen for this work, and local people were asked to participate in the visual survey. Since Bogotá has an extremely heterogeneous urban environment, this restriction still guarantees a significant image variability. As a result, a model with an accuracy of 81% was obtained, and it was used to predict a safety perception score for two other neighboring localities. In order to detect different patterns, prediction on neighboring localities was performed using their particular image sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Construction</head><p>Street Image Crawler A city street image crawler was built using the Google Street View API V3.0 along with a file that contained geographical limits of the target zones.</p><p>Image filtering Since some collected images have no a wide view of a street, but a close-up of a building facade, or a totally black image, it was required to remove these images from the collected set. This was done by extracting SIFT local descriptors from each image, and excluding images with a descriptor count less than 420.</p><p>Visual survey Visual surveys was carried out through a public web site. On this, users were shown two geotagged street city images, and asked to click on one in response to the questions "which place looks safer?". In this way a bit of citizenship perception is obtained. An image pair and its associated comparison are the unit of information that will be used during the training task. The visual survey tool is online in http://wmodi.com/ . <ref type="figure" target="#fig_0">Figure 1</ref> shows this site. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image feature extraction</head><p>IN THE VGG19 Keras 1 model it was removed its fully-connected layer at the top of the network and loaded with the weights trained on ImageNet. Then, this model was used to obtain a row vector representation of each published image. This is a 512 values vector, i.e, the model is used like an image feature extractor and no training is required for this task. It is important to note that the goal of this research is to train a model that will be able to predict a visual survey vote based on a vector representation of a pair of city street images. That is, each training item is composed of two image feature vectors and their associated vote code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training set construction</head><p>The construction of the training set involved the transformation of the collected visual survey votes into number vectors and its associated label (vote code). This task starts with the elaboration of a list of all the votes found in the database. From this list, image identifiers are replaced by their associated vector descriptors. Finally, images descriptors associated to each single vote are concatenated and annotated with the respective {1, 2}. Ties (code 0 votes) were not used, but just charged votes were used. If an actual vote has been annotated with vote code 1, it means that left image was better perceived than the right image. This means also that if image positions are exchanged, the resulting vote should be annotated with vote code 2. This fact has been used to double the initial data set size. The resulting set of 24,092 (5,946*2 + 6,100*2) votes was split into training, validation and testing sets. A distribution of 65-7-28 was used. In this way 15,636 votes were used for training, 1,754 for validation and 6,702 votes for testing. Each descriptor vector corresponding to each image of the VGG19 was normalized applying the following expression,</p><formula xml:id="formula_0">f i,j = (f i,j − µ i )/σ i ,</formula><p>where f i,j is the j-th component of the i-th vector, (µ) the mean, and (σ) the standard deviation. Each feature i of each row vector j is normalized by subtracting the associated mean and scaled by the associated standard deviation. Same normalization scheme was used on the image set of the neighboring localities, the model was used to predict on.</p><p>Training phase Training sessions were performed using the TensorFlow 2 machine learning framework. The implemented neural network configuration is shown in <ref type="figure" target="#fig_1">Figure 2</ref>  The input data is the concatenation of image 0 and 1 VGG19 based on a 512 vector descriptor. This is a 1024 bin vector. A dropout technique was used as regularization method. Dropout rates of 0.5, 0.45 and 0.3 were applied to the input, hidden and output layers, respectively. The learning rate was set to 0.00001, and a mini-batch size was defined as 64. The AdamOptimizer <ref type="bibr" target="#b4">[5]</ref> method was used along with Cross Entropy Loss. <ref type="figure">Figure 3</ref> shows training session cost curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3: Training session cost curves</head><p>Accuracy report <ref type="table" target="#tab_0">Table 1</ref> presents the confusion matrix for the testing set, for which an accuracy of 81.5% was achieved. Synthetic vote generation Synthetic votes were generated dividing each locality collected image set into two same size groups. Each group holds images homogeneously distributed over the target locality area. Each group was divided into 10 subgroups. Then, each image from one group was paired with a randomly (uniform) selected image from each subgroup in the other group.</p><p>Synthetic vote prediction In order to be able to make a map based on synthetic votes, the initial dummy vote label must be substituted by one based on the model prediction. The output of the softmax layer was used for this purpose. At first, the option with the higher probability was used to annotate the associated vote as 1 or 2. However, the absolute difference between the two probabilities must be higher than 0.25, otherwise the vote was annotated with the code 0. At the same time each image positive, negative and neutral perception counters were updated.</p><p>Image Score Based on Perception Counters In section "Vote Coding and Database Structures" it was mentioned that published images had an structure associated to them and how its fields are updated during a visual survey session. This holds the image identifier, a positive perception counter, a negative perception counter and a neutral perception counter. Every image used in the synthetic vote prediction task has the same structure, and its perception counters are updated in the same way as during a visual survey session. If an image neutral perception counter is greater than 0, this value is redistributed between this image positive and negative counters. This redistribution is done by factors that are worked out form the perception counter summation of all images, which this image tied with. Finally, each image counter summation is normalized [0,1] and each counter turned into positive, negative and neutral safety perception percentages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Actual and synthetic vote maps of same zone</head><p>For the initial target zone training images were obtained, and a perception maps was built based on both, actual and synthetic votes. This was done in order to verify that colors patterns found in actual vote perception map are present in the synthetic vote perception map. For further exploration, actual 3 and synthetic 4 vote perception map are available in the project web site. <ref type="figure" target="#fig_2">Figure 4</ref> shows the color gradient used on the map set. Here left green end indicates a 100% percent of positive safety perception, and right red end 0%.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and future work</head><p>This paper presented a model that allows to predict citizen's safety perception using visual information of street images. The obtained results show a prediction accuracy of 81%, which is higher than results obtained in recent state of the art methods. Up to our knowledge, this is the first time that this analysis is performed to Bogotá city. The presented method does not require a high computing capacity, that is, 1 model iterations per hour can be performed. This feature makes this approach appropriate for the development of an online tool. It is expected to carry out more exhaustive evaluations in order determine the robustness of the predictions as well as statistical stability. All these results and the results of an earlier model evaluated by us based on SVM with a smaller amount of votes are available at http://wmodi.com/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Visual Survey Site Image serving policies Image pair serving policy and database housekeeping try to fulfill the following requirements. -Each image should have the same vote share. -Repeated image pair comparisons should be reduced to a single or no vote. -Same image comparison is not allowed and should be removed. -Comparisons of near images should be removed.Vote Coding and Database StructuresIn each visual survey session, two street images were presented to the user. The user was asked to click on one image or on the equal button in order to answer the question: Which place looks safer?. Once the user click on an image or a button, the vote is coded as follows: If click on image 0 (left side image), vote is coded as 1; if click on button equal vote is coded as 0. Finally, if click on image 1 (right side image), vote is coded as 2. For each visual survey session, the resulting vote code along with the involved left and right image identifiers were stored. There is also a database structure associated to each published image. This holds the image identifier, a positive perception counter, a negative perception counter and a neutral perception counter. If user click on image 0 (left side image), this image positive perception counter is increased by 1 and image 1 (right side image) negative perception counter is increased by 1. Same logic applies if user click on image 1 (right side image). Finally, if user click on button equal both images neutral perception counter is increased by 1. This counters are used for each image basic perception percentage estimation.Collected Data 5,505 images of the target zone were published. In one year, 17,703 image pair votes were collected. Each image participated in 6 ±1 image pair comparison session. The collected vote distribution based on its code is as follow: 5,657 code 0 votes, 5,946 code 1 votes, and 6,100 code 2 votes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Full Connected Neural Network Setup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Color gradient reference for image safety perception score3.2 Safety perception score prediction for other localitiesModel was used to generate synthetic votes on image pairs of different city zones. These image maps are available on line at left 5 and right 6 image map links.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Left: U saquen zone 94,780 synthetic votes safety perception score map. Right: M artires zone 37,880 synthetic votes safety perception score map 3.3 Predicted image set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figures 6 and 7</head><label>7</label><figDesc>are samples of other zone images whose score has been predicted by the system based on synthetic votes. It is worth noting that the trained model predicts with high precision the perception of test images. For instance, the predicted vote of left image inFigure 6captures negative perception safety characteristics such as dirty houses, lonely streets, trash, etc., whilst the predicted vote of image in right image inFigure capturespositive perception safety characteristics such illumination, green zones, clean streets, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>M artires zone:0%,49%,86% positive safety perception images U saquen zone:6%,41%,87% positive safety perception images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Testing Confusion Matrix</cell></row><row><cell></cell><cell cols="2">Predicted Vote Code</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>Total</cell></row><row><cell>1</cell><cell>2,777</cell><cell>574</cell><cell>3,351</cell></row><row><cell>2</cell><cell>663</cell><cell>2,688</cell><cell>3,351</cell></row><row><cell cols="2">Total 3,440</cell><cell>3,262</cell><cell>6,702</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://keras.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.tensorflow.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://wmodi.com/chapinero 17703actualvote jun04 2018 imgscore 4 http://wmodi.com/chapinero 55040NNsyntheticvote jun04 2018 imgscore 5 http://wmodi.com/usaquen 94780NNsyntheticvote jun04 2018 imgscore 6 http://wmodi.com/martires 37880NNsyntheticvote jun04 2018 imgscore</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Eecs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkeley</forename><surname>Edu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Learning the City : Quantifying Urban Perception At A Global Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><forename type="middle">A</forename><surname>Hidalgo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hotspot mapping for perception of security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Galvis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2nd International Smart Cities Conference: Improving the Citizens Quality of Life, ISC2 2016 -Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="0" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TrueSkill: A Bayesian Skill Rating System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="569" to="576" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Do People Shape Cities, or Do Cities Shape People? The Co-evolution of Physical, Social, and Economic Change in Five Major</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott Duke Kominers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glaeser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Garlapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Eric</forename><surname>Heckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Strand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Tobio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petkova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>U.S. Cities</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Streetscore -Predicting the Perceived Safety of One Million Streetscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Philipoom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning high-level judgments of urban perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="494" to="510" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Predicting and Understanding Urban Perception with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Buló</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Collaborative Image of The City: Mapping the Inequality of Urban Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Salesses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schechtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><forename type="middle">A</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
