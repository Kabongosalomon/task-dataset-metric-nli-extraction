<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Generative Models using Denoising Density Estimators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><forename type="middle">A Bigdeli</forename><surname>Csem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neuchâtel</forename><surname>Switzerland</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>ETHZ</roleName><forename type="first">Tiziano</forename><surname>Portenier</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zurich</forename><surname>Switzerland</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>CSEM</roleName><forename type="first">L</forename><forename type="middle">Andrea</forename><surname>Dunbar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neuchâtel</forename><surname>Switzerland</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Generative Models using Denoising Density Estimators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning probabilistic models that can estimate the density of a given set of samples, and generate samples from that density, is one of the fundamental challenges in unsupervised machine learning. We introduce a new generative model based on denoising density estimators (DDEs), which are scalar functions parameterized by neural networks, that are efficiently trained to represent kernel density estimators of the data. Leveraging DDEs, our main contribution is a novel technique to obtain generative models by minimizing the KL-divergence directly. We prove that our algorithm for obtaining generative models is guaranteed to converge to the correct solution. Our approach does not require specific network architecture as in normalizing flows, nor use ordinary differential equation solvers as in continuous normalizing flows. Experimental results demonstrate substantial improvement in density estimation and competitive performance in generative model training.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning generative probabilistic models from raw data is one of the fundamental problems in unsupervised machine learning. These models enable sampling from the probability density represented by the input data, or also performing density estimation and inference of latent variables. Recently, the use of deep neural networks has led to significant advances in this area. For example, generative adversarial networks <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref> can be trained to sample very high dimensional densities, but they do not provide density estimation or inference. Inference in Boltzman machines <ref type="bibr" target="#b39">(Salakhutdinov and Hinton, 2009</ref>) is tractable only under approximations <ref type="bibr" target="#b45">(Welling and Teh, 2003)</ref>. Variational autoencoders <ref type="bibr" target="#b23">(Kingma and Welling, 2014)</ref> provide functionality for both (approximate) inference and sampling. Finally, normalizing flows <ref type="bibr" target="#b11">(Dinh et al., 2014)</ref> perform all three operations (sampling, density estimation, inference) efficiently.</p><p>In this paper we introduce a novel type of generative model based on what we call denoising density estimators (DDEs), which supports efficient sampling and density estimation. Our approach to construct a sampler is straightforward: assuming we have a density estimator that can be efficiently trained and evaluated, we learn a sampler by forcing its generated density to be the same as the input data density via minimizing their Kullback-Leibler (KL) divergence. In particular, we use the reverse KL divergence, which avoids saddle points when the two distributions are non-overlapping. In our approach, the density estimator is derived from the theory of denoising autoencoders, hence our term denoising density estimator. Compared to normalizing flows, a key advantage of our theory is that it does not require any specific network architecture, except differentiability, and we do not need to solve ordinary differential equations (ODE) like in continuous normalizing flows. In summary, our main contribution is a novel approach to obtain a generative model by explicitly estimating the energy (un-normalized density) of the generated and true data distributions and minimizing the statistical divergence of these densities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generative adversarial networks <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref> are currently the most widely studied type of generative probabilistic models for very high dimensional data. GANs are often difficult to train, however, and they can suffer from mode-collapse, sparking renewed interest in alternatives. A common approach is to formulate these models as mappings between a latent space and the data domain, and one way to categorize them is to consider the constraints on this mapping. In normalizing flows <ref type="bibr" target="#b11">(Dinh et al., 2014;</ref><ref type="bibr" target="#b36">Rezende and Mohamed, 2015)</ref> the mapping is invertible and differentiable, such that the data density can be estimated using the determinant of its Jacobian, and inference can be perfomed via the inverse mapping. Normalizing flows can be trained simply using maximum likelihood estimation (MLE) <ref type="bibr" target="#b12">(Dinh et al., 2017)</ref>. The challenge, however, is to design efficient computational structures for the required operations <ref type="bibr" target="#b18">(Huang et al., 2018;</ref><ref type="bibr" target="#b21">Kingma and Dhariwal, 2018)</ref>. <ref type="bibr" target="#b7">Chen et al. (2018)</ref> and <ref type="bibr" target="#b15">Grathwohl et al. (2019)</ref> derive continuous normalizing flows by parameterizing the dynamics (the time derivative) of an ODE using a neural network, but it comes at the cost of solving ODEs to produce outputs. In contrast, in variational techniques the relation between the latent variables and data is probabilistic, usually expressed as a Gaussian likelihood function. Hence computing the marginal likelihood requires integration over latent space. To make this tractable, it is common to bound the marginal likelihood using the evidence lower bound <ref type="bibr" target="#b23">(Kingma and Welling, 2014)</ref>. Recently, <ref type="bibr" target="#b25">Li and Malik (2018)</ref> proposed an approximate form of MLE, which they call implicit MLE (IMLE), that can also be performed without requiring invertible mappings. As a disadvantage, IMLE requires nearest neighbor queries in (high dimensional) data space.</p><p>Not all generative models include a latent space, for example autoregressive models <ref type="bibr" target="#b43">(van den Oord et al., 2016)</ref> or denoising autoencoders (DAEs) <ref type="bibr" target="#b1">(Alain and Bengio, 2014)</ref>. In particular, <ref type="bibr" target="#b1">Alain and Bengio (2014)</ref> and <ref type="bibr" target="#b40">Saremi and Hyvärinen (2019)</ref> use the well known relation between DAEs and the score of the corresponding data distributions <ref type="bibr" target="#b44">(Vincent, 2011;</ref><ref type="bibr" target="#b35">Raphan and Simoncelli, 2011)</ref> to construct an approximate Markov Chain sampling procedure. Similarly,  and  use DAEs to learn the gradient of image densities for optimizing maximum a-posteriori problems in image restoration. We build on DAEs, but formulate an estimator for the un-normalized, scalar density, rather than for the score (a vector field). This is crucial to allow us to train a generator instead of requiring Markov chain sampling, which has the disadvantages of requiring sequential sampling and producing correlated samples.</p><p>Instead of using a denoising objective, score-matching can also be achieved by minimizing Stein's loss for the true and estimated density gradients. <ref type="bibr" target="#b22">Kingma and LeCun (2010)</ref> use a regularized version of the loss to parametrize a product-of-experts model for images, and <ref type="bibr" target="#b26">Li et al. (2019)</ref> train deep density estimators based on exponential family kernels. These techniques require computation of third order derivatives, however, limiting the dimensionality of their models.  extend this approach by introducing a sliced score-matching objective that leads to more efficient training. Unlike these techniques, DDEs are optimized using a denoising objective, hence they can be optimized without approximations, nor higher order derivatives. This allows us to efficiently train an exact generator that scales well with the data dimensionality. In addition, Song and Ermon (2019) formulate a generative model using Langevin dynamics, which requires an iterative sampling procedure that provides exact sampling only asymptotically. Similarly, <ref type="bibr" target="#b9">Dai et al. (2019b)</ref> use adversarial training to learn dynamics for generating samples. Unlike these approaches, we do not require an iterative sampling scheme and our generator produces samples in single forward passes.</p><p>Other energy-based techniques for generative models include the work by <ref type="bibr" target="#b20">Kim and Bengio (2016)</ref>, who use directed graphs to learn densities in latent space and to train their generator. The approximation in this approach limits their generalization to complex and higher dimension datasets. Using kernel exponential families, <ref type="bibr" target="#b8">Dai et al. (2019a)</ref> train a density estimator at the same time as their dual generator. Similar to other score-matching optimizations, their approach requires quadratic computations with respect to the input dimensions at each gradient calculation. Moreover, they only report generated results on 2D toy examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Denoising Density Estimators (DDEs)</head><p>First we describe how to estimate a density using a variant of denoising autoencoders (DAEs). More precisely, this approach allows us to obtain the density smoothed by a Gaussian kernel, which is equivalent to kernel density estimation <ref type="bibr" target="#b33">(Parzen, 1962)</ref>, up to a normalizing factor. Originally, the optimal DAE r : R n → R n <ref type="bibr" target="#b44">(Vincent, 2011;</ref><ref type="bibr" target="#b1">Alain and Bengio, 2014)</ref> is defined as the function minimizing the following denoising loss,</p><formula xml:id="formula_0">L DAE (r; p, σ η ) = E x∼p,η∼N (0,σ 2 η ) r(x + η) − x 2 ,<label>(1)</label></formula><p>where the data x is distributed according to a density p over R n , and η ∼ N (0, σ 2 η ) represents n-dimensional, isotropic additive Gaussian noise with variance σ 2 η . It has been shown <ref type="bibr" target="#b37">(Robbins, 1956;</ref><ref type="bibr" target="#b35">Raphan and Simoncelli, 2011;</ref>) that the optimal DAE r * (x) minimizing L DAE can be expressed as follows, which is also known as Tweedie's formula,</p><formula xml:id="formula_1">r * (x) = x + σ 2 η ∇ x logp(x),<label>(2)</label></formula><p>where ∇ x is the gradient with respect to the input x,p(s) = [p * k](x) denotes the convolution between the data and noise distributions p(x), and k = N (0, σ 2 η ). Inspired by this result, we reformulate the DAE-loss as a noise estimation loss,</p><formula xml:id="formula_2">L NEs (f ; p, σ η ) = E x∼p,η∼N (0,σ 2 η ) f (x + η) + η/σ 2 η 2 ,<label>(3)</label></formula><p>where f : R n → R n is a vector field that estimates the noise vector −η/σ 2 η . Similar to <ref type="bibr" target="#b44">Vincent (2011)</ref> and <ref type="bibr" target="#b1">Alain and Bengio (2014)</ref>, we formulate the following proposition and provide the proof in the supplementary material:</p><formula xml:id="formula_3">Proposition 1. There is a unique minimizer f * (x) = arg min f L NEs (f ; p, σ η ) that satisfies f * (x) = ∇ x logp(x) = ∇ x log[p * k](x).</formula><p>(4) That is, the optimal estimator corresponds to the gradient of the logarithm of the Gaussian smoothed densityp(x), that is, the score of the density.</p><p>A key observation is that the desired vector-field f * is the gradient of a scalar function and conservative. Hence we can write the noise estimation loss in terms of a scalar function s : R n → R instead of the vector field f , which we call the denoising density estimation loss,</p><formula xml:id="formula_4">L DDE (s; p, σ η ) = E x∼p,η∼N (0,σ 2 η ) ∇ x s(x + η) + η/σ 2 η 2 .<label>(5)</label></formula><p>A similar formulation has recently been proposed by <ref type="bibr" target="#b40">Saremi and Hyvärinen (2019)</ref>. Our terminology is motivated by the following corollary:</p><formula xml:id="formula_5">Corollary 1. The minimizer s * (x) = arg min s L DDE (s; p) satisfies s * (x) = logp(x) + C, (6) with some constant C ∈ R.</formula><p>Proof. From Proposition 1 and the definition of L DDE (s; p) we know that ∇ x s * (x) = ∇ x logp(x), which leads immediately to the corollary.</p><p>In summary, we have shown how modifying the denoising autoencoder loss (Eq. 1) into a noise estimation loss based on the gradients of a scalar function (Eq. 5) allows us to derive a density estimator (Corollary 1), which we call the denoising density estimator (DDE). In practice, we approximate the DDE using a neural network s(x; θ). For illustration, <ref type="figure" target="#fig_1">Figure 1</ref> shows 2D distribution examples, which we approximate using a DDE implemented as a multi-layer perceptron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Generative Models using DDEs</head><p>By leveraging DDEs, our key contribution is to formulate a novel training algorithm to obtain generators for given densities, which can be represented by a set of samples or as a continuous function. In either case, we denote the smoothed data densityp, which is obtained by training a DDE in case the input is given as a set of samples as described in Section 3. We express our samplers using mappings x = g(z), where x ∈ R n , and z ∈ R m (usually n &gt; m) is a latent variable, which typically has a standard normal distribution. In contrast to normalizing flows, g(z) does not need to be invertible. Let us denote the distribution of x induced by the generator as q, that is q ∼ g(z), and also its Gaussian smoothed versionq = q * k.</p><p>We obtain the generator by minimizing the KL divergence D KL (q||p) between the density induced by the generatorq and the data densityp. Our algorithm is based on the following observation: Proposition 2. Given a scalar function ∆ : R n → R that satisfies the following conditions:</p><formula xml:id="formula_6">D KL (q||p) = q, logq − logp &gt; q + ∆, logq − logp ,<label>(7)</label></formula><p>∆, 1 = 0,</p><formula xml:id="formula_7">∆ 2 &lt; , (pointwise exponentiation) (9) then D KL (q||p) &gt; D KL (q + ∆||p) for small enough .<label>(8)</label></formula><p>Proof. We will use the first order approximation log(q + ∆) = logq + ∆/q + o(∆ 2 ), where the division is pointwise. Using ·, · to denote the inner product, we can write</p><formula xml:id="formula_8">D KL (q + ∆||p) = q + ∆, log(q + ∆) − logp = q + ∆, logq + ∆/q + o(∆ 2 ) − logp = q, logq − logp + ∆, logq − logp + q, ∆/q + ∆, ∆/q + O(∆ 2 ). (10) This means D KL (q + ∆||p) − D KL (q||p) = ∆, logq − logp + q, ∆/q + ∆, ∆/q + O(∆ 2 ) &lt; 0 (11)</formula><p>because the first term on the right hand side is negative (first assumption in Equation 7), the second term is zero (second assumption in Equation 8), and the third and fourth terms are quadratic in ∆ and can be ignored for ∆ &lt; when is small enough (third assumption in Equation 9).</p><p>Based on the above observation, Algorithm 1 minimizes D KL (q||p) by iteratively computing updated densitiesq + ∆ that satisfy the conditions from Proposition 2, hence D KL (q||p) &gt; D KL (q + ∆||p). This is guaranteed to converge to a global minimum, because D KL (q||p) is convex inq.</p><p>At the beginning of each iteration in Algorithm 1 (Line 3), by definition q is the density obtained by sampling our generator x = g(z; φ), z ∼ N (0, 1) (n-dimensional standard normal distribution), and the generator is a neural network with parameters φ. In addition,q = q * k is defined as the density obtained by sampling x = g(z; φ) + η, z ∼ N (0, 1), η ∼ N (0, σ 2 η ). Finally, the DDE sq correctly estimatesq, that is logq(x) = sq(x) + C. In each iteration, we update the generator such that its density is changed by a small ∆ that satisfies the conditions from Proposition 2. We achieve this by computing a gradient descent step of E x=g(z;φ)+η sq(x) − logp(x) + C with respect to the generator parameters φ (Line 4). The constant C can be ignored since we only need the gradient (q always integrate to one after any generator update). A small enough learning rate guarantees that condition one (Equation 7) in Proposition 2 is satisfied. The second condition (Equation 8) is satisfied because we update the distribution by updating its generator, and the third condition (Equation 9) is also satisfied under a small enough learning rate (and assuming the generator network is Lipschitz continuous). After updating the generator, we update the DDE to correctly estimate the new density produced by the updated generator (Line 6). Note that in practice, we perform fixed number of iterations (5-10 steps similar to GANs) to optimize the DDE, which did not lead to any instabilities.</p><p>Note that it is crucial in the first step in the iteration in Algorithm 1 that we sample using g(z; φ) + η and not g(z; φ). This allows us, in the second step, to use the updated g(z; φ) to train a DDE sq that exactly (up to a constant) matches the density generated by g(z; φ) + η. Even though in this approach we only minimize the KL divergence with the "noisy" input densityp, the sampler g(z; φ) still converges to a sampler of the underlying density p in theory (exact sampling).</p><p>Algorithm 1 Training steps for the generator. The input to the algorithm is a pre-trained optimal DDE on input data logp(x) and a learning rate δ.</p><p>1: Initialize generator parameters φ 2: Initialize DDE sq = arg min s L DDE (s; q, σ η ) with q ∼ g(z; φ), z ∼ N (0, 1) 3: while not converged do // sq is now the density (up to a constant) of g(z; φ) + η 8: end while Exact Sampling. Our objective involves reducing the KL divergence between the Gaussian smoothed generated densityq and the data densityp. This also implies that the density q obtained from sampling the generator g(z; φ) is identical with the data density p, without Gaussian smoothing, which can be expressed as the following corollary: Corollary 2. Letp andq be related to densities p and q, respectively, via convolutions using a Gaussian k, that isp = p * k,q = q * k. Then the smoothed densitiesp andq are the same if and only if the data density p and the generated density q are the same.</p><formula xml:id="formula_9">4: φ = φ − δ∇ φ E x=g(z;φ)+η sq(x) − logp(x) , with z ∼ N (0, 1), η ∼ N (0, σ 2 η ) 5: // q ∼ g(z; φ) now</formula><p>This follows immediately from the convolution theorem and the fact that the Fourier transform of Gaussian functions is non-zero everywhere, that is, Gaussian blur is invertible.</p><p>Relation to GANs. In the original GANs <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref>, the generator is trained to minimize the Jensen-Shannon divergence between generated and real data distributions. Our model is optimized to minimize the KL-divergence instead, which has been shown to achieve better likelihood scores compared to GANs <ref type="bibr" target="#b30">(Nowozin et al., 2016)</ref>. Moreover, we use the reverse KL-divergence loss in our training, which unlike forward KL-divergence, avoids saddle points when the two distributions are non-overlapping. This is because minimizing the reverse KL divergence can be reformulated as</p><formula xml:id="formula_10">arg miñ q D KL (q||p) = arg max q E x∼q [logp(x)] + H(q(x)),<label>(12)</label></formula><p>which includes a term that attempts to maximize the entropy H of the generated distributionq. Wasserstein-GANs address the same issue by using the Wasserstein distance between the two distributions to formulate their loss. These models, however, require the discriminator network to guarantee Lipschitz continuity, which is imposed either by weight-clipping <ref type="bibr" target="#b2">Arjovsky et al. (2017)</ref> or gradient penalty methods <ref type="bibr" target="#b16">(Gulrajani et al., 2017)</ref>. Our DDEs explicitly impose Gaussian-smoothness on the data distribution, which guarantees that the density is non-zero everywhere. Additionally, the DDEs are trained to exactly constrain their gradients with respect to their inputs (Equation 5), without requiring additional techniques to control gradient magnitudes or weight clipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>2D Comparisons. Similar to <ref type="bibr" target="#b15">Grathwohl et al. (2019)</ref>, we perform experiments for 2D density estimation and visualization over three datasets. Additionally, we learn generative models. For our DDE networks, we used multi-layer perceptrons with residual connections. All networks have 25 layers, each with 32 channels and Softplus activation. For training we use 2048 samples per iteration to estimate the expected values. <ref type="figure" target="#fig_1">Figure 1</ref> shows the comparison of our method with Glow <ref type="bibr" target="#b21">(Kingma and Dhariwal, 2018)</ref>, <ref type="bibr">BNAF (De Cao et al., 2019)</ref>, and FFJORD <ref type="bibr" target="#b15">(Grathwohl et al., 2019)</ref>. Our DDEs can estimate the density accurately and capture the underlying complexities of each density. Due to inherent smoothing as in kernel density estimation (KDE), our method induces a small blur to the density compared to BNAF. To demonstrate this effect, we show DDEs trained with both small and large noise standard deviations σ η = 0.05 and σ η = 0.2. However, our DDE can estimate the density coherently through the data domain, whereas BNAF produces noisy approximation across the data.</p><p>Generator training and sampling is demonstrated in <ref type="figure" target="#fig_1">Figure 1</ref> on the right. The sharp edges of the checkerboard samples imply that, due to invertibility of a small Gaussian blur, the generator learns to sample from the sharp target density even though the DDEs estimate noisy densities. While the generator update in theory requires DDE networks to be optimal at each gradient step, we take a limited number of 10 DDE gradient descent steps for each generator update to accelerate convergence. We summarize the training parameters used in these experiments the supplementary material.</p><p>MNIST. <ref type="figure">Figure 2</ref> illustrates our generative training on MNIST (LeCun, 1998) using Algorithm 1. We use a dense block architecture with fully connected layers here and refer to the supplementary material for the network and training details, including additional results for Fashion-MNIST <ref type="bibr" target="#b47">(Xiao et al., 2017)</ref>. <ref type="figure">Figure 2</ref> shows qualitatively that our generator is able to replicate the underlying distributions. In addition, latent-space interpolation demonstrates that the network learns an intuitive and interpretable mapping from normally distributed latent variables to samples of the data distribution.</p><p>CelebA. <ref type="figure" target="#fig_2">Figure 3</ref> shows additional experiments on the CelebA dataset <ref type="bibr" target="#b27">(Liu et al., 2015)</ref>. The images in the dataset have 32 × 32 × 3 dimensions and we normalize the pixel values to be in range [−0.5, 0.5]. To show the flexibility of our algorithm with respect to neural network architectures, here we use a style-based generator <ref type="bibr" target="#b19">(Karras et al., 2019)</ref> architecture for our generator network. Please refer to the supplementary material for network and training details. <ref type="figure" target="#fig_2">Figure 3</ref> shows that our approach can produce natural-looking images, and the model has learned to replicate the global distribution with a diverse set of images and different characteristics.</p><p>Quantitative Evaluation with Stacked-MNIST. We perform a quantitative evaluation of our approach based on the synthetic Stacked-MNIST <ref type="bibr" target="#b29">(Metz et al., 2016)</ref> dataset, which was designed to analyse mode-collapse in generative models. The dataset is constructed by stacking three randomly chosen digit images from MNIST to generate samples of size 28 × 28 × 3. This augments the number of classes to 10 3 , which are considered as distinct modes of the dataset. Mode-collapse can be quantified by counting the number of nodes generated by a model. Additionally, the quality of the distribution can be measured by computing the KL-divergence between the generated class distribution and the original dataset, which has a uniform distribution in terms of class labels. Similar to prior work <ref type="bibr" target="#b29">(Metz et al., 2016)</ref>, we use an external classifier to measure the number of classes that each generator produces by separately inferring the class of each channel of the images.  <ref type="figure">Figure 4</ref> reports the quantitative results for this experiment by comparing our method with well-tuned GAN models. DCGAN <ref type="bibr" target="#b34">(Radford et al., 2015)</ref> implements a basic GAN training strategy using a stable architecture. WGAN uses the Wasserstein distance , and WGAN+GP includes a gradient penalty to regularize the discriminator <ref type="bibr" target="#b16">(Gulrajani et al., 2017)</ref>. For a fair comparison, all methods use the DCGAN network architecture. Since our method requires two DDE networks, we have used fewer parameters in the DDEs so that in total we preserve the same number of parameters and capacity as the other methods. For each method, we generate batches of 512 samples per training iteration and count the number of classes within each batch (that is, the maximum number of different labels in each batch is 512). We also plot the reverse KL-divergence to the uniform ground truth class distribution. Using the two measurements we can see how well each method replicates the distribution in terms of diversity and balance. Without fine-tuning and changing the capacity of our network models, our approach is comparable to modern GANs such as WGAN and WGAN+GP, which outperform DCGAN by a large margin in this experiment.</p><p>We also report results for sampling techniques based on Score-Matching. We trained a Noise Conditional Score Network (NCSN) parametrized with a UNET architecture <ref type="bibr" target="#b38">(Ronneberger et al., 2015)</ref>, which is then followed by a sampling algorithm using the Annealed Langevin Dynamics (ALD) as described by . We refer to this method as UNET+ALD. We also implemented a model based on our approach called DDE+ALD, where we used our DDE network in combination with iterative Langevin sampling. While our training loss is equivalent to the score-matching objective, the DDE network outputs a scalar and explicitly enforces the score to be a conservative vector field by computing it as the gradient of its scalar output. DDE+ALD uses the spatial gradient of the DDE for iterative sampling with ALD , instead of our proposed direct, one-step generator as described in Section 4. We observe that DDE+ALD is more stable compared to the UNET+ALD baseline, even though the UNET achieves a lower loss during training. We believe that this is because DDEs guarantee conservativeness of the distribution gradients (i.e. scores), which leads to more diverse and stable data generation as we see in <ref type="figure">Figure 4</ref>. Furthermore, our approach with direct sampling outperforms both UNET+ALD and DDE+ALD.   <ref type="table" target="#tab_2">Table 2</ref>. We have omitted the results of the BSDS300 dataset <ref type="bibr" target="#b28">(Martin et al., 2001)</ref>, since we could not estimate the normalizing constant reliably (due to high dimensionality of the data). To train our DDEs, we used Multi-Layer Perceptrons (MLP) with residual connections between each layer. All networks have 25 layers, with 64 channels and Softplus activations, except for GAS and HEPMASS, which employ 128 channels. We trained the models for 400 epochs using learning rate of 2.5 × 10 −4 with linear decay with scale of 2 every 100 epochs. Similarly, we started the training by using noise standard deviation σ η = 0.1 and decreased it linearly with the scale of 1.1 up to a dataset specific value, which we set to 5 × 10 −2 for POWER, 4 × 10 −2 for GAS, 2 × 10 −2 for HEPMASS, and 0.15 for MINIBOON. We estimate the normalizing constant via importance sampling using a Gaussian distribution with the mean and variance of the DDE input distribution. We average 5 estimations using 51200 samples each (we used 10 times more samples for GAS), and we indicate the variance of this average in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Discussion. Our approach relies on a key hyperparameter σ η that determines the training noise for the DDE, which we currently set manually. In the future we will investigate strategies to determine this parameter in a data-dependent manner. Another challenge is to obtain high-quality results using complex, high-dimensional data such CIFAR or high-resolution images. In practice, one strategy is to combine our approach with latent embedding learning methods <ref type="bibr" target="#b6">(Bojanowski et al., 2018)</ref>, in a similar fashion as proposed by <ref type="bibr" target="#b17">Hoshen et al. (2019)</ref>. The robustness of our technique with very high-dimensional data could potentially also be improved by leveraging slicing techniques <ref type="bibr" target="#b46">Wu et al., 2019)</ref>. Finally, we uses three networks to learn a generator (a DDE each for the input and generated data, and the generator). Our generator training approach, however, is independent of the type of density estimator, and techniques other than DDEs could also be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented a novel approach to learn generative models using denoising density estimators (DDE), and our theoretical analysis proves that our training algorithm converges to a unique optimum. Further, our technique does not require specific neural network architectures or ODE integration. We achieve state of the art results on a standard log-likelihood evaluation benchmark compared to recent techniques based on normalizing flows, continuous flows, and autoregressive models, and we demonstrate successful generators on diverse image data sets. Finally, a quantitative evaluation using the stacked MNIST data set shows that our approach avoids mode collapse similarly as state of the art Wasserstein GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>We propose a novel generative model that also provides a density estimate, which has potentially very broad applicability with many types of data. Generative models could be used to easily author visual media, for example, which would allow individuals or small teams to create content for education, training, or entertainment very easily. The density estimate could be used to leverage the generative model as a prior in highly underconstrained inverse problems, such as image or video restoration and various computational imaging techniques. Nefarious applications include deepfakes that attempt to mislead the consumers of the generated content. Generative models also replicate any biases that are inherent in the training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>indicates the updated density using the updated φ 6: sq = arg min s L DDE (s; q, σ η ) // In practice, we only take few optimization steps 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Density estimation in 2D, showing that we can accurately capture these densities with few visual artifacts. The rightmost column shows samples generated using our generative model training.(a) Generated samples (b) Real samples (c) Interpolated samples using our model Figure 2: Generated MNIST (a), from the dataset (b), and latent space interpolation (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Generated results on 32 × 32 images from the celebA dataset<ref type="bibr" target="#b27">(Liu et al., 2015)</ref>.(a) Generated modes per batch (b) KL-divergence Figure 4: Mode-collapse experiment results on Stacked-MNIST as a function of training iterations (for discriminator or DDE). (a) Number of generated modes per batch of size 512. (b) Reverse KL-divergence between the generated and the data distribution in the logarithmic domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>summarizes the differences of our approach to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b12">Dinh et al. (2017)</ref> 0.17 ±.01 8.33 ±.14 −18.71 ±.02 −13.55 ±.49<ref type="bibr" target="#b21">Kingma and Dhariwal (2018)</ref> 0.17 ±.01 8.15 ±.40 −18.92 ±.08 −11.35 ±.07 Germain et al. (2015)* 0.40 ±.01 8.47 ±.02 −15.15 ±.02 −12.27 ±.47 Papamakarios et al. (2017) 0.24 ±.01 10.08 ±.02 −17.73 ±.02 −12.24 ±.45</figDesc><table><row><cell>Model</cell><cell>POWER d = 6, N ≈ 2M</cell><cell>GAS d = 8, N ≈ 1M</cell><cell>HEPMASS d = 21, N ≈ 500K</cell><cell>MINIBOON d = 43, N ≈ 36K</cell></row><row><cell>Papamakarios et al. (2017)*</cell><cell>0.30 ±.01</cell><cell>9.59 ±.02</cell><cell>−17.39 ±.02</cell><cell>−11.68 ±.44</cell></row><row><cell>Grathwohl et al. (2019)</cell><cell>0.46 ±.01</cell><cell>8.59 ±.12</cell><cell>−14.92 ±.08</cell><cell>−10.43 ±.04</cell></row><row><cell>Huang et al. (2018)</cell><cell>0.62 ±.01</cell><cell>11.96 ±.33</cell><cell>−15.09 ±.40</cell><cell>−8.86 ±.15</cell></row><row><cell>Oliva et al. (2018)</cell><cell>0.60 ±.01</cell><cell>12.06 ±.02</cell><cell>−13.78 ±.02</cell><cell>−11.01 ±.48</cell></row><row><cell>De Cao et al. (2019)</cell><cell>0.61 ±.01</cell><cell>12.06 ±.09</cell><cell>−14.71 ±.38</cell><cell>−8.95 ±.07</cell></row><row><cell>Li et al. (2019)</cell><cell>-</cell><cell>-</cell><cell>≤ −20</cell><cell>≤ −40</cell></row><row><cell>Ours</cell><cell>0.97 ±.18</cell><cell>9.73 ±1.14</cell><cell>-11.3 ±.16</cell><cell>-6.94 ±1.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average log-likelihood comparison in four datasets<ref type="bibr" target="#b3">(Asuncion and Newman, 2007)</ref>. The top rows includes dataset size and dimensionality, bottom rows are normalized by sampling. The upper section includes methods that estimate normalized densities. Results of Li et al. (2019) are read from the bar plots reported in their article. *Mixture of Gaussions. Best performances are in bold.Real Data Density Estimation. We follow the experiments in BNAF (De Cao et al., 2019) for density estimation, which includes the POWER, GAS, HEPMASS, and MINIBOON datasets<ref type="bibr" target="#b3">(Asuncion and Newman, 2007)</ref>. Since DDEs estimate densities up to their normalizing constant, we approximate the constant using Monte Carlo estimation here. Similarly,<ref type="bibr" target="#b26">Li et al. (2019)</ref> use sampling to estimate the normalizing constant. We show average log-likelihoods over test sets and compare to state-of-the-art methods for normalized density estimation in</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generative adversarial density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10782" to="10791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What regularized auto-encoders learn from the data-generating distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3743" to="3773" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia. PMLR</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bigdeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09964</idno>
		<title level="m">Image restoration using autoencoding priors</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep mean-shift priors for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bigdeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="763" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Pas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="600" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kernel exponential family estimation via doubly dual embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2321" to="2330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exponential family estimation via adversarial dynamics embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10977" to="10988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aziz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04676</idno>
		<title level="m">Block neural autoregressive flow</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Made: Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="881" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">; Z</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note>Ghahramani,</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<title level="m">Ffjord: Free-form continuous dynamics for scalable reversible generative models. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-adversarial image synthesis with generative latent nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural autoregressive flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep directed generative models with energy-based probability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03439</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularized estimation of image statistics by score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1126" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Implicit maximum likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1809.09087</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep kernels for exponential family densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Vancouver</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<title level="m">Unrolled generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09819</idno>
		<title level="m">Transformation autoregressive networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Least squares estimation without priors or supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raphan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="374" to="420" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Bach, F. and Blei, D.</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An empirical bayes approach to statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Third Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1956" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
	<note>Contributions to the Theory of Statistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<idno>abs/1903.02334</idno>
		<title level="m">Neural empirical bayes. ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>abs/1907.05600</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>abs/1905.07088</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D D</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee,</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Approximate inference in boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="50" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sliced wasserstein generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<title level="m">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
