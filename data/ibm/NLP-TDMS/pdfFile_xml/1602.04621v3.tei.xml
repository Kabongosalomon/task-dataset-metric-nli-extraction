<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Exploration via Bootstrapped DQN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
							<email>apritzel@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Roy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Exploration via Bootstrapped DQN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as -greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We study the reinforcement learning (RL) problem where an agent interacts with an unknown environment. The agent takes a sequence of actions in order to maximize cumulative rewards. Unlike standard planning problems, an RL agent does not begin with perfect knowledge of the environment, but learns through experience. This leads to a fundamental trade-off of exploration versus exploitation; the agent may improve its future rewards by exploring poorly understood states and actions, but this may require sacrificing immediate rewards. To learn efficiently an agent should explore only when there are valuable learning opportunities. Further, since any action may have long term consequences, the agent should reason about the informational value of possible observation sequences. Without this sort of temporally extended (deep) exploration, learning times can worsen by an exponential factor.</p><p>The theoretical RL literature offers a variety of provably-efficient approaches to deep exploration <ref type="bibr" target="#b8">[9]</ref>. However, most of these are designed for Markov decision processes (MDPs) with small finite state spaces, while others require solving computationally intractable planning tasks <ref type="bibr" target="#b7">[8]</ref>. These algorithms are not practical in complex environments where an agent must generalize to operate effectively. For this reason, large-scale applications of RL have relied upon statistically inefficient strategies for exploration <ref type="bibr" target="#b11">[12]</ref> or even no exploration at all <ref type="bibr" target="#b22">[23]</ref>. We review related literature in more detail in Section 4.</p><p>Common dithering strategies, such as -greedy, approximate the value of an action by a single number. Most of the time they pick the action with the highest estimate, but sometimes they choose another action at random. In this paper, we consider an alternative approach to efficient exploration inspired by Thompson sampling. These algorithms have some notion of uncertainty and instead maintain a distribution over possible values. They explore by randomly select a policy according to the probability it is the optimal policy. Recent work has shown that randomized value functions can implement something similar to Thompson sampling without the need for an intractable exact posterior update. However, this work is restricted to linearly-parameterized value functions <ref type="bibr" target="#b15">[16]</ref>. We present a natural extension of this approach that enables use of complex non-linear generalization methods such as deep neural networks. We show that the bootstrap with random initialization can produce reasonable uncertainty estimates for neural networks at low computational cost. Bootstrapped DQN leverages these uncertainty estimates for efficient (and deep) exploration. We demonstrate that these benefits can extend to large scale problems that are not designed to highlight deep exploration. Bootstrapped DQN substantially reduces learning times and improves performance across most games. This algorithm is computationally efficient and parallelizable; on a single machine our implementation runs roughly 20% slower than DQN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Uncertainty for neural networks</head><p>Deep neural networks (DNN) represent the state of the art in many supervised and reinforcement learning domains <ref type="bibr" target="#b11">[12]</ref>. We want an exploration strategy that is statistically computationally efficient together with a DNN representation of the value function. To explore efficiently, the first step to quantify uncertainty in value estimates so that the agent can judge potential benefits of exploratory actions. The neural network literature presents a sizable body of work on uncertainty quantification founded on parametric Bayesian inference <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>. We actually found the simple non-parametric bootstrap with random initialization <ref type="bibr" target="#b4">[5]</ref> more effective in our experiments, but the main ideas of this paper would apply with any other approach to uncertainty in DNNs.</p><p>The bootstrap princple is to approximate a population distribution by a sample distribution <ref type="bibr" target="#b5">[6]</ref>. In its most common form, the bootstrap takes as input a data set D and an estimator ψ. To generate a sample from the bootstrapped distribution, a data setD of cardinality equal to that of D is sampled uniformly with replacement from D. The bootstrap sample estimate is then taken to be ψ(D). The bootstrap is widely hailed as a great advance of 20th century applied statistics and even comes with theoretical guarantees <ref type="bibr" target="#b1">[2]</ref>. In <ref type="figure" target="#fig_1">Figure 1a</ref> we present an efficient and scalable method for generating bootstrap samples from a large and deep neural network. The network consists of a shared architecture with K bootstrapped "heads" branching off independently. Each head is trained only on its bootstrapped sub-sample of the data and represents a single bootstrap sample ψ(D). The shared network learns a joint feature representation across all the data, which can provide significant computational advantages at the cost of lower diversity between heads. This type of bootstrap can be trained efficiently in a single forward/backward pass; it can be thought of as a data-dependent dropout, where the dropout mask for each head is fixed for each data point <ref type="bibr" target="#b18">[19]</ref>.   <ref type="figure" target="#fig_1">Figure 1</ref> presents an example of uncertainty estimates from bootstrapped neural networks on a regression task with noisy data. We trained a fully-connected 2-layer neural networks with 50 rectified linear units (ReLU) in each layer on 50 bootstrapped samples from the data. As is standard, we initialize these networks with random parameter values, this induces an important initial diversity in the models. We were unable to generate effective uncertainty estimates for this problem using the dropout approach in prior literature <ref type="bibr" target="#b6">[7]</ref>. Further details are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bootstrapped DQN</head><p>For a policy π we define the value of an action a in state s Q π (s, a) :</p><formula xml:id="formula_0">= E s,a,π [ ∞ t=1 γ t r t ],</formula><p>where γ ∈ (0, 1) is a discount factor that balances immediate versus future rewards r t . This expectation indicates that the initial state is s, the initial action is a, and thereafter actions are selected by the policy π. The optimal value is Q * (s, a) := max π Q π (s, a). To scale to large problems, we learn a parameterized estimate of the Q-value function Q(s, a; θ) rather than a tabular encoding. We use a neural network to estimate this value.</p><p>The Q-learning update from state s t , action a t , reward r t and new state s t+1 is given by</p><formula xml:id="formula_1">θ t+1 ← θ t + α(y Q t − Q(s t , a t ; θ t ))∇ θ Q(s t , a t ; θ t )<label>(1)</label></formula><p>where α is the scalar learning rate and y Q t is the target value r t + γ max a Q(s t+1 , a; θ − ). θ − are target network parameters fixed θ − = θ t .</p><p>Several important modifications to the Q-learning update improve stability for DQN <ref type="bibr" target="#b11">[12]</ref>. First the algorithm learns from sampled transitions from an experience buffer, rather than learning fully online. Second the algorithm uses a target network with parameters θ − that are copied from the learning network θ − ← θ t only every τ time steps and then kept fixed in between updates. Double DQN <ref type="bibr" target="#b24">[25]</ref> modifies the target y Q t and helps further 1 :</p><formula xml:id="formula_2">y Q t ← r t + γ max a Q s t+1 , arg max a Q(s t+1 , a; θ t ); θ − .<label>(2)</label></formula><p>Bootstrapped DQN modifies DQN to approximate a distribution over Q-values via the bootstrap. At the start of each episode, bootstrapped DQN samples a single Q-value function from its approximate posterior. The agent then follows the policy which is optimal for that sample for the duration of the episode. This is a natural adaptation of the Thompson sampling heuristic to RL that allows for temporally extended (or deep) exploration <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>We implement this algorithm efficiently by building up K ∈ N bootstrapped estimates of the Q-value function in parallel as in <ref type="figure" target="#fig_1">Figure 1a</ref>. Importantly, each one of these value function function heads Q k (s, a; θ) is trained against its own target network Q k (s, a; θ − ). This means that each Q 1 , .., Q K provide a temporally extended (and consistent) estimate of the value uncertainty via TD estimates. In order to keep track of which data belongs to which bootstrap head we store flags w 1 , .., w K ∈ {0, 1} indicating which heads are privy to which data. We approximate a bootstrap sample by selecting k ∈ {1, .., K} uniformly at random and following Q k for the duration of that episode. We present a detailed algorithm for our implementation of bootstrapped DQN in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>The observation that temporally extended exploration is necessary for efficient reinforcement learning is not new. For any prior distribution over MDPs, the optimal exploration strategy is available through dynamic programming in the Bayesian belief state space. However, the exact solution is intractable even for very simple systems <ref type="bibr" target="#b7">[8]</ref>. Many successful RL applications focus on generalization and planning but address exploration only via inefficient exploration <ref type="bibr" target="#b11">[12]</ref> or even none at all <ref type="bibr" target="#b22">[23]</ref>. However, such exploration strategies can be highly inefficient.</p><p>Many exploration strategies are guided by the principle of "optimism in the face of uncertainty" (OFU). These algorithms add an exploration bonus to values of state-action pairs that may lead to useful learning and select actions to maximize these adjusted values. This approach was first proposed for finite-armed bandits <ref type="bibr" target="#b10">[11]</ref>, but the principle has been extended successfully across bandits with generalization and tabular RL <ref type="bibr" target="#b8">[9]</ref>. Except for particular deterministic contexts <ref type="bibr" target="#b26">[27]</ref>, OFU methods that lead to efficient RL in complex domains have been computationally intractable. The work of <ref type="bibr" target="#b19">[20]</ref> aims to add an effective bonus through a variation of DQN. The resulting algorithm relies on a large number of hand-tuned parameters and is only suitable for application to deterministic problems. We compare our results on Atari to theirs in Appendix D and find that bootstrapped DQN offers a significant improvement over previous methods.</p><p>Perhaps the oldest heuristic for balancing exploration with exploitation is given by <ref type="bibr">Thompson sampling [24]</ref>. This bandit algorithm takes a single sample from the posterior at every time step and chooses the action which is optimal for that time step. To apply the Thompson sampling principle to RL, an agent should sample a value function from its posterior. Naive applications of Thompson sampling to RL which resample every timestep can be extremely inefficient. The agent must also commit to this sample for several time steps in order to achieve deep exploration <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref>. The algorithm PSRL does exactly this, with state of the art guarantees <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. However, this algorithm still requires solving a single known MDP, which will usually be intractable for large systems.</p><p>Our new algorithm, bootstrapped DQN, approximates this approach to exploration via randomized value functions sampled from an approximate posterior. Recently, authors have proposed the RLSVI algorithm which accomplishes this for linearly parameterized value functions. Surprisingly, RLSVI recovers state of the art guarantees in the setting with tabular basis functions, but its performance is crucially dependent upon a suitable linear representation of the value function <ref type="bibr" target="#b15">[16]</ref>. We extend these ideas to produce an algorithm that can simultaneously perform generalization and exploration with a flexible nonlinear value function representation. Our method is simple, general and compatible with almost all advances in deep RL at low computational cost and with few tuning parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Deep Exploration</head><p>Uncertainty estimates allow an agent to direct its exploration at potentially informative states and actions. In bandits, this choice of directed exploration rather than dithering generally categorizes efficient algorithms. The story in RL is not as simple, directed exploration is not enough to guarantee efficiency; the exploration must also be deep. Deep exploration means exploration which is directed over multiple time steps; it can also be called "planning to learn" or "far-sighted" exploration. Unlike bandit problems, which balance actions which are immediately rewarding or immediately informative, RL settings require planning over several time steps <ref type="bibr" target="#b9">[10]</ref>. For exploitation, this means that an efficient agent must consider the future rewards over several time steps and not simply the myopic rewards. In exactly the same way, efficient exploration may require taking actions which are neither immediately rewarding, nor immediately informative.</p><p>To illustrate this distinction, consider a simple deterministic chain {s −3 , .., s +3 } with three step horizon starting from state s 0 . This MDP is known to the agent a priori, with deterministic actions "left" and "right". All states have zero reward, except for the leftmost state s −3 which has known reward &gt; 0 and the rightmost state s 3 which is unknown. In order to reach either a rewarding state or an informative state within three steps from s 0 the agent must plan a consistent strategy over several time steps. <ref type="figure" target="#fig_3">Figure 2</ref> depicts the planning and look ahead trees for several algorithmic approaches in this example MDP. The action "left" is gray, the action "right" is black. Rewarding states are depicted as red, informative states as blue. Dashed lines indicate that the agent can plan ahead for either rewards or information. Unlike bandit algorithms, an RL agent can plan to exploit future rewards. Only an RL agent with deep exploration can plan to learn.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Testing for deep exploration</head><p>We now present a series of didactic computational experiments designed to highlight the need for deep exploration. These environments can be described by chains of length N &gt; 3 in <ref type="figure" target="#fig_4">Figure 3</ref>. Each episode of interaction lasts N + 9 steps after which point the agent resets to the initial state s 2 . These are toy problems intended to be expository rather than entirely realistic. Balancing a well known and mildly successful strategy versus an unknown, but potentially more rewarding, approach can emerge in many practical applications. These environments may be described by a finite tabular MDP. However, we consider algorithms which interact with the MDP only through raw pixel features. We consider two feature mappings φ 1hot (s t ) :</p><formula xml:id="formula_3">= (1{x = s t }) and φ therm (s t ) := (1{x ≤ s t }) in {0, 1} N .</formula><p>We present results for φ therm , which worked better for all DQN variants due to better generalization, but the difference was relatively small -see Appendix C. Thompson DQN is the same as bootstrapped DQN, but resamples every timestep. Ensemble DQN uses the same architecture as bootstrapped DQN, but with an ensemble policy.</p><p>We say that the algorithm has successfully learned the optimal policy when it has successfully completed one hundred episodes with optimal reward of 10. For each chain length, we ran each learning algorithm for 2000 episodes across three seeds. We plot the median time to learn in <ref type="figure" target="#fig_5">Figure 4</ref>, together with a conservative lower bound of 99 + 2 N −11 on the expected time to learn for any shallow exploration strategy <ref type="bibr" target="#b15">[16]</ref>. Only bootstrapped DQN demonstrates a graceful scaling to long chains which require deep exploration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">How does bootstrapped DQN drive deep exploration?</head><p>Bootstrapped DQN explores in a manner similar to the provably-efficient algorithm PSRL <ref type="bibr" target="#b12">[13]</ref> but it uses a bootstrapped neural network to approximate a posterior sample for the value. Unlike PSRL, bootstrapped DQN directly samples a value function and so does not require further planning steps. This algorithm is similar to RLSVI, which is also provably-efficient <ref type="bibr" target="#b15">[16]</ref>, but with a neural network instead of linear value function and bootstrap instead of Gaussian sampling. The analysis for the linear setting suggests that this nonlinear approach will work well so long as the distribution {Q 1 , .., Q K } remains stochastically optimistic <ref type="bibr" target="#b15">[16]</ref>, or at least as spread out as the "correct" posterior.</p><p>Bootstrapped DQN relies upon random initialization of the network weights as a prior to induce diversity. Surprisingly, we found this initial diversity was enough to maintain diverse generalization to new and unseen states for large and deep neural networks. This is effective for our experimental setting, but will not work in all situations. In general it may be necessary to maintain some more rigorous notion of "prior", potentially through the use of artificial prior data to maintain diversity <ref type="bibr" target="#b14">[15]</ref>. One potential explanation for the efficacy of simple random initialization is that unlike supervised learning or bandits, where all networks fit the same data, each of our Q k heads has a unique target network. This, together with stochastic minibatch and flexible nonlinear representations, means that even small differences at initialization may become bigger as they refit to unique TD errors.</p><p>Bootstrapped DQN does not require that any single network Q k is initialized to the correct policy of "right" at every step, which would be exponentially unlikely for large chains N . For the algorithm to be successful in this example we only require that the networks generalize in a diverse way to the actions they have never chosen in the states they have not visited very often. Imagine that, in the example above, the network has made it as far as stateÑ &lt; N , but never observed the action right a = 2. As long as one head k imagines Q(Ñ , 2) &gt; Q(Ñ , 2) then TD bootstrapping can propagate this signal back to s = 1 through the target network to drive deep exploration. The expected time for these estimates at n to propagate to at least one head grows gracefully in n, even for relatively small K, as our experiments show. We expand upon this intuition with a video designed to highlight how bootstrapped DQN demonstrates deep exploration https://youtu.be/e3KuV_d0EMk. We present further evaluation on a difficult stochastic MDP in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Arcade Learning Environment</head><p>We now evaluate our algorithm across 49 Atari games on the Arcade Learning Environment <ref type="bibr" target="#b0">[1]</ref>. Importantly, and unlike the experiments in Section 5, these domains are not specifically designed to showcase our algorithm. In fact, many Atari games are structured so that small rewards always indicate part of an optimal policy. This may be crucial for the strong performance observed by dithering strategies 2 . We find that exploration via bootstrapped DQN produces significant gains versus -greedy in this setting. Bootstrapped DQN reaches peak performance roughly similar to DQN. However, our improved exploration mean we reach human performance on average 30% faster across all games. This translates to significantly improved cumulative rewards through learning.</p><p>We follow the setup of <ref type="bibr" target="#b24">[25]</ref> for our network architecture and benchmark our performance against their algorithm. Our network structure is identical to the convolutional structure of DQN <ref type="bibr" target="#b11">[12]</ref> except we split 10 separate bootstrap heads after the convolutional layer as per <ref type="figure" target="#fig_1">Figure 1a</ref>. Recently, several authors have provided architectural and algorithmic improvements to DDQN <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18]</ref>. We do not compare our results to these since their advances are orthogonal to our concern and could easily be incorporated to our bootstrapped DQN design. Full details of our experimental set up are available in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementing bootstrapped DQN at scale</head><p>We now examine how to generate online bootstrap samples for DQN in a computationally efficient manner. We focus on three key questions: how many heads do we need, how should we pass gradients to the shared network and how should we bootstrap data online? We make significant compromises in order to maintain computational cost comparable to DQN. <ref type="figure" target="#fig_6">Figure 5a</ref> presents the cumulative reward of bootstrapped DQN on the game Breakout, for different number of heads K. More heads leads to faster learning, but even a small number of heads captures most of the benefits of bootstrapped DQN. We choose K = 10.  The shared network architecture allows us to train this combined network via backpropagation. Feeding K network heads to the shared convolutional network effectively increases the learning rate for this portion of the network. In some games, this leads to premature and sub-optimal convergence. We found the best final scores by normalizing the gradients by 1/K, but this also leads to slower early learning. See Appendix D for more details.</p><p>To implement an online bootstrap we use an independent Bernoulli mask w 1 ,..,w K ∼Ber(p) for each head in each episode 3 . These flags are stored in the memory replay buffer and identify which heads are trained on which data. However, when trained using a shared minibatch the algorithm will also require an effective 1/p more iterations; this is undesirable computationally. Surprisingly, we found the algorithm performed similarly irrespective of p and all outperformed DQN, as shown in <ref type="figure" target="#fig_7">Figure 5b</ref>. This is strange and we discuss this phenomenon in Appendix D. However, in light of this empirical observation for Atari, we chose p=1 to save on minibatch passes. As a result bootstrapped DQN runs at similar computational speed to vanilla DQN on identical hardware 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Efficient exploration in Atari</head><p>We find that Bootstrapped DQN drives efficient exploration in several Atari games. For the same amount of game experience, bootstrapped DQN generally outperforms DQN with -greedy exploration. <ref type="figure" target="#fig_8">Figure 6</ref> demonstrates this effect for a diverse selection of games.  <ref type="figure" target="#fig_9">Figure 7</ref> shows that Bootstrapped DQN typically reaches human performance significantly faster. On most games where DQN does not reach human performance, bootstrapped DQN does not solve the problem by itself. On some challenging Atari games where deep exploration is conjectured to be important <ref type="bibr" target="#b24">[25]</ref> our results are not entirely successful, but still promising. In Frostbite, bootstrapped DQN reaches the second level much faster than DQN but network instabilities cause the performance to crash. In Montezuma's Revenge, bootstrapped DQN reaches the first key after 20m frames (DQN never observes a reward even after 200m frames) but does not properly learn from this experience 5 . Our results suggest that improved exploration may help to solve these remaining games, but also highlight the importance of other problems like network instability, reward clipping and temporally extended rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Overall performance</head><p>Bootstrapped DQN is able to learn much faster than DQN. <ref type="figure" target="#fig_10">Figure 8</ref> shows that bootstrapped DQN also improves upon the final score across most games. However, the real benefits to efficient exploration mean that bootstrapped DQN outperforms DQN by orders of magnitude in terms of the cumulative rewards through learning <ref type="figure" target="#fig_11">(Figure 9</ref>. In both figures we normalize performance relative to a fully random policy. The most similar work to ours presents several other approaches to improved exploration in Atari <ref type="bibr" target="#b19">[20]</ref> they optimize for AUC-20, a normalized version of the cumulative returns after 20m frames. According to their metric, averaged across the 14 games they consider, we improve upon both base DQN (0.29) and their best method (0.37) to obtain 0.62 via bootstrapped DQN. We present these results together with results tables across all 49 games in Appendix D.4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Visualizing bootstrapped DQN</head><p>We now present some more insight to how bootstrapped DQN drives deep exploration in Atari.</p><p>In each game, although each head Q 1 , .., Q 10 learns a high scoring policy, the policies they find are quite distinct. In the video https://youtu.be/Zm2KoT82O_M we show the evolution of these policies simultaneously for several games. Although each head performs well, they each follow a unique policy. By contrast, -greedy strategies are almost indistinguishable for small values of and totally ineffectual for larger values. We believe that this deep exploration is key to improved learning, since diverse experiences allow for better generalization.</p><p>Disregarding exploration, bootstrapped DQN may be beneficial as a purely exploitative policy. We can combine all the heads into a single ensemble policy, for example by choosing the action with the most votes across heads. This approach might have several benefits. First, we find that the ensemble policy can often outperform any individual policy. Second, the distribution of votes across heads to give a measure of the uncertainty in the optimal policy. Unlike vanilla DQN, bootstrapped DQN can know what it doesn't know. In an application where executing a poorly-understood action is dangerous this could be crucial. In the video https://youtu.be/0jvEcC5JvGY we visualize this ensemble policy across several games. We find that the uncertainty in this policy is surprisingly interpretable: all heads agree at clearly crucial decision points, but remain diverse at other less important steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Closing remarks</head><p>In this paper we present bootstrapped DQN as an algorithm for efficient reinforcement learning in complex environments. We demonstrate that the bootstrap can produce useful uncertainty estimates for deep neural networks. Bootstrapped DQN is computationally tractable and also naturally scalable to massive parallel systems. We believe that, beyond our specific implementation, randomized value functions represent a promising alternative to dithering for exploration. Bootstrapped DQN practically combines efficient generalization with exploration for complex nonlinear value functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDICES A Uncertainty for neural networks</head><p>In this appendix we discuss some of the experimental setup to qualitatively evaluate uncertainty methods for deep neural networks. To do this, we generated twenty noisy regression pairs x i , y i with:</p><formula xml:id="formula_4">y i = x i + sin(α(x i + w i )) + sin(β(x i + w i )) + w i</formula><p>where x i are drawn uniformly from (0, 0.6) ∪ (0.8, 1) and w i ∼ N (µ = 0, σ 2 = 0.03 2 ). We set α = 4 and β = 13. None of these numerical choices were important except to represent a highly nonlinear function with lots of noise and several clear regions where we should be uncertain. We present the regression data together with an indication of the generating distribution in <ref type="figure" target="#fig_1">Figure 10</ref>. Interestingly, we did not find that using dropout produced satisfying confidence intervals for this task. We present one example of this dropout posterior estimate in <ref type="figure" target="#fig_1">Figure 11a</ref>.</p><p>(a) Dropout gives strange uncertainty estimates.</p><p>(b) Screenshot from accompanying web demo to <ref type="bibr" target="#b6">[7]</ref>. Dropout converges with high certainty to the mean value. These results are unsatisfactory for several reasons. First, the network extrapolates the mean posterior far outside the range of any actual data for x = 0.75. We believe this is because dropout only perturbs locally from a single neural network fit, unlike bootstrap. Second, the posterior samples from the dropout approximation are very spiky and do not look like any sensible posterior sample. Third, the network collapses to almost zero uncertainty in regions with data.</p><p>We spent some time altering our dropout scheme to fix this effect, which might be undesirable for stochastic domains and we believed might be an artefact of our implementation. However, after further thought we believe this to be an effect which you would expect for dropout posterior approximations. In <ref type="figure" target="#fig_1">Figure 11b</ref> we present a didactic example taken from the author's website <ref type="bibr" target="#b6">[7]</ref>.</p><p>On the right hand side of the plot we generate noisy data with wildly different values. Training a neural network using MSE criterion means that the network will surely converge to the mean of the noisy data. Any dropout samples remain highly concentrated around this mean. By contrast, bootstrapped neural networks may include different subsets of this noisy data and so may produce a more intuitive uncertainty estimates for our settings. Note this isn't necessarily a failure of dropout to approximate a Gaussian process posterior, but this artefact could be shared by any homoskedastic posterior. The authors of <ref type="bibr" target="#b6">[7]</ref> propose a heteroskedastic variant which can help, but does not address the fundamental issue that for large networks trained to convergence all dropout samples may converge to every single datapoint... even the outliers.</p><p>In this paper we focus on the bootstrap approach to uncertainty for neural networks. We like its simplicity, connections to established statistical methodology and empirical good performance. However, the key insights of this paper is the use of deep exploration via randomized value functions. This is compatible with any approximate posterior estimator for deep neural networks. We believe that this area of uncertainty estimates for neural networks remains an important area of research in its own right.</p><p>Bootstrapped uncertainty estimates for the Q-value functions have another crucial advantage over dropout which does not appear in the supervised problem. Unlike random dropout masks trained against random target networks, our implementation of bootstrap DQN trains against its own temporally consistent target network. This means that our bootstrap estimates (in the sense of <ref type="bibr" target="#b4">[5]</ref>), are able to "bootstrap" (in the TD sense of <ref type="bibr" target="#b21">[22]</ref>) on their own estimates of the long run value. This is important to quantify the long run uncertainty over Q and drive deep exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Bootstrapped DQN implementation</head><p>Algorithm 1 gives a full description of Bootstrapped DQN. It captures two modes of operation where either k neural networks are used to estimate the Q k -value functions, or where one neural network with k heads is used to estimate k Q-value functions. In both cases, as this is largely a parameterisation issue, we denote the value function networks as Q, where Q k is output of the kth network or the kth head.</p><p>A core idea to the full bootstrapped DQN algorithm is the bootstrap mask m t . The mask m t decides, for each value function Q k , whether or not it should train upon the experience generated at step t. In its simplest form m t is a binary vector of length K, masking out or including each value function for training on that time step of experience (i.e., should it receive gradients from the corresponding (s t , a t , r t+1 , s t+1 , m t ) tuple). The masking distribution M is responsible for generating each m t . For example, when M yields m t whose components are independently drawn from a bernoulli distribution with parameter 0.5 then this corresponds to the double-or-nothing bootstrap <ref type="bibr" target="#b16">[17]</ref>. On the other hand, if M yields a mask m t with all ones, then the algorithm reduces to an ensemble method. Poisson masks M t [k] ∼ Poi(1) provides the most natural parallel with the standard non-parameteric boostrap since Bin(N, 1/N ) → Poi(1) as N → ∞. Exponential masks M t [k] ∼ Exp(1) closely resemble the standard Bayesian nonparametric posterior of a Dirichlet process <ref type="bibr" target="#b14">[15]</ref>.</p><p>Periodically, the replay buffer is played back to update the parameters of the value function network Q. The gradients of the kth value function Q k for the tth tuple in the replay buffer B, g k t is:</p><formula xml:id="formula_5">g k t = m k t (y Q t − Q k (s t , a t ; θ))∇ θ Q k (s t , a t ; θ)<label>(3)</label></formula><p>where y Q Algorithm 1 Bootstrapped DQN Obtain initial state from environment s 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Pick a value function to act using k ∼ Uniform{1, . . . , K} <ref type="bibr">6:</ref> for step t = 1, . . . until end of episode do <ref type="bibr">7:</ref> Pick an action according to a t ∈ arg max a Q k (s t , a) <ref type="bibr">8:</ref> Receive state s t+1 and reward r t from environment, having taking action a t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Sample bootstrap mask m t ∼ M <ref type="bibr">10:</ref> Add (s t , a t , r t+1 , s t+1 , m t ) to replay buffer B <ref type="bibr">11:</ref> end for <ref type="bibr" target="#b11">12</ref>: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiments for deep exploration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Bootstrap methodology</head><p>A naive implementation of bootstrapped DQN builds up K complete networks with K distinct memory buffers. This method is parallelizable up to many machines, however we wanted to produce an algorithm that was efficient even on a single machine. To do this, we implemented the bootstrap heads in a single larger network, like <ref type="figure" target="#fig_1">Figure 1a</ref> but without any shared network. We implement bootstrap by masking each episode of data according to w 1 , .., w K ∼ Ber(p). In <ref type="figure" target="#fig_1">Figure 12</ref> we demonstrate that bootstrapped DQN can implement deep exploration even with relatively small values of K. However, the results are more robust and scalable with larger K. We run our experiments on the example from <ref type="figure" target="#fig_4">Figure 3</ref>. Surprisingly, this method is even effective with p = 1 and complete data sharing between heads. This degenerate full sharing of information turns out to be remarkably efficient for training large and deep neural networks. We discuss this phenomenon more in Appendix D.</p><p>Generating good estimates for uncertainty is not enough for efficient exploration. In <ref type="figure" target="#fig_1">Figure 13</ref> we see that other methods trained with the same network architecture are totally ineffective at implementing deep exploration. The -greedy policy follows just one Q-value estimate. We allow this policy to be evaluated without dithering. The ensemble policy is trained exactly as per bootstrapped DQN except at each stage the algorithm follows the policy which is majority vote of the bootstrap heads. Thompson sampling is the same as bootstrapped DQN except a new head is sampled every timestep, rather than every episode.</p><p>We can see that only bootstrapped DQN demonstrates efficient and deep exploration in this domain.  <ref type="figure" target="#fig_5">Figure 4</ref> shows that bootstrapped DQN can implement effective (and deep) exploration where similar deep RL architectures fail. However, since the underlying system is a small and finite MDP there may be several other simpler strategies which would also solve this problem. We will now consider a difficult variant of this chain system with significant stochastic noise in transitions as depicted in <ref type="figure" target="#fig_1">Figure 14</ref>. Action "left" deterministically moves the agent left, but action "right" is only successful 50% of the time and otherwise also moves left. The agent interacts with the MDP in episodes of length 15 and begins each episode at s 1 . Once again the optimal policy is to head right. Bootstrapped DQN is unique amongst scalable approaches to efficient exploration with deep RL in stochastic domains. For benchmark performance we implement three algorithms which, unlike bootstrapped DQN, will receive the true tabular representation for the MDP. These algorithms are based on three state of the art approaches to exploration via dithering (greedy), optimism <ref type="bibr" target="#b8">[9]</ref> and posterior sampling <ref type="bibr" target="#b12">[13]</ref>. We discuss the choice of these benchmarks in Appendix C.  In <ref type="figure" target="#fig_1">Figure 15a</ref> we present the empirical regret of each algorithm averaged over 10 seeds over the first two thousand episodes. The empirical regret is the cumulative difference between the expected rewards of the optimal policy and the realized rewards of each algorithm. We find that bootstrapped DQN achieves similar performance to state of the art efficient exploration schemes such as PSRL even without prior knowledge of the tabular MDP structure and in noisy environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 A difficult stochastic MDP</head><p>Most telling is how much better bootstrapped DQN does than the state of the art optimistic algorithm UCRL2. Although <ref type="figure" target="#fig_1">Figure 15a</ref> seems to suggest UCRL2 incurs linear regret, actually it follows its boundsÕ(S √ AT ) <ref type="bibr" target="#b8">[9]</ref> where S is the number of states and A is the number of actions.</p><p>For the example in <ref type="figure" target="#fig_1">Figure 14</ref> we attempted to display our performance compared to several benchmark tabula rasa approaches to exploration. There are many other algorithms we could have considered, but for a short paper we chose to focus against the most common approach ( -greedy) the pre-eminent optimistic approach (UCRL2) and posterior sampling (PSRL).</p><p>Other common heuristic approaches, such as optimistic initialization for Q-learning can be tuned to work well on this domain, however the precise parameters are sensitive to the underlying MDP 6 . To make a general-purpose version of this heuristic essentially leads to optimistic algorithms. Since UCRL2 is originally designed for infinite-horizon MDPs, we use the natural adaptation of this algorithm, which has state of the art guarantees in finite horizon MDPs as well <ref type="bibr" target="#b3">[4]</ref>. <ref type="figure" target="#fig_1">Figure 15a</ref> displays the empirical regret of these algorithms together with bootstrapped DQN on the example from <ref type="figure" target="#fig_1">Figure 14</ref>. It is somewhat disconcerting that UCRL2 appears to incur linear regret, but it is proven to satisfy near-optimal regret bounds. Actually, as we show in <ref type="figure" target="#fig_1">Figure 15b</ref>, the algorithm produces regret which scales very similarly to its established bounds <ref type="bibr" target="#b8">[9]</ref>. Similarly, even for this tiny problem size, the recent analysis that proves a near optimal sample complexity in fixed horizon problems <ref type="bibr" target="#b3">[4]</ref> only guarantees that we will have fewer than 10 10 = 1 suboptimal episodes. While these bounds may be acceptable in worst caseÕ(·) scaling, they are not of much practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 One-hot features</head><p>In <ref type="figure" target="#fig_1">Figure 16</ref> we include the mean performance of bootstrapped DQN with one-hot feature encodings. We found that, using these features, bootstrapped DQN learned the optimal policy for most seeds, but was somewhat less robust than the thermometer encoding. Two out of ten seeds failed to learn the optimal policy within 2000 episodes, this is presented in <ref type="figure" target="#fig_1">Figure 16</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experiments for Atari</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Experimental setup</head><p>We use the same 49 Atari games as <ref type="bibr" target="#b11">[12]</ref> for our experiments. Each step of the agent corresponds to four steps of the emulator, where the same action is repeated, the reward values of the agents are clipped between -1 and 1 for stability. We evaluate our agents and report performance based upon the raw scores.</p><p>The convolutional part of the network used is identical to the one used in <ref type="bibr" target="#b11">[12]</ref>. The input to the network is 4x84x84 tensor with a rescaled, grayscale version of the last four observations. The first convolutional (conv) layer has 32 filters of size 8 with a stride of 4. The second conv layer has 64 filters of size 4 with stride 2. The last conv layer has 64 filters of size 3. We split the network beyond the final layer into K = 10 distinct heads, each one is fully connected and identical to the single head of DQN <ref type="bibr" target="#b11">[12]</ref>. This consists of a fully connected layer to 512 units followed by another fully connected layer to the Q-Values for each action. The fully connected layers all use Rectified Linear Units(ReLU) as a non-linearity. We normalize gradients 1/K that flow from each head.</p><p>We trained the networks with RMSProp with a momentum of 0.95 and a learning rate of 0.00025 as in <ref type="bibr" target="#b11">[12]</ref>. The discount was set to γ = 0.99, the number of steps between target updates was set to τ = 10000 steps. We trained the agents for a total of 50m steps per game, which corresponds to 200m frames. The agents were every 1m frames, for evaluation in bootstrapped DQN we use an ensemble voting policy. The experience replay contains the 1m most recent transitions. We update the network every 4 steps by randomly sampling a minibatch of 32 transitions from the replay buffer to use the exact same minibatch schedule as DQN. For training we used an -greedy policy with being annealed linearly from 1 to 0.01 over the first 1m timesteps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Gradient normalization in bootstrap heads</head><p>Most literature in deep RL for Atari focuses on learning the best single evaluation policy, with particular attention to whether this above or below human performance <ref type="bibr" target="#b11">[12]</ref>. This is unusual for the RL literature, which typically focuses upon cumulative or final performance.</p><p>Bootstrapped DQN makes significant improvements to the cumulative rewards of DQN on Atari, as we display in <ref type="figure" target="#fig_11">Figure 9</ref>, while the peak performance is much more We found that using bootstrapped DQN without gradient normalization on each head typically learned even faster than our implementation with rescaling 1/K, but it was somewhat prone to premature and suboptimal convergence. We present an example of this phenomenon in <ref type="figure" target="#fig_1">Figure 17</ref>. We found that, in order to better the benchmark "best" policies reported by DQN, it was very helpful for us to use the gradient normalization. However, it is not entirely clear whether this represents an improvement for all settings. In <ref type="figure" target="#fig_1">Figures 18a and 18b</ref> we present the cumulative rewards of the same algorithms on Beam Rider. Where an RL system is deployed to learn with real interactions, cumulative rewards present a better measure for performance. In these settings the benefits of gradient normalization are less clear. However, even with normalization 1/K bootstrapped DQN significantly outperforms DQN in terms of cumulative rewards. This is reflected most clearly in <ref type="figure" target="#fig_11">Figure 9</ref> and <ref type="table" target="#tab_2">Table 2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Sharing data in bootstrap heads</head><p>In this setting all network heads share all the data, so they are not actually a traditional bootstrap at all. This is different from the regression task in Section 2, where bootstrapped data was essential to obtain meaningful uncertainty estimates. We have several theories for why the networks maintain significant diversity even without data bootstrapping in this setting. We build upon the intuition of Section 5.2.</p><p>First, they all train on different target networks. This means that even when facing the same (s, a, r, s ) datapoint this can still lead to drastically different Q-value updates. Second, Atari is a deterministic environment, any transition observation is the unique correct datapoint for this setting. Third, the networks are deep and initialized from different random values so they will likely find quite diverse generalization even when they agree on given data. Finally, since all variants of DQN take many many frames to update their policy, it is likely that even using p = 0.5 they would still populate their replay memory with identical datapoints. This means using p = 1 to save on minibatch passes seems like a reasonable compromise and it doesn't seem to negatively affect performance too much in this setting. More research is needed to examine exactly where/when this data sharing is important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Results tables</head><p>In <ref type="table" target="#tab_1">Table 1</ref> the average score achieved by the agents during the most successful evaluation period, compared to human performance and a uniformly random policy. DQN is our implementation of DQN with the hyperparameters specified above, using the double Q-Learning update. <ref type="bibr" target="#b24">[25]</ref>. We find that peak final performance is similar under bootstrapped DQN to previous benchmarks.</p><p>To compare the benefits of exploration via bootstrapped DQN we benchmark our performance against the most similar prior work on incentivizing exploration in Atari <ref type="bibr" target="#b19">[20]</ref>. To do this, we compute the AUC-100 measure specified in this work. We present these results in <ref type="table" target="#tab_2">Table 2</ref> compare to their best performing strategy as well as their implementation of DQN. Importantly, bootstrapped DQN outperforms this prior work significantly. We now compare our method against the results in <ref type="bibr" target="#b19">[20]</ref>. In this paper they introduce a new measure of performance called AUC-100, which is something similar to normalized cumulative rewards up to 20 million frames.  <ref type="table" target="#tab_2">Table 2</ref>: AUC-100 for different agents compared to <ref type="bibr" target="#b19">[20]</ref> We see that, on average, both bootstrapped DQN implementations outperform Dynamic AE, the best algorithm from previous work. The only game in which Dynamic AE produces best results is Bowling, but this difference in Bowling is dominated by the implementation of DQN* vs DQN. Bootstrapped DQN still gives over 100% improvement over its relevant DQN baseline. Overall it is clear that Boot-DQN+ (bootstrapped DQN without rescaling) performs best in terms of AUC-100 metric. Averaged across the 14 games it is over 50% better than the next best competitor, which is bootstrapped DQN with gradient normalization. However, in terms of peak performance over 200m frames Boot-DQN generally reached higher scores. Boot-DQN+ sometimes plateaud early as in <ref type="figure" target="#fig_1">Figure 17</ref>. This highlights an important distinction between evaluation based on best learned policy versus cumulative rewards, as we discuss in Appendix D.2. Bootstrapped DQN displays the biggest improvements over DQN when doing well during learning is important.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Shared network architecture (b) Gaussian process posterior (c) Bootstrapped neural nets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Bootstrapped neural nets can produce reasonable posterior estimates for regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Bandit algorithm (b) RL+dithering (c) RL+shallow explore (d) RL+deep explore</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Planning, learning and exploration in RL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Scalable environments that requires deep exploration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Only Bootstrapped DQN demonstrates deep exploration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )</head><label>a</label><figDesc>Number of bootstrap heads K. (b) Probability of data sharing p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Examining the sensitivities of bootstrapped DQN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Bootstrapped DQN drives more efficient exploration. On games where DQN performs well, bootstrapped DQN typically performs better. Bootstrapped DQN does not reach human performance on Amidar (DQN does) but does on Beam Rider and Battle Zone (DQN does not). To summarize this improvement in learning time we consider the number of frames required to reach human performance. If bootstrapped DQN reaches human performance in 1/x frames of DQN we say it has improved by x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Bootstrapped DQN reaches human performance faster than DQN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Bootstrapped DQN typically improves upon the best policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Bootstrapped DQN improves cumulative rewards by orders of magnitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Underlying generating distribution. All our algorithms receive the same blue data. Pink points represent other samples, the mean function is shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Comparing the bootstrap to dropout uncertainty for neural nets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Bootstrapped DQN performs well even with small number of bootstrap heads K or high probability of sharing p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Shallow exploration methods do not work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>A stochastic MDP that requires deep exploration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(a) Bootstrapped DQN matches efficient tabular RL.(b) The regret bounds for UCRL2 are nearoptimal inÕ(·), but they are still not very practical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 :</head><label>15</label><figDesc>Learning and regret bounds on a stochastic MDP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 :</head><label>16</label><figDesc>Bootstrapped DQN also performs well with one-hot features, but learning is less robust.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 17 :</head><label>17</label><figDesc>Normalization fights premature convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>(a) Normalization does not help cumulative rewards. (b) Even over 200m frames the importance of exploration dominates the effects of an inferior final policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 18 :</head><label>18</label><figDesc>Planning, learning and exploration in RL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Value function networks Q with K outputs {Q k } K k=1 . Masking distribution M . 2: Let B be a replay buffer storing experience for training. 3: for each episode do</figDesc><table><row><cell>4:</cell></row></table><note>1: Input:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Maximal evaluation Scores achieved by agents</figDesc><table><row><cell>Tennis</cell><cell>-23.8</cell><cell>-8.3</cell><cell>0.0</cell><cell>11.8</cell><cell>-2.5</cell></row><row><cell>Time Pilot</cell><cell>3568.0</cell><cell>5229.2</cell><cell>9079.4</cell><cell>10075.8</cell><cell>5947</cell></row><row><cell>Tutankham</cell><cell>11.4</cell><cell>167.6</cell><cell>214.8</cell><cell>268.0</cell><cell>186.7</cell></row><row><cell>Up N Down</cell><cell>533.4</cell><cell>11693.2</cell><cell>26231.0</cell><cell>19743.5</cell><cell>8456</cell></row><row><cell>Venture</cell><cell>0.0</cell><cell>1187.5</cell><cell>212.5</cell><cell>239.7</cell><cell>380</cell></row><row><cell>Video Pinball</cell><cell>0.0</cell><cell>17667.9</cell><cell>811610.0</cell><cell cols="2">685911.0 42684</cell></row><row><cell>Wizard Of Wor</cell><cell>563.5</cell><cell>4756.5</cell><cell>6804.7</cell><cell>7655.7</cell><cell>3393</cell></row><row><cell>Zaxxon</cell><cell>32.5</cell><cell>9173.3</cell><cell>11491.7</cell><cell>12947.6</cell><cell>4977</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>displays the results for our reference DQN and bootstrapped DQN as Boot-DQN. We reproduce their reference results for DQN as DQN* and their best performing algorithm, Dynamic AE. We also present bootstrapped DQN without head rescaling as Boot-DQN+.</figDesc><table><row><cell></cell><cell cols="5">DQN* Dynamic AE DQN Boot-DQN Boot-DQN+</cell></row><row><cell>Alien</cell><cell>0.15</cell><cell>0.20</cell><cell>0.23</cell><cell>0.23</cell><cell>0.33</cell></row><row><cell>Asteroids</cell><cell>0.26</cell><cell>0.41</cell><cell>0.29</cell><cell>0.29</cell><cell>0.55</cell></row><row><cell>Bank Heist</cell><cell>0.07</cell><cell>0.15</cell><cell>0.06</cell><cell>0.09</cell><cell>0.77</cell></row><row><cell>Beam Rider</cell><cell>0.11</cell><cell>0.09</cell><cell>0.24</cell><cell>0.46</cell><cell>0.79</cell></row><row><cell>Bowling</cell><cell>0.96</cell><cell>1.49</cell><cell>0.24</cell><cell>0.56</cell><cell>0.54</cell></row><row><cell>Breakout</cell><cell>0.19</cell><cell>0.20</cell><cell>0.06</cell><cell>0.16</cell><cell>0.52</cell></row><row><cell>Enduro</cell><cell>0.52</cell><cell>0.49</cell><cell>1.68</cell><cell>1.85</cell><cell>1.72</cell></row><row><cell>Freeway</cell><cell>0.21</cell><cell>0.21</cell><cell>0.58</cell><cell>0.68</cell><cell>0.81</cell></row><row><cell>Frostbite</cell><cell>0.57</cell><cell>0.97</cell><cell>0.99</cell><cell>1.12</cell><cell>0.98</cell></row><row><cell>Montezuma Revenge</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Pong</cell><cell>0.52</cell><cell>0.56</cell><cell>-0.13</cell><cell>0.02</cell><cell>0.60</cell></row><row><cell>Qbert</cell><cell>0.15</cell><cell>0.10</cell><cell>0.13</cell><cell>0.16</cell><cell>0.24</cell></row><row><cell>Seaquest</cell><cell>0.16</cell><cell>0.17</cell><cell>0.18</cell><cell>0.23</cell><cell>0.44</cell></row><row><cell>Space Invaders</cell><cell>0.20</cell><cell>0.18</cell><cell>0.25</cell><cell>0.30</cell><cell>0.38</cell></row><row><cell>Average</cell><cell>0.29</cell><cell>0.37</cell><cell>0.35</cell><cell>0.41</cell><cell>0.62</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper we use the DDQN update for all DQN variants unless explicitly stated.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">By contrast, imagine that the agent received a small immediate reward for dying; dithering strategies would be hopeless at solving this problem, just like Section 5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">p=0.5 is double-or-nothing bootstrap<ref type="bibr" target="#b16">[17]</ref>, p=1 is ensemble with no bootstrapping at all.<ref type="bibr" target="#b3">4</ref> Our implementation K=10, p=1 ran with less than a 20% increase on wall-time versus DQN.<ref type="bibr" target="#b4">5</ref> An improved training method, such as prioritized replay<ref type="bibr" target="#b17">[18]</ref> may help solve this problem.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t is given by<ref type="bibr" target="#b1">(2)</ref>. Note that the mask m k t modulates the gradient, giving rise to the bootstrap behaviour.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Further, it is difficult to extend the idea of optimistic initialization with function generalization, especially for deep neural networks.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.4708</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Some asymptotic theory for the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David A</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1196" to="1217" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<title level="m">Weight uncertainty in neural networks. ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sample complexity of episodic fixed-horizon reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Dann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The jackknife, the bootstrap and other resampling plans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>SIAM</publisher>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02142</idno>
	</analytic>
	<monogr>
		<title level="m">Representing model uncertainty in deep learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient bayes-adaptive reinforcement learning using sample-based search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1025" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Near-optimal regret bounds for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Jaksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1563" to="1600" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On the Sample Complexity of Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Asymptotically efficient adaptive allocation rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tze</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">More) efficient reinforcement learning via posterior sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3003" to="3011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model-based reinforcement learning and the eluder dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1466" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bootstrapped thompson sampling and deep exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generalization and exploration via randomized value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Benjamin Van Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.0635</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bootstrapping data arrays of arbitrary order</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Art</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eckles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="895" to="927" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Prioritized experience replay. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Incentivizing exploration in reinforcement learning with deep predictive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bradly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00814</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A bayesian framework for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="943" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-03" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal difference learning and td-gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="68" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06461</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06581</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient exploration and value function generalization in deterministic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3021" to="3029" />
		</imprint>
	</monogr>
	<note>Random Human Bootstrapped DQN</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
