<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MALI: A MEMORY EFFICIENT AND REVERSE ACCU- RATE INTEGRATOR FOR NEURAL ODES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Nicha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dvornek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sekhar</forename><surname>Tatikonda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
							<email>james.duncan@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MALI: A MEMORY EFFICIENT AND REVERSE ACCU- RATE INTEGRATOR FOR NEURAL ODES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural ordinary differential equations (Neural ODEs) are a new family of deeplearning models with continuous depth. However, the numerical estimation of the gradient in the continuous case is not well solved: existing implementations of the adjoint method suffer from inaccuracy in reverse-time trajectory, while the naive method and the adaptive checkpoint adjoint method (ACA) have a memory cost that grows with integration time. In this project, based on the asynchronous leapfrog (ALF) solver, we propose the Memory-efficient ALF Integrator (MALI), which has a constant memory cost w.r.t number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). We validate MALI in various tasks: on image recognition tasks, to our knowledge, MALI is the first to enable feasible training of a Neural ODE on ImageNet and outperform a well-tuned ResNet, while existing methods fail due to either heavy memory burden or inaccuracy; for time series modeling, MALI significantly outperforms the adjoint method; and for continuous generative models, MALI achieves new state-of-the-art performance.Code is available at httpsmachine learning framework for solving high-dimensional mean field game and mean field control problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent research builds the connection between continuous models and neural networks. The theory of dynamical systems has been applied to analyze the properties of neural networks or guide the design of networks <ref type="bibr">(Weinan, 2017;</ref><ref type="bibr">Ruthotto &amp; Haber, 2019;</ref><ref type="bibr">Lu et al., 2018)</ref>. In these works, a residual block <ref type="bibr" target="#b12">(He et al., 2016)</ref> is typically viewed as a one-step Euler discretization of an ODE; instead of directly analyzing the discretized neural network, it might be easier to analyze the ODE.</p><p>Another direction is the neural ordinary differential equation (Neural ODE) , which takes a continuous depth instead of discretized depth. The dynamics of a Neural ODE is typically approximated by numerical integration with adaptive ODE solvers. Neural ODEs have been applied in irregularly sampled time-series <ref type="bibr">(Rubanova et al., 2019)</ref>, free-form continuous generative models <ref type="bibr" target="#b9">(Grathwohl et al., 2018;</ref><ref type="bibr" target="#b7">Finlay et al., 2020)</ref>, mean-field games <ref type="bibr">(Ruthotto et al., 2020)</ref>, stochastic differential equations  and physically informed modeling <ref type="bibr">(Sanchez-Gonzalez et al., 2019;</ref><ref type="bibr" target="#b20">Zhong et al., 2019)</ref>.</p><p>Though the Neural ODE has been widely applied in practice, how to train it is not extensively studied. The naive method directly backpropagates through an ODE solver, but tracking a continuous trajectory requires a huge memory.  proposed to use the adjoint method to determine the gradient in continuous cases, which achieves constant memory cost w.r.t integration time; however, as pointed out by <ref type="bibr" target="#b21">Zhuang et al. (2020)</ref>, the adjoint method suffers from numerical errors due to the inaccuracy in reverse-time trajectory. <ref type="bibr" target="#b21">Zhuang et al. (2020)</ref> proposed the adaptive checkpoint adjoint (ACA) method to achieve accuracy in gradient estimation at a much smaller memory cost compared to the naive method, yet the memory consumption of ACA still grows linearly with integration time. Due to the non-constant memory cost, neither ACA nor naive method are suitable for large scale datasets (e.g. ImageNet) or high-dimensional Neural ODEs (e.g. FFJORD <ref type="bibr" target="#b9">(Grathwohl et al., 2018)</ref>).</p><p>In this project, we propose the Memory-efficient Asynchronous Leapfrog Integrator (MALI) to achieve advantages of both the adjoint method and ACA: constant memory cost w.r.t integration Published as a conference paper at ICLR 2021 time and accuracy in reverse-time trajectory. MALI is based on the asynchronous leapfrog (ALF) integrator <ref type="bibr">(Mutze, 2013)</ref>. With the ALF integrator, each numerical step forward in time is reversible. Therefore, with MALI, we delete the trajectory and only keep the end-time states, hence achieve constant memory cost w.r.t integration time; using the reversibility, we can accurately reconstruct the trajectory from the end-time value, hence achieve accuracy in gradient. Our contributions are:</p><p>1. We propose a new method (MALI) to solve Neural ODEs, which achieves constant memory cost w.r.t number of solver steps in integration and accuracy in gradient estimation. We provide theoretical analysis.</p><p>2. We validate our method with extensive experiments: (a) for image classification tasks, MALI enables a Neural ODE to achieve better accuracy than a well-tuned ResNet with the same number of parameters; to our knowledge, MALI is the first method to enable training of Neural ODEs on a large-scale dataset such as ImageNet, while existing methods fail due to either heavy memory burden or inaccuracy. (b) In time-series modeling, MALI achieves comparable or better results than other methods. (c) For generative modeling, a FFJORD model trained with MALI achieves new state-of-the-art results on MNIST and Cifar10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 NUMERICAL INTEGRATION METHODS</head><p>An ordinary differential equation (ODE) typically takes the form</p><formula xml:id="formula_0">dz(t) dt = f θ (t, z(t)) s.t. z(t 0 ) = x, t ∈ [t 0 , T ], Loss = L(z(T ), y)<label>(1)</label></formula><p>where z(t) is the hidden state evolving with time, T is the end time, t 0 is the start time (typically 0), x is the initial state. The derivative of z(t) w.r.t t is defined by a function f , and f is defined as a sequence of layers parameterized by θ. The loss function is L(z(T ), y), where y is the target variable. Eq. 1 is called the initial value problem (IVP) because only z(t 0 ) is specified.</p><p>Algorithm 1: Numerical Integration Input initial state x, start time t 0 , end time T , error tolerance etol, initial stepsize h.</p><formula xml:id="formula_1">Initialize z(0) = x, t = t 0 While t &lt; T error est = ∞ While error est &gt; etol h ← h × DecayF actor z, error est = ψ h (t, z) If error est &lt; etol h ← h × IncreaseF actor t ← t + h, z ←ẑ</formula><p>Notations We summarize the notations following <ref type="bibr" target="#b21">Zhuang et al. (2020)</ref>.</p><formula xml:id="formula_2">• z i (t i )/z(τ i ): hidden state in forward/reverse time trajectory at time t i /τ i . • ψ h (t i , z i ): the numerical solution at time t i + h, starting from (t i , z i ) with a stepsize h. • N f , N z : N f is the number of layers in f in Eq. 1,</formula><p>N z is the dimension of z. • N t /N r : number of discretized points (outer iterations in Algo. 1) in forward / reverse integration. • m: average number of inner iterations in Algo. 1 to find an acceptable stepsize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Numerical Integration</head><p>The algorithm for general adaptive-stepsize numerical ODE solvers is summarized in Algo. 1 <ref type="bibr">(Wanner &amp; Hairer, 1996)</ref>. The solver repeatedly advances in time by a step, which is the outer loop in Algo. 1 (blue curve in <ref type="figure" target="#fig_8">Fig. 1</ref>). For each step, the solver decreases the stepsize until the estimate of error is lower than the tolerance, which is the inner loop in Algo. 1 (green curve in <ref type="figure" target="#fig_8">Fig. 1</ref>). For fixed-stepsize solvers, the inner loop is replaced with a single evaluation of ψ h (t, z) using predefined stepsize h. Different methods typically use different ψ, for example different orders of the Runge-Kutta method (Runge, 1895).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ANALYTICAL FORM OF GRADIENT IN CONTINUOUS CASE</head><p>We first briefly introduce the analytical form of the gradient in the continuous case, then we compare different numerical implementations in the literature to estimate the gradient. The analytical form </p><formula xml:id="formula_3">Naive Adjoint ACA MALI Computation N zN f × N t × m × 2 N zN f × (N t + N r ) × m N zN f × N t × (m + 1) N zN f × N t × (m + 2) Memory N zN f × N t × m N zN f N z(N f + N t ) N z(N f + 1) Computation graph depth N f × N t × m N f × N r N f × N t N f × N t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reverse accuracy</head><p>Figure 1: Illustration of numerical solver in forward-pass. For adaptive solvers, for each step forward-in-time, the stepsize is recursively adjusted until the estimated error is below predefined tolerance; the search process is represented by green curve, and the accepted step (ignore the search process) is represented by blue curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>In backward-pass, the adjoint method reconstructs trajectory as a separate IVP. Naive, ACA and MALI track the forward-time trajectory, hence are accurate. ACA and MALI only backpropagate through the accepted step, while naive method backpropagates through the search process hence has deeper computation graphs. of the gradient in the continuous case is</p><formula xml:id="formula_4">dL dθ = − 0 T a(t) ∂f (z(t), t, θ) ∂θ dt (2) da(t) dt + ∂f (z(t), t, θ) ∂z(t) a(t) = 0 ∀t ∈ (0, T ), a(T ) = ∂L ∂z(T )<label>(3)</label></formula><p>where a(t) is the "adjoint state". Detailed proof is given in <ref type="bibr">(Pontryagin, 1962)</ref>. In the next section we compare different numerical implementations of this analytical form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">NUMERICAL IMPLEMENTATIONS IN THE LITERATURE FOR THE ANALYTICAL FORM</head><p>We compare different numerical implementations of the analytical form in this section. The forwardpass and backward-pass of different methods are demonstrated in <ref type="figure" target="#fig_8">Fig. 1 and Fig. 2</ref> respectively. Forward-pass is similar for different methods. The comparison of backward-pass among different methods are summarized in <ref type="table">Table.</ref> 1. We explain methods in the literature below.</p><p>Naive method The naive method saves all of the computation graph (including search for optimal stepsize, green curve in <ref type="figure">Fig. 2)</ref> in memory, and backpropagates through it. Hence the memory cost is N z N f × N t × m and depth of computation graph are N f × N t × m, and the computation is doubled considering both forward and backward passes. Besides the large memory and computation, the deep computation graph might cause vanishing or exploding gradient <ref type="bibr">(Pascanu et al., 2013)</ref>.</p><p>Adjoint method Note that we use "adjoint state equation" to refer to the analytical form in Eq. 2 and 3, while we use "adjoint method" to refer to the numerical implementation by Chen et al. <ref type="bibr">(2018)</ref>. As in <ref type="figure" target="#fig_8">Fig. 1</ref> and 2, the adjoint method forgets forward-time trajectory (blue curve) to achieve memory cost N z N f which is constant to integration time; it takes the end-time state (derived from forward-time integration) as the initial state, and solves a separate IVP (red curve) in reverse-time.</p><p>Theorem 2.1. <ref type="bibr" target="#b21">(Zhuang et al., 2020)</ref> For an ODE solver of order p, the error of the reconstructed initial value by the adjoint method is</p><formula xml:id="formula_5">N −1 k=0 h p+1 k DΦ T t k (z k )l(t k , z k ) + (−h k ) p+1 DΦ t k T (z k )l(t k , z k ) + O(h p+1 ),</formula><p>where Φ is the ideal solution, DΦ is the Jacobian of Φ, l(t, z) and l(t, z) are the local error in forward-time and reverse-time integration respectively. Theorem 2.1 is stated as Theorem 3.2 in <ref type="bibr" target="#b21">Zhuang et al. (2020)</ref>; please see reference paper for detailed proof. To summarize, due to inevitable errors with numerical ODE solvers, the reverse-time trajectory (red curve, z(τ )) cannot match the forward-time trajectory (blue curve, z(t)) accurately. The error in z propagates to dL dθ by Eq. 2, hence affects the accuracy in gradient estimation. Adaptive checkpoint adjoint (ACA) To solve the inaccuracy of adjoint method, <ref type="bibr" target="#b21">Zhuang et al. (2020)</ref> proposed ACA: ACA stores forward-time trajectory in memory for backward-pass, hence guarantees accuracy; ACA deletes the search process (green curve in <ref type="figure">Fig. 2)</ref>, and only backpropagates through the accepted step (blue curve in <ref type="figure">Fig. 2</ref>), hence has a shallower computation graph (N f × N t for ACA vs N f × N t × m for naive method). ACA only stores {z(t i )} Nt i=1 , and deletes the computation graph for {f</p><formula xml:id="formula_6">z(t i ), t i } Nt i=1 , hence the memory cost is N z (N f + N t ).</formula><p>Though the memory cost is much smaller than the naive method, it grows linearly with N t , and can not handle very high dimensional models. In the following sections, we propose a method to overcome all these disadvantages of existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ASYNCHRONOUS LEAPFROG INTEGRATOR</head><p>In this section we give a brief introduction to the asynchronous leapfrog (ALF) method (Mutze, 2013), and we provide theoretical analysis which is missing in Mutze (2013). For general firstorder ODEs in the form of Eq. 1, the tuple (z, t) is sufficient for most ODE solvers to take a step numerically. For ALF, the required tuple is (z, v, t), where v is the "approximated derivative". Most numerical ODE solvers such as the Runge-Kutta method (Runge, 1895) track state z evolving with time, while ALF tracks the "augmented state" (z, v). We explain the details of ALF as below.</p><p>Algorithm 2: Forward of ψ in ALF Input (z in , v in , s in , h) where s in is current time, z in and v in are correponding values at time s in , h is stepsize.</p><formula xml:id="formula_7">Forward s 1 = s in + h/2 k 1 = z in + v in × h/2 u 1 = f (k 1 , s 1 ) v out = v in + 2(u 1 − v in ) z out = k 1 + v out × h/2 s out = s 1 + h/2 Output (z out , v out , s out , h)</formula><p>Algorithm 3: ψ −1 (Inverse of ψ) in ALF Input (z out , v out , s out , h) where s out is current time, z out and v out are corresponding values at s out , h is stepsize. Procedure of ALF Different ODE solvers have different ψ in Algo. 1, hence we only summarize ψ for ALF in Algo. 2. Note that for a complete algorithm of integration for ALF, we need to plug Algo. 2 into Algo. 1. The forward-pass is summarized in Algo. 2. Given stepsize h, with input (z in , v in , s in ), a single step of ALF outputs (z out , v out , s out ). <ref type="figure" target="#fig_0">Fig. 3</ref>, given (z 0 , v 0 , t 0 ), the numerical forwardtime integration calls Algo. 2 iteratively:</p><formula xml:id="formula_8">Inverse s 1 = s out − h/2 k 1 = z out − v out × h/2 u 1 = f (k 1 , s 1 ) v in = 2u 1 − v out z in = k 1 − v in × h/2 s in = s 1 − h/2 Output (z in , v in , s in , h)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As in</head><formula xml:id="formula_9">(z i , v i , t i , h i ) = ψ(z i−1 , v i−1 , t i−1 , h i ) s.t. h i = t i − t i−1 , i = 1, 2, ...N t<label>(4)</label></formula><p>Invertibility of ALF An interesting property of ALF is that ψ defines a bijective mapping; therefore, we can reconstruct (z in , v in , s in , h) from (z out , v out , s out , h), as demonstrated in Algo. 7. As in <ref type="figure" target="#fig_0">Fig. 3</ref>, we can reconstruct the entire trajectory given the state (z j , v j ) at time t j , and the discretized time points {t 0 , ...t Nt }. For example, given (z Nt , v Nt ) and {t i } Nt i=0 , the trajectory for Eq. 4 is reconstructed:</p><formula xml:id="formula_10">(z i−1 , v i−1 , t i−1 , h i ) = ψ −1 (z i , v i , t i , h i ) s.t. h i = t i − t i−1 , i = N t , N t − 1, ..., 1<label>(5)</label></formula><p>In the following sections, we will show the invertibility of ALF is the key to maintain accuracy at a constant memory cost to train Neural ODEs. Note that "inverse" refers to reconstructing the input from the output without computing the gradient, hence is different from "back-propagation".</p><p>Initial value For an initial value problem (IVP) such as Eq. 1,</p><formula xml:id="formula_11">typically z 0 = z(t 0 ) is given while v 0 is undetermined. We can construct v 0 = f (z(t 0 ), t 0 ), so the initial augmented state is (z 0 , v 0 ).</formula><p>Difference from midpoint integrator The midpoint integrator <ref type="bibr">(Süli &amp; Mayers, 2003)</ref> is similar to Algo. 2, except that it recomputes v in = f (z in , s in ) for every step, while ALF directly uses the input v in . Therefore, the midpoint method does not have an explicit form of inverse.</p><p>Local truncation error Theorem 3.1 indicates that the local truncation error of ALF is of order O(h 3 ); this implies the global error is O(h 2 ). Detailed proof is in Appendix A.3. Theorem 3.1. For a single step in ALF with stepsize h, the local truncation error of z is O(h 3 ), and the local truncation error of v is O(h 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A-Stability</head><p>The ALF solver has a limited stability region, but this can be solved with damping. The damped ALF replaces the update of v out in Algo.</p><formula xml:id="formula_12">2 with v out = v in + 2η(u 1 − v in ),</formula><p>where η is the "damping coefficient" between 0 and 1. We have the following theorem on its numerical stability. Theorem 3.2. For the damped ALF integrator with stepsize h, where σ i is the i-th eigenvalue of the Jacobian ∂f ∂z , then the solver is</p><formula xml:id="formula_13">A-stable if 1 + η(hσ i − 1) ± η 2hσ i + η(hσ i − 1) 2 &lt; 1, ∀i</formula><p>Proof is in Appendix A.4 and A.5. Theorem 3.2 implies the following: when η = 1, the damped ALF reduces to ALF, and the stability region is empty; when 0 &lt; η &lt; 1, the stability region is nonempty. However, stability describes the behaviour when T goes to infinity; in practice we always use a bounded T and ALF performs well. Inverse of damped ALF is in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MEMORY-EFFICIENT ALF INTEGRATOR (MALI) FOR GRADIENT ESTIMATION</head><p>An ideal solver for Neural ODEs should achieve two goals: accuracy in gradient estimation and constant memory cost w.r.t integration time. Yet none of the existing methods can achieve both goals. We propose a method based on the ALF solver, which to our knowledge is the first method to achieve the two goals simultaneously.</p><p>Algorithm 4: MALI to acheive accuracy at a constant memory cost w.r.t integration time Input Initial state z 0 , start time t 0 , end time T Forward Apply the numerical integration in Algo. 1, with the ψ function defined by Algo. 2. Delete computation graph on the fly, only keep end-time state (z Nt , v Nt ) Keep accepted discretized time points {t i } Nt i=0 (ignore process to search for optimal stepsize) Backward</p><p>Initialize</p><formula xml:id="formula_14">a(T ) = ∂L ∂z(T ) by Eq. 3, initialize dL dθ = 0 For i in {N t , N t − 1, ..., 2, 1}: Reconstruct (z i−1 , v i−1 ) from (z i , v i ) by Algo. 7 Local forward (z i , v i , t i , h i ) = ψ(z i−1 , v i−1 , t i−1 , h i ) Local backward, get ∂f (zi−1,ti−1,θ) ∂zi−1 and ∂f (zi−1,ti−1,θ) ∂θ</formula><p>Update a(t) and dL dθ by Eq. 2 and Eq. 3 discretized at time points t i−1 and t i Delete local computation graph Output the adjoint state a(t 0 ) (gradient w.r.t input z 0 ) and parameter gradient dL dθ Procedure of MALI Details of MALI are summarized in Algo. 4. For the forward-pass, we only keep the end-time state (z Nt , v Nt ) and the accepted discretized time points (blue curves in <ref type="figure" target="#fig_8">Fig. 1</ref> and 2). We ignore the search process for optimal stepsize (green curve in <ref type="figure" target="#fig_8">Fig. 1 and 2)</ref>, and delete other variables to save memory. During the backward pass, we can reconstruct the forward-time trajectory as in Eq. 5, then calculate the gradient by numerical discretization of Eq. 2 and Eq. 3.  </p><formula xml:id="formula_15">(N f + 1), where N z N f is due to evaluating f (z, t)</formula><p>and is irreducible for all methods. Compared with the adjoint method, MALI only requires extra N z memory to record v Nt , and also has a constant memory cost w.r.t time step N t . The memory cost is N z (N f + 1).</p><p>Accuracy Our method guarantees the accuracy of reverse-time trajectory (e.g. blue curve in <ref type="figure">Fig. 2</ref> matches the blue curve in <ref type="figure" target="#fig_8">Fig. 1</ref>), because ALF is explicitly invertible for free-form f (see Algo. 7). Therefore, the gradient estimation in MALI is more accurate compared to the adjoint method.</p><p>Computation cost Recall that on average it takes m steps to find an acceptable stepsize, whose error estimate is below tolerance. Therefore, the forward-pass with search process has computation burden N z × N f × N t × m. Note that we only reconstruct and backprop through the accepted step and ignore the search process, hence it takes another N z × N f × N t × 2 computation. The overall computation burden is N z N f × N t × (m + 2) as in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Shallow computation graph Similar to ACA, MALI only backpropagates through the accepted step (blue curve in <ref type="figure">Fig. 2</ref>) and ignores the search process (green curve in <ref type="figure">Fig. 2</ref>), hence the depth of computation graph is N f × N t . The computation graph of MALI is much shallower than the naive method, hence is more robust to vanishing and exploding gradients <ref type="bibr">(Pascanu et al., 2013)</ref>.</p><p>Summary The adjoint method suffers from inaccuracy in reverse-time trajectory, the naive method suffers from exploding or vanishing gradient caused by deep computation graph, and ACA finds a balance but the memory grows linearly with integration time. MALI achieves accuracy in reversetime trajectory, constant memory w.r.t integration time, and a shallow computation graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">VALIDATION ON A TOY EXAMPLE</head><p>We compare the performance of different methods on a toy example, defined as</p><formula xml:id="formula_16">L(z(T )) = z(T ) 2 s.t. z(0) = z 0 , dz(t)/dt = αz(t)<label>(6)</label></formula><p>The analytical solution is</p><formula xml:id="formula_17">z(t) = z 0 e αt , L = z 2 0 e 2αT , dL/dz 0 = 2z 0 e 2αT , dL/dα = 2T z 2 0 e 2αT<label>(7)</label></formula><p>We plot the amplitude of error between numerical solution and analytical solution varying with T (integrated under the same error tolerance, rtol = 10 −5 , atol = 10 −6 ) in   have similar errors, both outperforming other methods. We also plot the memory consumption for different methods on a Neural ODE with the same input in <ref type="figure" target="#fig_1">Fig. 4</ref>. As the error tolerance decreases, the solver evaluates more steps, hence the naive method and ACA increase memory consumption, while MALI and the adjoint method have a constant memory cost. These results validate our analysis in Sec. 3.2 and <ref type="table" target="#tab_0">Table 1</ref>, and shows MALI achieves accuracy at a constant memory cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IMAGE RECOGNITION WITH NEURAL ODE</head><p>We validate MALI on image recognition tasks using Cifar10 and ImageNet datasets. Similar to <ref type="bibr" target="#b21">Zhuang et al. (2020)</ref>, we modify a ResNet18 into its corresponding Neural ODE: the forward function is y = x + f θ (x) and y = x + T 0 f θ (z)dt for the residual block and Neural ODE respectively, where the same f θ is shared. We compare MALI with the naive method, adjoint method and ACA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Cifar10</head><p>Results of 5 independent runs on Cifar10 are summarized in <ref type="figure" target="#fig_14">Fig. 5</ref>. MALI achieves comparable accuracy to ACA, and both significantly outperform the naive and the adjoint method. Furthermore, the training speed of MALI is similar to ACA, and both are almost two times faster than the adjoint memthod, and three times faster than the naive method. This validates our analysis on accuracy and computation burden in <ref type="table" target="#tab_0">Table 1</ref>. Accuracy on ImageNet Due to the heavy memory burden caused by large images, the naive method and ACA are unable to train a Neural ODE on ImageNet with 4 GPUs; only MALI and the adjoint method are feasible due to the constant memory. We also compare the Neural ODE to a standard ResNet. As shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, the accuracy of the Neural ODE trained with MALI closely follows ResNet, and significantly outperforms the adjoint method (top-1 validation: 70% v.s. 63%).</p><p>Invariance to discretization scheme A continuous model should be invariant to discretization schemes (e.g. different types of ODE solvers) as long as the discretization is sufficiently accurate. We test the Neural ODE using different solvers without re-training; since ResNet is often viewed as a one-step Euler discretization of an ODE <ref type="bibr" target="#b10">(Haber &amp; Ruthotto, 2017)</ref>, we perform similar experiments. As shown in <ref type="table" target="#tab_2">Table 2</ref>, Neural ODE consistently achieves high accuracy (∼70%), while ResNet drops to random guessing (∼0.1%) because ResNet as a one-step Euler discretization fails to be a meaningful dynamical system <ref type="bibr">(Queiruga et al., 2020)</ref>.  <ref type="table" target="#tab_3">Table 3</ref>. For Neural ODE, due to its invariance to discretization scheme, we derive the gradient for attack using a certain solver (row in   <ref type="table" target="#tab_3">Table 3</ref>), and inference on the perturbed images using various solvers. For different combinations of solvers and perturbation amplitudes, Neural ODE consistently outperforms ResNet.</p><p>Summary In image recognition tasks, we demonstrate Neural ODE is accurate, invariant to discretization scheme, and more robust to adversarial attack than ResNet. Note that detailed explanation on the robustness of Neural ODE is out of the scope for this paper, but to our knowledge, MALI is the first method to enable training of Neural ODE on large datasets due to constant memory cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TIME-SERIES MODELING</head><p>We apply MALI to latent-ODE (Rubanova et al., 2019) and Neural Controlled Differential Equation (Neural CDE) <ref type="bibr" target="#b15">(Kidger et al., 2020a;</ref><ref type="bibr">b)</ref>. Our experiment is based on the official implementation from the literature. We report the mean squared error (MSE) on the Mujoco test set in <ref type="table" target="#tab_4">Table 4</ref>, which is generated from the "Hopper" model using DeepMind control suite (Tassa et al., 2018); for all experiments with different ratios of training data, MALI achieves similar MSE to ACA, and both outperform the adjoint and naive method. We report the test accuracy on the Speech Command dataset for Neural CDE in <ref type="table" target="#tab_5">Table 5</ref>; MALI achieves a higher accuracy than competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CONTINUOUS GENERATIVE MODELS</head><p>We apply MALI on FFJORD <ref type="bibr" target="#b9">(Grathwohl et al., 2018)</ref>, a free-from continuous generative model, and compare with several variants in the literature <ref type="bibr" target="#b7">(Finlay et al., 2020;</ref><ref type="bibr" target="#b15">Kidger et al., 2020a)</ref>. Our experiment is based on the official implementaion of <ref type="bibr" target="#b7">Finlay et al. (2020)</ref>; for a fair comparison, we train with MALI, and test with the same solver as in the literature <ref type="bibr" target="#b9">(Grathwohl et al., 2018;</ref><ref type="bibr" target="#b7">Finlay et al., 2020)</ref>, the Dopri5 solver with rtol = atol = 10 −5 from the torchdiffeq package (Chen et al., 2018). Bits per dim (BPD, lower is better) on validation set for various datasets are reported in <ref type="table" target="#tab_6">Table 6</ref>. For continuous models, MALI consistently generates the lowest BPD, and outperforms the Vanilla FFJORD (trained with adjoint), RNODE (regularized FFJORD) and the SemiNorm Adjoint <ref type="bibr" target="#b15">(Kidger et al., 2020a)</ref>. Furthermore, FFJORD trained with MALI achieves comparable BPD to stateof-the-art discrete-layer flow models in the literature. Please see Sec. B.3 for generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORKS</head><p>Besides ALF, the symplectic integrator <ref type="bibr">(Verlet, 1967;</ref><ref type="bibr" target="#b19">Yoshida, 1990)</ref> is also able to reconstruct trajectory accurately, yet it's typically restricted to second order Hamiltonian systems (De <ref type="bibr" target="#b4">Almeida, 1990)</ref>, and are unsuitable for general ODEs. Besides aforementioned methods, there are other methods for gradient estimation such as interpolated adjoint <ref type="bibr" target="#b3">(Daulbaev et al., 2020)</ref> and spectral method (Quaglino et al., 2019), yet the implementations are involved and not publicly available. Other works focus on the theoretical properties of Neural ODEs <ref type="bibr" target="#b6">(Dupont et al., 2019;</ref><ref type="bibr">Tabuada &amp; Gharesifard, 2020;</ref><ref type="bibr">Massaroli et al., 2020)</ref>. Neural ODE is recently applied to stochastic differential equation , jump differential equation <ref type="bibr" target="#b14">(Jia &amp; Benson, 2019)</ref> and auto-regressive models (Wehenkel &amp; Louppe, 2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Based on the asynchronous leapfrog integrator, we propose MALI to estimate the gradient for Neural ODEs. To our knowledge, our method is the first to achieve accuracy, fast speed and a constant memory cost. We provide comprehensive theoretical analysis on its properties. We validate MALI Published as a conference paper at ICLR 2021  <ref type="figure" target="#fig_3">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16  A.5 Damped ALF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</ref>   <ref type="figure">Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</ref>  For the ease of reading, we write the algorithm for ψ in ALF below, which is the same as Algo. 2 in the main paper, but uses slightly different notations for the ease of analysis.</p><p>Algorithm 1: Forward of ψ in ALF Input ( z in , v in , s in , h) = ( z 0 , v 0 , s 0 , h) where s 0 is current time, z 0 and v 0 are correponding values at time s 0 ; stepsize h. Forward</p><formula xml:id="formula_18">s 1 = s 0 + h/2 (1) z 1 = z 0 + v 0 × h/2 (2) v 1 = f ( z 1 , s 1 ) (3) v 2 = v 1 + ( v 1 − v 0 ) (4) z 2 = z 1 + v 2 × h/2 (5) s 2 = s 1 + h/2 (6) Output ( z out , v out , s out , h) = ( z 2 , v 2 , s 2 , h)</formula><p>For simplicity, we can re-write the forward of ALF as</p><formula xml:id="formula_19">z 2 v 2 =   z 0 + hf ( z 0 + h 2 v 0 , s 0 + h 2 ) 2f ( z 0 + h 2 v 0 , s 0 + h 2 ) − v 0   (7)</formula><p>Similarly, the inverse of ALF can be written as</p><formula xml:id="formula_20">z 0 v 0 =   z 2 − hf ( z 2 − h 2 v 2 , s 2 − h 2 ) 2f ( z 2 − h 2 v 2 , s 2 − h 2 ) − v 2   (8) A.2 PRELIMINARIES For an ODE of the form dz(t) dt = f (z(t), t)<label>(9)</label></formula><p>We have:</p><formula xml:id="formula_21">d 2 z(t) dt 2 = d dt f (z(t), t) = ∂f (z(t), t) ∂t + ∂f (z(t), t) ∂z dz(t) dt<label>(10)</label></formula><p>For the ease of notation, we re-write Eq. 10 as</p><formula xml:id="formula_22">d 2 z(t) dt 2 = f t + f z f<label>(11)</label></formula><p>where f t and f z represents the partial derivative of f w.r.t t and z respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 LOCAL TRUNCATION ERROR OF ALF</head><p>Theorem A.1 (Theorem 3.1 in the main paper). For a single step in ALF with stepsize h, the local truncation error of z is O(h 3 ), and the local truncation errof of v is O(h 2 ).</p><p>Proof. Under the same notation as Algo. 1, denote the ground-truth state of z and v starting from ( z 0 , s 0 ) as z and v respectively. Then the local truncation error is</p><formula xml:id="formula_23">L z = z(s 0 + h) − z 2 , L v = v(s 0 + h) − v 2<label>(12)</label></formula><p>We estimate L z and L v in terms of polynomial of h.</p><p>Under mild assumptions that f is smooth up to 2nd order almost everywhere (this is typically satisfied with neural networks with bounded weights), hence Taylor expansion is meaningful for f . By Eq. 11, the Taylor expansion of z around point ( z 0 , v 0 , s 0 ) is</p><formula xml:id="formula_24">z(s 0 + h) = z 0 + h dz dt + h 2 2 d 2 z dt 2 + O(h 3 ) (13) = z 0 + hf ( z 0 , s 0 ) + h 2 2 f t ( z 0 , s 0 ) + f z ( z 0 , s 0 )f ( z 0 , s 0 ) + O(h 3 )<label>(14)</label></formula><p>Next, we analyze accuracy of the numerical approximation. For simplicity, we directly analyze Eq. 7 by performing Taylor Expansion on f .</p><formula xml:id="formula_25">f ( z 0 + h 2 v 0 , s 0 + h 2 ) = f ( z 0 , s 0 ) + h 2 f t ( z 0 , s 0 ) + h v 0 2 f z ( z 0 , s 0 ) + O(h 2 )<label>(15)</label></formula><formula xml:id="formula_26">z 2 = z 0 + hf ( z 0 + h 2 v 0 , s 0 + h 2 )<label>(16)</label></formula><p>Plug Eq. 14, Eq. 15 and E.q. 16 into the definition of L z , we get</p><formula xml:id="formula_27">L z = z(s 0 + h) − z 2 (17) = z 0 + hf ( z 0 , s 0 ) + h 2 2 f t ( z 0 , s 0 ) + f z ( z 0 , s 0 )f ( z 0 , s 0 ) − z 0 + h f ( z 0 , s 0 ) + h 2 f t ( z 0 , s 0 ) + h v 0 2 f z ( z 0 , s 0 ) + O(h 3 ) (18) = h 2 2 f z ( z 0 , s 0 ) f ( z 0 , s 0 ) − v 0 + O(h 3 )<label>(19)</label></formula><p>Therefore Next we analyze the local truncation error in v, denoted as L v . Denote the ground truth as</p><formula xml:id="formula_28">, if f ( z 0 , s 0 ) − v 0 is of order O(1), L z is of order O(h 2 ); if f ( z 0 , s 0 ) − v</formula><formula xml:id="formula_29">v(t 0 + h), we have v(s 0 + h) = f z(s 0 + h), s 0 + h (20) = f ( z 0 , s 0 ) + hf t ( z 0 , s 0 ) + z(s 0 + h) − z 0 f z ( z 0 , s 0 ) + O(h 2 )<label>(21)</label></formula><p>Next we analyze the error in the numerical approximation. Plug Eq. 15 into Eq. 7,</p><formula xml:id="formula_30">v 2 = 2f ( z 0 + h 2 v 0 , s 0 + h 2 ) − v 0 (22) = f ( z 0 , s 0 ) + f ( z 0 , s 0 ) − v 0 + hf t ( z 0 , s 0 ) + h v 0 f z ( z 0 , s 0 ) + O(h 2 )<label>(23)</label></formula><p>From Eq. 14, Eq. 21 and Eq. 23, we have</p><formula xml:id="formula_31">L v = v(s 0 + h) − v 2 (24) = f ( z 0 , s 0 ) − v 0 + z(s 0 + h) − z 0 + h v 0 f z ( z 0 , s 0 ) + O(h 2 ) (25) = f ( z 0 , s 0 ) − v 0 + h f ( z 0 , s 0 ) − v 0 f z ( z 0 , s 0 ) + O(h 2 )<label>(26)</label></formula><p>The last equation is derived by plugging in Eq. 14. Note that Eq. 26 holds for every single step forward in time, and at the start time of integration, we have f ( z 0 , s 0 ) − v 0 = 0 due to our initialization as in Sec. 3.1 of the main paper. Therefore, by induction, L v is of order O(h 2 ) for consecutive steps. Proof. See <ref type="figure">(Silvester, 2000)</ref> for a detailed proof.</p><p>Theorem A.2. For ALF integrator with stepsize h, if hσ i is 0 or is imaginary with norm no larger than 1, where σ i is the i-th eigenvalue of the Jacobian ∂f ∂z , then the solver is on the critical boundary of A-stability; otherwise, the solver is not A-stable.</p><p>Proof. A solver is A-stable is equivalent to the eigenvalue of the numerical forward has a norm below 1. We calculate the eigenvalue of ψ below.</p><p>For the function defined by Eq. 7, the Jacobian is</p><formula xml:id="formula_32">J =   ∂ z2 ∂z0 ∂ z2 ∂ v0 ∂ v2 ∂z0 ∂ v2 ∂ v0   =   I + h ∂f ∂z h 2 2 ∂f ∂z 2 × ∂f ∂z h ∂f ∂z − I  <label>(27)</label></formula><p>We determine the eigenvalue of J by solving the equation</p><formula xml:id="formula_33">det(J − λI) =   h ∂f ∂z + (1 − λ)I h 2 2 ∂f ∂z 2 × ∂f ∂z h ∂f ∂z − (1 + λ)I   = 0<label>(28)</label></formula><p>It's trivial to check J satisfies conditions for Lemma A.1.1.Therefore, we have</p><formula xml:id="formula_34">det(J − λI) = det h ∂f ∂z + (1 − λ)I h ∂f ∂z − (1 + λ)I − h 2 2 ∂f ∂z 2 × ∂f ∂z (29) = det − 2λh ∂f ∂z + (λ 2 − 1)I<label>(30)</label></formula><p>Suppose the eigen-decompostion of ∂f ∂z can be written as</p><formula xml:id="formula_35">∂f ∂z = Λ    σ 1 σ 2 ... σ N    Λ −1<label>(31)</label></formula><p>Note that I = ΛIλ −1 , hence we have</p><formula xml:id="formula_36">det(J − λI) = det Λ − 2λh    σ 1 σ 2 ... σ N    + (λ 2 − 1)I Λ −1 (32) = N i=1 (λ 2 − 2hσ i λ − 1)<label>(33)</label></formula><p>Hence the eigenvalues are</p><formula xml:id="formula_37">λ i± = hσ i ± h 2 σ 2 i + 1<label>(34)</label></formula><p>A-stability requires |λ i± | &lt; 1, ∀i, and has no solution.</p><p>The critical boundary is |λ i± | = 1, the solution is: hσ i is 0 or on the imaginary line with norm no larger than 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 DAMPED ALF</head><p>Algorithm 2: Forward of ψ in Damped ALF (η ∈ (0, 1] ) Input ( z in , v in , s in , h) = ( z 0 , v 0 , s 0 , h) where s 0 is current time, z 0 and v 0 are correponding values at time s 0 ; stepsize h. Forward</p><formula xml:id="formula_38">s 1 = s 0 + h/2 (35) z 1 = z 0 + v 0 × h/2 (36) v 1 = f ( z 1 , s 1 ) (37) v 2 = v 0 + 2η( v 1 − v 0 ) (38) z 2 = z 1 + v 2 × h/2 (39) s 2 = s 1 + h/2 (40) Output ( z out , v out , s out , h) = ( z 2 , v 2 , s 2 , h) Algorithm 3: ψ −1 (Inverse of ψ) in Damped ALF (η ∈ (0, 1] ) Input ( z out , v out , s out , h) where s out is current time, z out and v out are corresponding values at s out , h is stepsize. Inverse ( z 2 , v 2 , s 2 , h) = ( z out , v out , s out , h) (41) s 1 = s 2 − h/2 (42) z 1 = z 2 − v 2 × h/2 (43) v 1 = f ( z 1 , s 1 ) (44) v 0 = ( v 2 − 2η v 1 )/(1 − 2η) (45) z 0 = z 1 − v 0 × h/2 (46) s 0 = s 1 − h/2 (47) Output ( z in , v in , s in , h) = ( z 0 , v 0 , s 0 , h)</formula><p>The main difference between ALF and Damped ALF is marked in blue in Algo. 2. In ALF, the</p><formula xml:id="formula_39">update of v 2 is v 2 = ( v 1 − v 0 ) + v 1 = 2( v 1 − v 0 ) + v 0 ;</formula><p>while in Damped ALF, the update is scaled by a factor η between 0 and 1, so the update is v 2 = 2η( v 1 − v 0 ) + v 0 . When η = 1, Damped ALF reduces to ALF.</p><p>Similar to Sec. A.1, we can write the forward as For simplicity, we can re-write the forward of ALF as</p><formula xml:id="formula_40">z 2 v 2 =   z 0 + ηhf ( z 0 + h 2 v 0 , s 0 + h 2 ) + (1 − η)h v 0 2ηf ( z 0 + h 2 v 0 , s 0 + h 2 ) + (1 − 2η) v 0  <label>(48)</label></formula><p>Similarly, the inverse of ALF can be written as Proof. The proof is similar to Thm. A.3. By similar calculations using the Taylor Expansion in Eq. 15 and Eq. 14, we have</p><formula xml:id="formula_41">z 0 v 0 =   z 2 − h 1−η 1−2η v 2 + h η 1−2η f ( z 2 − h 2 v 2 , s 2 − h 2 ) 1 1−2η v 2 − 2η 1−2η f ( z 2 − h 2 v 2 , s 2 − h 2 )  <label>(49)</label></formula><formula xml:id="formula_42">z 2 −z(s 0 + h) = (1 − η)h v 0 + hη f ( z 0 , s 0 ) + h 2 f t ( z 0 , s 0 ) + h v 0 2 f z ( z 0 , s 0 ) − h f ( z 0 , s 0 ) + h 2 f t z 0 , s 0 + h 2 f z ( z 0 , s 0 )f ( z 0 , s 0 ) + O(h 2 ) (50) = (1 − η)h v 0 − f ( z 0 , s 0 ) + η − 1 2 h 2 f t ( z 0 , s 0 ) + h 2 2 η v 0 − f ( z 0 , s 0 ) f z ( z 0 , s 0 ) + O(h 2 )<label>(51)</label></formula><p>Using Eq. 21, Eq. 15 and Eq. 14, we havẽ</p><formula xml:id="formula_43">v 2 − v 2 = (1 − 2η) v 0 + (2η − 1)f ( z 0 , s 0 ) + (1 − η)hf t ( z 0 , s 0 ) + z(s 0 + h) − z 0 − ηh v 0 f z ( z 0 , s 0 ) + O(h 2 ) (52) = (2η − 1) f ( z 0 , s 0 ) − z 0 + (1 − η)hf t ( z 0 , s 0 ) + η hf ( z 0 , s 0 ) − h v 0 f z ( z 0 , s 0 ) + O(h 2 )<label>(53)</label></formula><p>Note that when η = 1, Eq. 51 reduces to Eq. 19, and Eq. 53 reduces to Eq. 26. By initialization, we have |f ( z 0 , s 0 ) − v 0 | = 0 at initial time, hence by induction, the local truncation error for z is O(h 2 ); the local truncation error for v is O(h) when η &lt; 1, and is O(h 2 ) when η = 1.</p><p>Theorem A.4 (Theorem 3.2 in the main paper). For Dampled ALF integrator with stepsize h, where σ i is the i-th eigenvalue of the Jacobian ∂f ∂z , then the solver is</p><formula xml:id="formula_44">A-stable if 1 + η(hσ − 1) ± η 2hσ i + η(hσ i − 1) 2 &lt; 1, ∀i.</formula><p>Proof. The Jacobian of the forward-pass of a single step damped ALF is</p><formula xml:id="formula_45">J =   I + ηh ∂f ∂z (1 − η)hI + η h 2 2 ∂f ∂z 2η ∂f ∂z ηh ∂f ∂z + (1 − 2η)I  <label>(54)</label></formula><p>when η = 1, J reduces to Eq. 27. We can determine the eigenvalue of J using similar techniques. Assume the eigenvalues for ∂f ∂z are {σ i }, then we have</p><formula xml:id="formula_46">det(J − λI) = det   (1 − λ)I + ηh ∂f ∂z (1 − η)hI + η h 2 2 ∂f ∂z 2η ∂f ∂z ηh ∂f ∂z + (1 − 2η − λ)I   (55) = det (1 − λ)I + ηh ∂f ∂z ηh ∂f ∂z + (1 − 2η − λ)I − (1 − η)hI + η h 2 2 ∂f ∂z 2η ∂f ∂z (56) = N i=1 1 + η(hσ i − 1) ± η 2hσ i + η(hσ i − 1) 2<label>(57)</label></formula><p>when η &lt; 1, it's easy to check that 1+η(hσ i −1)± η 2hσ i + η(hσ i − 1) 2 &lt; 1 has non-empty solutions for hσ.</p><p>For a quick validation, we plot the region of A-stability on the imaginary plane for a single eigenvalue in <ref type="figure" target="#fig_8">Fig. 1</ref>. As η increases, the area of stability decreases. When η = 1, the system is no-where A-stable, and the boundary for A-stability is on the imaginary axis <ref type="bibr">[−i, i]</ref> where i is the imaginary unit. We directly modify a ResNet18 into a Neural ODE, where the forward of a residual block (y = x + f (x)) and the forward of an ODE block (y = x + T 0 f (z, t)dt where T = 1) share the same parameterization f , hence they have the same number of parameters. Our experiment is based on the official implementation by <ref type="bibr" target="#b21">Zhuang et al. (2020)</ref> and an open-source repository <ref type="bibr">(Liu, 2017)</ref>.</p><p>All models are trained with SGD optimizer for 90 epochs, with an initial learning rate of 0.01, and decayed by a factor of 10 at 30th epoch and 60th epoch respectively. Training scheme is the same for all models (ResNet, Neural ODE trained with adjoint, naive, ACA and MALI). For ACA, we follow the settings in <ref type="bibr" target="#b21">(Zhuang et al., 2020)</ref> and use the official implementation torch ACA 1 , and use a Heun-Euler solver with rtol = 10 −1 , atol = 10 −2 during training. For MALI, we use an adaptive version and set rtol = 10 −1 , atol = 10 −2 . For the naive and adjoint method, we use the default Dopri5 solver from the torchdiffeq 2 package with rtol = atol = 10 −5 . We train all models for 5 independent runs, and report the mean and standard deviation across runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 EXPERIMENTS ON IMAGENET</head><p>Training scheme We conduct experiments on ImageNet with ResNet18 and Neural-ODE18. All models are trained on 4 GTX-1080Ti GPUs with a batchsize of 256. All models are trained for 80 epochs, with an initial learning rate of 0.1, and decayed by a factor of 10 at 30th and 60th epoch. Note that due to the large size input 256 × 256, the naive method and ACA requires a huge memory, and is infeasible to train. MALI and the adjoint method requires a constant memory hence is suitable for large-scale experiments. For both MALI and the adjoint menthod, we use a fixed stepsize of 0.25, and integrates from 0 to T = 1. As shown in <ref type="table">Table.</ref> 2 in the main paper, a stepsize of 0.25 is sufficiently small to train a meaningful continuous model that is robust to discretization scheme.</p><p>Invariance to discretization scheme To test the influence of discretization scheme, we test our Neural ODE with different solvers without re-training. For fixed-stepsize solvers, we tested various step sizes including {0.1, 0.15, 0.25, 0.5, 1.0}; for adaptive solvers, we set rtol=0.1, atol=0.01 for MALI and Heun-Euler method, and set rtol = 10 −2 , atol = 10 −3 for RK23 solver, and set rtol = 10 −4 , atol = 10 −5 for Dopri5 solver. As shown in <ref type="table">Table.</ref> 2, Neural ODE trained with MALI is robust to discretization scheme, and MALI significantly outperforms the adjoint method in terms of accuracy (70% v.s. 63% top-1 accuracy on the validation dataset). An interesting finding is that when trained with MALI which is a second-order solver, and tested with higher-order solver (e.g.  Adversarial robustness Besides the high accuracy and robustness to discretization scheme, another advantage of Neural ODE is the robustness to adversarial attack. The adversary robustness of Neural ODE is extensively studied in <ref type="bibr" target="#b11">(Hanshu et al., 2019)</ref>, but not only validated on small-scale datasets such as Cifar10. To our knowledge, our method is the first to enable effectuve training of Neural ODE on large-scale datasets such as ImageNet and achieve a high accuracy, and we are the first to validate the robustness of Neural ODE on ImageNet. We use the advertorch 3 toolbox to perform adversarial attack. We test the performance of ResNet and Neural ODE under FGSM attack. To be more convincing, we conduct experiment on the pretrained ResNet18 provided by the official PyTorch website 4 . Since Neural ODE is invariant to discretization scheme, it's possible to derive the gradient for attack using one ODE solver, and inference on the perturbed image using another solver. As summarized in <ref type="table">Table.</ref> 3, Neural ODE consistently achieves a higher accuracy than ResNet under the same attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 TIME SERIES MODELING</head><p>We conduct experiments on Latent-ODE models <ref type="bibr">(Rubanova et al., 2019)</ref> and Neural CDE (controlled differential equation) <ref type="bibr" target="#b15">(Kidger et al., 2020a)</ref>. For all experiments, we use the official implementation, and only replace the solver with MALI. The latent-ODE model is trained on the Mujoco dataset processed with code provided by the official implementation, and we experiment with different ratios (10%,20%,50%) of training data as described in <ref type="bibr">(Rubanova et al., 2019)</ref>. All models are trained for 300 epochs with Adamax optimizer, with an initial learning rate of 0.01 and scaled by 0.999 for each epoch. For the Neural CDE model, for the naive method, ACA and MALI, we perform 5 independent runs and report the mean value and standard deviation; results for the adjoint and seminorm adjoint are from <ref type="bibr" target="#b15">(Kidger et al., 2020a)</ref>. For Neural CDE, we use MALI with ALF solver with a fixed stepsize of 0.25, and train the model for 100 epochs with an initial learning rate of 0.004.  Our experiment is based on the official implementation of <ref type="bibr" target="#b7">(Finlay et al., 2020)</ref>, with the only difference in ODE solver. For a fair comparison, we only use MALI for training, and use Dopri5 solver from torchdiffeq package (Chen et al., 2018) with rtol = atol = 10 −5 . For MALI, we use adaptive ALF solver with rtol = 10 −2 , atol = 10 −3 , and use an initial stepsize of 0.25. Integration time is from 0 to 1.</p><p>On MNIST and CIFAR dataset, we set the regularization coefficients for kinetic energy and Frobenius norm of the derivative function as 0.05. We train the model for 50 epochs with an initial learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.2 ADDTIONAL RESULTS</head><p>We show generated examples on MNIST dataset in <ref type="figure" target="#fig_0">Fig. 3</ref>, results for Cifar10 dataset in <ref type="figure" target="#fig_1">Fig. 4</ref>, and results for ImageNet64 in <ref type="figure" target="#fig_14">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ERROR IN GRADIENT ESTIMATION FOR TOY EXAMPLES WHEN t &lt; 1</head><p>We plot the error in gradient estimation for the toy example defined by Eq.6 in the main paper in <ref type="figure" target="#fig_3">Fig. 6</ref>. Note that the integration time T is set as smaller than 1, while the main paper is larger than 20. We observe the same results, MALI and ACA generate smaller error than the adjoint and the naive method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 RESULTS OF DAMPED MALI</head><p>For all experiments in the main paper, we set η = 1 and did not use damping. For completeness, we experimented with damped MALI using different values of η. As shown in <ref type="table">Table.</ref> 7, MALI is robust to different η values.     <ref type="figure" target="#fig_3">Figure 6</ref>: Comparison of error in gradient estimation for the toy example by Eq.6 of the main paper, when t &lt; 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>With ALF method, given any tuple (zj, vj, tj) and discretized time points {ti} N t i=1 , we can reconstruct the entire trajectory accurately due to the reversibility of ALF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of error in gradient in Eq. 6. (a) error in dL dz 0 . (b) error in dL dα . (c) memory cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig 4. ACA and MALI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Top-1 accuracy on Ima-geNet validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Robustness to adversarial attack<ref type="bibr" target="#b11">Hanshu et al. (2019)</ref> demonstrated that Neural ODE is more robust to adversarial attack than ResNet on small-scale datasets such as Cifar10. We validate this result on the large-scale ImageNet dataset. The top-1 accuracy of NeuralODE and ResNet under  FGSM attack (Goodfellow et al., 2014)  are summarized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>0 is of order O(h) or smaller, then L z is of order O(h 3 ). Specifically, at the start time of integration, we have f ( z 0 , s 0 ) − v 0 = 0 , by induction, L z at end time is O(h 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>A. 4</head><label>4</label><figDesc>STABILITY ANALYSIS Lemma A.1.1. For a matrix of the form A B C D , if A, B, C, D are square matrices of the same shape, and CD = DC, then we have det A B C D = det(AD − BC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem A. 3 .</head><label>3</label><figDesc>For a single step in Damped ALF with stepsize h, the local truncation error of z is O(h 2 ), and the local truncation errof of v is O(h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 :</head><label>1</label><figDesc>Region of A-stability for eigenvalue on the imaginary plane for damped ALF. From left to right, the region of stability for η = 0.25, η = 0.7,η = 0.8 respectively. As η increases to 1, the area of stability region decreases.B EXPERIMENTAL DETAILS B.1 IMAGE RECOGNITION B.1.1 EXPERIMENT ON CIFAR10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Training curve on ImageNet.(b) Validation curve on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Real samples from MNIST dataset.(b) Generated samples from FFJORD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 :</head><label>3</label><figDesc>Results on MNIST dataset. B.3 CONTINUOUS GENERATIVE MODELS B.3.1 TRAINING DETAILS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Real samples from CIFAR10 dataset.(b) Generated samples from FFJORD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 :</head><label>4</label><figDesc>Results on Cifar10 dataset. (a) Real samples from ImageNet64 dataset. (b) Generated samples from FFJORD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 5 :</head><label>5</label><figDesc>Results on ImageNet64 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between different methods for gradient estimation in continuous case. MALI achieves reverse accuracy, constant memory w.r.t number of solver steps in integration, shallow computation graph and low computation cost.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 5:Results  on Cifar10. From left to right: (1) box plot of test accuracy (first 4 columns are Neural ODEs, last is ResNet); (2) test accuracy ±std v.s. training epoch for Neural ODE; (3) test accuracy ±std v.s. training time of 90 epochs for Neural ODE.Constant memory cost w.r.t number of solver steps in integration We delete the computation graph and only keep the end-time state to save memory. The memory cost is N z</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Top-1 test accuracy of Neural ODE and ResNet on ImageNet. Neural ODE is trained with MALI, and ResNet is trained as the original model; Neural ODE is tested using different solvers without retraining.</figDesc><table><row><cell></cell><cell cols="5">Fixed-stepsize solvers of various stepsizes</cell><cell></cell><cell cols="3">Adaptive-stepsize solver of various tolerances</cell></row><row><cell></cell><cell>Stepsize</cell><cell>1</cell><cell>0.5</cell><cell>0.25</cell><cell>0.15</cell><cell>0.1</cell><cell>Tolerance</cell><cell cols="2">1.00E+00 1.00E-01 1.00E-02</cell></row><row><cell></cell><cell>MALI</cell><cell cols="5">42.33 66.4 69.59 70.17 69.94</cell><cell>MALI</cell><cell>62.56</cell><cell>69.89</cell><cell>69.87</cell></row><row><cell>Neural</cell><cell>Euler</cell><cell cols="6">21.94 61.25 67.38 68.69 70.02 Heun-Euler</cell><cell>68.48</cell><cell>69.87</cell><cell>69.88</cell></row><row><cell>ODE</cell><cell>RK2</cell><cell>42.33</cell><cell>69</cell><cell cols="3">69.72 70.14 69.92</cell><cell>RK23</cell><cell>50.77</cell><cell>69.89</cell><cell>69.93</cell></row><row><cell></cell><cell>RK4</cell><cell cols="5">12.6 69.99 69.91 70.21 69.96</cell><cell>Dopri5</cell><cell>52.3</cell><cell>68.58</cell><cell>69.71</cell></row><row><cell>ResNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.09</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top-1 accuracy under FGSM attack. is the perturbation amplitude. For Neural ODE models, row names represent the solvers to derive the gradient for attack, and column names represent solvers for inference on the perturbed image.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">= 1/255</cell><cell></cell><cell></cell><cell cols="2">= 2/255</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">MALI Heun-Euler RK23 Dopri5 MALI Heun-Euler RK23 Dopri5</cell></row><row><cell></cell><cell>MALI</cell><cell>14.69</cell><cell>14.72</cell><cell>14.77</cell><cell>15.71</cell><cell>10.38</cell><cell>10.46</cell><cell>10.62</cell><cell>10.62</cell></row><row><cell>Neural</cell><cell cols="2">Heun-Euler 14.77</cell><cell>14.75</cell><cell>14.80</cell><cell>15.74</cell><cell>10.63</cell><cell>10.47</cell><cell>10.44</cell><cell>10.49</cell></row><row><cell>ODE</cell><cell>RK23</cell><cell>14.82</cell><cell>14.77</cell><cell>14.79</cell><cell>15.69</cell><cell>10.78</cell><cell>10.53</cell><cell>10.48</cell><cell>10.56</cell></row><row><cell></cell><cell>Dopri5</cell><cell>14.82</cell><cell>14.78</cell><cell>14.79</cell><cell>15.15</cell><cell>10.76</cell><cell>10.49</cell><cell>10.48</cell><cell>10.51</cell></row><row><cell></cell><cell>ResNet</cell><cell></cell><cell>13.02</cell><cell></cell><cell></cell><cell></cell><cell>9.57</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test MSE (×0.01) on Mujoco dataset (lower is better). Results marked with superscript numbers correspond to literature in the footnote.</figDesc><table><row><cell>Percentage of training data</cell><cell cols="2">RNN 1 RNN-GRU 1</cell><cell cols="4">Latent-ODE Adjoint 1 Naive 2 ACA 2 MALI</cell></row><row><cell>10%</cell><cell>2.45 1</cell><cell>1.97 2</cell><cell>0.47 1</cell><cell>0.36 2</cell><cell>0.31 2</cell><cell>0.35</cell></row><row><cell>20%</cell><cell>1.71 1</cell><cell>1.42 1</cell><cell>0.44 1</cell><cell>0.30 2</cell><cell>0.27 2</cell><cell>0.27</cell></row><row><cell>50%</cell><cell>0.79 1</cell><cell>0.75 1</cell><cell>0.40 1</cell><cell>0.29 2</cell><cell>0.26 2</cell><cell>0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Test ACC on Speech</figDesc><table><row><cell cols="2">Command Dataset</cell></row><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Adjoint 3</cell><cell>92.8 ± 0.4</cell></row><row><cell>SemiNorm 3</cell><cell>92.9 ± 0.4</cell></row><row><cell>Naive</cell><cell>93.2 ± 0.2</cell></row><row><cell>ACA</cell><cell>93.2 ± 0.2</cell></row><row><cell>MALI</cell><cell>93.7 ± 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Bits per dim (BPD) of generative models, lower is better. Results marked with superscript numbers correspond to literature in the footnote. RNODE 5 SemiNorm 3 MALI RealNVP 6 i-ResNet 7 Glow 8 Flow++ 9 Residual Flow 10 Algorithm of ALF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.3 Local truncation error of ALF . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.4 Stability analysis . . .</figDesc><table><row><cell cols="2">Dataset Vanilla 4 MNIST 0.99 4</cell><cell cols="2">Continuous Flow (FFJORD) 0.97 5 0.96 3</cell><cell>0.87</cell><cell>1.06 6</cell><cell>1.05 7</cell><cell cols="2">Discrete Flow 1.05 8</cell><cell>-</cell><cell>0.97 10</cell></row><row><cell>CIFAR10</cell><cell>3.40 4</cell><cell>3.38 5</cell><cell>3.35 3</cell><cell>3.27</cell><cell>3.49 6</cell><cell>3.45 7</cell><cell>3.35 8</cell><cell cols="2">3.28 9</cell><cell>3.28 10</cell></row><row><cell>ImageNet64</cell><cell>-</cell><cell>3.83 5</cell><cell>-</cell><cell>3.71</cell><cell>3.98 6</cell><cell>-</cell><cell>3.81 8</cell><cell></cell><cell>-</cell><cell>3.76 10</cell></row><row><cell cols="11">with extensive experiments, and achieved new state-of-the-art results in various tasks, including</cell></row><row><cell cols="8">image recognition, continuous generative modeling, and time-series modeling.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Experiment on Cifar10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.1.2 Experiments on ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.2 Time series modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.3 Continuous generative models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3.1 Training details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3.2 Addtional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.4 Error in gradient estimation for toy examples when t &lt; 1 . . . . . . . . . . . . . . 21 B.5 Results of damped MALI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 A THEORETICAL PROPERTIES OF ALF INTEGRATOR A.1 ALGORITHM OF ALF</figDesc><table><row><cell>19</cell></row><row><cell>B.1.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Queiruga et al. (2020)  argues that many numerical discretizations fail to be meaningful dynamical systems, while our experiments demonstrate that our model is continuous hence invariant to discretization schemes.</figDesc><table><row><cell>Figure 2: Results on ImageNet.</cell></row><row><cell>RK4), our Neural ODE achieves 70.21% top-1 accuracy, which is higher than both the same solver</cell></row><row><cell>during training (MALI, 69.59% accuracy) and the ResNet18 (70.09% accuracy).</cell></row><row><cell>Furthermore, many papers claim ResNet to be an approximation for an ODE (Lu et al., 2018).</cell></row><row><cell>However,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Results of damped MALI with different η values. We report the test accuracy of Neural CDE on Speech Command dataset, and the test MSE of latent-ODE on Mujoco data. Error in the estimation of gradient w.r.t parameter α.</figDesc><table><row><cell>η</cell><cell></cell><cell>1.0</cell><cell>0.95</cell><cell>0.9</cell><cell>0.85</cell></row><row><cell cols="2">Test Accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">on Speech Commands</cell><cell cols="4">93.7 ± 0.3 93.7 ± 0.1 93.5 ± 0.2 93.7 ± 0.3</cell></row><row><cell cols="2">(Higher is better)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test MSE of latent ODE</cell><cell>10% training data</cell><cell>0.35</cell><cell>0.36</cell><cell>0.33</cell><cell>0.33</cell></row><row><cell>on Mujoco (Lower is better)</cell><cell>20% training data</cell><cell>0.27</cell><cell>0.25</cell><cell>0.26</cell><cell>0.27</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">1. Rubanova et al. (2019); 2. Zhuang et al. (2020); 3. Kidger et al. (2020a); 4. Chen et al. (2018); 5. Finlay et al. (2020); 6. Dinh et al. (2016); 7. Behrmann et al. (2019); 8. Kingma &amp; Dhariwal (2018); 9. Ho et al. (2019); 10. Chen et al. (2019)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/juntang-zhuang/torch_ACA 2 https://github.com/rtqichen/torchdiffeq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/BorealisAI/advertorch 4 https://pytorch.org/docs/stable/torchvision/models.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Residual flows for invertible generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9916" to="9926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Interpolated adjoint method for neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Talgat</forename><surname>Daulbaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr</forename><surname>Katrutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Gusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05271</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hamiltonian systems: chaos and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo M Ozorio De</forename><surname>Almeida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3140" to="3150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How to train your neural ode: the world of jacobian and kinetic regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Finlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levon</forename><surname>Nurbekyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">M</forename><surname>Oberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<title level="m">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On robustness of neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan Hanshu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Jiawei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Flow++: Improving flowbased generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00275</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural jump stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junteng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9847" to="9858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hey, that&apos;s not an ODE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09457</idno>
	</analytic>
	<monogr>
		<title level="m">Faster ODE Adjoints with 12 Lines of Code</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Kam Leonard</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01328</idno>
		<title level="m">Scalable gradients for stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Construction of higher order symplectic integrators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haruo</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics letters A</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">5-7</biblScope>
			<biblScope unit="page" from="262" to="268" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaofeng Desmond</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12077</idno>
		<title level="m">Biswadip Dey, and Amit Chakraborty. Symplectic ode-net: Learning hamiltonian dynamics with control</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive checkpoint adjoint method for gradient estimation in neural ode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicha</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sekhar</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Papademetris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
