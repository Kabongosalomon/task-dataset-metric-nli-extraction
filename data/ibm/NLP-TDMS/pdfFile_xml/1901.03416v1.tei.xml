<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PREVENTING POSTERIOR COLLAPSE WITH δ-VAES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Razavi</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
							<email>pooleb@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Oriol</surname></persName>
							<email>vinyals@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">PREVENTING POSTERIOR COLLAPSE WITH δ-VAES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019 Aäron van den Oord Deepmind</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the phenomenon of "posterior collapse," current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data. In this paper, we propose an alternative that utilizes the most powerful generative models as decoders, whilst optimising the variational lower bound all while ensuring that the latent variables preserve and encode useful information. Our proposed δ-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 × 32.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep latent variable models trained with amortized variational inference have led to advances in representation learning on high-dimensional datasets <ref type="bibr" target="#b23">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b31">Rezende et al., 2014)</ref>. These latent variable models typically have simple decoders, where the mapping from the latent variable to the input space is uni-modal, for example using a conditional Gaussian decoder. This typically results in representations that are good at capturing the global structure in the input, but fail at capturing more complex local structure (e.g., texture <ref type="bibr" target="#b26">(Larsen et al., 2016)</ref>). In parallel, advances in autoregressive models have led to drastic improvements in density modeling and sample quality without explicit latent variables <ref type="bibr" target="#b37">(van den Oord et al., 2016b)</ref>. While these models are good at capturing local statistics, they often fail to produce globally coherent structures <ref type="bibr" target="#b28">(Ostrovski et al., 2018)</ref>.</p><p>Combining the power of tractable densities from autoregressive models with the representation learning capabilities of latent variable models could result in higher-quality generative models with useful latent representations. While much prior work has attempted to join these two models, a common problem remains. If the autoregressive decoder is expressive enough to model the data density, then the model can learn to ignore the latent variables, resulting in a trivial posterior that collapses to the prior. This phenomenon has been frequently observed in prior work and has been referred to as optimisation challenges of VAEs by <ref type="bibr" target="#b5">Bowman et al. (2015)</ref>, the information preference property by , and the posterior collapse problems by several others (e.g., <ref type="bibr" target="#b38">van den Oord et al. (2017)</ref>; <ref type="bibr" target="#b21">Kim et al. (2018)</ref>). Ideally, an approach that mitigates posterior collapse would not alter the evidence lower bound (ELBO) training objective, and would allow the practitioner to leverage the most recent advances in powerful autoregressive decoders to improve performance. To the best of our knowledge, no prior work has succeeded at this goal. Most common approaches either change the objective <ref type="bibr" target="#b18">(Higgins et al., 2017;</ref><ref type="bibr" target="#b1">Alemi et al., 2017;</ref><ref type="bibr" target="#b45">Zhao et al., 2017;</ref><ref type="bibr" target="#b27">Lucas &amp; Verbeek, 2017)</ref>, or weaken the decoder <ref type="bibr" target="#b5">(Bowman et al., 2015;</ref><ref type="bibr" target="#b16">Gulrajani et al., 2016)</ref>. Additionally, these approaches are often challenging to tune and highly sensitive to hyperparameters <ref type="bibr" target="#b1">(Alemi et al., 2017;</ref>.</p><p>In this paper, we propose δ-VAEs, a simple framework for selecting variational families that prevent posterior collapse without altering the ELBO training objective or weakening the decoder. By restricting the parameters or family of the posterior, we ensure that there is a minimum KL divergence, δ, between the posterior and the prior.</p><p>We demonstrate the effectiveness of this approach at learning latent-variable models with powerful decoders on images (CIFAR-10, and ImageNet 32 × 32), and text (LM1B). We achieve state of the art log-likelihood results with image models by additionally introducing a sequential latent-variable model with an anti-causal encoder structure. Our experiments demonstrate the utility of δ-VAEs at learning useful representations for downstream tasks without sacrificing performance on density modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MITIGATING POSTERIOR COLLAPSE WITH δ-VAES</head><p>Our proposed δ-VAE builds upon the framework of variational autoencoders (VAEs) <ref type="bibr" target="#b23">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b31">Rezende et al., 2014)</ref> for training latent-variable models with amortized variational inference. Our goal is to train a generative model p(x, z) to maximize the marginal likelihood log p(x) on a dataset. As the marginal likelihood requires computing an intractable integral over the unobserved latent variable z, VAEs introduce an encoder network q(z|x) and optimize a tractable lower bound (the ELBO): log p(x) ≥ E z∼q(z|x) [log p(x|z)] − D KL (q(z|x) p(z)). The first term is the reconstruction term, while the second term (KL) is the rate term, as it measures how many nats on average are required to send through the latent variables from the encoder (q(z|x)) to the decoder (p(z|x)) <ref type="bibr" target="#b19">(Hoffman et al., 2016;</ref><ref type="bibr" target="#b1">Alemi et al., 2017)</ref>.</p><p>The problem of posterior collapse is that the rate term, D KL (q(z|x) p(z)) reduces to 0. In this case, the approximate posterior q(z|x) equals the prior p(z), thus the latent variables do not convey any information about the input x. A necessary condition if we want representations to be meaningful is to have the rate term be positive.</p><p>In this paper we address the posterior collapse problem with structural constraints so that the KL divergence between the posterior and prior is lower bounded by design. This can be achieved by choosing families of distributions for the prior and approximate posterior, p θ (z) and q φ (z|x) such that min θ,φ D KL (q φ (z|x) p θ (z)) ≥ δ. We refer to δ as the committed rate of the model.</p><p>Note that a trivial choice for p and q to have a non-zero committed rate is to set them to Gaussian distributions with fixed (but different) variance term. We study a variant of this case in the experiments, and provide more details of this setup in Appendix D. In the following section we describe our choices for p θ and q φ , but others should also be explored in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">δ-VAE WITH SEQUENTIAL LATENT VARIABLES</head><p>Data such as speech, natural images and text exhibit strong spatio-temporal continuity. Our aim is to model variations in such data through latent variables, so that we have control over not just the global characteristics of the generated samples (e.g., existence of an object), but also can influence their finer, often shifting attributes such as texture and pose in the case of natural images, tone, volume and accent in the case of speech, or style and sentiment in the case of natural language. Sequences of latent variables can be an effective modeling tool for expressing the occurrence and evolution of such features throughout the sequence.</p><p>To construct a δ-VAE in the sequential setting, we combine a mean field posterior with a correlated prior in time. We model the posterior distribution of each timestep as q(z t |x) = N (z t ; µ t (x), σ t (x)). For the prior, we use an first-order linear autoregressive process (AR(1)), where z t = αz t−1 + t with t zero mean Gaussian noise with constant variance σ 2 . The conditional probability for the latent variable can be expressed as p(z t |z &lt;t ) = N (z t ; αz t−1 , σ ). This process is wide-sense stationary (that is, having constant sufficient statistics through its time evolution) if |α| &lt; 1. If so, then z t has zero mean and variance of σ 2 1−α 2 . It is thus convenient to choose σ = √ 1 − α 2 . The mismatch in the correlation structure of the prior and the posterior results in the following positive lower bound on the KL-divergence between the two distributions (see Appendix C for derivation):</p><formula xml:id="formula_0">D KL (q(z|x) p(z)) ≥ 1 2 d k=1 (n − 2) ln(1 + α 2 k ) − ln(1 − α 2 k ) (1)</formula><p>where n is the length of the sequence and d is the dimension of the latent variable at each timestep. The committed rate between the prior and the posterior is easily controlled by equating the right hand side of the inequality in equation 1 to a given rate δ and solving for α. In <ref type="figure">Fig. 1</ref>, we show the scaling of the minimum rate as a function of α and the behavior of δ-VAE in 2d. Figure 1: Effect of δ in a toy model. Fitting an uncorrelated Gaussian for the posterior, q φ (z), to a correlated Gaussian prior, p α (z), by minimizing D KL (q φ (z) p α (z)) over φ. Left: committed rate (δ) as a function of the prior squared correlation α and the dimensionality n. Right: contours of the optimal posterior and prior in 2d. As the correlation increases, the minimum rate grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">RELATION TO PROBABILISTIC SLOWNESS PRIOR</head><p>The AR(1) prior over the latent variables specifies the degree of temporal correlation in the latent space. As the correlation α approaches one, the prior trajectories get smoother . On the other hand in the limit of α approaching 0, the prior becomes the same as the independent standard Gaussian prior where there are no correlations between timesteps. This pairing of independent posterior with a correlated prior is related to the probabilistic counterpart to Slow Feature Analysis <ref type="bibr" target="#b42">(Wiskott &amp; Sejnowski, 2002)</ref> in <ref type="bibr" target="#b35">Turner &amp; Sahani (2007)</ref>. SFA has been shown to be an effective method for learning invariant spatio-temporal features <ref type="bibr" target="#b42">(Wiskott &amp; Sejnowski, 2002)</ref>. In our models, we infer latent variables with multiple dimensions per timestep, each with a different slowness filter imposed by a different value of α, corresponding to features with different speed of variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ANTI-CAUSAL ENCODER NETWORK</head><p>Having a high capacity autoregressive network as the decoder implies that it can accurately estimate p(x t |x &lt;t ). Given this premise, what kind of complementary information can latent variables provide? Encoding information about the past seems wasteful as the autoregressive decoder has full access to past observations already. On the other hand, if we impose conditional independence between observations and latent variables at other timesteps given the current one (i.e., p(x t |z) = p(x t |z t )), there will then be at best (by the data processing inequality (Cover &amp; <ref type="bibr" target="#b10">Thomas, 2006</ref>)) a break-even situation between the KL cost of encoding information in z t and the resulting improvement in the reconstruction loss. There is therefore no advantage for the model to utilize the latent variable even if it would transmit to the decoder the unobserved x t . The situation is different when z t can inform the decoder at multiple timesteps, encoding information about x t and x &gt;t . In this setting, the decoder pays the KL cost for the mutual information once, but is able to leverage the transmitted information multiple times to reduce its entropy about future predictions.</p><p>To encourage the generative model to leverage the latents for future timesteps, we introduce an anti-causal structure for the encoder where the parameters of the variational posterior for a timestep cannot depend on past observations <ref type="figure" target="#fig_2">(Fig. 2</ref>). Alternatively, one can consider a non-causal structure that allows latents be inferred from all observations. In this non-causal setup there is no temporal order in either the encoder or the decoder, thus the model resembles a standard non-temporal latent variable model. While the anti-causal structure is a subgraph of the non-causal structure, we find that the anti-causal structure often performs better, and we compare both approaches in different settings in Appendix F.1.   <ref type="bibr">(2018)</ref>. Unlike prior models, we use sequential latent variables to generate the image row by row. This differs from <ref type="bibr" target="#b15">Gregor et al. (2016)</ref>, where the latent variables are sequential but the entire image is generated at each timestep. Our sequential image generation model resembles latent variable models used for timeseries <ref type="bibr" target="#b9">(Chung et al., 2015;</ref><ref type="bibr" target="#b4">Babaeizadeh et al., 2017;</ref><ref type="bibr" target="#b13">Denton &amp; Fergus, 2018)</ref> but does not rely on KL annealing, and has an additional autoregressive dependence of the outputs over time (rows of the image). Another difference between our work and previous sequential latent variable models is our proposed anti-causal structure for the inference network (see Sect. 2.2). We motivate this structure from a coding efficiency and representation learning standpoint and demonstrate its effectiveness empirically in Sect. 4. For textual data, we use the Transformer architecture from <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref> as our main blueprint for the decoder. As shown in Sect. 4, our method is able to learn informative latent variables while preserving the performance of these models in terms of likelihoods.</p><p>To prevent posterior collapse, most prior work has focused on modifying the training objective.  <ref type="formula" target="#formula_4">2016)</ref> use free-bits to allow the model to hit a target minimum rate, but the objective is non-smooth which leads to optimization difficulties in our hands, and deviations from a lower bound on likelihood when the soft version is used with a coefficient less than 1. <ref type="bibr" target="#b27">Lucas &amp; Verbeek (2017)</ref> add an auxiliary objective that reconstructs a low-resolution version of the input to prevent posterior collapse. <ref type="bibr" target="#b1">Alemi et al. (2017)</ref> argue that the ELBO is a defective objective function for representation learning as it does not distinguish between models with different rates, and advocate for model selection based on downstream tasks. Their method for sweeping models was to use β-VAE with different coefficients, which can be challenging as the mapping from β to rate is highly nonlinear, and model-and data-dependent. While we adopt the same perspective as Alemi et al. <ref type="formula" target="#formula_4">(2017)</ref>, we present a new way of achieving a target rate while optimizing the vanilla ELBO objective.</p><p>Most similar to our approach is work on constraining the variational family to regularize the model. VQ-VAE (van den Oord et al., 2017) uses discrete latent variables obtained by vector quantization of the latent space that, given a uniform prior over the outcome, yields a fixed KL divergence equal to log K, where K is the size of the codebook. A number of recent papers have also used the von Mises-Fisher (vMF) distribution to obtain a fixed KL divergence and mitigate the posterior collapse problem. In particular, <ref type="bibr" target="#b17">Guu et al. (2017)</ref>; <ref type="bibr" target="#b43">Xu &amp; Durrett (2018)</ref>; Davidson et al. <ref type="formula" target="#formula_1">(2018)</ref> use vMF(µ, κ) with a fixed κ as their posterior, and the uniform distribution (i.e. vMF(·, 0)) as the prior. The mismatching prior-posterior thus give a constant KL divergence. As such, this approach can be considered as the continuous analogue of VQ-VAE. Unlike the VQ-VAE and vMF approaches which have a constant KL divergence for every data point, δ-VAE can allow higher KL for different data points. This allows the model to allocate more bits for more complicated inputs, which has been shown to be useful for detecting outliers in datasets <ref type="bibr" target="#b2">(Alemi et al., 2018)</ref>. As such, δ-VAE may be considered a generalisation of these fixed-KL approaches.</p><p>The Associative Compression Networks (ACN) <ref type="bibr" target="#b14">(Graves &amp; Menick, 2014</ref>  <ref type="formula" target="#formula_4">(2017)</ref>, we use a single channel network to output the components of discretised mixture of logistics distributions for each channel, and linear dependencies between the RGB colour channels. As in PixelSNAIL, we use attention layers interleaved with masked gated convolution layers. We use the same architecture of gated convolution introduced in van den <ref type="bibr" target="#b37">Oord et al. (2016b)</ref>. We also use the multi-head attention module of <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref>. To condition the decoder, similar to Transformer and unlike PixelCNN variants that use 1x1 convolution, we use attention over the output of the encoder. The decoder-encoder attention is causally masked to realize the anti-causal inference structure, and is unmasked for the non-causal structure.</p><p>Encoder. Our encoder also uses the same blueprint as the decoder. To introduce the anti-causal structure the input is reversed, shifted and cropped by one in order to obtain the desired future context. Using one latent variable for each pixel is too inefficient in terms of computation so we encode each row of the image with a multidimensional latent variable. </p><p>and train an auxiliary prior over the course of learning to match the aggregate posterior, but that does not influence the training of the encoder or decoder. We used a simple autoregressive model for the auxiliary prior p aux : a single-layer LSTM network with conditional-Gaussian outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">DENSITY ESTIMATION RESULTS</head><p>We begin by comparing our approach to prior work on CIFAR-10 and downsampled ImageNet 32x32 in <ref type="table">Table 1</ref>. As expected, we found that the capacity of the employed autoregressive decoder had a large impact on the overall performance. Nevertheless, our models with latent variables have a negligible gap compared to their powerful autoregressive latent-free counterparts, while also learning informative latent variables. In comparison, ) had a 0.03 bits per dimension gap between their latent variable model and PixelCNN++ architecture 1 . On ImageNet 32x32, our latent variable model achieves on par performance with purely autoregressive Image Transformer . On CIFAR-10 we achieve a new state of the art of 2.83 bits per dimension, again matching the performance of our autoregressive baseline. Note that the values for KL appear quite small as they are reported in bits per dimension (e.g. 0.02 bits/dim translates to 61 bits/image encoded in the latents). The results on CIFAR-10 also demonstrate the effect of the auxiliary prior on improving the efficiency of the latent code; it leads to more than 50% (on average 30 bits per image) reduction in the rate of the model to achieve the same performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">UTILIZATION OF LATENT VARIABLES</head><p>In this section, we aim to demonstrate that our models learn meaningful representations of the data in the latent variables. We first investigate the effect of z on the generated samples from the model. <ref type="figure">Fig. 3</ref> depicts samples from an ImageNet model (see Appendix for CIFAR-10), where we sample from the decoder network multiple times conditioned on a fixed sample from the auxiliary prior. We see similar global structure (e.g. same color background, scale and structure of objects) but very different details. This indicates that the model is using the latent variable to capture global structure, while the autoregressive decoder is filling in local statistics and patterns.</p><p>For a more quantitative assessment of how useful the learned representations are for downstream tasks, we performed linear classification from the representation to the class labels on CIFAR-10. We also study the effect of the chosen rate of the model on classification accuracy as illustrated in <ref type="figure">Fig. 4b, along</ref> with the performance of other methods. We find that generally a model with higher rate gives better classification accuracy, with our highest rate model, encoding 92 bits per image, giving the best accuracy of 68%. However, we find that improved log-likelihood does not necessarily lead to better linear classification results. We caution that an important requirement for this task is the linear separability of the learned feature space, which may not align with the desire to learn highly compressed representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDIES</head><p>We performed more extensive comparisons of δ-VAE with other approaches to prevent posterior collapse on the CIFAR-10 dataset. We employ the same medium sized encoder and decoder for evaluating all methods as detailed in Appendix E. <ref type="figure">Fig. 4a</ref> reports the rate-distortion results of our experiments for the CIFAR-10 test set. To better highlight the difference between models and to put into perspective the amount of information that latent variables capture about images, the rate and distortion results in <ref type="figure">Fig. 4a</ref> are reported in bits per images. We only report results for models that encode a non-negligible amount information in latent variables. Unlike the committed information rate approach of δ-VAE, most alternative solutions required considerable amount of effort to get the training converge or prevent the KL from collapsing altogether. For example, with linear annealing of <ref type="bibr">KL (Bowman et al., 2015)</ref>, despite trying a wide range of values for the end step of the annealing schedule, we were not able to train a model with a significant usage of latent variables; the KL collapsed as soon as β approached 1.0. A practical advantage of our approach is its simple formula to choose the target minimum rate of the model. Targeting a desired rate in β-VAE, on the other hand, proved to be difficult, as many of our attempts resulted in either collapsed KL, or very large KL values that led to inefficient inference. As reported in , we also observed that optimising models with the free-bits loss was challenging and sensitive to hyperparameter values.</p><p>To assess each methods tendency to overfit across the range of rates, we also report the rate-distortion results for CIFAR-10 training sets in Appendix F. While beta-VAEs do find points along the ratedistortion optimal frontier on the training set, we found that they overfit more than delta-VAEs, with delta-VAEs dominating the rate-distortion frontier on heldout data.</p><p>Next, we compare the performance of the anti-causal encoder structure with the non-causal structure on the CIFAR-10 dataset discussed in Sect. 2.2. The results for several configurations of our model are reported in the Appendix <ref type="table" target="#tab_8">Table 6</ref>. In models where the decoder is not powerful enough (such as our 6-layer PixelCNN that has no attention and consequently a receptive field smaller than the causal context for most pixels), the anti-causal structure does not perform as well as the noncausal structure. The performance gap is however closed as the decoder becomes more powerful and its receptive field grows by adding self-attention and more layers. We observed that the anticausal structure outperforms the non-causal encoder for very high capacity decoders, as well as for medium size models with a high rate. We also repeated these experiments with both anti-causal and non-causal structures but without imposing a committed information rate or using other mitigation strategies, and found that neither structure by itself is able to mitigate the posterior collapse issue; in both cases the KL divergence drops to negligible values (&lt; 10 −8 bits per dimension) only after a few thousand training steps and never recovers.  <ref type="figure">Figure 4</ref>: Comparison of CIFAR-10 test performance of δ-VAEs vs. models trained with free-bits and β-VAE across many rates. δ-VAE is significantly more stable, achieves competitive density estimation results across different rates, and its learned representations perform better in the downstream linear classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TEXT</head><p>For our experiments on natural language, we used the 1 Billion Words or LM1B <ref type="bibr" target="#b6">(Chelba et al., 2013)</ref> dataset in its processed form in the Tensor2Tensor  codebase 2 . Our employed architecture for text closely follows the Transformer network of <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref>. Our sequence of latent variables has the same number of elements as in the number of tokens in the input, each having two dimensions with α = 0.2 and 0.4. Our decoder uses causal self-attention as in <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref>. For the anti-causal structure in the encoder, we use the inverted causality masks as in the decoder to only allow looking at the current timestep and the future.</p><p>Quantitatively, our model achieves slightly worse log-likelihood compared to its autoregressive counterpart <ref type="table" target="#tab_3">(Table 2)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>In this work, we have demonstrated that δ-VAEs provide a simple, intuitive, and effective solution to posterior collapse in latent variable models, enabling them to be paired with powerful decoders. Unlike prior work, we do not require changes to the objective or weakening of the decoder, and we can learn useful representations as well as achieving state-of-the-art likelihoods. While our work presents two simple posterior-prior pairs, there are a number of other possibilities that could be explored in future work. Our work also points to at least two interesting challenges for latentvariable models: (1) can they exceed the performance of a strong autoregressive baseline, and (2) can they learn representations that improve downstream applications such as classification?</p><formula xml:id="formula_2">A DERIVATION OF THE KL DIVERGENCE FOR SEQUENTIAL LATENT VARIABLES D KL (q(z|x) p(z)) = z q(z|x) log q(z|x) p(z) dz = z n i=1 q(z i |x)( n i=1 log q(z i |x) − log p(z))dz = z n i=1 q(z i |x)( n i=1 log q(z i |x) − log p(z 1 ) − n i=2 log p(z i |z i−1 ))dz = z n i=1 q(z i |x)( n i=1 log q(z i |x))dz − z1 q(z 1 |x) log p(z 1 )dz 1 − i=n i=2 [ zi−1 q(z i−1 |x) zi q(z i |x) log p(z i |z i−1 )dz i dz i−1 ] = z1 q(z 1 |x) log q(z 1 |x)dz 1 − z1 q(z 1 ) log p(z 1 )dz 1 + n i=2 [ zi q(z i |x) log q(z i |x)dz i − zi−1 q(z i−1 |x) zi q(z i |x) log p(z i |z i−1 )dz i dz i−1 ] = D KL (q(z 1 |x) p(z 1 )) + n i=2 [ zi−1 q(z i−1 |x) zi q(z i |x) log q(z i |x) p(z i |z i−1 ) dz i dz i−1 ] = D KL (q(z 1 |x) p(z 1 )) + n i=2 E zi−1∼q(zi−1|x) [D KL (q(z i |x) p(z i |z i−1 ))]</formula><p>B DERIVATION OF THE KL-DIVERGENCE BETWEEN AR(1) AND DIAGONAL GAUSSIAN, AND ITS LOWER-BOUND</p><formula xml:id="formula_3">z i ∈ R d , z 0 ∈ {0} d p(z 1 ) = N (0, 1) p(z i |z i−1 ) = N (αz i−1 , 1 − α 2 ) i &gt; 1 q(z i |x) = N (µ θ i (x), σ θ i (x))</formula><p>Noting the analytic form for the KL-divergence for two uni-variate Gaussian distributions:</p><formula xml:id="formula_4">D KL (N (µ q , σ q ) N (µ p , σ p )) = 1 2 [ln(( σ p σ q ) 2 ) + σ 2 q + (µ p − µ q ) 2 σ 2 p − 1]<label>(2)</label></formula><p>we now derive the lower-bound for KL-divergence. Without loss of generality and to avoid clutter, we have assume the mean vector µ i has equal values in each dimension.?.</p><formula xml:id="formula_5">D KL (q(z|x) p(z)) = E zi−1∼q(zi−1|x) [ n i=1 D KL (N (µ qi , σ qi ) N (µ pi , σ pi ))] = 1 2 E zi−1 [− ln(σ 2 1 ) + σ 2 1 − 1 + µ 2 1 + n i=2 ln( 1 − α 2 σ 2 i ) + σ 2 i 1 − α 2 − 1 + (µ i − αz i−1 ) 2 1 − α 2 ] = 1 2 (f 1 (σ 2 1 ) + µ 2 1 + n i=2 f 1 ( σ 2 i 1 − α 2 ) + 1 1 − α 2 E zi−1 [(µ i − αz i−1 ) 2 ])</formula><p>Where f a (x) = ax−ln(x)−1. Using the fact that σ 2 i = E[(z i −µ i ) 2 ] = E(z 2 i )−µ 2 i , the expectation inside the summation can be simplified as follows.</p><formula xml:id="formula_6">E zi−1 [(µ i − αz i−1 ) 2 )]) = = E zi−1 [µ 2 i − 2µ i αz i−1 + α 2 z 2 i−1 ] = µ 2 i − 2αµ i E zi−1 [z i−1 ] + α 2 E zi−1 [z 2 i−1 ] = µ 2 i − 2αµ i µ i−1 + α 2 σ 2 i−1 + α 2 µ 2 i−1 = (µ i − αµ i−1 ) 2 + α 2 σ 2 i−1</formula><p>Plugging this back gives us the following analytic form for the KL-divergence for the sequential latent variable z.</p><formula xml:id="formula_7">D KL (q(z|x) p(z)) = 1 2 (f 1 (σ 2 1 ) + µ 2 1 + n i=2 [f 1 ( σ 2 i 1 − α 2 ) + (µ i − αµ i−1 ) 2 + α 2 σ 2 i−1 1 − α 2 ])<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DERIVATION OF THE LOWER-BOUND</head><p>Removing non-negative quadratic terms involving µ i in equation 3 and expanding back f inside the summation yields</p><formula xml:id="formula_8">D KL (q(z|x) p(z)) ≥ 1 2 (f 1 (σ 2 1 ) + α 2 σ 2 1 1 − α 2 + n−1 i=2 [ σ 2 i (1 + α 2 ) 1 − α 2 − ln( σ 2 i 1 − α 2 ) − 1] + f 1 ( σ 2 n 1 − α 2 )) = 1 2 (f 1 1−α 2 (σ 2 1 ) + n−1 i=2 f 1+α 2 ( σ 2 i 1 − α 2 ) + f 1 ( σ 2 n 1 − α 2 ))</formula><p>Consider f a (x) = ax − ln(x) − 1 and its first and second order derivatives, f a (x) = a − 1 x and f a (x) ≥ 0. Thus, f a is convex and obtains its minimum value of ln(a) at x = a −1 . Substituting σ 2 1 = 1 − α 2 , σ 2 n = 1 − α 2 and σ 2 i = 1−α 2 1+α 2 yields the following lower-bound for the KL:</p><formula xml:id="formula_9">D KL (q(z|x) p(z)) ≥ 1 2 [(n − 2) ln(1 + α 2 ) − ln(1 − α 2 )]</formula><p>When using multi-dimensional z i at each timestep, the committed rate is the sum of the KL for each individual dimension:</p><formula xml:id="formula_10">D KL (q(z|x) p(z)) ≥ 1 2 [ D k=1 (n − 2) ln(1 + α 2 k ) − ln(1 − α 2 k )] D INDEPENDENT δ-VAES</formula><p>The most common choice for variational families is to assume that the components of the posterior are independent, for example using a multivariate Gaussian with a diagonal covariance: q φ (z|x) = N (z; µ q (x), σ q (x)). When paired with a standard Gaussian prior, p(z) = N (z; 0, 1), we can guarantee a committed information rate δ by constraining the mean and variance of the variational family (see Appendix C)</p><formula xml:id="formula_11">µ 2 q ≥ 2δ + 1 + ln(σ 2 q ) − σ 2 q<label>(4)</label></formula><p>We can, thus, numerically solve ln(σ 2 q ) − σ 2 q + 2r + 1 ≥ 0 to obtain the feasible interval [σ l q , σ u q ] where the above equation has a solution for µ q , and the committed rate δ. Posterior parameters can thus be parameterised as:</p><formula xml:id="formula_12">σ q = σ l q + (σ u q − σ l q ) 1 1 + e −σ φ (x) (5) µ q = 2δ + 1 + ln(σ 2 q ) − σ 2 q + max(0, µ φ (x))<label>(6)</label></formula><p>Where φ parameterises the data-dependent part of µ q ad σ q , which allow the rate to go above the designated lower-bound δ.</p><p>We compare this model with the temporal version of δ-VAE discussed in the paper and report the results in <ref type="table">Table 3</ref>. While independent δ-VAE also prevents the posterior from collapsing to prior, its performance in density modeling lags behind temporal δ-VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test ELBO (KL) Accuracy Independent δ-VAE (δ = 0.08) 3.08 (0.08) 66% Temporal δ-VAE (δ = 0.08)</p><p>3.02 (0.09) 65% <ref type="table">Table 3</ref>: Comparison of independent Gaussian delta-VAE and temporal delta-VAE with AR(1) prior on CIFAR-10 both targeting the same rate. While both models achieve a KL around the target rate and perform similarly in the downstream linear classification task, the temporal model with AR <ref type="formula">(1)</ref> prior achieves significantly better marginal likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ARCHITECTURE DETAILS E.1 IMAGE MODELS</head><p>In this section we provide the details of our architecture used in our experiments. The overall architecture diagram is depicted in <ref type="figure" target="#fig_7">Fig. 5</ref>. To establish the anti-causal context for the inference network we first reverse the input image and pad each spatial dimension by one before feeding it to the encoder. The output of the encoder is cropped and reversed again. As show in <ref type="figure" target="#fig_7">Fig. 5, this</ref> gives each pixel the anti-causal context (i.e., pooling information from its own value and future values). We then apply average pooling to this representation to give us row-wise latent variables, on which the decoder network is conditioned.  The exact hyper-parameters of our network is detailed in <ref type="table">Table 4</ref>. We used dropout only in our decoder and applied it the activations of the hidden units as well as the attention matrix. As in <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>, we used rectified linear units and layer normalization <ref type="bibr" target="#b3">(Ba et al., 2016)</ref> after the multi-head attention layers. We found layer normalization to be essential for stabilizing training. For optimization we used the Adam optimizer <ref type="bibr" target="#b22">(Kingma &amp; Ba, 2014)</ref>. We used the learning rate schedule proposed in <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> with a few tweaks as in the formulae:  <ref type="table">Table 4</ref>: Hyperparameter values for the models used for experiments. The subscripts e, d, aux respectively denote the encoder, the decoder, and the LSTM auxiliary prior. l is the number of layers, h is the hidden size of each layer, r is the size of the residual filter, a is the number of attention layers interspersed with gated convolution layers of PixelCNN, n dmol is the number of components in the discrete mixture of logistics distribution, do d is the probability of dropout applied to the decoder, z is the dimensionality of the latent variable used for each row, and the alpha column gives the range of of the AR(1) prior hyper-parameter for each latent.</p><formula xml:id="formula_14">LR imageN et = 0.18 × h −0.</formula><p>We developed our code using Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. Our experiments on natural images were conducted on Google Cloud TPU accelerators. For ImageNet, we used 128 TPU cores with batch size of 1024. We used 8 TPU cores for CIFAR-10 with batch size of 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 TEXT MODELS</head><p>The architecture of our model for text experiment is closely based on the Transformer network of <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref>. We realize the encoder anti-causal structure by inverting the causal attention masks to upper triangular bias matrices. The exact hyper-parameters are summarized in <ref type="table">Table 5</ref>.  <ref type="table">Table 5</ref>: Hyperparameter values for our LM1B experiments. l is the number of layers, h is the hidden size of each layer, r is the size of the residual filters, do is the probability of dropout, z is the dimensionality of the latent variable, and the alpha column gives the range of of the AR(1) prior hyper-parameter for each latent dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ABLATION STUDIES</head><p>For our ablation studies on CIFAR-10, we trained our model with the configuration listed in <ref type="table">Table 4</ref>. After training the model, we inferred the mean of the posterior distribution corresponding to each training example in the CIFAR-10 test set, and subsequently trained a multi-class logistic regression classifier on top of it. For each model, the linear classifier was optimized for 100 epochs using the Adam optimizer with the starting learning rate of 0.003. The learning rate was decayed by a factor of 0.3 every 30 epochs.</p><p>We also report the rate-distortion curves for the CIFAR-10 training set in <ref type="figure">Fig. 6</ref>. In contrast to the graph of <ref type="figure">Fig. 4a</ref> for the test set, δ-VAE achieves relatively higher negative log-likelihood compared to other methods on the training seen, especially for larger rates. This suggests that δ-VAE is less prone to overfitting compared to β-VAE and free-bits.  <ref type="figure">Figure 6</ref>: Rate-Distortion for CIFAR-10 training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 ENCODER ABLATION</head><p>In <ref type="table" target="#tab_8">table Table 6</ref>, we report the details of evaluating our proposed anti-causal encoder architecture (discussed in Sect. 2.2) against the non-causal architecture in which there is no restriction in the connectivity of the encoder network. The reported experiments are conducted on the CIFAR-10 dataset. We trained 4 different configurations of our model to provide comparison in different capacity and information rate regimes, using the temporal δ-VAE approach to prevent posterior collapse. We found that the anti-causal structure is beneficial when the decoder has sufficiently large receptive field, and also when encoding relatively high amount of information in latent variables.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G VISUALIZATION OF THE LATENT SPACE</head><p>It is generally expected that images from the same class are mapped to the same region of the latent space. <ref type="figure" target="#fig_9">Fig. 7</ref> illustrates the t-SNE (van der Maaten &amp; Hinton, 2008) plot of latent variables inferred from 3000 examples from the test set of CIFAR-10 colour coded based on class labels. As can also be seen on the right hand plot classes that are closest are also mostly the one that have close semantic and often visual relationships (e.g., cat and dog, or deer and horse).    and KL of 0.10 (0.01) bits/dim. Notice that in the high rate model that has a larger value of α, samples from the AR(1) prior can turn out too smooth compared to natural images. This is because of the gap between the prior and the marginal posterior, that is closed by the auxiliary prior. ==== Interpolating dimension 0 ==== The company's stock price is also up for a year-on-year rally, when the The company's shares are trading at a record high for the year, when they were trading at The company's shares were trading at $3.00, down from their 52-week low The company's shares fell $1.14, or 5.7 percent, to $ UNK The company, which is based in New York, said it would cut 1,000 jobs in the The two-day meeting, held at the White House, was a rare opportunity for the United States The company, which is based in New York, said it was looking to cut costs, but added The company is the only company to have a significant presence in China. The company is the only company to have a significant presence in the North American market. The two men, who were arrested, have been released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H ADDITIONAL SAMPLES</head><p>==== Interpolating dimension 1 ==== In the meantime, however, the company is taking the necessary steps to keep the company in the UNK In the meantime, however, the company is expected to take some of the most aggressive steps in the In the meantime, the company is expected to report earnings of $2.15 to $4. The two men, who were both in their 20s , were arrested on suspicion of causing death by dangerous The company said it was "disappointed" by a decision by the U.S. Food and Drug The company said it would continue to provide financial support to its business and financial services clients.</p><p>The new plan would also provide a new national security dimension to U.S.-led efforts to "I've always been a great customer and thereś always thereś a good chance "It's a great personal decision... <ref type="figure" target="#fig_2">Figure 12</ref>: One at a time interpolation of latent dimensions of a sample from the AR(1) prior. The sentences of each segment are generated by sampling a 32 element sequence of 2D random vectors from the autoregressive prior, fixing one dimension interpolating the other dimension linearly between µ ± 3σ.</p><p>The company is now the world's cheapest for consumers . The company is now the world's biggest producer of oil and gas, with an estimated annual revenue of $2.2 billion.</p><p>The company is now the world's third-largest producer of the drug, after Pfizer and AstraZeneca, which is based in the UK.</p><p>The company is now the world's biggest producer of the popular games console, with sales of more than $1bn (312m) in the US and about $3bn in the UK. The company is now the world's largest company, with over $7.5 billion in annual revenue in 2008, and has been in the past for more than two decades.</p><p>The company is now the world's second-largest, after the cellphone company, which is dominated by the iPhone, which has the iPhone and the ability to store in -store, rather than having to buy, the product, said, because of the Apple-based device.</p><p>The company is now the world's biggest manufacturer of the door-to-door design for cars and the auto industry.</p><p>The company is now the world's third-largest maker of commercial aircraft, behind Boeing and Airbus.</p><p>The company is now the world's largest producer of silicon, and one of the biggest producers of silicon in the world. The company is now the world's largest maker of computer -based software, with a market value of $4.2 billion (2.6 billion) and an annual turnover of $400 million (343 million). <ref type="figure">Figure 14</ref>: Text completion samples. For each sentence we prime the decoder with a fragment of a random sample from the validation set (shown in bold), and condition the decoder on interpolations between two samples from the latent space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Generative structures for the inference of sequential latent variables. The anti-causal structure introduces an inductive bias to encode in each latent variable information about the future 3 RELATED WORK The main focus of our work is on representation learning and density modeling in latent variable models with powerful decoders. Earlier work has focused on this kind of architecture, but has addressed the problem of posterior collapse in different ways.In terms of our architecture, the decoders for our image models build on advances in autoregressive modeling from van den Oord et al.(2016b); Salimans et al. (2017); Chen et al. (2017); Parmar et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b5">Bowman et al. (2015)</ref>;<ref type="bibr" target="#b44">Yang et al. (2017)</ref>;<ref type="bibr" target="#b21">Kim et al. (2018)</ref> and<ref type="bibr" target="#b16">Gulrajani et al. (2016)</ref> use an annealing strategy, where they anneal the weight on the rate from 0 to 1 over the course of training. This approach does not directly optimize a lower bound on likelihood for most of training, and tuning the annealing schedule to prevent collapse can be challenging (seeSect. 4). Similarly,<ref type="bibr" target="#b18">Higgins et al. (2017)</ref> proposes using a fixed coefficient &gt; 1 on the rate term to learn disentangled representations.<ref type="bibr" target="#b45">Zhao et al. (2017)</ref> adds a term to the objective to pick the model with maximal rate.; Kingma et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Auxiliary Prior. Tomczak &amp; Welling (2017); Hoffman et al. (2016); Jimenez Rezende &amp; Viola (2018) show that VAE performance can suffer when there is a significant mismatch between the prior and the aggregate posterior, q(z) = E x∼D [q(z|x)]. When such a gap exists, the decoder is likely to have never seen samples from regions of the prior distribution where the aggregate posterior assigns small probability density. This phenomenon, also known as the "posterior holes" problem (Jimenez Rezende &amp; Viola, 2018), can be exacerbated in δ-VAEs, where the systematic mismatch between the prior and the posterior might induce a large gap between the prior and aggregate posterior. Increasing the complexity of the variational family can reduce this gap (Rezende &amp; Mohamed, 2015), but require changes in the objective to control the rate and prevent posterior collapse (Kingma et al., 2016). To address this limitation, we adopt the approaches of van den Oord et al. (2017); Roy et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Multiple decoding of the same z (b) Random samples from the our Auxiliary prior Figure 3: Random samples from our ImageNet 32 × 32 model. Each column inFig. 3ashows multiple samples from p(x|z) for a fixed z ∼ p aux (z). Each image inFig. 3bis decoded using a different sample from p aux (z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, but makes considerable use of latent variables, as demonstrated by the samples and interpolations in Appendix H. AR(1) ELBO (KL) Aux prior ELBO (KL) AR baseline NLL δ-VAE ≤ 3.61(0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Architecture for images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>t-SNE plot of the posterior mean for 3000 CIFAR-10 images. Note the adjacent groups and mixed regions of the plot: cats and dogs images are mostly interspersed as are automobiles and trucks.The highest concentration of horses are on top of the region right above where deer examples are.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Additional ImageNet samples. Top left: Each column is interpolation in the latent space. Top right: "Day dream" samples where we alternate between sampling x ∼ p(x|z) and z ∼ q(z|x). Bottom left: Each half-column contains in order an original image from the validation set, occlusion of that image, and two reconstructions from different posterior samples. Bottom right: Each halfcolumn contains in order an original image from the validation set, followed by 3 reconstructions from different posterior samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Additional CIFAR-10 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Additional unconditional random samples from Imagenet 32x32. Each half-column in each block contains 4 decodings of the same sample z ∼ p aux (z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) is a new method for learning latent variables with powerful decoders that exploits the associations between training examples in the dataset by amortizing the description length of the code among many similar training examples. ACN deviates from the i.i.d training regime of the classical methods in statistics and machine learning, and is considered a procedure for compressing whole datasets rather than individual training examples. GECO (Jimenez Rezende &amp; Viola, 2018) is a recently proposed method to stabilize the training of β-VAEs by finding an automatic annealing schedule for the KL that satisfies a tolerance constraint for maximum allowed distortion, and solving the resulting Lagrange multiplier for the KL penalty. The value of β, however, does not necessarily approach one, which means that the optimized objective may not be a lower bound for the marginal likelihood.We applied our method to generative modeling of images on the CIFAR-10 (Krizhevsky et al.) and downsampled ImageNet<ref type="bibr" target="#b12">(Deng et al., 2009</ref>) (32 × 32 as prepared in van den Oord et al. (2016a)) datasets. We describe the main components in the following. The details of our hyperparameters can be found in Appendix E.Decoder: Our decoder network is closest to PixelSNAIL but also incorporates elements from the original GatedPixelCNN (van den<ref type="bibr" target="#b37">Oord et al., 2016b)</ref>. In particular, as introduced by<ref type="bibr" target="#b33">Salimans et al. (2017)</ref> and used in Chen et al.</figDesc><table><row><cell>4 EXPERIMENTS</cell></row><row><cell>4.1 NATURAL IMAGES</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The result of our text experiments on LM1B in nats / token.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>5 d min(step num 0.35 , step num × 4000 1.5 ) LR cif ar10 = 0.36 × h −0.5 d min(step num 0.35 , step num × 8000 1.5 )</figDesc><table><row><cell></cell><cell cols="2">LR ablation</cell><cell></cell><cell></cell><cell></cell><cell>= 0.0001</cell></row><row><cell cols="7">We use multi-dimensional latent variables per each timestep, with different slowness factors lin-</cell></row><row><cell cols="7">early spaced between a chosen interval. For our ablation studies, we chose corresponding hyper-</cell></row><row><cell cols="7">parameters of each method we compare against to target rates between 25-100 bits per image.</cell></row><row><cell></cell><cell>l e /l d</cell><cell>h e /h d /h aux</cell><cell>r e /r d</cell><cell cols="3">a e /a d ah n dmol do d z</cell><cell>α</cell></row><row><cell>Best</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Imagenet</cell><cell>6/20</cell><cell cols="3">128/512/1024 1024/2048 2/5</cell><cell>8</cell><cell>32</cell><cell>0.3 16 [0.5, 0.95]</cell></row><row><cell cols="6">CIFAR-10 20/30 128/256/1024 1024/1024 11/16 8</cell><cell>32</cell><cell>0.5 8</cell><cell>[0.3, 0.99]</cell></row><row><cell>Ablations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[0.5,</cell></row><row><cell cols="2">CIFAR-10 8/8</cell><cell cols="3">128/128/1024 1024/1024 2/2</cell><cell>2</cell><cell>32</cell><cell>0.2 32</cell><cell>0.68-0.99]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation of anti-causal vs. non-causal structure. l: number of layers, h: hidden size, a: number of attention layers. Subscripts e and d respectively denote encoder and decoder sizes, when they were different. The low-rate (high-rate) models had latent dimension of 8 (64) with alpha linearly placed in [0.5, 0.95] ([0.5, 0.99]) which gives the total rate of 79.44 (666.6) bits per image.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Top left: Each column is interpolation in the latent space. Top right: "Day dream" samples where we alternate between sampling x ∼ p(x|z) and z ∼ q(z|x). Bottom left: Each half-column contains in order an original image from the test set, occlusion of that image, and two reconstructions from different posterior samples. Bottom right: Each half-column contains in order an original image from the test set, followed by 3 reconstructions from different posterior samples.</figDesc><table><row><cell>Figure 10: Random samples from the auxiliary (left) and AR(1) (right) priors of our high-rate (top)</cell></row><row><cell>and low-rate(bottom) CIFAR-10 models. The high-rate (low-rate) model has -ELBO of 2.90 (2.83)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">the exact results for the autoregressive baseline was not reported in </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tensorflow/tensor2tensor</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Danilo J. Rezende, Sander Dieleman, Jeffrey De Fauw, Jacob Menick, Nal Kalchberner, Andy Brock, Karen Simonyan and Jeff Donahue for their help, insightful discussions and valuable feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>==== Interpolating dimension 1 ==== The company said the company, which employs more than 400 people, did not respond to requests for comment, but did not respond to an email seeking comment, which The new rules, which are expected to take effect in the coming weeks, will allow the government to take steps to ensure that the current system does not take too "The only thing that could be so important is the fact that the government is not going to be able to get the money back, so the people are taking "I'm not sure if the government will be able to do that," he said. "We are not going to get any more information about the situation," said Mr. O'Brien. "It's a very important thing to have a president who has a strong and strong relationship with our country," said Mr. Obama, who has been the "It's a very important thing to have a president who has a great chance to make a great president," said Mr. Obama, a former senator from "It's a very important decision," said Mr. Obama. "It's a very difficult decision to make," said Mr. McCain. <ref type="figure">Figure 13</ref>: One at a time interpolation of latent dimensions of a sample from the auxiliary prior. The generation procedure is identical to <ref type="figure">Fig. 12</ref> with the exception that the initial vector is sampled from the auxiliary prior.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<idno>As- sociation. ISBN 978-1-931971-33-1</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3026877.3026899" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fixing a Broken ELBO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<idno>1938-7228</idno>
		<ptr target="http://arxiv.org/abs/1711" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00906</idno>
		<title level="m">Uncertainty in the variational information bottleneck</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>abs/1710.11252</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1002</idno>
		<ptr target="http://arxiv.org/abs/1511.06349" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<idno>abs/1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational Lossy Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.02731" />
	</analytic>
	<monogr>
		<title level="m">Iclr</title>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">PixelSNAIL: An Improved Autoregressive Generative Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1712.09763" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Recurrent Latent Variable Model for Sequential Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>10495258</idno>
		<ptr target="http://arxiv.org/abs/1506.02216" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28 (NIPS 2015)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Series in Telecommunications and Signal Processing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>ISBN 0471241954</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Tim R Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub M</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomczak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00891</idno>
		<title level="m">Hyperspherical variational auto-encoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1802.07687</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Associative Compression Networks for Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards conceptual compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3549" to="3557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">PixelVAE: A Latent Variable Model for Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><forename type="middle">Ali</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.psych.53.100901.135239</idno>
		<ptr target="http://arxiv.org/abs/1611.05013" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generating Sentences by Editing Prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno>1938-7228</idno>
		<ptr target="http://arxiv.org/abs/1709.08878" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="437" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Shakir Mohamed, and Alexander Lerchner. β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ELBO surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brain</surname></persName>
		</author>
		<ptr target="http://approximateinference.org/accepted/HoffmanJohnson2016.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
	<note type="report_type">Taming VAEs. ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-amortized variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/kim18e.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving Variational Inference with Inverse Autoregressive Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>10495258</idno>
		<ptr target="http://arxiv.org/abs/1606.04934" />
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/˜kriz/cifar.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Boesen Lindbo Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11479</idno>
		<title level="m">Auxiliary guided autoregressive variational autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Autoregressive quantile networks for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/ostrovski18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>1938-7228</idno>
		<ptr target="http://arxiv.org/abs/1802.05751" />
	</analytic>
	<monogr>
		<title level="j">Image Transformer</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045118.3045281" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>10495258. doi: 10.1051/ 0004-6361/201527329</idno>
		<ptr target="http://arxiv.org/abs/1401.4082" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11063</idno>
		<title level="m">Theory and experiments on vector quantized autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno>9781467324755</idno>
		<ptr target="http://arxiv.org/abs/1701.05517" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1705.07120</idno>
		<ptr target="http://arxiv.org/abs/1705.07120" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A maximum-likelihood interpretation for slow feature analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahani</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2007.19.4.1022</idno>
		<idno>0899-7667. doi</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.2007.19.4.1022" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1022" to="1038" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1601.06759</idno>
		<ptr target="http://arxiv.org/abs/1601.06759" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pixel Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1601.06759" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1711.00937</idno>
		<ptr target="http://arxiv.org/abs/1711.00937" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Visualizing high-dimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X16001837</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>abs/1803.07416</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurenz</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976602317318938</idno>
		<idno>0899-7667. doi: 10.1162/ 089976602317318938</idno>
		<ptr target="http://dx.doi.org/10.1162/089976602317318938" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="770" />
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spherical latent spaces for stable variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improved Variational Autoencoders for Text Modeling using Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.08139" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">I&apos;m not going to take any chances,&quot; he said. We are not going to get any more information on the situation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.02262" />
	</analytic>
	<monogr>
		<title level="m">InfoVAE: Balancing Learning and Inference in Variational Autoencoders. ICML Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>I&apos;ve got to take the best possible shot at the top,&quot; he said &quot;I&apos;m not going to take any chances,&quot; he said. said a spokesman for the U. N. mission in Afghanistan, which is to be formally We are not going to get the money back,&quot; he said. We are not going to get the money back,&quot; said one of the co -workers. &quot;We are not going to get the money back,&quot; said the man. &quot;We are not going to get a lot of money back,&quot; said the man</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
