<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
							<email>yongjian.cyj@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
							<email>mingyangli@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular 3D object detection is an essential component in autonomous driving while challenging to solve, especially for those occluded samples which are only partially visible. Most detectors consider each 3D object as an independent training target, inevitably resulting in a lack of useful information for occluded samples. To this end, we propose a novel method to improve the monocular 3D object detection by considering the relationship of paired samples. This allows us to encode spatial constraints for partially-occluded objects from their adjacent neighbors. Specifically, the proposed detector computes uncertaintyaware predictions for object locations and 3D distances for the adjacent object pairs, which are subsequently jointly optimized by nonlinear least squares. Finally, the onestage uncertainty-aware prediction structure and the postoptimization module are dedicatedly integrated for ensuring the run-time efficiency. Experiments demonstrate that our method yields the best performance on KITTI 3D detection benchmark, by outperforming state-of-the-art competitors by wide margins, especially for the hard samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection plays an essential role in various computer vision applications such as autonomous driving, unmanned aircrafts, robotic manipulation, and augmented reality. In this paper, we tackle this problem by using a monocular camera, primarily for autonomous driving use cases. Most existing methods on 3D object detection require accurate depth information, which can be obtained from either 3D LiDARs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">43]</ref> or multicamera systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref>. Due to the lack of directly computable depth information, 3D object detection using a monocular camera is generally considered a much more challenging problem than using LiDARs or multi-camera systems. Despite the difficulties in computer vision algorithm design, solutions relying on a monocular camera can potentially allow for low-cost, low-power, and deployment-flexible systems in real applications. Therefore, there is a growing trend on performing monocular 3D object detection in research community in recent years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Existing monocular 3D object detection methods have achieved considerable high accuracy for normal objects in autonomous driving. However, in real scenarios, there are a large number of objects that are under heavy occlusions, which pose significant algorithmic challenges. Unlike objects in the foreground which are fully visible, useful information for occluded objects is naturally limited. Straightforward methods on solving this problem are to design networks to exploit useful information as much as possible, which however only lead to limited improvement. Inspired by image captioning methods which seek to use scene graph and object relationships <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">40]</ref> , we propose to fully leverage the spatial relationship between close-by objects instead of individually focusing on information-constrained occluded objects. This is well aligned with human's intuition that human beings can naturally infer positions of the occluded cars from their neighbors on busy streets.</p><p>Mathematically, our key idea is to optimize the predicted 3D locations of objects guided by their uncertainty-aware spatial constraints. Specifically, we propose a novel detector to jointly compute object locations and spatial constraints between matched object pairs. The pairwise spatial constraint is modeled as a keypoint located in the geometric center between two neighboring objects, which effectively encodes all necessary geometric information. By doing that, it enables the network to capture the geometric context among objects explicitly. During the prediction, we impose aleatoric uncertainty into the baseline 3D object detector to model the noise of the output. The uncertainty is learned in an unsupervised manner, which is able to enhance the network robustness properties significantly. Finally, we formulate the predicted 3D locations as well as their pairwise spatial constraints into a nonlinear least squares problem to optimize the locations with a graph optimization framework. The computed uncertainties are used to weight each term in the cost function. Experiments on challenging KITTI 3D datasets demonstrate that our method outperforms the state-of-the-art competing approaches by wide margins. We also note that for hard samples with heavier occlusions, our method demonstrates massive improvement. In summary, the key contributions of this paper are as follows:</p><p>• We design a novel 3D object detector using a monocular camera by capturing spatial relationships between paired objects, allowing largely improved accuracy on occluded objects.</p><p>• We propose an uncertainty-aware prediction module in 3D object detection, which is jointly optimized together with object-to-object distances.</p><p>• Experiments demonstrate that our method yields the best performance on KITTI 3D detection benchmark, by outperforming state-of-the-art competitors by wide margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we first review methods on monocular 3D object detection for autonomous driving. Related algorithms on object relationship and uncertainty estimation are also briefly discussed. Monocular 3D Object Detection. Monocular image is naturally of limited 3D information compared with multibeam LiDAR or stereo vision. Prior knowledge or auxiliary information are widely used for 3D object detection. Mono3D <ref type="bibr" target="#b4">[5]</ref> focuses on the fact that 3D objects are on the ground plane. Prior 3D shapes of vehicles are also leveraged to reconstruct the bounding box for autonomous driving <ref type="bibr" target="#b25">[26]</ref>. Deep MANTA <ref type="bibr" target="#b3">[4]</ref> predicts 3D object information utilizing key points and 3D CAD models. SubCNN <ref type="bibr" target="#b37">[38]</ref> learns viewpoint-dependent subcategories from 3D CAD models to capture both shape, viewpoint and occlusion patterns. In <ref type="bibr" target="#b0">[1]</ref>, the network learns to estimate correspondences between detected 2D keypoints and 3D counterparts. 3D-RCNN <ref type="bibr" target="#b18">[19]</ref> introduces an inverse-graphics framework for all object instances from an image. A differentiable Render-and-Compare loss allows 3D results to be learned through 2D information. In <ref type="bibr" target="#b16">[17]</ref>, a sparse LiDAR scan is used in the training stage to generate training data, which removes the necessity of using inconvenient CAD dataset. An alternative family of methods is to predict a stand-alone depth or disparity information of the monocular image at the first stage <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. Although they only require the monocular image at testing time, ground-truth depth information is still necessary for the model training.</p><p>Compared with the aforementioned works in monocular 3D detection, some algorithms consist of only the RGB image as input rather than relying on external data, network structures or pre-trained models. Deep3DBox <ref type="bibr" target="#b24">[25]</ref> infers 3D information from a 2D bounding box considering the geometrical constraints of projection. OFTNet <ref type="bibr" target="#b30">[31]</ref> presents a orthographic feature transform to map image-based features into an orthographic 3D space. ROI-10D <ref type="bibr" target="#b23">[24]</ref> proposes a novel loss to properly measure the metric misalignment of boxes. MonoGRNet <ref type="bibr" target="#b28">[29]</ref> predicts 3D object localizations from a monocular RGB image considering geometric reasoning in 2D projection and the unobserved depth dimension. Current state-of-the-art results for monocular 3D object detection are from MonoDIS <ref type="bibr" target="#b33">[34]</ref> and M3D-RPN <ref type="bibr" target="#b2">[3]</ref>. Among them, MonoDIS <ref type="bibr" target="#b33">[34]</ref> leverages a novel disentangling transformation for 2D and 3D detection losses, which simplifies the training dynamics. M3D-RPN <ref type="bibr" target="#b2">[3]</ref> reformulates the monocular 3D detection problem as a standalone 3D region proposal network. However, all the object detectors mentioned above focus on predicting each individual object from the image. The spatial relationship among objects is not considered. Our work is originally inspired by CenterNet <ref type="bibr" target="#b42">[42]</ref>, in which each object is identified by points. Specifically, we model the geometric relationship between objects by using a single point similar to CenterNet, which is effectively the geometric center between them.</p><p>Visual Relationship Detection. Relationship plays an essential role for image understanding. To date, it is widely applied in image captioning. Dai et al. <ref type="bibr" target="#b9">[10]</ref> proposes a relational network to exploit the statistical dependencies between objects and their relationships. MSDB <ref type="bibr" target="#b20">[21]</ref> presents a multi-level scene description network to learn features of different semantic levels. Yao et al. <ref type="bibr" target="#b40">[40]</ref> proposes an attention-based encoder-decoder framework. through graph convolutional networks and long short-term memory (LSTM) for scene generation. However, these methods are mainly for tackling the effects of visual relationships in representing and describing an image. They usually extract object proposals directly or show full trust for the predicted bounding boxes. By contrast, our method focuses 3D object detection, which is to refine the detection results based on spatial relationships. This is un-explored in existing work.</p><p>Uncertainty Estimation in object detection. The computed object locations and pairwise 3D distances of our method are all predicted with uncertainties. This is inspired by the aleatoric uncertainty of deep neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. Instead of fully trusting the results of deep neural networks, we can extract how uncertain the predictions. This is crucial for various perception and decision making tasks, especially for autonomous driving, where human lives may be endangered due to inappropriate choices. This concept has been applied in 3D Lidar object detection <ref type="bibr" target="#b11">[12]</ref> and pedestrian localization <ref type="bibr" target="#b1">[2]</ref>, where they mainly consider uncertainties as additional information for reference. In <ref type="bibr" target="#b36">[37]</ref>, uncertainty is used to approximate object hulls with bounded collision probability for subsequent trajectory planning tasks. Gaussian-YOLO <ref type="bibr">[</ref>  improves the detection results by predicting the localization uncertainty. These approaches only use uncertainty to improve the training quality or to provide an additional reference. By contrast, we use uncertainty to weight the cost function for post-optimization, integrating the detection estimates and predicted uncertainties in global context optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We adopt a one-stage architecture, which shares a similar structure with state-of-the-art anchor-free 2D object detectors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">42]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, it is composed of a backbone network and several task-specific dense prediction branches. The backbone takes a monocular image I with a size of (W s × Hs) as input, and outputs the feature map with a size of (W ×H ×64), where s is our backbone's down-sampling factor. There are eleven output branches with a size of W × H × m, where m means the channel of each output branch, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Eleven output branches are divided into three parts: three for 2D object detection, six for 3D object detection, and two for pairwise constraint prediction. We introduce each module in details as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">2D Detection</head><p>Our 2D detection module is derived from the CenterNet <ref type="bibr" target="#b42">[42]</ref> with three output branches. The heatmap with a size of (W × H × c) is used for keypoint localization and classification. Keypoint types include c = 3 in KITTI3D object detection. Details about extracting the object location c g = (u g , v g ) from the output heatmap can be referred in <ref type="bibr" target="#b42">[42]</ref>. The other two branches, with two channels for each, output the size of the bounding box (w b , h b ) and the offset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Detection</head><p>The object center in world space is represented as c w = (x, y, z). Its projection in the feature map is c o = (u, v) as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Similar to <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34]</ref>, we predict its offset (∆ u , ∆ v ) to the keypoint location c g and the depth z in two separate branches. With the camera intrinsic matrix K, the derivation from predictions to the 3D center c w is as  follows:</p><formula xml:id="formula_0">K =   f x 0 a x 0 f y a y 0 0 1   . (1) c w = ( u g + ∆ u − a x f x z, v g + ∆ v − a y f y z, z)<label>(2)</label></formula><p>Given the difficulty to regress depth directly, depth prediction branch outputs inverse depthẑ similar to <ref type="bibr" target="#b10">[11]</ref>, transforming the absolute depth by inverse sigmoid transformation z = 1/σ(ẑ) − 1. The dimension branch regresses the size (w, h, l) of the object in meters directly. The branches for depth, offset and dimensions in both 2D and 3D detection are trained with the L1 loss following <ref type="bibr" target="#b42">[42]</ref>. As presented in <ref type="figure" target="#fig_1">Figure 2</ref>, we estimate the object's local orientation α following <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b42">[42]</ref>. Compared to global orientation β in the camera coordinate system, the local orientation accounts for the relative rotation of the object to the camera viewing angle γ = arctan(x/z). Therefore, using the local orientation is more meaningful when dealing with image features. Similar to <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b42">42]</ref>, we represent the orientation using eight scalars, where the orientation branch is trained by M ultiBin loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pairwise Spatial Constraint</head><p>In addition to the regular 2D and 3D detection pipelines, we propose a novel regression target, which is to estimate the pairwise geometric constraint among adjacent objects via a keypoint on the feature map. Pair matching strategy for training and inference is shown in <ref type="figure">Figure 4a</ref>. For arbitrary sample pair, we define a range circle by setting the distance of their 2D bounding box centers as the diameter. This pair is neglected if it contains other object centers. <ref type="figure">Figure  4b</ref> shows an example image with all effective sample pairs. Given a selected pair of objects, their 3D centers in world space are c w i = (x i , y i , z i ) and c w j = (x j , y j , z j ) and their 2D bounding box centers on the feature map are</p><formula xml:id="formula_1">c b i = (u b i , v b i ) and c b j = (u b j , v b j )</formula><p>. The pairwise constraint keypoint locates on the feature map as p b ij = (c b i + c b j )/2. The regression target for the related keypoint is the 3D distance of these two objects. We first locate the middle point</p><formula xml:id="formula_2">p w ij = (c w i + c w j )/2 = (p w x , p w y , p w z ) ij in 3D space. Then, the 3D absolute distance k v ij = (k v x , k v y , k v z )</formula><p>ij along the view point direction, as shown in <ref type="figure" target="#fig_3">Figure 3b</ref>, are taken as the regression target which is the distance branch of the pair constraint output in <ref type="figure" target="#fig_0">Figure 1</ref>. Notice that p b is not the projected point of p w on the feature map, like c w and c b in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>For training, k v ij can be easily collected through the groundtruth 3D object centers from the training data as: where − − → | · | means extract absolute value of each entry in the vector. k w ij = c w i − c w j is the 3D distance in camera coordinate, γ ij = arctan(p w x /p w z ) is the view direction of their middle point p w ij , and R(γ ij ) is its rotation matrix along the Y axis as</p><formula xml:id="formula_3">k v ij = −−−−−−−→ R(γ ij )k w ij ,<label>(3)</label></formula><formula xml:id="formula_4">R(γ ij ) =   cos(γ ij ) 0 − sin(γ ij ) 0 1 0 sin(γ ij ) 0 cos(γ ij )   .<label>(4)</label></formula><p>The 3D distance k w in camera coordinate is not considered because it is invariant from different view angles, as shown in <ref type="figure">Figure 5a</ref>. As in estimation of the orientation γ, 3D absolute distance k v in the local coordinate of p w is more meaningful considering the appearance change through viewing angles.</p><p>In inference, we first estimate objects' 2D locations and extract pairwise constraint keypoint located in the middle of predicted 2D bounding box centers. The predictedk v is extracted in the dense feature map of the distance branch based on the keypoint location. We do not consider offsets for this constraint keypoint both in training and reference, and round the middle point p b ij of paired objects' 2D centers to the nearest grid point on the feature map directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Uncertainty</head><p>Following the heteroscedastic aleatoric uncertainty setup in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, we represent a regression task with L1 loss as</p><formula xml:id="formula_5">[ỹ,σ] = f θ (x),<label>(5)</label></formula><formula xml:id="formula_6">L(θ) = √ 2 σ y −ỹ + logσ.<label>(6)</label></formula><p>Here, x is the input data, y andỹ are the groundtruth regression target and the predicted result.σ is another output of the model and can represent the observation noise of the data x. θ is the weight of the regression model. As mentioned in <ref type="bibr" target="#b14">[15]</ref>, aleatoric uncertaintyσ(x) makes the loss more robust to noisy input in a regression task. In this paper, we add three uncertainty branches as shown as σ blocks in <ref type="figure" target="#fig_0">Figure 1</ref> for the depth prediction σ z , 3D center offset σ uv and pairwise distance σ k respectively. They are mainly used to weight the error terms as presented in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Spatial Constraint Optimization</head><p>As the main contribution of this paper, we propose a post-optimization process from a graph perspective. Suppose that in one image, the network outputs N effective objects, and there are M pair constraints among them based on the strategy in Section 3.4. Those paired objects are regarded as vertices {ξ i } N G i=1 with size of N G and the M paired constraints are regarded as edges of the graph. Each vertex may connect multiple neighbors. Predicted objects not connected by other vertices are not updated anymore in the post-optimization. The proposed spatial constraint optimization is formulated as a nonlinear least square problem as arg min</p><formula xml:id="formula_7">(ui,vi,zi) N G i=1 e T We,<label>(7)</label></formula><p>where e is the error vector and W is the weight matrix for different errors. W is a diagonal matrix with dimension 3N G + 3M . For each vertex ξ i , there are three variables (u i , v i , z i ), which are the projected center (u i , v i ) of the 3D bounding box on the feature map and the depth z i as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We introduce each minimization term in the following.</p><p>Pairwise Constraint Error For each pairwise constraint connecting ξ i and ξ j , there are three error terms (e x ij , e y ij , e z ij ) measuring the inconsistency between network estimated 3D distancek v ij and the distance k v ij obtained by 3D locations c w i and c w j of the two associated objects. c w i and c w j can be represented by variables (u i , v i , z i ), (u j , v j , z j ) and the known intrinsic matrix through Equation 2. Thus, error terms (e x ij , e y ij , e z ij ) are the absolute difference betweenk v ij and k v ij along three axis as following.</p><formula xml:id="formula_8">k v ij = − −−−−−−−−−−−− → R(γ ij )(c w i − c w j ) (8) (e x ij , e y ij , e z ij ) T = − −−−−−− → k v ij − k v ij (9)</formula><p>Object Location Error For each vertex ξ i , there are three error terms (e u i , e v i , e z i ) to regularize the optimization variables with the predicted values from the network. We use this term to constraint the deviation between network estimated object location and the optimized location as follows.</p><formula xml:id="formula_9">e u i = ũ g i +∆ u i − u i (10) e v i = ṽ g i +∆ v i − v i (11) e z i = |z i − z i |<label>(12)</label></formula><p>Weight Matrix The weight matrix W is constructed by the uncertainty outputσ of the network. The weight of the error is higher when the uncertainty is lower, which means we have more confidence in the predicted output. Thus, we use 1/σ as the element of W. For pairwise inconsistency, the weights for the three error terms (e x ij , e y ij , e z ij ) are the same as the predicted 1/σ ij as shown in <ref type="figure" target="#fig_5">Figure 6a</ref>. For object location error, the weight is 1/σ z i for depth error e z i and 1/σ uv i for both e u i and e v i as shown in <ref type="figure" target="#fig_5">Figure 6b</ref>. We visualize an example pair for the spatial constraint optimization in <ref type="figure" target="#fig_5">Figure 6</ref>. Uncertainties give us confidence ranges to tune variables so that both the pairwise constraint error and the object location error can be jointly minimized. We use g2o <ref type="bibr" target="#b17">[18]</ref> to conduct this graph optimization structure during implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>We conduct experiments on the challenge KITTI 3D object detection dataset <ref type="bibr" target="#b13">[14]</ref>. It is split to 3712 training samples and 3769 validation samples as <ref type="bibr" target="#b5">[6]</ref>. Samples are labeled from Easy, Moderate, to Hard according to its condition of truncation, occlusions and bounding box height. <ref type="table">Table 1</ref> shows counts of groundtruth pairwise constraints through the proposed pair matching strategy from all the training samples. <ref type="table" target="#tab_1">Car  14357 11110  13620  Pedestrian 2207 1187  1614  Cyclist  734  219  371   Table 1</ref>: Count of objects, pairs and paired objects of each category in the KITTI training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Count object pair paired object</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>We adopt the modified DLA-34 <ref type="bibr" target="#b41">[41]</ref> as our backbone. The resolution of the input image is set to 380 × 1280. The feature map of the backbone output is with a size of 96 × 320 × 64. Each of the eleven output branches connects the backbone feature with two additional convolution layers with sizes of 3 × 3 × 256 and 1 × 1 × m, where m is the feature channel of the related output branch. Convolution layers connecting output branches maintain the same feature width and height. Thus, the feature size of each output branch is 96 × 320 × m.</p><p>We train the whole network in an end-to-end manner for 70 epochs with a batchsize of 32 on four GPUs simultaneously. The initial learning rate is 1.25e-4, dropped by multiplying 0.1 both at 45 and 60 epochs. It is trained with Adam optimizer with weight decay as 1e-5. We conduct different data augmentation strategies during training, as random cropping and scaling for 2D detection, and random horizontal flipping for both 3D detection and pairwise constraints prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>Following <ref type="bibr" target="#b33">[34]</ref>, we use 40-point interpolated average precision metric AP 40 that averaging precision results on 40 recall positions instead of 0. The previous metric AP 11 of KITTI3D average precision on 11 recall positions, which may trigger bias to some extent. The precision is evaluated at both the bird-eye view 2D box AP bv and the 3D bounding box AP 3D in world space. We report average precision with intersection over union (IoU) using both 0.5 and 0.7 as thresholds.</p><p>For the evaluation and ablation study, we show experimental results from three different setups. Baseline is derived from CenterNet <ref type="bibr" target="#b42">[42]</ref> with an additional output branch to represent the offset of the 3D projected center to the located keypoint. +σ z + σ uv adds two uncertainty prediction branches on Baseline which consists of all the three 2D detection branches and six 3D detection branches as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. MonoPair is the final proposed method integrating the eleven prediction branches and the pairwise spatial constraint optimization.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative and Qualitative Results</head><p>We first show the performance of our proposed MonoPair on KITTI3D validation set for car, compared with other state-of-the-art (SOTA) monocular 3D detectors including MonoDIS <ref type="bibr" target="#b33">[34]</ref>, MonoGRNet <ref type="bibr" target="#b28">[29]</ref> and M3D-RPN <ref type="bibr" target="#b2">[3]</ref> in <ref type="table" target="#tab_1">Table 2</ref>. Since MonoGRNet and M3D-RPN have not published their results through AP 40 , we evaluate the related values through their published detection results or models. <ref type="table" target="#tab_1">Table 2</ref>, although our baseline is only comparable or a little worse than SOTA detector M3D-RPN, MonoPair outperforms all the other detectors mostly by a large margin, especially for hard samples with augmentations from the uncertainty and the pairwise spatial constraint. <ref type="table" target="#tab_2">Table 3</ref> shows results of our MonoPair on the KITTI3D test set for car. From the KITTI 3D object de-tection benchmark 1 , we achieve the highest score for Moderate samples and rank at the first place among those 3D monocular object detectors without using additional information. AP 2D and AOS are metrics for 2D object detection and orientation estimations following the benchmark. Apart from the Easy result of AP b v and AP 3D , our method outperforms M3D-RPN for a large margin, especially for Hard samples. It proves the effects of the proposed pairwise constraint optimization targeting for highly occluded samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>We show the pedestrian and cyclist detection results on the KITTI test set in <ref type="table" target="#tab_3">Table 4</ref>. Because MonoDIS <ref type="bibr" target="#b33">[34]</ref> and MonoGRNet <ref type="bibr" target="#b28">[29]</ref> do not report their performance on pedestrian and cyclist categories, we only compare our method with M3D-RPN <ref type="bibr" target="#b2">[3]</ref>. It presents a significant improvement from our MonoPair. Even though the relatively few training samples of pedestrian and cyclist, the proposed pairwise spatial constraint goes much deeper by utilizing object relationships compared with target-independent detectors.</p><p>Besides, compared with those methods relying on timeconsuming region proposal network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref>, our one-stage anchor-free detector is more than two times faster on an Nvidia GTX 1080 Ti. It can perform inference in real-time as 57 ms per image, as shown in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>We conduct two ablation studies for different uncertain terms and the count of pairwise constraints both on KITTI3D validation set through AP 40 . We only show results from Moderate samples here.    <ref type="table">Table 6</ref>: Ablation study for improvements among different pair counts through 0.7 IoU.</p><p>For uncertainty study, except the Baseline and +σ z + σ uv setups mentioned above, we add σ z and σ uv methods by only predict the depth or projected offset uncertainty based on the Baseline. From <ref type="table" target="#tab_5">Table 5</ref>, uncertainties prediction from both depth and offset show considerable development above the baseline, where the improvement from depth is larger. The results match the fact that depth prediction is a much more challenging task and it can benefit more from the uncertainty term. It proves the necessity of imposing uncertainties for 3D object prediction, which is rarely considered by previous detectors.</p><p>In terms of the pairwise constraint, we divide the validation set to different parts based on the count of groundtruth pairwise constraints. The Uncert. in <ref type="table">Table 6</ref> represents +σ z + σ uv for simplicity. By checking both the AP bv and AP 3D in <ref type="table">Table 6</ref>, the third group with 5 to 8 pairs shows higher average precision improvement. A possible explanation is that fewer pairs may not provide enough constraints, and more pairs may increase the complexity of the optimization.</p><p>Also, to prove the utilization of using uncertainties to weigh related errors, we tried various strategies for weight matrix designing, for example, giving more confidence for objects closed to the camera or setting the weight matrix as identity. However, none of those strategies showed improvements in the detection performance. On the other hand, the baseline is easily dropped to be worse because of coarse post-optimization. It shows that setting the weight matrix of the proposed spatial constraint optimization is nontrivial. And uncertainties, besides its original function to enhance network training, is naturally a meaningful choice for weights of different error terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We proposed a novel post-optimization method for 3D object detection with uncertainty-aware training from a monocular camera. By imposing aleatoric uncertainties into the network and considering spatial relationships for objects, our method has achieved the state-of-the-art performance on KITTI 3D object detection benchmark using a monocular camera without additional information. By exploring the spatial constraints of object pairs, we observed the enormous potential of geometric relationships in object detection, which was rarely considered before. For future work, finding spatial relationships across object categories and innovating pair matching strategies would be exciting next steps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our architecture. A monocular RGB image is taken as the input to the backbone network and trained with supervision. Eleven different prediction branches, with feature map as W × H × m, are divided into three parts: 2D detection, 3D detection and pair constraint prediction. The width and height of the output feature (W, H) are as the same as the backbone output. Dash lines represent forward flows of the neural network. The heatmap and offset of 2D detection are also utilized to locate the 3D object center and the pairwise constraint keypoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) 3D world space (b) feature map coordinate (c) top view image plane Visualization of notations for (a) 3D bounding box in world space, (b) locations of an object in the output feature map, and (c) orientation of the object from the top view. 3D dimensions are in meters, and all values in (b) are in the feature coordinate. The vertical distance y is invisible and skipped in (c). vector (δ u , δ v ) from the located keypoint c g to the bounding box center c b = (u b , v b ) respectively. As shown in Figure 2, those values are in units of the feature map coordinate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Pairwise spatial constraint definition. c w i and c w j are centers of two 3D bounding boxes where p w ij is their middle point. 3D distance in camera coordinate k w ij and local coordinate k v ij are shown in (a) and (b) respectively. The distance along y axis is skipped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Pair matching strategy for training and inference.(a) camera coordinate (b) local coordinate The same pairwise spatial constraint in camera and local coordinates from various viewing angles. The spatial constraint in camera coordinate is invariant among different view angles. Considering the different projected form of the car, we use the 3D absolute distance in local coordinate as the regression target of spatial constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of optimization for an example pair including. In (a), The predicted pairwise constraintk v ij and its uncertaintyσ k ij is located by predicted 2D bounding box centers(ũ b i ,ṽ b i ) and (ũ b j ,ṽ b j )on the feature map. The 3D prediction results (green points) are shown in (b). All uncertainties are represented as arrows to show a confidence range. We show variables in (c) for this optimization function as red points. The final optimized results are presented in (d). Our method is mainly supposed to work for occluded samples. The relatively long distance among the paired cars is for simplicity in visualization. Properties along v direction is skipped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results in KITTI validation set. Cyan, yellow and grey mean predictions of car, pedestrian and cyclist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>27.91 24.65 20.00 17.50 15.57 3.46 3.31 3.21 0.60 0.66 0.77 45 12.58 10.66 11.06 7.60 6.37 -MonoGRNet[29]* 52.13 35.99 28.72 47.59 32.28 25.50 19.72 12.81 10.15 11.90 7.56 5.76 60 M3D-RPN[3]* 53.35 39.60 31.76 48.53 35.94 28.59 20.85 15.62 11.88 14.53 11.07 8.65 161 Baseline 53.06 38.51 32.56 47.63 33.19 28.68 19.83 12.84 10.42 13.06 7.81 6.49 47 +σ z + σ uv 59.22 46.90 41.38 53.44 41.46 36.28 21.71 17.39 15.10 14.75 11.42 9.76 50 MonoPair 61.06 47.63 41.92 55.38 42.39 37.99 24.12 18.17 15.76 16.28 12.30 10.42 57 AP 40 scores on KITTI3D validation set for car. * indicates that the value is extracted by ourselves from the public pretrained model or results provided by related paper author. E, M and H represent Easy, Moderate and Hard samples. ] 89.04 85.08 69.26 88.38 82.81 67.08 21.02 13.67 10.23 14.76 9.71 7.42 MonoPair 96.61 93.55 83.55 91.65 86.11 76.45 19.28 14.83 12.89 13.04 9.99 8.65</figDesc><table><row><cell>H</cell><cell>E</cell><cell>M</cell><cell>H (ms)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>AP 40 scores on KITTI3D test set for car referred from the KITTI benchmark website.</figDesc><table><row><cell>Cat</cell><cell>Method</cell><cell>E</cell><cell>AP bv M</cell><cell>H</cell><cell>E</cell><cell>AP 3D M</cell><cell>H</cell></row><row><cell>Ped</cell><cell cols="7">M3D-RPN[3] 5.65 4.05 3.29 4.92 3.48 2.94 MonoPair 10.99 7.04 6.29 10.02 6.68 5.53</cell></row><row><cell>Cyc</cell><cell cols="7">M3D-RPN[3] 1.25 0.81 0.78 0.94 0.65 0.47 MonoPair 4.76 2.87 2.42 3.79 2.12 1.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>AP 40 scores on pedestrian and cyclist samples from the KITTI3D test set at 0.7 IoU threshold. It can be referred from the KITTI benchmark website.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>AP bv AP 3D AP bv AP 3D Baseline 38.51 33.19 12.84 7.81 +σ uv 42.79 38.75 14.38 8.96 +σ z 45.09 40.46 15.79 10.15 +σ z + σ uv 46.90 41.46 17.39 11.42</figDesc><table><row><cell>Uncertainty</cell><cell>IoU≥0.5</cell><cell>IoU≥0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study for different uncertainty terms.</figDesc><table><row><cell cols="2">pairs images</cell><cell cols="4">AP bv Uncert. MonoPair Uncert. MonoPair AP 3D</cell></row><row><cell cols="3">0-1 1404 10.40</cell><cell>10.44</cell><cell>5.41</cell><cell>6.02</cell></row><row><cell cols="3">2-4 1176 13.25</cell><cell>14.00</cell><cell>8.46</cell><cell>8.97</cell></row><row><cell>5-8</cell><cell>887</cell><cell>20.45</cell><cell>22.32</cell><cell>14.63</cell><cell>15.54</cell></row><row><cell>9-</cell><cell>302</cell><cell>25.49</cell><cell>25.87</cell><cell>17.98</cell><cell>1894</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cvlibs.net/datasets/kitti/eval object.php?obj benchmark=3d</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Barabanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vyacheslav</forename><surname>Murashkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05618</idno>
		<idno>arXiv: 1905.05618. 2</idno>
		<title level="m">Monocular 3d Object Detection via Geometric Reasoning on Keypoints</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monoloco: Monocular 3d pedestrian localization and uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV), October</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Céline</forename><surname>Teulière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular 3d Object Detection for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d Object Proposals Using Stereo Imagery for Accurate Object Class Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayoung</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuk-Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04620</idno>
		<idno>arXiv: 1904.04620. 2</idno>
		<title level="m">Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3076" to="3086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3266" to="3273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Geometry and uncertainty in deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Guy</forename><surname>Kendall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">g 2 o: A general framework for graph optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Kümmerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Grisetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hauke</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3607" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Continuous Fusion for Multi-sensor 3d Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="663" to="678" />
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Wanli Ouyang, and Xin Fan</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d Bounding Box Estimation Using Deep Learning and Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reconstructing vehicles from a single image: Shape priors for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falak</forename><surname>Gv Sai Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Madhava</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust object proposals re-ranking for object detection in autonomous driving using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Wook</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="110" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8851" to="8858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Triangulation learning network: From monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08188</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Roarnet: A robust 3d object detection based on region approximation refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwook</forename><surname>Paul Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2510" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<idno>Octo- ber 2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07179</idno>
		<idno>arXiv: 1812.07179. 2</idno>
		<title level="m">Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3d Object Detection for Autonomous Driving</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Capturing Object Detection Uncertainty in Multi-Layer Grid Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Wirges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Reith-Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11284</idno>
		<idno>arXiv: 1901.11284. 2</idno>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04693</idno>
		<idno>arXiv: 1604.04693. 2</idno>
		<title level="m">Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection</title>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-level Fusion Based 3d Object Detection from Monocular Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krhenbhl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<idno>arXiv: 1904.07850</idno>
		<title level="m">Objects as Points</title>
		<imprint>
			<date type="published" when="2019-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
