<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding the Difficulty of Training Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding the Difficulty of Training Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>‡ Microsoft Research § Microsoft Dynamics 365 AI</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cuttingedge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand what complicates Transformer training from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially -for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize stabilize the early stage's training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance 1 .</p><p>2 As in <ref type="figure">Figure 2</ref>, Post-LN places layer norm outside of residual blocks, and Pre-LN moves them to the inside.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b3">(Vaswani et al., 2017)</ref> have led to a series of breakthroughs in various deep learning tasks <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr" target="#b4">Velickovic et al., 2018)</ref>. They do not contain recurrent connections and can parallelize all computations in the same layer, thus improving effectiveness, efficiency, and scalability. Training Transformers, however, requires extra efforts. For example, although stochastic gradient descent (SGD) is the standard algorithm for conventional RNNs and CNNs, it converges to bad/suspicious local optima for Trans-  formers <ref type="bibr" target="#b10">(Zhang et al., 2019b)</ref>. Moreover, comparing to other neural architectures, removing the warmup stage in Transformer training results in more severe consequences such as model divergence <ref type="bibr">(Popel and Bojar, 2018;</ref><ref type="bibr">Liu et al., 2020a)</ref>.</p><p>Here, we conduct comprehensive analyses in empirical and theoretical manners to answer the question: what complicates Transformer training. Our analysis starts from the observation: the original Transformer (referred to as Post-LN) is less robust than its   <ref type="bibr">(Baevski and Auli, 2019;</ref><ref type="bibr" target="#b8">Xiong et al., 2019;</ref><ref type="bibr">Nguyen and Salazar, 2019)</ref>. We recognize that gradient vanishing issue is not the direct reason causing such difference, since fixing this issue alone cannot stabilize Post-LN training. It implies that, besides unbalanced gradients, there exist other factors influencing model training greatly.</p><p>With further analysis, we recognize that for each Transformer residual block, the dependency on its  Layer Norm</p><p>x <ref type="bibr">(pd)</ref> x <ref type="bibr">(od)</ref> x (oe)  residual branch 3 plays an essential role in training stability. First, we find that a Post-LN layer has a heavier dependency on its residual branch than a Pre-LN layer. As in <ref type="figure" target="#fig_7">Figure 7</ref>, at initialization, a Pre-LN layer has roughly the same dependency on its residual branch and any previous layer, whereas a Post-LN layer has a stronger dependency on its residual branch (more discussions are elaborated in Section 4.1). We find that strong dependencies of Post-LN amplify fluctuations brought by parameter changes and destabilize the training (as in Theorem 2 and <ref type="figure" target="#fig_4">Figure 4)</ref>. Besides, the loose reliance on residual branches in Pre-LN generally limits the algorithm's potential and often produces inferior models.</p><p>In light of our analysis, we propose Admin, an adaptive initialization method which retains the merits of Pre-LN stability without hurting the performance. It restricts the layer dependency on its residual branches in the early stage and unleashes the model potential in the late stage. We conduct experiments on IWSLT'14 De-En, WMT'14 En-De, and WMT'14 En-Fr; Admin is more stable, converges faster, and achieves better performance. For example, without introducing any additional hyper-parameters, Admin successfully stabilizes 72-layer Transformer training on WMT'14 En-Fr and achieves a 43.80 BLEU score.</p><p>3 For a residual block x + f (x), its shortcut output refers to x, its residual branch output refers to f (x), and the dependency on its residual branch refers to <ref type="bibr">Var[f (x)]</ref> Var[x+f (x)] .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Transformer Architectures and Notations. The Transformer architecture contains two types of sublayers, i.e., Attention sub-layers and Feedforward (FFN) sub-layers. They are composed of mainly three basic modules <ref type="bibr" target="#b3">(Vaswani et al., 2017)</ref>, i.e., Layer Norm (f LN ), Multi-head Attention (f ATT ), and Feedforward Network (f FFN ). As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, the Pre-LN Transformer and the Post-LN Transformer organize these modules differently. For example, a Pre-LN encoder organizes the Self-Attention sublayer as x</p><formula xml:id="formula_0">(pe) 2i−1 = x (pe) 2i−2 + f S-ATT (f LN (x (pe)</formula><p>2i−2 )) and a Post-LN encoder as x</p><formula xml:id="formula_1">(oe) 2i−1 = f LN (x (oe) 2i−2 + f S-ATT (x (oe) 2i−2 )), where x (·)</formula><p>2i−2 is the input of the ith Transformer layer and x (·) 2i−1 is the output of the i-th Self-Attention sub-layer. Here, we refer</p><formula xml:id="formula_2">f S-ATT (f LN (x (pe) 2i−2 )) and f S-ATT (x (oe)</formula><p>2i−2 ) as the residual branches and their outputs as the residual outputs, in contrast to layer/sub-layer outputs, which integrates residual outputs and shortcut outputs.</p><p>Notation elaborations are shown in <ref type="figure" target="#fig_2">Figure 2</ref>. In particular, we use superscripts to indicate network architectures (i.e., the Pre-LN Encoder), use subscripts to indicate layer indexes (top layers have larger indexes), all inputs and outputs are formulated as Sequence-Len × Hidden-Dim. </p><formula xml:id="formula_3">(x) = φ(xW (1) )W (2) , where φ(·)</formula><p>is the nonlinear function 4 , and W (·) are parameters.</p><p>Multi-head Attention. Multi-head Attentions allows the network to have multiple focuses in a single layer and plays a crucial role in many tasks <ref type="bibr" target="#b12">(Chen et al., 2018)</ref>.</p><p>It is defined as (with H heads):</p><formula xml:id="formula_4">f ATT (q, k, v) = H h=1 f s (qW (Q) h W (K) h k T )vW (V 1 ) h W (V 2 ) h , where f s is the row-wise softmax function and W (·) h are parameters. W (Q) h and W (V 1 ) h are D × D H matrices, W (K) h and W (V 2 ) h are D H × D matrices,</formula><p>where D is the hidden state dimension. Parameters without subscript refer the concatenation of all Hhead parameters, e.g.,</p><formula xml:id="formula_5">W (Q) = [W (Q) 1 , · · · , W (Q) H ].</formula><p>In Transformer, this module is used in two different settings:</p><formula xml:id="formula_6">Encoder-Attention (f E-ATT (x) = f ATT (x, x (·e) , x (·e)</formula><p>) and x (·e) is the encoder output), and Self-Attention (f S-ATT (x) = f ATT (x, x, x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unbalanced Gradients</head><p>In this study, we strive to answer the question: what complicates Transformer training. Our analysis starts from the observation: Pre-LN training is more robust than Post-LN, while Post-LN is more likely to reach a better performance than Pre-LN. In a parameter grid search (as in <ref type="figure" target="#fig_0">Figure 10</ref>), Pre-LN converges in all 15 settings, and Post-LN diverges in 7 out of 15 settings; when Post-LN converges, it outperforms Pre-LN in 7 out of 8 settings. We seek to reveal the underlying factor that destabilizes Post-LN training and restricts the performance of Pre-LN.</p><p>In this section, we focus on the unbalanced gradients (e.g., gradient vanishing). We find that, although Post-LN suffers from gradient vanishing and Pre-LN does not, gradient vanishing is not the direct reason causing the instability of Post-LN. Specifically, we first theoretically and empirically establish that only Post-LN decoders suffer from gradient vanishing and Post-LN encoders do not. We then observe that fixing the gradient vanishing issue alone cannot stabilize training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gradients at Initialization</head><p>As gradient vanishing can hamper convergence from the beginning, it has been regarded as the major issue causing unstable training. Also, recent studies show that this issue exists in the Post-LN Transformer, even after using residual connections <ref type="bibr" target="#b8">(Xiong et al., 2019)</ref>. Below, we establish that only Post-LN decoders suffer from the gradient vanishing, and neither Post-LN encoders, Pre-LN encoders, nor Pre-LN decoders.</p><p>We use ∆x to denote gradients, i.e., ∆x = ∂L ∂x where L is the training objective. Following previous studies (Glorot and Bengio, 2010), we analyze the gradient distribution at the very beginning of training and find only Encoder-Attention sub-layers in Post-LN suffers from gradient vanishing. First, we conduct analysis from a theoretical Num of Sub-Layers (FFN or Self-Attention) in the Encoder Random Perturbations, i.e., Gradient Updates, i.e., The update magnitude is consistent, even with unbalanced gradients.  To make sure that the assumptions of Theorem 2 match the real-world situation, we further conduct empirical verification. At initialization, we calculate ||∆x (·) i || 2 for 18-layer Transformers 5</p><formula xml:id="formula_7">|F(x 0 , W ) F(x 0 , W ⇤ )| 2 2 W ⇤ = W + Post-LN is less stable than Pre-LN Post-LN: |F F ⇤ | 2 2 = O(N ) Pre-LN |F F ⇤ | 2 2 = O(log N ) Admin : R 2 = 0.99 R 2 = 0.99 W ⇤ = W + Adam(r W L(F))</formula><formula xml:id="formula_8">5 Note if E[∆x (p·) i−1 ] = 0, Var[∆x (p·) i−1 ] ≈ |∆x (p·) i−1 | 2 2 .</formula><p>and visualize <ref type="figure">Figure 3</ref>. It verifies that only Post-LN decoders suffer from the gradient vanishing. Besides, we can observe that the dropping of gradient norms mostly happens in the backpropagation from encoder-attention outputs (encoder-attention bars) to its inputs (self-attention bars, since the output of self-attention is the input of encoder-attention). This pattern is further explained in Appendix A.3.</p><formula xml:id="formula_9">||∆x (·) i || 2 max j ||∆x (·) j || 2 in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Impact of the Gradient Vanishing</head><p>Now, we explore whether gradient vanishing is the direct cause of training instability.</p><p>First, we design a controlled experiment to show the relationship between gradient vanishing and training stability. We construct a hybrid Transformer by combining a Post-LN encoder and a Pre-LN decoder. As in Section 3.1, only Post-LN decoders suffer from gradient vanishing, but not Post-LN encoders. Therefore, this hybrid Transformer does not suffer from gradient vanishing. As shown in <ref type="table" target="#tab_5">Table 1</ref>, fixing gradient vanishing alone (i.e., changing Post-LN decoders to Pre-LN decoders) fails to stabilize model training. This observation provides evidence supporting that the gradient vanishing issue is not the direct cause of unstable Post-LN training.</p><p>Moreover, we observe that gradients of all attention modules are unbalanced, while adaptive optimizers mostly address this issue. As in Figure 5, adaptive optimizers successfully assign different learning rates to different parameters and lead to consistent update magnitudes even with unbalanced gradients. It explains why the standard SGD fails in training Transformers (i.e., lacking the ability to handle unbalanced gradients) and necessitates using adaptive optimizers. More discussions are included in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Instability from Amplification Effect</head><p>We find that unbalanced gradients are not the root cause of the instability of Post-LN, which implies the existence of other factors influencing model training. Now, we go beyond gradient vanishing and introduce the amplification effect. Specifically, we first examine the difference between Pre-LN and Post-LN, including their early-stage and latestage training. Then, we show that Post-LN's training instability is attributed to layer dependency's amplification effect, which intensifies gradient updates and destabilizes training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Impact of Layer Norms Positions</head><p>As described in Section 2, both Pre-LN and Post-LN employ layer norm to regularize inputs and outputs. Different residual outputs are aggregated and normalized in residual networks before serving as inputs of other layers (i.e., residual outputs will be scaled to ensure the integrated input to have a consistent variance). To some extend, layer norm treats the variance of residual outputs as weights to average them. For example, for Post-LN Self-Attention,</p><formula xml:id="formula_10">we have x (o·) 2i−1 = x (o·) 2i−2 +a (o·) 2i−1 Var[x (o·) 2i−2 ]+Var[a (o·) 2i−1 ] at initial- ization. Larger Var[a (o·) 2i−2 ] not only increases the proportion of a (o·) 2i−2 in x (o·)</formula><p>2i−2 but decreases the proportion of other residual outputs. Intuitively, this is similar to the weight mechanism of the weighted average.</p><p>The position of layer norms is the major difference between Pre-LN and Post-LN and makes them aggregate residual outputs differently (i.e., using different weights). As in <ref type="figure" target="#fig_5">Figure 6</ref>, all residual outputs in Pre-LN are only normalized once before feeding into other layers (thus only treating residual output variances as weights); in Post-LN, most Post-LN layer outputs always depend more on its residual</p><p>Pre-LN layer outputs learn to depend more on its residual residual outputs are normalized more than once, and different residual outputs are normalized for different times. For example, if all layers are initialized in the same way, output variances of different Pre-LN residual branches would be similar, and the aggregation would be similar to the simple average. Similarly, for Post-LN, nearby residual outputs are normalized by fewer times than others, thus having relatively larger weights. We proceed to calculate and analyze these weights to understand the impact of layer norm positions. First, we use a i to refer a i √ Var a i (i.e., normalized outputs of i-th residual branch) and x i to refer x i √ Var x i (i.e., normalized outputs of i-th layer or normalized inputs of (i+1)-th residual branch). Then, we describe their relationships as x i = j≤i β i,j a j , where β i,j integrates scaling operations of all layer norms (including Var[a i ]). For</p><formula xml:id="formula_11">example, Pre-LN sets β i,j = √ Var[a j ] √ Var[ k≤i a k ]</formula><p>. Intuitively, β i,j describes the proportion of j-th residual branch outputs in i-th layer outputs, thus reflects the dependency among layers.</p><p>We visualize β i,j in <ref type="figure" target="#fig_7">Figure 7</ref>. For a Post-LN layer, its outputs rely more on its residual branch from the initialization to the end. At initialization, Pre-LN layer outputs have roughly the same reliance on all previous residual branches. As the training advances, each layer starts to rely more on its own residual outputs. However, comparing to Post-LN, Pre-LN layer outputs in the final model still has less reliance on their residual branches.</p><p>Intuitively, it is harder for Pre-LN layers to depend too much on their own residual branches. In Pre-LN, layer outputs (i.e., x (p·) i ) are not normalized, and their variances are likely to be larger for higher layers 6 . Since</p><formula xml:id="formula_12">β i,i = √ Var[a i ] Var[x (p·) i−1 +a i ] , β i,i</formula><p>is likely to be smaller for higher layers, which restricts i-th layer outputs from depending too much on its residual branch and inhibits the network from reaching its full potential. In other words, Pre-LN restricts the network from being too deep (i.e., if it is hard to distinguish x (p·) i and x (p·) i+1 , appending one layer would be similar to doubling the width of the last layer), while Post-LN gives the network the choice of being wider or deeper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Amplification Effect at Initialization</head><p>Although depending more on residual branches allows the model to have a larger potential, it amplifies the fluctuation brought by parameter changes. For a network x = F(x 0 , W ) where x 0 is the model input and W is the parameter, the output change caused by parameter perturbations is</p><formula xml:id="formula_13">Var[F(x 0 , W )−F(x 0 , W * )], where W * = W +δ.</formula><p>Its relationship with N is described in Theorem 2, and the derivation is elaborated in Appendix B.</p><p>THEOREM 2. -Consider a N -layer Transformer x = F( x 0 , W ) at initialization, where x 0 is the input and W is the parameter. If the layer dependency stays the same after a parameter change (i.e., β i,j has the same value after changing W to W * , where W is randomly initialized and δ = W * − W is independent to W ), the output change (i.e.,</p><formula xml:id="formula_14">Var[F(x 0 , W ) − F(x 0 , W * )]) can be estimated as N i=1 β 2 i,i C where C is a constant. If Var[a i ]</formula><p>is the same for all layers, Pre-LN sets β 2 i,i as 1/i, and Post-LN sets β 2 i,i as a constant. Thus, we have Corollary 1 and 2 as below.</p><formula xml:id="formula_15">COROLLARY 1. -For a N -layer Pre-LN F, we have Var[F(x 0 , W ) − F(x 0 , W * )] = O(log N ). COROLLARY 2. -For a N -layer Post-LN F, we have Var[F(x 0 , W ) − F(x 0 , W * )] = O(N ).</formula><p>They show that, since Post-LN relies more on residual branches than Pre-LN (i.e., has a larger β 2 i,i ), the perturbation is amplified to a larger magnitude. To empirically verify these relationships, we calculate |F(x 0 , W ) − F(x 0 , W * )| ] increases as i becomes larger ure 4. In Corollary 2, N is linearly associated with |F − F * | 2 2 for Post-LN; and in Corollary 1, log N is linearly associated with |F − F * | 2 2 for Pre-LN. These relationships match the observation in our experiments (as in <ref type="figure" target="#fig_4">Figure 4</ref>). For further verification, we measure their correlation magnitudes by R 2 and find R 2 = 0.99 in both cases.</p><p>Moreover, we replace the random noise δ with optimization updates (i.e., setting W * = W + Adam(∆W ), where opt(·) is update calculated by the Adam optimizer) and visualize output shifts. This replacement makes the correlation between |F − F * | 2 2 and N (for Post-LN) or log N (for Pre-LN) to be weaker (i.e., R 2 = 0.75). Still, as in <ref type="figure" target="#fig_4">Figure 4</ref>, the output shift |F − F * | 2 2 for Post-LN is larger than Pre-LN by multiple magnitudes.</p><p>Intuitively, large output shifts would destabilize the training <ref type="bibr">(Li et al., 2018)</ref>. Also, as elaborated in Appendix B, the constant C in Theorem 2 is related to network derivatives and would be smaller as training advances, which explains why warmup is also helpful for the standard SGD. Therefore, we conjecture it is the large output shift of Post-LN results in unstable training. We proceed to stabilize Post-LN by controlling the dependency on residual branches in the early stage of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Admin -Adaptive Model Initialization</head><p>In light of our analysis, we add additional parameters (i.e., ω) to control residual dependencies of Post-LN and stabilize training by adaptively initializing ω to ensure an O(log N ) output change.</p><p>Due to different training configurations and model specificities (e.g., different models may use different activation functions and dropout ratios), it is hard to derive a universal initialization method. Instead, we decompose model initialization into two phrases: Profiling and Initialization. Specifically, Admin adds new parameters ω and constructs its i-th sub-layer as</p><formula xml:id="formula_16">x i = f LN (b i ), where b i = x i−1 · ω i + f i (x i−1 )</formula><p>, ω i is a D-dimension vector and · is element-wise product. Then the Profiling phrase and Initialization phrase are:</p><p>Profiling. After initializing the network with a standard method (initializing ω i as 1), conduct forward propagation without parameter updating and record the output variance of residual branches (i.e., calculate Var[f i (x i−1 )]). Since all elements in the same parameter/output matrix are independent to each other and are subject to the same distribution, it is sufficient to use a small number of instances in  this phrase. In our experiments, the first batch (no more than 8192 tokens) is used.</p><formula xml:id="formula_17">Initialization. Set ω i = j&lt;i Var[f j (x j−1 )</formula><p>] and initialize all other parameters with the same method used in the Profiling phrase.</p><p>In the early stage, Admin sets β 2 i,i to approximately 1 i and ensures an O(log N ) output change, thus stabilizing training. Model training would become more stable in the late stage (the constant C in Theorem 2 is related to parameter gradients), and each layer has the flexibility to adjust ω and depends more on its residual branch to calculate the layer outputs. After training finishes, Admin can be reparameterized as the conventional Post-LN structure (i.e., removing ω). More implementation details are elaborated in Appendix C.</p><p>To verify our intuition, we calculate the layer dependency of 18-Layer models and visualize the result in <ref type="figure" target="#fig_10">Figure 8</ref>. <ref type="figure" target="#fig_7">Figures 7 and 8</ref> show that Admin avoids over-large dependencies at initialization and unleashes the potential to make the layer outputs depend more on their residual outputs in the final model. Moreover, we visualize the output change of Admin in <ref type="figure" target="#fig_4">Figure 4</ref>. Benefiting from the adaptive initialization, the output change of Admin gets roughly the same increase speed as Pre-LN, even constructed in the Post-LN manner. Also, although Admin is formulated in a Post-LN manner and suffers from gradient vanishing, 18-layer Admin successfully converges and outperforms 18-layer Pre-LN (as in <ref type="table" target="#tab_6">Table 2</ref>). This evidence supports our intuition that the large dependency on residual branches amplifies the output fluctuation and destabilizes training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on IWSLT'14 De-En, WMT'14 En-De, and WMT'14 En-Fr. More details are elaborated in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance Comparison</head><p>We use BLEU as the evaluation matric and summarize the model performance in <ref type="table" target="#tab_6">Table 2</ref>. On the WMT'14 dataset, we use Transformer-base models with 6, 12, or 18 layers. Admin achieves a better performance than Post-LN and Pre-LN in all three settings. Specifically, 12-Layer and 18-Layer Post-LN diverges without the adaptive initialization. Pre-LN converges in all settings, but it results in sub-optimal performance. Admin not only stabilizes the training of deeper models but benefits more from the increased model capacity then Pre-LN, which verifies our intuition that the Pre-LN structure limits the model potential. As in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure" target="#fig_11">Figure 9</ref>, although the 6-layer Pre-LN converges faster than Post-LN, its final performance is worse than Post-LN. In contrast, Admin not only achieves the same convergence speed with Pre-LN in the early stage but reaches a good performance in the late stage. We use 6-layer Transformer-small (its hidden dimension is smaller than the base model) on the IWSLT'14 dataset, and all methods perform similarly. Still, as in <ref type="figure" target="#fig_0">Figure 10</ref>, Admin outperforms the other two by a small margin. Together with WMT'14 results, it implies the training stability is related to layer number. For shallow networks, the stability difference between Post-LN and Pre-LN is not significant (as in <ref type="figure" target="#fig_4">Figure 4</ref>), and all methods reach reasonable performance. It is worth mentioning that attention and activation dropouts have an enormous impact on IWSLT'14, which is smaller than WMT'14 datasets. To further explore the potential of Admin, we train Transformers with a larger size. Specifically, we expand the Transformer-base configuration to have a 60-layer encoder and a 12-layer decoder. As in <ref type="table" target="#tab_6">Table 2</ref>, our method achieves a BLEU score of 43.8 on the WMT'14 En-Fr dataset, the new state-of-the-art without using additional annotations (e.g., back-translation). More discussions are conducted in Appendix F to compare this model with the current state of the art. Furthermore, in-depth analyses are summarized in <ref type="bibr">Liu et al. (2020b)</ref>, including systematic evaluations on the model performance (with TER, ME-TEOR, and BLEU), comprehensive discussions on model dimensions (i.e., depth, head number, and hidden dimension), and fine-grained error analysis. It is worth mentioning that the 60L-12L Admin model achieves a 30.1 BLEU score on WMT'14 En-De <ref type="bibr">(Liu et al., 2020b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Connection to Warmup</head><p>Our previous work <ref type="bibr">(Liu et al., 2020a</ref>) establishes that the need for warmup comes from the unstable adaptive learning rates in the early stage. Still, removing the warmup phrase results in more severe consequences for Transformers than other architectures. Also, warmup has been found to be useful for the vanilla SGD <ref type="bibr" target="#b8">(Xiong et al., 2019)</ref>.</p><p>Theorem 1 establishes that</p><formula xml:id="formula_18">Var[F(x 0 , W ) − F(x 0 , W * )] ≈ N i=1 β 2 i,i C where C = Var[G i ( x * i−1 , W i ) − G i ( x * i−1 , W * i )].</formula><p>In the early stage of training, the network has larger parameter gradients and thus larger C. Therefore, using a small learning rate at initialization helps to alleviate the massive output shift of Post-LN. We further conduct experiments to explore whether more prolonged warmups can make up the stability difference between Post-LN and Pre-LN. We observe that 18-layer Post-LN training still fails after extending the warmup phrase from 8 thousand updates to 16, 24, and 32 thousand. It shows that learning rate warmup alone cannot neutralize the 0 .9 9 9 0 .9 9 5 0 .  instability of Post-LN. Intuitively, massive output shifts not only require a small learning rate but also unsmoothes the loss surface <ref type="bibr">(Li et al., 2018)</ref> and make the training ill-conditioned. Admin regularizes the model behavior at initialization and stabilizes the training. To explore whether Admin is able to stabilize the training alone, we remove the warmup phase and conduct a grid search on optimizer hyper-parameters. The results are visualized in <ref type="figure" target="#fig_0">Figure 10</ref>. It shows that as Post-LN is more sensitive to the choice of hyperparameters, Admin successfully stabilizes the training without hurting its potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparing to Other Initializations</head><p>We compare our methods with three initialization methods, i.e., <ref type="bibr">ReZero (Bachlechner et al., 2020)</ref>, FixUp <ref type="bibr" target="#b9">(Zhang et al., 2019a), and</ref><ref type="bibr">LookLinear (Balduzzi et al., 2017a)</ref>. Specifically, we first conduct experiments with 18-layer Transformers on the WMT'14 De-En dataset. In our experiments, we observe that all of ReZero (which does not contain layer normalization), FixUp (which also does not contain layer normalization), and LookLinear (which is incorporated with Post-LN) leads to di-vergent training. With further analysis, we find that the half-precision training and dropout could destabilize FixUp and ReZero, due to the lack of layer normalization. Simultaneously, we find that even for shadow networks, having an over small reliance on residual branches hurts the model performance, which also supports our intuition. For example, as elaborated in Appendix E, applying ReZero to Transformer-small leads to a 1-2 BLEU score drop on the IWSLT'14 De-En dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Transformer. Transformer <ref type="bibr" target="#b3">(Vaswani et al., 2017)</ref> has led to a series of breakthroughs in various domains <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr" target="#b4">Velickovic et al., 2018;</ref><ref type="bibr">Huang et al., 2019;</ref><ref type="bibr">Parmar et al., 2018;</ref><ref type="bibr" target="#b0">Ramachandran et al., 2019)</ref>. <ref type="bibr">Liu et al. (2020a)</ref> show that compared to other architectures, removing the warmup phase is more damaging for Transformers, especially Post-LN. Similarly, it has been found that the original Transformer (referred to as Post-LN) is less robust than its <ref type="bibr">Pre-LN variant (Baevski and Auli, 2019;</ref><ref type="bibr">Nguyen and Salazar, 2019;</ref>. Our studies go beyond the existing literature on gradient vanishing <ref type="bibr" target="#b8">(Xiong et al., 2019)</ref> and identify an essential factor influencing Transformer training greatly.</p><p>Deep Network Initialization. It has been observed that deeper networks can lead to better performance. For example, <ref type="bibr">Dong et al. (2020)</ref> find that the network depth players a similar role with the sample number in numerical ODE solvers, which hinders the system from getting more precise results. Many attempts have been made to clear obstacles for training deep networks, including various initialization methods. Based on the independence among initialized parameters, one method is derived and found to be useful to handle the gradient vanishing <ref type="bibr">(Glorot and Bengio, 2010)</ref>. Similar methods are further developed for ReLU networks <ref type="bibr">(He et al., 2015)</ref>. <ref type="bibr">He et al. (2016)</ref> find that deep network training is still hard even after addressing the gradient vanishing issue and propose residual networks. <ref type="bibr">Balduzzi et al. (2017b)</ref> identifies the shattered gradient issue and proposes LookLinear initialization.</p><p>On the other hand, although it is observed that scaling residual outputs to smaller values helps to stabilize training <ref type="bibr">(Hanin and Rolnick, 2018;</ref><ref type="bibr">Mishkin and Matas, 2015;</ref><ref type="bibr" target="#b9">Zhang et al., 2019a;</ref><ref type="bibr">Bachlechner et al., 2020;</ref><ref type="bibr">Goyal et al., 2017)</ref>, there is no systematic analysis on what complicates Transformer training or its underlying connection to the dependency on residual branches. Here, we identify that unbalanced gradients are not the direct cause of the Post-LN instability, recognize the amplification effect, and propose a novel adaptive initialization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we study the difficulties of training Transformers in theoretical and empirical manners. Our study in Section 3 suggests that the gradient vanishing problem is not the root cause of unstable Transformer training. Also, the unbalanced gradient distribution issue is mostly addressed by adaptive optimizers. In Section 4, we reveal the root cause of the instability to be the strong dependency on residual branches, which amplifies the fluctuation caused by parameter changes and destabilizes model training. In light of our analysis, we propose Admin, an adaptive initialization method to stabilize Transformers training. It controls the dependency at the beginning of training and maintains the flexibility to capture those dependencies once training stabilizes. Extensive experiments verify our intuitions and show that, without introducing additional hyper-parameters, Admin achieves more stable training, faster convergence, and better performance.</p><p>Our work opens up new possibilities to not only further push the state-of-the-art but understand deep network training better. It leads to many interesting future works, including generalizing Theorem 2 to other models, designing new algorithms to automatically adapt deep networks to different training configurations, upgrading the Transformer architecture, and applying our proposed Admin to conduct training in a larger scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Gradients at Initialization</head><p>Here, we first reveal that Pre-LN does not suffer from the gradient vanishing. Then we establish that only the Post-LN decoder suffers from the gradient vanishing, but not the Post-LN encoder. For simplicity, we use ∆x to denote gradients, i.e., ∆x = ∂L ∂x where L is the training objective. Following the previous study <ref type="bibr">(Bengio et al., 1994;</ref><ref type="bibr">Glorot and Bengio, 2010;</ref><ref type="bibr">He et al., 2015;</ref><ref type="bibr" target="#b1">Saxe et al., 2013)</ref>, we analyze the gradient distribution at the very beginning of training, assume that the randomly initialized parameters and the partial derivative with regard to module inputs are independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Pre-LN Analysis</head><p>For Pre-LN encoders, we have x ]. Thus, lower layers have larger gradients than higher layers, and gradients do not vanish in the backpropagation. i−1 are associated with not only the residual connection but the layer normalization, which makes it harder to establish the connection on their gradients. After making assumptions on the model initialization, we find that lower layers in Post-LN encoder also have larger gradients than higher layers, and gradients do not vanish in the backpropagation through the encoder. Proof. We first prove Var[∆x</p><formula xml:id="formula_19">(pe) 2i = x (pe) 2i−1 + f FFN (f LN (x (pe) 2i−1 )) and ∆x (pe) 2i−1 = ∆x (pe) 2i (1 + ∂f FFN (f LN (x (pe) 2i−1 )) ∂x (pe) 2i</formula><formula xml:id="formula_20">(oe) 2i−1 ] ≥ Var[∆x (oe)</formula><p>2i ], i.e., the backpropagation through FFN sublayers does not suffer from gradient vanishing. In Post-LN encoders, the output of FFN sublayers is calculated as x</p><formula xml:id="formula_21">(oe) 2i = f LN (b (oe) 2i ) where b (oe) 2i = x (oe) 2i−1 + max(0, x<label>(oe)</label></formula><p>2i−1 W (1) )W <ref type="bibr">(2)</ref> . Since at initialization, W (1) and W (2) are independently randomized by symmetric distributions, we have E[b <ref type="bibr">(oe)</ref> 2i ] = 0 and </p><formula xml:id="formula_22">x (oe) 2i = x (oe) 2i−1 + max(x (oe) 2i−1 W (1) , 0)W (2) σ b,2i where σ 2 b,2i = Var[b<label>(</label></formula><p>Assuming different terms are also independent in the backpropagation, we have</p><formula xml:id="formula_24">Var[∆x (oe) 2i−1 ] ≥ Var[ 1 σ b,2i (∆x (oe) 2i + ∆x (oe) 2i ∂ max(x (oe) 2i−1 W (1) , 0)W (2) ∂x (oe) 2i−1 )].</formula><p>At initialization, <ref type="bibr">He et al. (2015)</ref> establishes that</p><formula xml:id="formula_25">Var[∆x (oe) 2i ∂ max(x (oe) 2i−1 W (1) , 0)W (2) ∂x (oe) 2i−1 ] = 1 2 DD f Var[w (1) ] Var[w (2) ] Var[∆x (oe) 2i ].</formula><p>Therefore, we have 2i−1 ], i.e., the backpropagation through Self-Attention sublayers do not suffer from gradient vanishing. In Post-LN encoders, the output of Self-Attention sublayers are calculated as x</p><formula xml:id="formula_26">Var[∆x (oe) 2i−1 ] ≥ 1 σ 2 b,2i (1 + 1 2 DD f Var[w (1) ] Var[w (2) ]) Var[∆x (oe) 2i ].<label>(2)</label></formula><formula xml:id="formula_27">(oe) 2i−1 = f LN (b (oe) 2i−1 ) where b (oe) 2i−1 = x (oe) 2i−2 + a (oe) 2i−1 and a (od) 2i−1 = h f s (x (oe) 2i−2 W (Q) h W (K) h x T (oe) 2i−2 )x (oe) 2i−2 W (V 1 ) h W (V 2 ) h . At initialization, since W (Q) , W (K) , W (V 1 ) , and W (V 2 ) are independently randomized by symmetric distributions, we have E[b (od) 2i−1 ] = 0, thus x (oe) 2i−1 = b (oe) 2i−1 σ b,2i−1 , where σ 2 b,2i−1 = Var[b (oe) 2i−1 ] = Var[x (oe) 2i−2 ] + Var[a (oe) 2i−1 ]. Referring E[f s 2 (x (oe) 2i−2 W (Q) h W (K) h x T (oe) 2i−2 )] as P h , we have Var[a (od) 2i−1 ] = Var[x (oe) 2i−2 W (V 1 ) h W (V 2 ) h ]HP h .</formula><p>Similar to <ref type="bibr">He et al. (2015)</ref>, we have</p><formula xml:id="formula_28">Var[x (oe) 2i−2 W (V 1 ) h W (V 2 ) h ] = D 2 H Var[x (oe) 2i−2 ] Var[w (V 1 ) ] Var[w (V 2 ) ]. Since x (oe)</formula><p>2i−2 is the output of layer norm, we have Var[x (oe) 2i−2 ] = 1. Thus,</p><formula xml:id="formula_29">σ 2 b,2i−1 = 1 + D 2 P h Var[x (oe) 2i−2 ] Var[w (V 1 ) ] Var[w (V 2 ) ].<label>(4)</label></formula><p>In the backpropagation, we have  <ref type="table" target="#tab_5">1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18</ref> Although the gradient distribution is unbalanced (e.g., W (V 1) and W (V 2) have larger gradients than W (K) and W (Q) ), adaptive optimizers lead to consistent update magnitudes for different parameters.</p><formula xml:id="formula_30">Var[∆x (oe) 2i−2 ] ≥ Var[ 1 σ b,2i−1 (∆x (oe) 2i−1 + ∆x (oe) 2i−1 h ∂f s (x (oe) 2i−2 W (Q) h W (K) h x T (oe) 2i−2 )x (oe) 2i−2 W (V 1 ) h W (V 2 ) h ∂x (oe) 2i−2</formula><p>Epoch # (iterations over the training set) <ref type="figure" target="#fig_0">Figure 11</ref>: Relative Norm of Gradient (∆W i , where W i is the checkpoint of i-th epoch) and Update (|W i+1 − W i |) of Self-Attention Parameters in 12-Layer Pre-LN.</p><p>Therefore, we have </p><formula xml:id="formula_31">Var[∆x (oe) 2i−2 ] ≥ 1 σ 2 b,2i−1 (1 + D 2 P h Var[w (V 1 ) ] Var[w (V 2 ) ]) Var[∆x (oe) 2i−1 ].<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Post-LN Decoder Analysis</head><p>In Post-LN, the Encoder-Attention sub-layer suffers from gradient vanishing. The Encoder-Attention sub-layer calculates outputs as x</p><formula xml:id="formula_32">(od) 3i−1 = f LN (b (od) 3i−1 ) where b (od) 3i−1 = x (od) 3i−2 + a<label>(od)</label></formula><p>3i−1 and a (od)</p><formula xml:id="formula_33">3i−1 = h f s (x (od) 3i−2 W (Q) h W (K) h x T (oe) )x (oe) W (V 1 ) h W (V 2 ) h</formula><p>. Here x (oe) is encoder outputs and f s is the row-wise softmax function. In the backpropagation, ∆x ] + 1 ≤ σ 2 b,3i−1 . Thus, those backpropagations suffer from gradient vanishing. This observation is further verified in <ref type="figure">Figure 3</ref>, as the encoder attention bars (gradients of encoder-attention outputs) are always shorter than self-attention bars (gradients of encoder-attention inputs), while adjacent self-attention bars and fully connected bars usually have the same length.</p><formula xml:id="formula_34">(od) 3i−2 ≈ ∆x (od) 3i−1 σ b,3i−1 (1 + ∂a (od) 3i−1 x (od) 3i−2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Distributes of Unbalanced Gradients</head><p>As in <ref type="figure">Figure 5</ref> and <ref type="figure" target="#fig_0">Figure 11</ref>, the gradient distribution of Attention modules is unbalanced even for Pre-LN. Specifically, parameters within the softmax function (i.e., W (K) and W (V 1 ) ) suffer from gradient vanishing (i.e., ∂fs(x 0 ,··· ,x i ,··· ) ∂x i ≤ 1) and have smaller gradients than other parameters. With further analysis, we find it is hard to neutralize the gradient vanishing of softmax. Unlike conventional non-linear functions like ReLU or sigmoid, softmax has a dynamic input length (i.e., for the sentences with different lengths, inputs of softmax have different dimensions). Although this setting allows Attention modules to handle sequential inputs, it restricts them from having stable and consistent backpropagation. Specifically, let us consider the comparison between softmax and sigmoid. For the sigmoid function, although its derivation is smaller than 1, this damping effect is consistent for all inputs. Thus, sigmoid can be neutralized by a larger initialization <ref type="bibr">(Glorot and Bengio, 2010)</ref>. For softmax, its damping effect is different for different inputs and cannot be neutralized by a static initialization.</p><p>Also, we observe that adaptive optimizers largely address this issue. Specifically, we calculate the norm of parameter change in consequent epochs (e.g., |W</p><formula xml:id="formula_35">(K) t+1 − W (K) t | where W (K) t</formula><p>is the checkpoint saved after t epochs) and visualize the relative norm (scaled by the largest value in the same network) in <ref type="figure" target="#fig_0">Figure 11</ref>. Comparing the relative norm of parameter gradients and parameter updates, we notice that: although the gradient distribution is unbalanced, adaptive optimizers successfully assign different learning rates to different parameters and lead to consistent update magnitudes. This result explains why the vanilla SGD fails for training Transformer (i.e., lacking the ability to handle unbalanced gradient distributions). Besides, it implies that the unbalanced gradient distribution (e.g., gradient vanishing) has been mostly addressed by adaptive optimizers and may not significantly impact the training instability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 2</head><p>Here, we elaborate the derivation for Theorem 2, which establishes the relationship between layer number and output fluctuation brought by parameter change.</p><p>THEOREM 2. -Consider a N -layer Transformer x = F( x 0 , W ), where x 0 is the input and W is the parameter. If the layer dependency stays the same after a parameter change (i.e., β i,j has the same value after changing W to W * , where W is randomly initialized and δ = W * − W is independent to W ), the output change (i.e., Var[F(x 0 , W ) − F(x 0 , W * )]) can be estimated as N i=1 β 2 i,i C where C is a constant. Proof. We refer the module in i sub-layer as a i = G i ( x i−1 , W i ), where x i = j≤i β i,j a j is the normalized residual output and a i = a i √ Var a i is the normalized module output. The final output is marked as x = F(x 0 , W ) = j≤N β N,j a j . To simplify the notation, we use the superscript * to indicate variables related to W * , e.g., x * = F(x 0 , W * ) and a * i = G i ( x * i−1 , W * i ). At initialization, all parameters are initialized independently. Thus ∀i = j, a i and a j are independent and 1 = Var[ j≤i β i,j a j ] = j≤i β 2 i,j . Also, since k-layer and (k + 1)-layer share the residual connection to previous layers, ∀i, j ≤ k we have </p><p>Now, we proceed to analyze Var[ a i − a * i ]. Specifically, we have</p><formula xml:id="formula_37">Var[ a i − a * i ] = Var[G i ( x i−1 , W i ) − G i ( x * i−1 , W * i )] = Var[G i ( x i−1 , W i ) − G i ( x * i−1 , W i ) + G i ( x * i−1 , W * i ) − G i ( x * i−1 , W * i )] = Var[G i ( x i−1 , W i ) − G i ( x * i−1 , W i )] + Var[G i ( x * i−1 , W i ) − G i ( x * i−1 , W * i )].<label>(8)</label></formula><p>Since W is randomly initialized, Var[G i ( x * i−1 , W i ) − G i ( x * i−1 , W * i )] should have the same value for all layers, thus we use a constant C to refer its value (</p><formula xml:id="formula_38">C = Var[G i ( x * i−1 , W i ) − G i ( x * i−1 , W * i )] and C ≈ |δ| · |∇G i ( x * i−1 , W i )|). As to Var[G i ( x i−1 , W i ) − G i ( x * i−1 , W i )]</formula><p>, since the sub-layer of Transformers are mostly using linear weights with ReLU nonlinearity and 1 = Var </p><formula xml:id="formula_39">[G i ( x i−1 , W i )] = Var[ x i−1 ], we have Var[G i ( x i−1 , W i ) − G i ( x * i−1 , W i )] ≈ Var[ x i−1 − x * i−1 ].</formula><formula xml:id="formula_40">Var[ x i − x * i ] = β 2 i,i Var[ a i − a * i ] + (1 − β 2 i,i ) Var[ x i − x * i ] ≈ β 2 i,i (Var[ x i−1 − x * i−1 ] + C) + (1 − β 2 i,i ) Var[ x i − x * i ] = Var[ x i − x * i ] + β 2 i,i C Therefore, we have Var[F(x 0 , W ) − F(x 0 , W * )] ≈ N i=1 β 2 i,i C.</formula><p>Here, we first conduct comparisons with <ref type="bibr">ReZero (Bachlechner et al., 2020)</ref> under two configurationsthe first employs the original ReZero model, and the second adds layer normalizations in a Post-LN manner. As summarized in <ref type="table">Table 3</ref>, the ReZero initialization leads to a performance drop, no matter layer normalization is used or not. It verifies our intuition that over small dependency restricts the model potential. At the same time, we find that adding layer normalization to ReZero helps to improve the performance. Intuitively, as dropout plays a vital role in regularizing Transformers, layer normalization helps to not only stabilize training but alleviate the impact of turning off dropouts during the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Performance on the WMT'14 En-Fr</head><p>To explore the potential of Admin, we conduct experiments with 72-layer Transformers on the WMT'14 En-Fr dataset (with a 60-layer encoder and 12-layer decoder, we add less layers to decoder to encourage the model to rely more on the source context). As in <ref type="table">Table 4</ref>, Admin (60L-12L) achieves a BLEU score of 43.80, the new state-of-the-art on this long-standing benchmark. This model has a 60-layer encoder and a 12-layer decoder, which is significantly deeper than other baselines. Still, since the number of parameters increases in a quadratic speed with regard to hidden dimensions and a linear speed with regard to layer numbers, our model has roughly the same number of parameters with other baselines. It is worth mentioning that Admin even achieves better performance than all variants of pre-trained T5 models, which demonstrates the great potential of our proposed method. Also, Admin achieves a better performance than Pre-LN (60L-12L), which further verifies that the Pre-LN architecture restricts deep models' potential.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Lacking enough robustness and stability, the 18-Layer Post-LN Transformer training (i.e.the original architecture) diverges and is omitted in the left graph. Admin not only stabilizes model training but unleashes the model potential for better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>decoder : sub-layers outputs (i.e., FFN, Self-Attention and Encoder-Attention) Notation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The Architecture and notations of Pre-LN Transformers (Left) and Post-LN Transformers (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Encoder output changes for parameter changes, i.e., |F(x 0 , W ) − F(x 0 , W * )| 2 2 where W * − W is random perturbations (left) or gradient updates (right). Intuitively, very large |F − F * | indicates the training to be ill-conditioned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The major difference between Pre-LN and Post-LN is the position of layer norms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, Post-LN layer has a larger dependency on its residual branch. branch outputs.branch outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>β i,j in 6-Layer Post-LN and Pre-LN on the WMT-14 En-De dataset (contains 12 sub-layers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>2 2 for Pre-LN and Post-LN and visualize the results in Fig-6 If a0 and a1 are independent, Var[a0 + a1] = Var[a0] + Var[a1]; also, in our experiments Var[x (p·) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>β i,j of 18-Layer Admin (Post-LN) and Pre-LN on the WMT-14 En-De dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Development PPL on the WMT'14 En-De dataset and the IWLST'14 De-En dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>BLEU score of Post-LN, Pre-LN and Admin on the IWSLT'14 De-En dataset (x-axis is the β 2 for adaptive optimizers and y-axis is the learning rate). Pre-LN converges in all settings while Post-LN diverges in 7 out of 15 settings. When Post-LN converges, it outperforms Pre-LN in 7 out of 8 settings. Admin stabilizes Post-LN training and outperforms Pre-LN (its best performance is comparable with Post-LN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>REMARK 1. -For Pre-LN, if ∀i, ∆x (p·) i and the derivatives of modules in the i-th sub-layer are independent, then ∀i ≤ j, Var[∆x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>THEOREM 1. -For Post-LN Encoders, if γ and ν in the Layer Norm are initialized as 1 and 0 respectively; all other parameters are initialized by symmetric distributions with zero mean; x (oe) i and ∆x (oe) i are subject to symmetric distributions with zero mean; the variance of x (oe) i is 1 (i.e., normalized by Layer Norm); ∆x (oe) i and the derivatives of modules in i-th sub-layer are independent, we have Var[∆x i−1 ] ≥ Var[∆x i ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>k+1 . Thus ∀i ≤ k, β 2 i,k+1 = (1 − β 2 k,k )β 2 i,k and Var[ x i − x * i ] = Var[ j≤i β i,j ( a j − a * j )] = j≤i β 2 i,j Var[ a j − a * j ] = β 2 i,i Var[ a i − a * i ] + (1 − β 2 i,i ) Var[ x i − x * i ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table :</head><label>:</label><figDesc>intermediate output : residual output N: layer # D: hidden # H: head # ⇥N ⇥N</figDesc><table><row><cell>x (pe)</cell></row><row><cell>Layer Norm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>⇤: normalized outputs, i.e., Var[⇤] = 1 Var[·]: dimension-wise variance</figDesc><table><row><cell cols="2">x</cell><cell>(pd) 3i</cell><cell cols="2">a b</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>(od) 3i</cell></row><row><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>b</cell><cell>(od) 3i</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a</cell><cell>(od) 3i</cell></row><row><cell>x</cell><cell cols="2">(pd) 3i 1</cell><cell cols="2">x</cell><cell>(pe) 2i</cell><cell cols="3">x</cell><cell>(oe) 2i</cell><cell>x</cell><cell>(od) 3i 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">b</cell><cell>(oe) 2i</cell><cell>b</cell><cell>(od) 3i 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">a</cell><cell>(oe) 2i</cell><cell>a</cell><cell>(od) 3i 1</cell></row><row><cell>x</cell><cell cols="2">(pd) 3i 2</cell><cell>x</cell><cell cols="2">(pe) 2i 1</cell><cell>x</cell><cell cols="2">(oe) 2i 1</cell><cell>x</cell><cell>(od) 3i 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">b</cell><cell>(oe) 2i 1</cell><cell>b</cell><cell>(od) 3i 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">a</cell><cell>(oe) 2i 1</cell><cell>a</cell><cell>(od) 3i 2</cell></row><row><cell>x</cell><cell cols="2">(pd) 3i 3</cell><cell>x</cell><cell cols="2">(pe) 2i 2</cell><cell>x</cell><cell cols="2">(oe) 2i 2</cell><cell>x</cell><cell>(od) 3i 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Layer Norm. Layer norm (Ba et al., 2016) plays a vital role in Transformer architecture. It is defined the gradient norm of sub-layer outputs, scaled by the largest gradient norm in the same network.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Pre-LN Encoder</cell><cell></cell><cell cols="3">Post-LN Encoder</cell><cell></cell><cell cols="3">Pre-LN Deocder</cell><cell></cell><cell cols="3">Post-LN Deocder</cell><cell></cell></row><row><cell>10 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10°1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Self Attention (PostLN Decoder)</cell><cell></cell><cell></cell><cell cols="5">Encoder Attention (PostLN Decoder)</cell><cell></cell><cell></cell><cell cols="4">Feedforward (PostLN Decoder)</cell></row><row><cell>10 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">Gradient vanishing only happens in backpropagations for Encoder-Attention sub-layers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">i.e., from Encoder-Attention outputs to Self-Attention outputs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10°1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell></row><row><cell cols="18">Figure 3: Relative gradient norm histogram (on a log scale) of 18-layer Transformers on the WMT'14 En-De</cell></row><row><cell cols="8">dataset, i.e., Feedforward Network. Transformers use two-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">layer perceptrons as feedforward networks, i.e.,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>f FFN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>as f LN (x) = γ x−µ σ + ν, where µ and σ are the mean and standard deviation of x.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Changing decoders from Post-LN to Pre-LN</cell></row><row><cell>fixes gradient vanishing, but does not stabilize model</cell></row><row><cell>training successfully. Encoder/Decoder have 18 layers.</cell></row><row><cell>perspective. Similar to Xiong et al. (2019), we</cell></row><row><cell>establish that Pre-LN networks do not suffer from</cell></row><row><cell>gradient vanishing (as elaborated in Appendix A.1).</cell></row><row><cell>Unlike Xiong et al. (2019), we recognize that not</cell></row><row><cell>all Post-LN networks suffer from gradient vanish-</cell></row><row><cell>ing. As in Theorem 1, we establish that Post-LN</cell></row><row><cell>Encoder networks do not suffer from gradient van-</cell></row><row><cell>ishing. Detailed derivations are elaborated in Ap-</cell></row><row><cell>pendix A.2.</cell></row><row><cell>THEOREM 1. -For Post-LN Encoders, if γ and</cell></row><row><cell>ν in the Layer Norm are initialized as 1 and 0 re-</cell></row><row><cell>spectively; all other parameters are initialized by</cell></row><row><cell>symmetric distributions with zero mean; x ∆x (oe) are subject to symmetric distributions with (oe) and i i zero mean; the variance of x (oe) is 1 (i.e., normal-i ized by Layer Norm); ∆x (oe) i and the derivatives</cell></row><row><cell>of modules in i-th sub-layer are independent, we</cell></row><row><cell>have Var[∆x i−1 ] ≥ Var[∆x i ].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>BLEU on IWSLT'14 De-En and WMT'14 En-Fr/De (AL-BL refers A-layer encoder &amp; B-layer decoder).</figDesc><table><row><cell>Dataset</cell><cell>IWSLT'14 De-En</cell><cell cols="2">WMT'14 En-Fr</cell><cell></cell><cell>WMT'14 En-De</cell><cell></cell></row><row><cell>Enc #-Dec #</cell><cell>6L-6L (small)</cell><cell>6L-6L</cell><cell>60L-12L</cell><cell>6L-6L</cell><cell cols="2">12L-12L 18L-18L</cell></row><row><cell>Post-LN</cell><cell>35.64±0.23</cell><cell>41.29</cell><cell>failed</cell><cell>27.80</cell><cell>failed</cell><cell>failed</cell></row><row><cell>Pre-LN</cell><cell>35.50±0.04</cell><cell>40.74</cell><cell>43.10</cell><cell>27.27</cell><cell>28.26</cell><cell>28.38</cell></row><row><cell>Admin</cell><cell>35.67±0.15</cell><cell>41.47</cell><cell>43.80</cell><cell>27.90</cell><cell>28.58</cell><cell>29.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Thus, we can rewrite Equation 8 and getVar[ a i − a * i ] ≈ Var[ x i−1 − x * i−1 ] + C</figDesc><table><row><cell>With Equation 7, we have</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Implementations are released at: https://github. com/LiyuanLucasLiu/Transforemr-Clinic</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Our analysis uses ReLU as the activation function, while Admin can be applied to other non-linear functions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">b,2i−1 (Var[∆x (oe) 2i−1 ] + Var[∆x (oe) 2i−1 h f s (x (oe) 2i−2 W (Q) h W (K) h x T (oe) 2i−2 ) ∂x (oe) 2i−2 W (V 1 ) h W (V 2 ) h ∂x (oe) 2i−2 ])At initialization, we assume ∆x(oe) 2i−1 and model parameters are independent (He et al., 2015), thusVar[∆x (oe) 2i−1 h f s (x (oe) 2i−2 W (Q) h W (K) h x T (oe) 2i−2 ) ∂x (oe) 2i−2 W (V 1 ) h W (V 2 ) h ∂x (oe) 2i−2 ] =D 2 P h Var[∆x (oe) 2i−1 ] Var[w (V 1 ) ] Var[w (V 2 ) ]</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>We thank all reviewers for their constructive comments; Chengyu Dong, Haoming Jiang, Jingbo Shang, Xiaotao Gu, and Zihan Wang for valuable discussions and comments; Jingbo Shang for sharing GPU machines; and Microsoft for setting up GPU machines. The research was sponsored in part by DARPA No. W911NF-17-C-0099 and No. FA8750-19-2-1004, National Science Foundation IIS-19-56151, IIS-17-41317, IIS 17-04532, and IIS  16-18481, and DTRA HDTRA11810026.   </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Admin Implementation Details</head><p>As introduced in Section 4.3, we introduce a new set of parameters to rescale the module outputs. Specifically, we refer these new parameters as ω and construct the Post-LN sub-layer as:</p><p>where · is the element-wise product.</p><p>After training, Admin can be reparameterized as the conventional Post-LN structure (i.e., removing ω i ). Specifically, we consider</p><p>Then, for feedforward sub-layers, we have</p><p>It can be reparameterized by changing γ, ν,</p><p>For Self-Attention sub-layers, we have</p><p>It can be reparameterized by changing γ, ν, W</p><p>For Encoder-Attention sub-layers, we have</p><p>It can be reparameterized by changing γ, ν, W</p><p>It is easy to find b i = b i in all three situations. From the previous analysis, it is easy to find that introducing the additional parameter ω i is equivalent to rescale some model parameters. In our experiments on IWSLT14 De-En, we find that directly rescaling initialization parameters can get roughly the same performance with introducing ω i . However, it is not very stable when conducting training in a half-precision manner. Accordingly, we choose to add new parameters ω i instead of rescaling parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Setup</head><p>Our experiments are based on the implementation from the fairseq package <ref type="bibr">(Ott et al., 2019)</ref>. As to pre-processing, we follow the public released script from previous work <ref type="bibr">(Ott et al., 2019;</ref><ref type="bibr" target="#b18">Lu et al., 2020)</ref>. For WMT'14 datasets, evaluations are conducted on the provided 'newstest14' file, and more details about them can be found in <ref type="bibr">Bojar et al. (2014)</ref>. For the IWSLT'14 De-En dataset, more analysis and details can be found in <ref type="bibr">Cettolo et al. (2014)</ref>.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno>abs/1312.6120</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth growing for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shu Xin Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>On layer normalization in the transformer architecture. ArXiv, abs/2002.04745</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Why adam beats sgd for attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sai Praneeth Karimireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyeon</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surinder</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
		<idno>abs/1912.03194</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Muse: Parallel multi-scale attention for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<idno>abs/1911.09483</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Big-Rnmt+ (chen</surname></persName>
		</author>
		<idno>377 M 1024 × 8192 6L-6L 41.12</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Dynamicconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno>2019a) 213 M 1024 × 4096 7L-7L 43.2 DG-Transformer</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Prime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno>252 M 1024 × 4096 6L-6L 43.48</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Pre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ln</surname></persName>
		</author>
		<idno>60L-12L) 262 M 512 × 2048 60L-12L 43.10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Admin</surname></persName>
		</author>
		<idno>60L-12L) 262 M 512 × 2048 60L-12L 43.80</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">14 En-De and WMT&apos;14 En-Fr datasets. Specifically, on the IWSLT&apos;14 De-En dataset, we use word embedding with 512 dimensions and 6-layer encoder/decoder with 4 heads and 1024 feedforward dimensions; on the WMT&apos;14 En-De and WMT&apos;14 En-Fr datasets, we use word embedding with 512 dimension and 8-head encoder/decoder with 2048 hidden dimensions. Label smoothed cross entropy is used as the objective function</title>
	</analytic>
	<monogr>
		<title level="m">As to model specifics, we directly adopt Transformer-small configurations on the IWSLT&apos;14 De-En dataset and stacks more layers over the Transformer-base model on the WMT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>with an uncertainty = 0.1 (Szegedy et al.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Specifically, for the WMT&apos;14 En-De and WMT&apos;14 En-Fr dataset, all dropout ratios (including (activation dropout and attention dropout) are set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">For Model training, we use RAdam as the optimizer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>2020a) and adopt almost all hyperparameter settings from. to 0.1. For the IWSLT&apos;14</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">As to optimizer, we set (β 1 , β 2 ) = (0.9, 0.98), use inverse sqrt learning rate scheduler with a warmup phrase (8000 steps on the WMT&apos;14 En-De/Fr dataset, and 6000 steps on the IWSLT&apos;14 De-En dataset). The maximum learning rate is set to 1e −3 on the WMT&apos;14 En-De dataset and 7e −4 on the IWSLT&apos;14 De-En and WMT&apos;14 En-Fr datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">after-layer dropout is set to 0.3, and a weight decay of 0.0001 is used</title>
		<imprint/>
	</monogr>
	<note>We conduct training for 100 epochs on the WMT&apos;14 En-De dataset, 90 epochs on the IWSLT&apos;14</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the IWSLT&apos;14 De-En dataset, we conduct training on one NVIDIA GeForce GTX 1080 Ti GPU and set the maximum batch size to be 4000. On the WMT&apos;14 En-De dataset, we conduct training on four NVIDIA Quadro R8000 GPUs and set maximum batch size</title>
	</analytic>
	<monogr>
		<title level="m">De-En dataset and 50 epochs on the WMT&apos;14 En-Fr dataset, while the last 10 checkpoints are averaged before inference</title>
		<imprint/>
	</monogr>
	<note>per GPU) as 8196. On the WMT&apos;14</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the WMT&apos;14 En-De dataset, 6L-6L models (w. 63 M Param.) take ∼ 1 day to train, 12L-12L (w. 107M Param.) models take ∼ 2 days to train, and 18L-18L (w. 151M Param.) models take ∼ 3 days to train. On the WMT&apos;14 En-Fr dataset, 6L-6L models (w. 67 M Param.) takes ∼ 2 days to train, and 60L-12L models (w. 262M Param.) takes ∼ 2.5 days to train. All training is conducted in half-precision with dynamic scaling</title>
	</analytic>
	<monogr>
		<title level="m">En-Fr dataset, we conduct training with the Nvidia DGX-2 server (6L-6L uses 4 NVIDIA TESLA V100 GPUs and 60L-16L uses 16 NVIDIA TESLA V100 GPUs) and set the maximum batch size (per GPU) as 5000. On the IWSLT&apos;14 De-En dataset</title>
		<imprint/>
	</monogr>
	<note>Transformer-small models (w. 37 M Param.) take a few hours to train. with a 256-update scaling window and a 0.03125 minimal scale. All our implementations and pre-trained models would be released publicly</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
