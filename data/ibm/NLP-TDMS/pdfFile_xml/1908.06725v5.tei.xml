<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Xiu</forename><surname>Ye</surname></persName>
							<email>zxye@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Speech Lab, DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
							<email>w.wang@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Speech Lab, DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
							<email>zhling@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The state-of-the-art pre-trained language representation models, such as Bidirectional Encoder Representations from Transformers (BERT), rarely incorporate commonsense knowledge or other knowledge explicitly. We propose a pre-training approach for incorporating commonsense knowledge into language representation models. We construct a commonsense-related multi-choice question answering dataset for pre-training a neural language representation model. The dataset is created automatically by our proposed "align, mask, and select" (AMS) method. We also investigate different pre-training tasks. Experimental results demonstrate that pre-training models using the proposed approach followed by fine-tuning achieve significant improvements over previous state-of-the-art models on two commonsense-related benchmarks, including CommonsenseQA and Winograd Schema Challenge. We also observe that finetuned models after the proposed pre-training approach maintain comparable performance on other NLP tasks, such as sentence classification and natural language inference tasks, compared to the original BERT models. These results verify that the proposed approach, while significantly improving commonsense-related NLP tasks, does not degrade the general language representation capabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, significant progress has been made in language representation models <ref type="bibr" target="#b8">(Pennington et al., 2014;</ref><ref type="bibr" target="#b9">Peters et al., 2017;</ref><ref type="bibr" target="#b2">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b13">Radford et al., 2018;</ref><ref type="bibr" target="#b0">Devlin et al., 2019;</ref>. These models can be categorized into feature-based approaches and fine-tuning approaches. In particular, a pre-training/fine-tuning * Work was done during an internship at DAMO Academy, Alibaba Group.  technique, Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref>, was proposed and has quickly created state-of-theart models for a wide variety of NLP tasks such as question answering (QA), text classification, and natural language inference (NLI) <ref type="bibr" target="#b16">(Rajpurkar et al., 2016;</ref>.</p><p>However, commonsense reasoning remains a challenging task for modern machine learning methods. For example, recently <ref type="bibr" target="#b22">Talmor et al. (2019)</ref> proposed a commonsense-related task, Com-monsenseQA(CSQA), and showed that the BERT model accuracy remains dozens of points lower than the human accuracy on questions about commonsense knowledge. Some examples from CSQA are shown in Part A of <ref type="table" target="#tab_1">Table 1</ref>. As can be seen from the examples, although it is easy for humans to answer the questions based on their world knowledge, it is a great challenge for machines when there is limited training data. We hypothesize that exploiting knowledge graphs (KGs) representing commonsense in QA modeling may help the model choose correct answers. For example, as shown in Part B of <ref type="table" target="#tab_1">Table 1</ref>, some triples from Concept-Net <ref type="bibr" target="#b19">(Speer et al., 2017)</ref> are related to the questions above. Exploiting these triples in QA modeling may help the models make the correct decision.</p><p>In this paper, we propose a pre-training approach that can leverage commmonsense KGs, such as ConceptNet <ref type="bibr" target="#b19">(Speer et al., 2017)</ref>, to improve the commonsense reasoning capability of language representation models, such as BERT, without sacrificing the language representation capabilities of the models. That is, we also aim to maintain comparable performances on other NLP tasks with the original BERT models. It is challenging to incorporate the commonsense knowledge into language representation models since the commonsense knowledge is usually represented in a structured format, such as (concept 1 , relation, concept 2 ) in Concept-Net, which is inconsistent with the data used for pre-training language representation models. For example, BERT is pre-trained on the BooksCorpus and English Wikipedia that are composed of unstructured natural language sentences. To tackle this challenge, inspired by the distant supervision approach <ref type="bibr" target="#b7">(Mintz et al., 2009)</ref>, we propose an "align, mask and select" (AMS) method to automatically construct a multi-choice question-answering dataset, by aligning a commonsense KG with a large text corpus and constructing natural language sentences with labeled concepts. We then replace the masked language model (MLM) and next sentence prediction (NSP) tasks used for the original BERT pre-training stage with a multi-choice question answering task based on this dataset.</p><p>In summary, our contributions are threefold. First, we propose a pre-training approach for incorporating commonsense knowledge into language representation models for improving the commonsense reasoning capabilities of these models. This pre-training approach is agnostic to the language representation models. We propose the AMS method to automatically construct a multi-choice QA dataset and facilitate the proposed pre-training approach. Second, experiments demonstrate that the pre-trained models from the proposed approach with fine-tuning achieve significant improvements over previous SOTA models on two commonsenserelated NLP benchmarks, CSQA and Winograd Schema Challenge (WSC), and maintain compa-rable performances on sentence classification and NLI tasks on the GLUE dataset, demonstrating that the proposed approach does not degrade the language representation capabilities of the models. Third, extensive ablation analysis conducted on different data creation approaches and pre-training tasks helps shed light on pre-training strategies for incorporating commonsense knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language Representation Model</head><p>Language representation models have demonstrated effectiveness for improving many NLP tasks.</p><p>The early feature-based approaches <ref type="bibr" target="#b6">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b8">Pennington et al., 2014;</ref><ref type="bibr" target="#b10">Peters et al., 2018)</ref> only use the pre-trained language representations as input features for other models. In contrast, the fine-tuning approaches <ref type="bibr" target="#b2">(Howard and Ruder, 2018;</ref><ref type="bibr" target="#b13">Radford et al., 2018;</ref><ref type="bibr" target="#b0">Devlin et al., 2019;</ref><ref type="bibr" target="#b27">Yang et al., 2019)</ref> introduce minimal task-specific parameters trained on the downstream tasks while fine-tuning pre-trained parameters. Recently, there are works incorporating entity knowledge <ref type="bibr" target="#b21">Sun et al., 2019b)</ref> and embedding multiple knowledge bases <ref type="bibr" target="#b11">(Peters et al., 2019)</ref> into language representation models . In this work, we focus on incorporating commonsense knowledge in the pre-training stage. <ref type="bibr" target="#b12">(Petroni et al., 2019)</ref> evaluated pretrained language models on factual and commonsense knowledge probing tasks. <ref type="bibr" target="#b29">(Zhong et al., 2018)</ref> ensembled commonsense knowledge based models with standard QA models and improved the commonsense reasoning ability. Other works directly incorporate commonsense knowledge into language representation models. <ref type="bibr" target="#b20">Sun et al. (2019a)</ref> proposed to directly pre-train BERT on commonsense knowledge triples. For any triple (concept 1 , relation, concept 2 ), they took the concatenation of concept 1 and relation as the question and concept 2 as the correct answer. Distractors were formed by randomly picking words or phrases in ConceptNet. However, we hypothesize that the language representations learned in <ref type="bibr" target="#b20">Sun et al. (2019a)</ref> may be tampered since the inputs to the model constructed this way are not natural language sentences. To address this issue, we propose a pre-training approach for incorporating commonsense knowledge that includes a method to construct large-scale natural language (1) A triple from ConceptNet (population, AtLocation, city)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Commonsense Reasoning</head><p>(2) Align with the English Wikipedia dataset to obtain a sentence containing "population" and "city"</p><p>The largest city by population is Birmingham, which has long been the most industrialized city.</p><p>(3) Mask "city" with a special token "[QW]"</p><p>The largest [QW] by population is Birmingham, which has long been the most industrialized city?  sentences. <ref type="bibr" target="#b15">Rajani et al. (2019)</ref> collected the Common Sense Explanations (CoS-E) dataset and applied a Commonsense Auto-Generated Explanations (CAGE) framework to language representation models, which required a large amount of human efforts. In contrast, we propose the AMS method, inspired by the distant supervision approaches, to automatically construct a multi-choice QA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distant Supervision</head><p>The distant supervision approach was originally proposed for generating training data for relation classification. The approach in <ref type="bibr" target="#b7">(Mintz et al., 2009)</ref> assumes that if two entities/concepts participate in a relation, all sentences that mention these two entities/concepts express that relation. It is inevitable that there exists noise in the data labeled by distant supervision <ref type="bibr" target="#b17">(Riedel et al., 2010)</ref>. In this paper, instead of employing the relation labels labeled by distant supervision, we focus on the aligned entities/concepts. We propose the AMS method to construct a multi-choice QA dataset that aligns sentences with commonsense knowledge triples, masks the aligned entities/concepts in sentences and treat the masked sentences as questions, and selects several entities/concepts from knowledge graphs as distractor choices.</p><p>3 Proposed Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Commonsense Knowledge Base</head><p>We use ConceptNet 1 <ref type="bibr" target="#b19">(Speer et al., 2017)</ref>, one of the most widely used commonsense knowledge bases. ConceptNet is a semantic network that represents the large sets of words and phrases and the commonsense relationships between them (36 core relations). It contains over 21 million edges and over 8 million nodes. Each instance in ConceptNet can be represented as a triple (concept 1 , relation, concept 2 ), indicating relation between the two concepts concept 1 and concept 2 . For example, the triple (semicarbazide, IsA, chemical compound) means that "semicarbazide is a kind of chemical compounds"; the triple (cooking dinner, Causes, cooked food) means that "the effect of cooking dinner is cooked food", etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing Pre-training Dataset</head><p>We first filter the triples in ConceptNet as follows:</p><p>(1) Filter triples in which one of the concepts is not English words.</p><p>(2) Filter triples with the general relations "RelatedTo" and "IsA", which hold a large proportion in ConceptNet. (3) Filter triples in which one of the concepts has more than four words or the edit distance (character-level) between the two concepts is less than four. After filtering, we obtain 606,564 triples. Each training sample is generated by three steps in the AMS method: align, mask, and select. Each training sample consists of a question and five candidate answers, following the form of the CSQA dataset.</p><p>An example of constructing one training sample is shown in <ref type="table" target="#tab_2">Table 2</ref>. Firstly, we align each triple (concept 1 , relation, concept 2 ) in the filtered triple set to the English Wikipedia dataset to extract the sentences containing the two concepts. This align step matches the two concepts exactly. In future work, we will explore entity linking. Secondly, we mask the concept 1 or concept 2 in one sentence with a special token [QW] and treat this sentence as a question, where QW is a replacement word of the question words "what", "where", etc. And the masked concept 1 or concept 2 is the correct answer for this question. Thirdly, for generating the distractors, <ref type="bibr" target="#b20">Sun et al. (2019a)</ref> randomly picked words or phrases in ConceptNet as the distractors. In our work, in order to generate more confusing distractors than the random selection approach, we select distractors sharing the same other unmasked concept, i.e., concept 2 or concept 1 , and the same relation with the correct answer. That is, we search ( * , relation, concept 2 ) or (concept 1 , relation, * ) in ConceptNet to select distractors, where * is a wildcard character that can match any word or phrase. For each question, we reserve four distractors and one correct answer. If there are fewer than four distractors, we discard this question. If there are more than four distractors, we randomly select four distractors from them. After applying the AMS method, we create 16,324,846 multi-choice QA samples and denote this dataset D AM S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-training BERT CS</head><p>We explore a multi-choice QA task for pre-training the English BERT base and large models on D AM S . The resulting models are denoted BERT CS base and BERT CS large , respectively. We then evaluate the performance of fine-tuning the BERT CS models on several NLP tasks (Section 4).</p><p>We concatenate the question with each candidate in D AM S to construct a standard input sequence for BERT CS (i.e., "[CLS] the largest [QW] by ...?</p><p>[SEP] city [SEP]", where [CLS] and [SEP] are two special tokens), and the hidden representations over the <ref type="bibr">[CLS]</ref> token are run through a softmax layer to predict whether the candidate is the correct answer. The objective function is defined as follows:</p><formula xml:id="formula_0">L = −logp(c i |s),<label>(1)</label></formula><formula xml:id="formula_1">p(c i |s) = exp(w T c i ) N k=1 exp(w T c k ) ,<label>(2)</label></formula><p>where c i is the correct answer, w the parameters in the softmax layer, N the total number of candidates, and c i the vector representation of the token [CLS].</p><p>To reduce the large cost of training BERT CS models from scratch, we initialize the BERT CS models (for both BERT base and BERT large models) with the parameter weights released by Google 2 . We pre-train BERT CS models with the batch size 160, the initial learning rate 2e-5, and the max sequence length 128 for 1 epoch. Pre-training is conducted on 16 NVIDIA V100 GPU cards with 32G memory for about 3 days for the BERT CS large model and 1 day for the BERT CS base model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Evaluations are conducted on two aspects. First, we evaluate whether the proposed approach improves the commonsense reasoning capability. Second, we investigate whether the proposed approach, while targeting improving the commonsense reasoning capability, can maintain general language representation capabilities comparable with the original BERT model, e.g., on general text classification and natural language inference (NLI) tasks. In the first set of experiments, we evaluate the BERT CS models on the benchmark CommonsenseQA (CSQA) and Winograd Schema Challenge (WSC) datasets.</p><p>In the second set of experiments, we evaluate the BERT CS models on the General Language Understanding Evaluation (GLUE) benchmark. When fine-tuning on the commonsense-related, multi-choice QA tasks, e.g., CSQA and WSC, we fine-tune all parameters in BERT CS, including the last softmax layer from the token [CLS]; whereas for the second set of experiments, we randomly initialize the classifier layer and train it from scratch. Additionally, as described in <ref type="bibr" target="#b0">Devlin et al. (2019)</ref>, sometimes fine-tuning on BERT is observed to be unstable on small datasets. Hence, we run experiments with 5 different random seeds and select the best model based on the development set for all of the fine-tuning experiments in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CommonsenseQA</head><p>The CSQA dataset consists of 12,247 questions with one correct answer and four distractors. Our experiments are conducted on the more challenging  <ref type="table">Table 5</ref>: Accuracy (%) of different models on the WSC dataset together with its subsets and the WNLI test set. MTP denotes masked token prediction, which is employed in <ref type="bibr" target="#b3">Kocijan et al. (2019)</ref>. MCQA denotes the multichoice question-answering format, which is employed in this paper.</p><p>random split, which is the main evaluation split <ref type="bibr" target="#b22">Talmor et al. (2019)</ref>. The statistics of the CSQA dataset are summarized in <ref type="table" target="#tab_4">Table 3</ref>. Same as the pre-training stage, the input data for fine-tuning BERT CS is formed by concatenating each question-answer pair as a sequence. The hidden representations over the <ref type="bibr">[CLS]</ref> token are run through a softmax layer to create the predictions. The objective function is the same as Equations 1 and 2. We fine-tune BERT CS on CSQA for 2 epochs with a learning rate of 1e-5 and a batch size of 16. <ref type="table" target="#tab_5">Table 4</ref> shows the accuracy on the CSQA test set from the baseline BERT models, the previous SOTA model <ref type="bibr">CoS-E (Rajani et al., 2019)</ref>, and our BERT CS models. CoS-E model requires a large amount of human efforts to collect the Common Sense Explanations (CoS-E) dataset. In comparison, our multi-choice QA dataset D AM S is constructed automatically. The BERT CS models significantly outperform the baseline BERT models with BERT CS large achieving 5.5% absolute gain over the baseline BERT large model and 4.0% absolute gain over the previous SOTA CoS-E model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Winograd Schema Challenge</head><p>The WSC task <ref type="bibr" target="#b4">(Levesque et al., 2012)</ref> is introduced for testing AI agents for commonsense knowledge and is considered one of the most difficult commonsense reasoning datasets <ref type="bibr" target="#b30">(Zhou et al., 2019)</ref>. WSC consists of 273 instances for pronoun disambiguation. For an example sentence "The delivery truck zoomed by the school bus because it was going so fast." and a corresponding question "What does the word it refers to?", the machine is expected to answer "delivery truck" instead of "school bus".</p><p>We follow <ref type="bibr" target="#b3">Kocijan et al. (2019)</ref> and employ the WSCR dataset <ref type="bibr" target="#b14">(Rahman and Ng, 2012)</ref> as the train-ing data. The WSCR dataset is partitioned into a training set of 1,322 examples and a test set of 564 examples. We use the WSCR training partition for fine-tuning pre-trained BERT CS models (Section 3.3) and the WSCR test partition for validating BERT CS models, respectively, and test the fine-tuned BERT CS models on the WSC dataset. We transform the pronoun disambiguation problem into a multi-choice QA problem. We mask the pronoun word with a special token [QW] to construct a question, and put the two candidate phrases as candidate answers. The remaining procedures are the same as the CSQA task. We use the same loss function as <ref type="bibr" target="#b3">Kocijan et al. (2019)</ref>. That is, if c 1 is correct and c 2 is not, the loss is L = − logp(c 1 |s)+ α · max(0, logp(c 2 |s) − logp(c 1 |s) + β),</p><p>(3) where p(c 1 |s) follows Equation 2 with N = 2, α and β are hyper-parameters. Similar to <ref type="bibr" target="#b3">Kocijan et al. (2019)</ref>, we search α ∈ {2.5, 5, 10, 20} and β ∈ {0.05, 0.1, 0.2, 0.4} by optimizing the accuracy on the WSCR test partition (i.e., the development set for the WSC dataset). We set the batch size 16 and the learning rate 1e-5. We evaluate our models on the WSC dataset and its various partitions defined in <ref type="bibr" target="#b23">Trichelair et al. (2019)</ref>. We also evaluate the fine-tuned BERT CS model (without using the WNLI training data for further fine-tuning) on the WNLI test set, one of the GLUE tasks . We first transform the examples in WNLI from the premise-hypothesis format into the pronoun disambiguation problem format <ref type="bibr" target="#b3">(Kocijan et al., 2019)</ref> and then transform it into the multi-choice QA format.</p><p>The results on the WSC dataset and its various  <ref type="table">Table 6</ref>: The results of different models on the GLUE test sets. We use the same measure criterion as <ref type="bibr" target="#b0">Devlin et al. (2019)</ref>. BERT CS large achieves comparable performance with BERT large and BERT CS base slightly better performance than BERT base , verifying that our multi-choice QA based pre-training approach can maintain the performance on common NLP tasks.</p><p>partitions and the WNLI test set are shown in Table 5. Note that non-assoc. denotes instances in WSC that no antecedent is statistically preferred, and assoc. denotes the rest instances. Columns unswitched and switched denote the accuracy on the unswitched and switched switchable subset defined in <ref type="bibr" target="#b23">(Trichelair et al., 2019)</ref>, and consist. denotes the percentage of predictions that change after candidates in the switchable subset are switched.</p><p>Higher scores for non-assoc. and const. suggest a better commonsense reasoning capability <ref type="bibr" target="#b23">(Trichelair et al., 2019)</ref>. Note that the results for <ref type="bibr" target="#b18">Ruan et al. (2019)</ref> are fine-tuned on the whole WSCR dataset, including its training and test set partitions. Results for Ensemble 14 LMs (Trinh and Le, 2018) and Knowledge Hunter <ref type="bibr" target="#b1">(Emami et al., 2018)</ref> are cited from <ref type="bibr" target="#b23">Trichelair et al. (2019)</ref>. Results for "BERT large + MTP" is cited from <ref type="bibr" target="#b3">Kocijan et al. (2019)</ref> as the baseline of applying BERT to the WSC task. As can be seen from the table, our "BERT CS large + MCQA" achieves the best performance on all of the evaluation criteria except being the second best on consist. 3 , and achieves a 3.3% absolute improvement on the WSC dataset over the previous SOTA results from <ref type="bibr" target="#b3">Kocijan et al. (2019)</ref>. Further analysis shows that "BERT large + MCQA" achieves better performance than "BERT large + MTP" on the WSC and WNLI testsets and achieves significant improvements on consist., suggesting that MCQA may be a better problem formatting method than MTP for the WSC task. Since the CSQA dataset is created using concepts from Con-ceptNet and D AM S is created using ConceptNet, D AM S may be considered as an augmented data for CSQA 4 . However, these WSC results demon-3 Knowledge Hunter achieves a better consist. score by a rule-based method <ref type="bibr" target="#b23">(Trichelair et al., 2019)</ref>. <ref type="bibr">4</ref> We will compare the BERT CS results in <ref type="table" target="#tab_5">Table 4</ref> with fine-tuning the original BERT on CSQA-train + DAMS. strate that the proposed approach is general for improving commonsense reasoning capabilities of the pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GLUE</head><p>The GLUE benchmark ) is a collection of diverse natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI. To investigate whether our pre-training approach can maintain the performance on common text classification and NLI tasks, we evaluate BERT CS on 8 GLUE datasets and compare the performance with the baseline BERT models. Following <ref type="bibr" target="#b0">Devlin et al. (2019)</ref>, we use batch size 32 and fine-tune for 3 epochs for all GLUE tasks, and select the finetuning learning rate among 1e-5, 2e-5, and 3e-5 based on the performance on the development set. Results are presented in <ref type="table">Table 6</ref>. We observe that BERT CS large achieves comparable performance with BERT large and BERT CS base achieves slightly better performance than BERT base . We hypothesize that the commonsense knowledge may not be required for the GLUE tasks. On the other hand, these results demonstrate that our proposed pre-training approach does not degrade the language representation capabilities of BERT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pre-training Strategies</head><p>We conduct several experiments investigating different data creation approaches and pre-training tasks on the BERT base model. For simplicity, we discard the subscript base in this subsection.</p><p>In order to compare the efficacy of our data creation approach versus the data creation approach  We create several other model counterparts: First, distractors are formed by randomly picking concept 1 or concept 2 in ConceptNet instead of those sharing the same concept 2 or concept 1 and the same relation with the correct answers. We denote the resulting model from this dataset BERT CS random. Second, we randomly mask 15% WordPiece tokens <ref type="bibr" target="#b26">(Wu et al., 2016)</ref> of the question as in <ref type="bibr" target="#b0">Devlin et al. (2019)</ref> and then conduct both multi-choice QA task and MLM task simultaneously. The resulting model is denoted BERT CS MLM. Third, instead of pre-training BERT with a multi-choice QA task that chooses the correct answer from several candidate answers, we mask concept 1 and concept 2 and pre-train BERT with the MLM task. We denote the resulting model from this pre-training task BERT MLM.</p><p>All these BERT models are fine-tuned on the CSQA training set with the same hyper-parameters as described in Section 4.1 and the results are shown in <ref type="table" target="#tab_9">Table 7</ref>. Comparing model 1 and model 2, we find that pre-training on ConceptNet benefits the CSQA task even with the triples as input instead of sentences. Further comparing model 2 and model 6, we find that constructing natural language sentences as input for pre-training BERT performs better on the CSQA task than pre-training using triples. We also conduct more detailed comparisons between fine-tuning model 1 and model 2 on GLUE tasks. The results are shown in Ta-ble 6. BERT triple base yields much worse results than BERT base and BERT CS base , demonstrating that pre-training directly on triples may hurt the sentence representation capabilities of BERT.</p><p>Comparing model 3 and model 6, we find that pre-training BERT benefits from a more difficult dataset. In our selection method, all candidate answers share the same (concept 1 , relation) or (relation, concept 2 ), that is, these candidates have close meanings. These more confusing candidates force BERT CS to distinguish meanings of synonyms, resulting in a more powerful BERT CS model. Comparing model 5 and model 6, we find that the multichoice QA task works better than the MLM task as the pre-training task for the target multi-choice QA task. We hypothesize that, for the MLM task, BERT is required to predict each masked wordpiece (in concepts) independently; whereas, for the multi-choice QA task, BERT is required to model the whole concepts instead of paying much attention to a single wordpiece. Particularly, our approach creates semantically confusing distractors and benefits incorporating commonsense knowledge and learning deep semantics. Comparing model 4 and model 6, we observe that adding the MLM task hurts the performance of BERT CS. This is probably because masking 15% words in questions cause the questions to be far from natural language sentences hence cause a pretrain-finetune discrepancy <ref type="bibr" target="#b27">(Yang et al., 2019)</ref>. In contrast, the MCQA task only masks one concept, which relieves this discrepancy. We hypothesize with these advantages of MCQA over MLM, BERT CS does not degrade the language representation capabilities of BERT models. Among all the models in this analysis, the proposed BERT CS achieves the best performance on the CSQA development set.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Curve</head><p>We investigate the performance from BERT CS on the CSQA development set against the pre-training steps. For every 10,000 training steps, we save the model as the initial model for fine-tuning. For each model, we run experiments for 10 times with the same pre-trained checkpoint but use different random seeds. Due to instability of fine-tuning BERT <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref>, we remove the results that are significantly lower than the mean. <ref type="figure" target="#fig_1">Figure 1</ref> shows the mean and standard deviation of accuracy. The performance of BERT CS base converges around 50,000 training steps while BERT CS large still improves at 100,000 steps, suggesting that BERT CS large is probably more powerful for incorporating commonsense knowledge. We also observe that pre-training with 2 epochs produces worse performance than with 1 epoch, probably due to over-fitting. Pre-training with more QA samples may benefit the BERT CS models and we will explore this in the future work. <ref type="table" target="#tab_11">Table 8</ref> shows several cases from the WSC dataset. Questions 1 and 2 only differ in the words "compassionate" and "cruel". BERT CS large chooses correct answers for both questions while BERT large chooses the same choice "Bill" for both questions. We hypothesize that BERT large tends to choose the closer candidates. To investigate this hypothesis, we split the WSC test set into CLOSE and FAR subsets, based on whether the correct answer is closer or farther to the pronoun than another candidate. We find that BERT CS large achieves the same 82.4% accuracy on the CLOSE set as BERT large and significantly better accuracy on the FAR set than BERT large (68.6% versus 60.6%), suggesting that BERT CS large is probably less influenced by proximity and more focused on semantics. Questions 3 and 4 only differ in "large" and "small". Neither BERT CS large nor BERT large chooses the correct answers, probably due to "suitcase is large" and "trophy is small" being frequent in the pretraining text. Next, we plan to explore reducing the sensitivity to language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a pre-training approach for incorporating commonsense knowledge into language representation models and a method for automatically constructing a multi-choice QA dataset for pretraining. Experiments demonstrate that the proposed approach significantly outperforms SOTA on commonsense-related CSQA and WSC tasks, while maintaining comparable performance on GLUE tasks to the BERT models. In future work, we will use it to incorporate commonsense knowledge into models such as XLNet <ref type="bibr" target="#b27">(Yang et al., 2019)</ref> and RoBERTa .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4)</head><label></label><figDesc>Select distractors by searching (population, AtLocation, * ) in ConceptNet (population, AtLocation, Michigan) (population, AtLocation, Petrie dish) (population, AtLocation, area with people inhabiting) (population, AtLocation, country) 5) Generate a multi-choice question answering sample question: The largest [QW] by population is Birmingham, which has long been the most industrialized city? candidates: city, Michigan, Petrie dish, area with people inhabiting, country</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>BERT CS base and BERT CS large accuracy on the CSQA development set against the number of pre-training steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>A) Some examples from CSQA datasetWhat can eating lunch cause that is painful? headache, gain weight, farts, bad breath, heartburnWhat is the main purpose of having a bath? cleanness, water, exfoliation, hygiene, wetness Where could you find a shark before it was caught? business, marine museum, pool hall, tomales bay, desert</figDesc><table><row><cell>B) Some triples from ConceptNet</cell></row><row><cell>(eating dinner, Causes, heartburn)</cell></row><row><cell>(eating dinner, MotivatedByGoal, not get headache)</cell></row><row><cell>(lunch, Synonym, dinner)</cell></row><row><cell>(have bath, HasSubevent, cleaning)</cell></row><row><cell>(shark, AtLocation, tomales bay)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Some examples from the CSQA dataset shown in Part A and some related triples from ConceptNet shown in Part B. The correct answers in Part A are in boldface.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The detailed procedure of constructing a multichoice question answering sample with the proposed AMS method by masking concept 2 . The * in the fourth step is a wildcard character. The correct answer for the question is underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The statistics of CSQA and WSC datasets.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>BERT base</cell><cell>53.0</cell></row><row><cell>BERT large</cell><cell>56.7</cell></row><row><cell>CoS-E (Rajani et al., 2019)</cell><cell>58.2</cell></row><row><cell>BERT CS base</cell><cell>56.2</cell></row><row><cell>BERT CS large</cell><cell>62.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (%) of different models on the CSQA test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>ModelWSC non-assoc. assoc. unswitched switched consist. WNLI    </figDesc><table><row><cell>Ensemble 14 LMs</cell><cell>63.7</cell><cell>60.6</cell><cell>83.8</cell><cell>63.4</cell><cell>53.4</cell><cell>44.3</cell><cell>-</cell></row><row><cell>Knowledge Hunter</cell><cell>57.1</cell><cell>58.3</cell><cell>50.0</cell><cell>58.8</cell><cell>58.8</cell><cell>90.1</cell><cell>-</cell></row><row><cell>BERT large + MTP</cell><cell>70.3</cell><cell>70.8</cell><cell>67.6</cell><cell>73.3</cell><cell>70.1</cell><cell>59.5</cell><cell>70.5</cell></row><row><cell>Ruan et al. (2019)</cell><cell>71.1</cell><cell>69.5</cell><cell>81.1</cell><cell>74.1</cell><cell>72.5</cell><cell>66.4</cell><cell>-</cell></row><row><cell>Kocijan et al. (2019)</cell><cell>72.2</cell><cell>71.6</cell><cell>75.7</cell><cell>74.8</cell><cell>72.5</cell><cell>61.1</cell><cell>71.9</cell></row><row><cell>BERT large + MCQA</cell><cell>71.4</cell><cell>69.9</cell><cell>81.1</cell><cell>71.8</cell><cell>64.9</cell><cell>82.4</cell><cell>78.5</cell></row><row><cell>BERT CS large + MCQA</cell><cell>75.5</cell><cell>73.7</cell><cell>86.5</cell><cell>74.8</cell><cell>73.3</cell><cell>86.3</cell><cell>83.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Ablation analysis: Model accuracy (%) from different pre-training strategies on the CSQA development set. The source data and pre-training tasks are employed to pre-train BERT CS. MCQA denotes the multi-choice question answering task and MLM denotes the masked language modeling task.</figDesc><table><row><cell>in Sun et al. (2019a), same as Sun et al. (2019a),</cell></row><row><cell>we collect 606,564 triples from ConceptNet and</cell></row><row><cell>construct 1,213,128 questions, each with a correct</cell></row><row><cell>answer and four distractors. This dataset is de-</cell></row><row><cell>noted the TRIPLES dataset. We pre-train BERT</cell></row><row><cell>models on the TRIPLES dataset with the same</cell></row><row><cell>hyper-parameters as the BERT CS models and the</cell></row><row><cell>resulting model is denoted BERT triple.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Question Candidates BERT large BERT CS large 1) Dan had to stop Bill from toying with the injured bird. [He] is very compassionate.</figDesc><table><row><cell></cell><cell>A) Dan B) Bill</cell><cell>B</cell><cell>A</cell></row><row><cell>2) Dan had to stop Bill from toying with the injured bird. [He]</cell><cell>A) Dan B) Bill</cell><cell>B</cell><cell>B</cell></row><row><cell>is very cruel.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3) The trophy doesn't fit into the brown suitcase because [it] is</cell><cell>A) the trophy</cell><cell>B</cell><cell>B</cell></row><row><cell>too large.</cell><cell>B) the suitcase</cell><cell></cell><cell></cell></row><row><cell>4) The trophy doesn't fit into the brown suitcase because [it] is</cell><cell>A) the trophy</cell><cell>A</cell><cell>A</cell></row><row><cell>too small.</cell><cell>B) the suitcase</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Several cases from the WSC dataset. The pronouns in questions are in square brackets. The correct answers and correct model predictions are in boldface.</figDesc><table><row><cell></cell><cell>0.68</cell><cell>base large</cell><cell></cell><cell></cell></row><row><cell>commonsenseQA Dev accuracy</cell><cell>0.60 0.62 0.64 0.66</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.58</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>20000</cell><cell>40000 pretraining steps 60000</cell><cell>80000</cell><cell>100000</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/commonsense/ conceptnet5/wiki</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A knowledge hunting framework for common sense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noelia</forename><forename type="middle">De</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01375</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A surprisingly robust trick for winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Vid Kocijan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana-Maria</forename><surname>Cretu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yordan</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Yordanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06290</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1161</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunderstand-ingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resolving complex cases of definite pronouns: The Winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="777" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Explain yourself! leveraging language models for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Nazneen Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02361</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring unsupervised pretraining and sentence structure modelling for winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ping</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09705</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probing prior knowledge needed in challenging chinese machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09679</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">ERNIE: Enhanced Representation through Knowledge Integration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How reasonable are common-sense reasoning tasks: A case-study on the Winograd schema challenge and SWAG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Trichelair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3380" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<title level="m">ERNIE: Enhanced Language Representation with Informative Entities</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving question answering by commonsense-based pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03568</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Evaluating commonsense in pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11931</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
