<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EfficientNetV2: Smaller Models and Faster Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
						</author>
						<title level="a" type="main">EfficientNetV2: Smaller Models and Faster Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.</p><p>Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose an improved method of progressive learning, which adaptively adjusts regularization (e.g., dropout and data augmentation) along with image size.</p><p>With progressive learning, our EfficientNetV2 significantly outperforms previous models on Im-ageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our Effi-cientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Training efficiency is important to deep learning as the sizes of models and training data are increasingly larger. For example, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, with an unprecedented model and training data sizes, demonstrates the remarkable capability in few shot learning, but it requires weeks of training with thousands of GPUs, making it difficult to retrain or improve.   EfficientNetV2 models are trained with progressive learning. Our EfficientNetV2 trains 5x -11x faster than others, while using up to 6.8x fewer parameters. Details are in <ref type="table" target="#tab_8">Table 7</ref> and <ref type="figure">Figure 5</ref>.</p><p>Training efficiency has gained significant interests recently. For instance, NFNets <ref type="bibr" target="#b4">(Brock et al., 2021)</ref> aim to improve training efficiency by removing the expensive batch normalization; ResNet-RS <ref type="bibr" target="#b2">(Bello et al., 2021)</ref> improves training efficiency by optimizing scaling hyperparameters; Lambda Networks <ref type="bibr" target="#b2">(Bello, 2021)</ref> and BotNet  improve training speed by using attention layers in Con-vNets; Vision Transformers <ref type="bibr" target="#b10">(Dosovitskiy et al., 2021)</ref> improves training efficiency on large-scale datasets by using Transformer blocks. However, these methods often come with expensive overhead on parameter size, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(b).</p><p>In this paper, we use an combination of training-aware neural architecture search (NAS) and scaling to improve both training speed and parameter efficiency. Given the parame-ter efficiency of EfficientNets <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a)</ref>, we start by systematically studying the training bottlenecks in Effi-cientNets. Our study shows in EfficientNets: (1) training with very large image sizes is slow; (2) depthwise convolutions are slow in early layers.</p><p>(3) equally scaling up every stage is sub-optimal. Based on these observations, we design a search space enriched with additional ops such as Fused-MBConv, and apply training-aware NAS and scaling to jointly optimize model accuracy, training speed, and parameter size. Our found networks, named EfficientNetV2, train up to 4x faster than prior models <ref type="figure" target="#fig_3">(Figure 3</ref>), while being up to 6.8x smaller in parameter size.</p><p>Our training can be further sped up by progressively increasing image size during training. Many previous works, such as progressive resizing <ref type="bibr" target="#b16">(Howard, 2018)</ref>, FixRes <ref type="bibr" target="#b40">(Touvron et al., 2019)</ref>, and Mix&amp;Match <ref type="bibr" target="#b15">(Hoffer et al., 2019)</ref>, have used smaller image sizes in training; however, they usually keep the same regularization for all image sizes, causing a drop in accuracy. We argue that keeping the same regularization for different image sizes is not ideal: for the same network, small image size leads to small network capacity and thus requires weak regularization; vice versa, large image size requires stronger regularization to combat overfitting (see Section 4.1). Based on this insight, we propose an improved method of progressive learning: in the early training epochs, we train the network with small image size and weak regularization (e.g., dropout and data augmentation), then we gradually increase image size and add stronger regularization. Built upon progressive resizing <ref type="bibr" target="#b16">(Howard, 2018)</ref>, but by dynamically adjusting regularization, our approach can speed up the training without causing accuracy drop.</p><p>With the improved progressive learning, our EfficientNetV2 achieves strong results on ImageNet, CIFAR-10, CIFAR-100, Cars, and Flowers dataset. On ImageNet, we achieve 85.7% top-1 accuracy while training 3x -9x faster and being up to 6.8x smaller than previous models ( <ref type="figure" target="#fig_1">Figure 1</ref>). Our Ef-ficientNetV2 and progressive learning also make it easier to train models on larger datasets. For example, ImageNet21k <ref type="bibr" target="#b31">(Russakovsky et al., 2015)</ref> is about 10x larger than ImageNet ILSVRC2012, but our EfficientNetV2 can finish the training within two days using moderate computing resources of 32 TPUv3 cores. By pretraining on the public ImageNet21k 2 , our EfficientNetV2 achieves 87.3% top-1 accuracy on Ima-geNet ILSVRC2012, outperforming the recent ViT-L/16 by 2.0% accuracy while training 5x-11x faster <ref type="figure" target="#fig_1">(Figure 1</ref>).</p><p>Our contributions are threefold:</p><p>• We introduce EfficientNetV2, a new family of smaller and faster models. Found by our training-aware NAS and scaling, EfficientNetV2 outperform previous models in both training speed and parameter efficiency. <ref type="bibr">2</ref> We do not compare results on non-public JFT or Instagram .</p><p>• We propose an improved method of progressive learning, which adaptively adjusts regularization along with image size. We show that it speeds up training, and simultaneously improves accuracy.</p><p>• We demonstrate up to 11x faster training speed and up to 6.8x better parameter efficiency on ImageNet, CIFAR, Cars, and Flowers dataset, than prior art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Training and Parameter efficiency: Many works, such as DenseNet <ref type="bibr" target="#b18">(Huang et al., 2017)</ref> and EfficientNet <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a)</ref>, focus on parameter efficiency, aiming to achieve better accuracy with less parameters. More recent works aim to improve training or inference speed instead of parameter efficiency. For example, RegNet <ref type="bibr" target="#b29">(Radosavovic et al., 2020)</ref>, ResNeSt <ref type="bibr" target="#b50">(Zhang et al., 2020)</ref>, TResNet <ref type="bibr" target="#b30">(Ridnik et al., 2020)</ref>, and EfficientNet-X <ref type="bibr" target="#b24">(Li et al., 2021)</ref> focus on GPU and/or TPU inference speed; Lambda Networks <ref type="bibr" target="#b2">(Bello, 2021)</ref>, NFNets <ref type="bibr" target="#b4">(Brock et al., 2021)</ref>, BoTNets , ResNet-RS <ref type="bibr" target="#b2">(Bello et al., 2021)</ref> focus on TPU training speed. However, their training speed often comes with the cost of more parameters. This paper aims to significantly improve both training and parameter efficiency than prior art.</p><p>Progressive Training: Previous works have proposed different kinds of progressive training, which dynamically change the training settings or networks, for GANs <ref type="bibr" target="#b20">(Karras et al., 2018)</ref>, transfer learning <ref type="bibr" target="#b20">(Karras et al., 2018)</ref>, adversarial learning <ref type="bibr" target="#b47">(Yu et al., 2019)</ref>, and language models <ref type="bibr" target="#b28">(Press et al., 2021)</ref>. Progressive resizing <ref type="bibr" target="#b16">(Howard, 2018)</ref> is mostly related to our approach, which aims to improve training speed. However, it usually comes with the cost of accuracy drop. For example, Fastai team use progressive resizing in the DAWNBench competition for fast training, but it has to increase the final image size with higher inference cost to meet the accuracy constraint <ref type="bibr" target="#b16">(Howard, 2018)</ref>. Another closely related work is Mix&amp;Match <ref type="bibr" target="#b15">(Hoffer et al., 2019)</ref>, which randomly sample different image size for each batch. Both progressive resizing and Mix&amp;Match use the same regularization for all image sizes, causing a drop in accuracy. In this paper, our main difference is to adaptively adjust regularization as well so that we can improve both training speed and accuracy. Our approach is also partially inspired by curriculum learning <ref type="bibr" target="#b3">(Bengio et al., 2009)</ref>, which schedules training examples from easy to hard. Our approach also gradually increases learning difficulty by adding more regularization, but we don't selectively pick training examples.</p><p>Neural Architecture Search (NAS): By automating the network design process, NAS has been used to optimize the network architecture for image classification <ref type="bibr" target="#b51">(Zoph et al., 2018)</ref>, object detection <ref type="bibr" target="#b39">Tan et al., 2020)</ref>, segmentation , hyperparameters <ref type="bibr" target="#b9">(Dong et al., 2020)</ref>, and other applications <ref type="bibr" target="#b11">(Elsken et al., 2019)</ref>. Previous NAS works mostly focus on improving FLOPs efficiency <ref type="bibr" target="#b37">(Tan &amp; Le, 2019b;</ref><ref type="bibr">a)</ref> or inference efficiency <ref type="bibr" target="#b6">Cai et al., 2019;</ref><ref type="bibr" target="#b44">Wu et al., 2019;</ref><ref type="bibr" target="#b24">Li et al., 2021)</ref>. Unlike prior works, this paper uses NAS to optimize training and parameter efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EfficientNetV2 Architecture Design</head><p>In this section, we study the training bottlenecks of Efficient-Net <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a)</ref>, and introduce our training-aware NAS and scaling, as well as EfficientNetV2 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of EfficientNet</head><p>EfficientNet <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a</ref>) is a family of models that are optimized for FLOPs and parameter efficiency. It leverages NAS to search for the baseline EfficientNet-B0 model that has better trade-off on accuracy and FLOPs. The baseline model is then scaled up with a simple compound scaling strategy to obtain a family of models B1-B7. While many recent works have claimed large gains on training or inference speed, they are often much worse than EfficientNet in terms of parameters and FLOPs efficiency <ref type="table" target="#tab_1">(Table 1)</ref>. In this paper, we aim to improve the training speed while maintaining the parameter efficiency.  <ref type="bibr" target="#b2">(Bello et al., 2021)</ref> 84.4% 192M 128B NFNet-F1 <ref type="bibr" target="#b4">(Brock et al., 2021)</ref> 84.7% 133M 36B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Understanding Training Efficiency</head><p>We study the training bottlenecks of EfficientNet <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a)</ref>, henceforth is also called EfficientNetV1, and a few simple techniques to improve training speed.</p><p>Training with very large image sizes is slow: As pointed out by previous works <ref type="bibr" target="#b29">(Radosavovic et al., 2020)</ref>, Efficient-Net's large image size results in significant memory usage.</p><p>Since the total memory on GPU/TPU is fixed, we have to train these models with smaller batch size, which drastically slows down the training. A simple improvement is to apply FixRes <ref type="bibr" target="#b40">(Touvron et al., 2019)</ref>, by using a smaller image size for training than for inference. As shown in <ref type="table" target="#tab_2">Table 2</ref>, smaller image size leads to less computations and enables large batch size, and thus improves training speed by up to 2.2x. Notably, as pointed out in <ref type="bibr" target="#b41">(Touvron et al., 2020;</ref><ref type="bibr" target="#b4">Brock et al., 2021)</ref>, using smaller image size for training also leads to slightly better accuracy. But unlike <ref type="bibr" target="#b40">(Touvron et al., 2019)</ref>, we do not finetune any layers after training.</p><p>In Section 4, we will explore a more advanced training approach, by progressively adjusting image size and regu- Depthwise convolutions are slow in early layers: Another training bottleneck of EfficientNet comes from the extensive depthwise convolutions <ref type="bibr" target="#b33">(Sifre, 2014)</ref>. Depthwise convolutions have fewer parameters and FLOPs than regular convolutions, but they often cannot fully utilize modern accelerators. Recently, Fused-MBConv is proposed in <ref type="bibr" target="#b13">(Gupta &amp; Tan, 2019)</ref> and later used in <ref type="bibr" target="#b46">Xiong et al., 2020;</ref><ref type="bibr" target="#b24">Li et al., 2021)</ref> to better utilize mobile or server accelerators. It replaces the depthwise conv3x3 and expansion conv1x1 in MBConv <ref type="bibr" target="#b32">(Sandler et al., 2018;</ref><ref type="bibr" target="#b36">Tan &amp; Le, 2019a</ref>) with a single regular conv3x3, as shown in <ref type="figure" target="#fig_2">Figure  2</ref>. To systematically compares these two building blocks, we gradually replace the original MBConv in EfficientNet-B4 with Fused-MBConv <ref type="table" target="#tab_3">(Table 3)</ref>. When applied in early stage 1-3, Fused-MBConv can improve training speed with a small overhead on parameters and FLOPs, but if we replace all blocks with Fused-MBConv (stage 1-7), then it significantly increases parameters and FLOPs while also slowing down the training. Finding the right combination of these two building blocks, MBConv and Fused-MBConv, is nontrivial, which motivates us to leverage neural architecture search to automatically search for the best combination.  Equally scaling up every stage is sub-optimal: Efficient-Net equally scales up all stages using a simple compound scaling rule. For example, when depth coefficient is 2, then all stages in the networks would double the number of layers. However, these stages are not equally contributed to the training speed and parameter efficiency. In this paper, we will use a non-uniform scaling strategy to gradually add more layers to later stages. In addition, EfficientNets aggressively scale up image size, leading to large memory consumption and slow training. To address this issue, we slightly modify the scaling rule and restrict the maximum image size to a smaller value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training-Aware NAS and Scaling</head><p>To this end, we have learned multiple design choices for improving training speed. To search for the best combinations of those choices, we now propose a training-aware NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS Search:</head><p>Our training-aware NAS framework is largely based on previous NAS works <ref type="bibr" target="#b36">Tan &amp; Le, 2019a)</ref>, but aims to jointly optimize accuracy, parameter efficiency, and training efficiency on modern accelerators. Specifically, we use EfficientNet as our backbone. Our search space is a stage-based factorized space similar to , which consists of the design choices for convolutional operation types {MBConv, Fused-MBConv}, number of layers, kernel size {3x3, 5x5}, expansion ratio {1, 4, 6}. On the other hand, we reduce the search space size by (1) removing unnecessary search options such as pooling skip ops, since they are never used in the original EfficientNets;</p><p>(2) reusing the same channel sizes from the backbone as they are already searched in <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a)</ref>.</p><p>Since the search space is smaller, we can simply apply random search on much larger networks that have comparable size as EfficientNet-B4. Specifically, we sample up to 1000 models and train each model about 10 epochs with reduced image size for training. Our search reward combines the model accuracy A, the normalized training step time S, and the parameter size P , using a simple weighted product A · S w · P v , where w = -0.07 and v = -0.05 are the hyperparameters that are empirically determined to balance the trade-offs similar to .</p><p>EfficientNetV2 Architecture: <ref type="table" target="#tab_4">Table 4</ref> shows the architecture for our searched model EfficientNetV2-S. Compared to the EfficientNet backbone, our searched EfficientNetV2 has several major distinctions: (1) The first difference is EfficientNetV2 extensively uses both MBConv <ref type="bibr" target="#b32">(Sandler et al., 2018;</ref><ref type="bibr" target="#b36">Tan &amp; Le, 2019a)</ref> and the newly added fused-MBConv <ref type="bibr" target="#b13">(Gupta &amp; Tan, 2019)</ref> in the early layers.</p><p>(2) Secondly, EfficientNetV2 prefers smaller expansion ratio for MBConv since smaller expansion ratios tend to have less memory access overhead.</p><p>(3) Thirdly, EfficientNetV2 prefers smaller 3x3 kernel sizes, but it adds more layers to EfficientNetV2 Scaling: We scale up EfficientNetV2-S to obtain EfficientNetV2-M/L using similar compound scaling as <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a)</ref>, with a few additional optimizations:</p><p>(1) we restrict the maximum inference image size to 480, as very large images often lead to expensive memory and training speed overhead;</p><p>(2) as a heuristic, we also gradually add more layers to later stages (e.g., stage 5 and 6 in <ref type="table" target="#tab_4">Table  4</ref>) in order to increase the network capacity without adding much runtime overhead. Training Speed Comparison: <ref type="figure" target="#fig_3">Figure 3</ref> compares the training step time for our new EfficientNetV2, where all models are trained with fixed image size without progressive learning. For EfficientNet <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a)</ref>, we show two curves: one is trained with the original inference size, and the other is trained with about 30% smaller image size, same as NFNet <ref type="bibr" target="#b40">(Touvron et al., 2019;</ref><ref type="bibr" target="#b4">Brock et al., 2021)</ref>. All models are trained with 350 epochs, except NFNets are trained with 360 epochs, so all models have a similar number of training steps. Interestingly, we observe that when trained properly, EfficientNets still achieve pretty strong performance trade-off. More importantly, with our trainingaware NAS and scaling, our proposed EfficientNetV2 model train much faster than the other recent models. These results also align with our inference results as shown in <ref type="table" target="#tab_8">Table 7</ref> and <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Progressive Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motivation</head><p>As discussed in Section 3, image size plays an important role in training efficiency. In addition to FixRes <ref type="bibr" target="#b40">(Touvron et al., 2019)</ref>, many other works dynamically change image sizes during training <ref type="bibr" target="#b16">(Howard, 2018;</ref><ref type="bibr" target="#b15">Hoffer et al., 2019)</ref>, but they often cause a drop in accuracy.</p><p>We hypothesize the accuracy drop comes from the unbalanced regularization: when training with different image sizes, we should also adjust the regularization strength accordingly (instead of using a fixed regularization as in previous works). In fact, it is common that large models require stronger regularization to combat overfitting: for example, EfficientNet-B7 uses larger dropout and stronger data augmentation than the B0. In this paper, we argue that even for the same network, smaller image size leads to smaller network capacity and thus needs weaker regularization; vice versa, larger image size leads to more computations with larger capacity, and thus more vulnerable to overfitting.</p><p>To validate our hypothesis, we train a model, sampled from our search space, with different image sizes and data augmentations <ref type="table">(Table 5)</ref>. When image size is small, it has the best accuracy with weak augmentation; but for larger images, it performs better with stronger augmentation. This insight motivates us to adaptively adjust regularization along with image size during training, leading to our improved method of progressive learning. <ref type="table">Table 5</ref>. ImageNet top-1 accuracy. We use RandAug <ref type="bibr" target="#b8">(Cubuk et al., 2020)</ref>, and report mean and stdev for 3 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size=128</head><p>Size=192 Size=300</p><p>RandAug <ref type="formula">magnitude=5</ref>   <ref type="figure" target="#fig_5">Figure 4</ref> illustrates the training process of our improved progressive learning: in the early training epochs, we train the network with smaller images and weak regularization, such that the network can learn simple representations easily and fast. Then, we gradually increase image size but also making learning more difficult by adding stronger regularization. Our approach is built upon <ref type="bibr" target="#b16">(Howard, 2018</ref>) that progressively changes image size, but here we adaptively adjust regularization as well.   <ref type="formula">(epoch=1)</ref>, and then gradually increase the learning difficulty with larger image sizes and stronger regularization: larger dropout rate, Ran-dAugment magnitude, and mixup ratio (e.g., epoch=300).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Progressive Learning with adaptive Regularization</head><p>Formally, suppose the whole training has N total steps, the target image size is S e , with a list of regularization magnitude Φ e = {φ k e }, where k represents a type of regularization such as dropout rate or mixup rate value. We divide the training into M stages: for each stage 1 ≤ i ≤ M , the model is trained with image size S i and regularization magnitude Φ i = {φ k i }. The last stage M would use the targeted image size S e and regularization Φ e . For simplicity, we heuristically pick the initial image size S 0 and regularization Φ 0 , and then use a linear interpolation to determine the value for each stage. Algorithm 1 summarizes the procedure. At the beginning of each stage, the network will inherit all weights from the previous stage. Unlike transformers, whose weights (e.g., position embedding) may depend on input length, ConvNet weights are independent to image sizes and thus can be inherited easily.  Our improved progressive learning is generally compatible to existing regularization. For simplicity, this paper mainly studies the following three types of regularization:</p><formula xml:id="formula_0">for i = 0 to M − 1 do Image size: Si ← S0 + (Se − S0) · i M −1 Regularization: Ri ← {φ k i = φ k 0 + (φ k e − φ k 0 ) · i M −1 }<label>Train</label></formula><p>• Dropout <ref type="bibr" target="#b35">(Srivastava et al., 2014)</ref>: a network-level regularization, which reduces co-adaptation by randomly dropping channels. We will adjust the dropout rate γ.</p><p>• RandAugment <ref type="bibr" target="#b8">(Cubuk et al., 2020)</ref>: a per-image data augmentation, with adjustable magnitude .</p><p>• Mixup <ref type="bibr" target="#b49">(Zhang et al., 2018)</ref>: a cross-image data augmentation. Given two images with labels (x i , y i ) and (x j , y j ), it combines them with mixup ratio λ:</p><p>x i = λx j + (1 − λ)x i andỹ i = λy j + (1 − λ)y i . We would adjust mixup ratio λ during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Main Results</head><p>This section presents our experimental setups, the main results on ImageNet, and the transfer learning results on CIFAR-10, CIFAR-100, Cars, and Flowers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ImageNet ILSVRC2012</head><p>Setup: ImageNet ILSVRC2012 <ref type="bibr" target="#b31">(Russakovsky et al., 2015)</ref> contains about 1.28M training images and 50,000 validation images with 1000 classes. During architecture search or hyperparameter tuning, we reserve 25,000 images (about 2%) from the training set as minival for accuracy evaluation. We also use minival to perform early stopping. Our ImageNet training settings largely follow EfficientNets <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a)</ref>: RMSProp optimizer with decay 0.9 and momentum 0.9; batch norm momentum 0.99; weight decay 1e-5. Each model is trained for 350 epochs with total batch size 4096. Learning rate is first warmed up from 0 to 0.256, and then decayed by 0.97 every 2.4 epochs. We use exponential moving average with 0.9999 decay rate, RandAugment <ref type="bibr" target="#b8">(Cubuk et al., 2020)</ref>, Mixup <ref type="bibr" target="#b49">(Zhang et al., 2018)</ref>, Dropout <ref type="bibr" target="#b35">(Srivastava et al., 2014)</ref>, and stochastic depth <ref type="bibr" target="#b17">(Huang et al., 2016)</ref> with 0.8 survival probability. For progressive learning, we divide the training process into four stages with about 87 epochs per stage: the early stage uses a small image size with weak regularization, while the later stages use larger image sizes with stronger regularization, as described in Algorithm 1. <ref type="table" target="#tab_7">Table 6</ref> shows the minimum (for the first stage) and maximum (for the last stage) values of image size and regularization. For simplicity, all models use the same minimum values of size and regularization, but they adopt different maximum values, as larger models generally require more regularization to combat overfitting. Following <ref type="bibr" target="#b41">(Touvron et al., 2020)</ref>, our maximum image size for training is about 20% smaller than inference, but we don't finetune any layers after training.</p><p>Results: As shown in <ref type="table" target="#tab_8">Table 7</ref>, our EfficientNetV2 models are significantly faster and achieves better accuracy and parameter efficiency than previous ConvNets and Transformers on ImageNet. In particular, our EfficientNetV2-M achieves comparable accuracy to EfficientNet-B7 while training 11x faster using the same computing resources. Our EfficientNetV2 models also significantly outperform all recent RegNet and ResNeSt, in both accuracy and inference speed. <ref type="figure" target="#fig_1">Figure 1</ref> further visualizes the comparison on train-ing speed and parameter efficiency. Notably, this speedup is a combination of progressive training and better networks, and we will study the individual impact for each of them in our ablation studies.</p><p>Recently, Vision Transformers have demonstrated impressive results on ImageNet accuracy and training speed. However, here we show that properly designed ConvNets with improved training method can still largely outperform vision transformers in both accuracy and training efficiency.</p><p>In particular, our EfficientNetV2-L achieves 85.7% top-1 accuracy, surpassing ViT-L/16(21k), a much larger transformer model pretrained on a larger ImageNet21k dataset.</p><p>Here, ViTs are not well tuned on ImageNet ILSVRC2012; DeiTs use the same architectures as ViTs, but achieve better results by adding more regularization.</p><p>Although our EfficientNetV2 models are optimized for training, they also perform well for inference, because training speed often correlates with inference speed. <ref type="figure">Figure 5</ref> visualizes the model size, FLOPs, and inference latency based on <ref type="table" target="#tab_8">Table 7</ref>. Since latency often depends on hardware and software, here we use the same PyTorch Image Models codebase <ref type="bibr" target="#b43">(Wightman, 2021)</ref> and run all models on the same machine using the batch size 16. In general, our models have slightly better parameters/FLOPs efficiency than Effi-cientNets, but our inference latency is up to 3x faster than EfficientNets. Compared to the recent ResNeSt that are specially optimized for GPUs, our EfficientNetV2-M achieves 0.6% better accuracy with 2.8x faster inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ImageNet21k</head><p>Setup: ImageNet21k <ref type="bibr" target="#b31">(Russakovsky et al., 2015)</ref> contains about 13M training images with 21,841 classes. The original ImageNet21k doesn't have train/eval split, so we reserve randomly picked 100,000 images as validation set and use the remaining as training set. We largely reuse the same training settings as ImageNet ILSVRC2012 with a few changes: (1) we change the training epochs to 60 or 30 to reduce training time, and use cosine learning rate decay that can adapt to different steps without extra tuning;</p><p>(2) since each image has multiple labels, we normalize the labels to have sum of 1 before computing softmax loss. After pretrained on ImageNet21k, each model is finetuned on ILSVRC2012 for 15 epochs using cosine learning rate decay.</p><p>Results: <ref type="table" target="#tab_8">Table 7</ref> shows the performance comparison, where models tagged with 21k are pretrained on Ima-geNet21k and finetuned on ImageNet ILSVRC2012. Compared to the recent ViT-L/16(21k), our EfficientNetV2-L(21k) improves the top-1 accuracy by 1.5% (85.3% vs. 86.8%), using 2.5x fewer parameters and 3.6x fewer FLOPs, while running 6x -7x faster in training and inference.</p><p>We would like to highlight a few interesting observations:  <ref type="bibr" target="#b29">(Radosavovic et al., 2020)</ref> 81.7% 39M 8B 21 -RegNetY-16GF <ref type="bibr" target="#b29">(Radosavovic et al., 2020)</ref> 82.9% 84M 16B 32 -ResNeSt-101 <ref type="bibr" target="#b50">(Zhang et al., 2020)</ref> 83.0% 48M 13B 31 -ResNeSt-200 <ref type="bibr" target="#b50">(Zhang et al., 2020)</ref> 83.9% 70M 36B 76 -ResNeSt-269 <ref type="bibr" target="#b50">(Zhang et al., 2020)</ref> 84.5% 111M 78B 160 -TResNet-L <ref type="bibr" target="#b30">(Ridnik et al., 2020)</ref> 83.8% 56M -45 -TResNet-XL <ref type="bibr" target="#b30">(Ridnik et al., 2020)</ref> 84.3% 78M -66 -EfficientNet-X <ref type="bibr" target="#b24">(Li et al., 2021)</ref> 84.7% 73M 91B --NFNet-F0 <ref type="bibr" target="#b4">(Brock et al., 2021)</ref> 83.6% 72M 12B 30 8.9 NFNet-F1 <ref type="bibr" target="#b4">(Brock et al., 2021)</ref> 84.7% 133M 36B 70 20 NFNet-F2 <ref type="bibr" target="#b4">(Brock et al., 2021)</ref> 85.1% 194M 63B 124 36 NFNet-F3 <ref type="bibr" target="#b4">(Brock et al., 2021)</ref> 85.7% 255M 115B 203 65 NFNet-F4 <ref type="bibr" target="#b4">(Brock et al., 2021)</ref> 85.9% 316M 215B 309 126 ResNet-RS <ref type="bibr" target="#b2">(Bello et al., 2021)</ref> 84.4% 192M 128B -61 LambdaResNet-420-hybrid <ref type="bibr" target="#b2">(Bello, 2021)</ref> 84.9% 125M --67 BotNet-T7-hybrid  84.7% 75M 46B -95 BiT-M-R152x2 (21k) <ref type="bibr" target="#b21">(Kolesnikov et al., 2020)</ref> 85.2% 236M 135B 500 -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision Transformers</head><p>ViT-B/32 <ref type="bibr" target="#b10">(Dosovitskiy et al., 2021)</ref> 73.4% 88M 13B 13 -ViT-B/16 <ref type="bibr" target="#b10">(Dosovitskiy et al., 2021)</ref> 74.9% 87M 56B 68 -DeiT-B (ViT+reg) <ref type="bibr" target="#b42">(Touvron et al., 2021)</ref> 81.8% 86M 18B 19 -DeiT-B-384 (ViT+reg) <ref type="bibr" target="#b42">(Touvron et al., 2021)</ref> 83.1% 86M 56B 68 -T2T-ViT-19 <ref type="bibr" target="#b48">(Yuan et al., 2021)</ref> 81.4% 39M 8.4B --T2T-ViT-24 <ref type="bibr" target="#b48">(Yuan et al., 2021)</ref> 82.2% 64M 13B --ViT-B/16 (21k) <ref type="bibr" target="#b10">(Dosovitskiy et al., 2021)</ref> 84.6% 87M 56B 68 -ViT-L/16 (21k) <ref type="bibr" target="#b10">(Dosovitskiy et al., 2021)</ref> 85 We do not include models pretrained on non-public Instagram/JFT images, or models with extra distillation or ensemble.  <ref type="figure">Figure 5</ref>. Model Size, FLOPs, and Inference Latency -Latency is measured with batch size 16 on V100 GPU. 21k denotes pretrained on ImageNet21k images, others are just trained on ImageNet ILSVRC2012. Our EfficientNetV2 has slightly better parameter efficiency with EfficientNet, but runs 3x faster for inference. • Scaling up data size is more effective than simply scaling up model size in high-accuracy regime: when the top-1 accuracy is beyond 85%, it is very difficult to further improve it by simply increasing model size due to the severe overfitting. However, the extra Im-ageNet21K pretraining can significantly improve accuracy. The effectiveness of large datasets is also observed in previous works <ref type="bibr" target="#b26">(Mahajan et al., 2018;</ref><ref type="bibr" target="#b45">Xie et al., 2020;</ref><ref type="bibr" target="#b10">Dosovitskiy et al., 2021)</ref>.</p><p>• Pretraining on ImageNet21k could be quite efficient.</p><p>Although ImageNet21k has 10x more data, our training approach enables us to finish the pretraining of Effi-cientNetV2 within two days using 32 TPU cores (instead of weeks for ViT <ref type="bibr" target="#b10">(Dosovitskiy et al., 2021)</ref>). This is more effective than training larger models on Ima-geNet. We suggest future research on large-scale models use the public ImageNet21k as a default dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Transfer Learning Datasets</head><p>Setup: We evaluate our models on four transfer learning datasets: CIFAR-10, CIFAR-100, Flowers and Cars. <ref type="table" target="#tab_11">Table  9</ref> includes the statistics of these datasets.  <ref type="bibr" target="#b23">(Krizhevsky &amp; Hinton, 2009)</ref> 50,000 10,000 100 Flowers <ref type="bibr" target="#b27">(Nilsback &amp; Zisserman, 2008)</ref> 2,040 6,149 102 Cars <ref type="bibr">(Krause et al., 2013) 8,144 8,041 196</ref> For this experiment, we use the checkpoints trained on Ima-geNet ILSVRC2012. For fair comparison, no ImageNet21k images are used here. Our finetuning settings are mostly the same as ImageNet training with a few modifications similar to <ref type="bibr" target="#b10">(Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b42">Touvron et al., 2021)</ref>: We use smaller batch size 512, smaller initial learning rate 0.001 with cosine decay. For all datasets, we train each model for fixed 10,000 steps. Since each model is finetuned with very few steps, we disable weight decay and use a simple cutout data augmentation.</p><p>Results: <ref type="table" target="#tab_10">Table 8</ref> compares the transfer learning performance. In general, our models outperform previous Con-vNets and Vision Transformers for all these datasets, sometimes by a non-trivial margin: for example, on CIFAR-100, EfficientNetV2-L achieves 0.6% better accuracy than prior GPipe/EfficientNets and 1.5% better accuracy than prior ViT/DeiT models. These results suggest that our models also generalize well beyond ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Comparison to EfficientNet</head><p>In this section, we will compare our EfficientNetV2 (V2 for short) with EfficientNets (Tan &amp; Le, 2019a) (V1 for short) under the same training and inference settings.</p><p>Performance with the same training: <ref type="table" target="#tab_1">Table 10</ref> shows the performance comparison using the same progressive learning settings. As we apply the same progressive learning to EfficientNet, its training speed (reduced from 139h to 54h) and accuracy (improved from 84.7% to 85.0%) are better than the original paper <ref type="bibr" target="#b36">(Tan &amp; Le, 2019a)</ref>. However, as shown in <ref type="table" target="#tab_1">Table 10</ref>, our EfficientNetV2 models still outperform EfficientNets by a large margin: EfficientNetV2-M reduces parameters by 17% and FLOPs by 37%, while running 4.1x faster in training and 3.1x faster in inference than EfficientNet-B7. Since we are using the same training settings here, we attribute the gains to the EfficientNetV2 architecture.  <ref type="bibr">(-17%)</ref> 24 <ref type="bibr">(-37%)</ref> 13 <ref type="bibr">(-76%)</ref> 57 <ref type="bibr">(-66%)</ref> Scaling Down: Previous sections mostly focus on largescale models. Here we compare smaller models by scaling down our EfficientNetV2-S using similar compound scaling coefficients as EfficientNet. For easy comparison, all models are trained without progressive learning. Compared to these small-size EfficientNets (V1), our new EfficientNetV2 models are generally faster while maintaining comparable parameter efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Progressive Learning for Different Networks</head><p>We ablate the performance of our progressive learning for different networks. <ref type="table" target="#tab_1">Table 12</ref> shows the performance comparison between our progressive training and the baseline training, using the same ResNet and EfficientNet models.</p><p>Here, the baseline ResNets have higher accuracy than the original paper <ref type="bibr" target="#b14">(He et al., 2016)</ref> because they are trained with our improved training settings (see Section 5) using more epochs and better optimizers. We also increase the image size from 224 to 380 for ResNets to further increase the network capacity and accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Importance of Adaptive Regularization</head><p>A key insight from our training approach is the adaptive regularization, which dynamically adjusts regularization according to image size. This paper chooses a simple progressive approach for its simplicity, but it is also a general method that can be combined with other approaches as well. <ref type="table" target="#tab_1">Table 13</ref> studies our adaptive regularization on two training settings: one is to progressively increase image size from small to large <ref type="bibr" target="#b16">(Howard, 2018)</ref>, and the other is to randomly sample a different image size for each batch as proposed in Mix&amp;Match <ref type="bibr" target="#b15">(Hoffer et al., 2019)</ref>. Because TPU needs to recompile the graph for each new size, here we randomly sample a image size every eight epochs instead of every batch. Compared to the vanilla approaches of progressive or random resizing that use the same regularization for all image sizes, our adaptive regularization improves the accuracy by 0.7%. <ref type="figure" target="#fig_8">Figure 6</ref> further compares the training curve for the progressive approach. Our adaptive regularization uses much smaller regularization for small images at the early training epochs, allowing models to converge faster and achieve better final accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper presents EfficientNetV2, a new family of smaller and faster neural networks for image recognition. Optimized with training-aware NAS and model scaling, our Efficient-NetV2 significantly outperforms previous models, while being much faster and more efficient in parameters. To further speed up the training, we propose an improved method of progressive learning, that jointly increases image size and regularization during training. Extensive experiments show our EfficientNetV2 achieves strong results on Ima-geNet, and CIFAR/Flowers/Cars. Compared to EfficientNet and more recent works, our EfficientNetV2 trains up to 11x faster while being up to 6.8x smaller.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Source Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. Design Choices for Progressive Learning</head><p>As shown in Algorithm 1, we need to empirically determine the number of training stages M and the starting image size S 0 . <ref type="table" target="#tab_1">Table 14</ref> shows the training time and ImageNet top-1 accuracy for different design choices of M and S 0 . Overall, the learning is pretty robust to different design choices: for all 4/8/16 training stages with 128/256 initial image sizes, their accuracies are comparable. However, those different choices would lead to different training cost: larger initial image size would increase the training time as larger images require more computations; more training stages will also slightly increase the training time, as we need to recompile the model graph for each stage in our current TensorFlow framework <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. In this paper, we use stages M = 4 and initial image size S 0 = 128 in default for all our experiments. <ref type="table" target="#tab_1">Table 14</ref>. Training stage and initial image size choices -Large initial image sizes or more training stages would increase the training time but doesn't improve accuracy. Overall, our improved method of progressive learning is robust to different choices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3. Latency Measurements</head><p>Properly comparing the runtime latency is non-trivial, as latency often depends on the underlining hardware, software, and implementations. We observe that even for the same model, there are often large discrepancy between different papers. Here, we have tried out best to measure the inference latency for all models using the same Pytorch Image Models codebase <ref type="bibr" target="#b43">(Wightman, 2021)</ref>, and the same hardware/software on the same machine. Despite that, we would like to point out that the latency numbers may still vary for different hardware/software/implementations. Batch size is another often overlooked factor. We observe that different models can have different behavior when batch size changes. <ref type="table" target="#tab_1">Table 15</ref> shows a few model examples: compared to NFNet and EfficientNetV2-M, the latency for EfficientNet-B7 becomes significantly worse for large batch size. We suspect this is because EfficientNet-B7 uses much larger image size, leading to expensive memory access overhead. This paper follows previous works <ref type="bibr" target="#b50">(Zhang et al., 2020)</ref> and uses batch size 16 in default unless explicitly specified.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>ImageNet ILSVRC2012 top-1 Accuracy vs. Training Time and Parameters -Models tagged with 21k are pretrained on ImageNet21k, and others are directly trained on ImageNet ILSVRC2012. Training time is measured with 32 TPU cores. All</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Structure of MBConv and Fused-MBConv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>ImageNet accuracy and training step time on TPUv3 -Lower step time is better; all models are trained with fixed image size without progressive learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Training process in our improved progressive learning -It starts with small image size and weak regularization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1</head><label>1</label><figDesc>Progressive learning with adaptive regularization. Input: Initial image size S0 and regularization {φ k 0 }. Input: Final image size Se and regularization {φ k e }. Input: Number of total training steps N and stages M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>GPU V100 Latency (batch 16)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Training curve comparison -Our adaptive regularization converges faster and achieves better final accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7</head><label>7</label><figDesc>provides the original Panda and Snoek images used for mixup inFigure 4. These two images are first augmented with RandAug and then combined with Mixup to generate the final image inFigure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Source files for the mixup of Panda and Snoek.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>EfficientNets have good parameter and FLOPs efficiency.</figDesc><table><row><cell></cell><cell cols="3">Top-1 Acc. Params FLOPs</cell></row><row><cell>EfficientNet-B6 (Tan &amp; Le, 2019a)</cell><cell>84.3%</cell><cell>43M</cell><cell>19B</cell></row><row><cell>ResNet-RS-420</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>EfficientNet-B6 accuracy and training throughput for different batch sizes and image size.</figDesc><table><row><cell></cell><cell cols="3">TPUv3 imgs/sec/core</cell><cell cols="2">V100 imgs/sec/gpu</cell></row><row><cell></cell><cell cols="5">Top-1 Acc. batch=32 batch=128 batch=12 batch=24</cell></row><row><cell>train size=512</cell><cell>84.3%</cell><cell>42</cell><cell>OOM</cell><cell>29</cell><cell>OOM</cell></row><row><cell>train size=380</cell><cell>84.6%</cell><cell>76</cell><cell>93</cell><cell>37</cell><cell>52</cell></row><row><cell cols="2">larization during training.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Replacing MBConv with Fused-MBConv. No fused denotes all stages use MBConv, Fused stage1-3 denotes replacing MBConv with Fused-MBConv in stage {2, 3, 4}.</figDesc><table><row><cell></cell><cell cols="3">Params FLOPs Top-1</cell><cell>TPU</cell><cell>V100</cell></row><row><cell></cell><cell>(M)</cell><cell>(B)</cell><cell cols="3">Acc. imgs/sec/core imgs/sec/gpu</cell></row><row><cell>No fused</cell><cell>19.3</cell><cell cols="2">4.5 82.8%</cell><cell>262</cell><cell>155</cell></row><row><cell>Fused stage1-3</cell><cell>20.0</cell><cell cols="2">7.5 83.1%</cell><cell>362</cell><cell>216</cell></row><row><cell>Fused stage1-5</cell><cell>43.4</cell><cell cols="2">21.3 83.1%</cell><cell>327</cell><cell>223</cell></row><row><cell>Fused stage1-7</cell><cell>132.0</cell><cell cols="2">34.4 81.7%</cell><cell>254</cell><cell>206</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>EfficientNetV2-S architecture -MBConv and Fused-MBConv blocks are described inFigure 2.</figDesc><table><row><cell>Stage</cell><cell>Operator</cell><cell cols="3">Stride #Channels #Layers</cell></row><row><cell>0</cell><cell>Conv3x3</cell><cell>2</cell><cell>24</cell><cell>1</cell></row><row><cell>1</cell><cell>Fused-MBConv1, k3x3</cell><cell>1</cell><cell>24</cell><cell>2</cell></row><row><cell>2</cell><cell>Fused-MBConv4, k3x3</cell><cell>2</cell><cell>48</cell><cell>4</cell></row><row><cell>3</cell><cell>Fused-MBConv4, k3x3</cell><cell>2</cell><cell>64</cell><cell>4</cell></row><row><cell>4</cell><cell>MBConv4, k3x3, SE0.25</cell><cell>2</cell><cell>128</cell><cell>6</cell></row><row><cell>5</cell><cell>MBConv6, k3x3, SE0.25</cell><cell>1</cell><cell>160</cell><cell>9</cell></row><row><cell>6</cell><cell>MBConv6, k3x3, SE0.25</cell><cell>2</cell><cell>272</cell><cell>15</cell></row><row><cell>7</cell><cell>Conv1x1 &amp; Pooling &amp; FC</cell><cell>-</cell><cell>1792</cell><cell>1</cell></row><row><cell cols="5">compensate the reduced receptive field resulted from the</cell></row><row><cell cols="5">smaller kernel size. (4) Lastly, EfficientNetV2 completely</cell></row><row><cell cols="5">removes the last stride-1 stage in the original EfficientNet,</cell></row><row><cell cols="5">perhaps due to its large parameter size and memory access</cell></row><row><cell cols="2">overhead.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>78.3 ±0.16 81.2 ±0.06 82.5 ±0.05 RandAug magnitude=10 78.0 ±0.08 81.6 ±0.08 82.7 ±0.08 RandAug magnitude=15 77.7 ±0.15 81.5 ±0.05 83.2 ±0.09</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>the model for N M steps with Si and Ri. end for</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Progressive training settings for EfficientNetV2.</figDesc><table><row><cell></cell><cell>S</cell><cell></cell><cell>M</cell><cell></cell><cell>L</cell><cell></cell></row><row><cell></cell><cell cols="6">min max min max min max</cell></row><row><cell>Image Size</cell><cell cols="6">128 300 128 380 128 380</cell></row><row><cell>RandAugment</cell><cell>5</cell><cell>15</cell><cell>5</cell><cell>20</cell><cell>5</cell><cell>25</cell></row><row><cell>Mixup alpha</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell>0</cell><cell>0.4</cell></row><row><cell>Dropout rate</cell><cell>0.1</cell><cell>0.3</cell><cell>0.1</cell><cell>0.4</cell><cell>0.1</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>EfficientNetV2 Performance Results on ImageNet<ref type="bibr" target="#b31">(Russakovsky et al., 2015)</ref> -Infer-time is measured on V100 GPU FP16 with batch size 16 using the same codebase<ref type="bibr" target="#b43">(Wightman, 2021)</ref>; Train-time is the total training time normalized for 32 TPU cores. Models marked with 21k are pretrained on ImageNet21k with 13M images, and others are directly trained on ImageNet ILSVRC2012 with 1.28M images from scratch. All EfficientNetV2 models are trained with our improved method of progressive learning.</figDesc><table><row><cell>Model</cell><cell cols="5">Top-1 Acc. Params FLOPs Infer-time(ms) Train-time (hours)</cell></row><row><cell>EfficientNet-B3 (Tan &amp; Le, 2019a)</cell><cell>81.5%</cell><cell>12M</cell><cell>1.9B</cell><cell>19</cell><cell>10</cell></row><row><cell>EfficientNet-B4 (Tan &amp; Le, 2019a)</cell><cell>82.9%</cell><cell>19M</cell><cell>4.2B</cell><cell>30</cell><cell>21</cell></row><row><cell>EfficientNet-B5 (Tan &amp; Le, 2019a)</cell><cell>83.7%</cell><cell>30M</cell><cell>10B</cell><cell>60</cell><cell>43</cell></row><row><cell>EfficientNet-B6 (Tan &amp; Le, 2019a)</cell><cell>84.3%</cell><cell>43M</cell><cell>19B</cell><cell>97</cell><cell>75</cell></row><row><cell>EfficientNet-B7 (Tan &amp; Le, 2019a)</cell><cell>84.7%</cell><cell>66M</cell><cell>38B</cell><cell>170</cell><cell>139</cell></row><row><cell>RegNetY-8GF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ConvNets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&amp; Hybrid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Transfer Learning Performance Comparison -All models are pretrained on ImageNet ILSVRC2012 and finetuned on downstream datasets. Transfer learning accuracy is averaged over five runs.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="5">Params ImageNet Acc. CIFAR-10 CIFAR-100 Flowers</cell><cell>Cars</cell></row><row><cell>ConvNets</cell><cell>GPipe (Huang et al., 2019) EfficientNet-B7 (Tan &amp; Le, 2019a)</cell><cell>556M 66M</cell><cell>84.4 84.7</cell><cell>99.0 98.9</cell><cell>91.3 91.7</cell><cell>98.8 98.8</cell><cell>94.7 94.7</cell></row><row><cell></cell><cell>ViT-B/32 (Dosovitskiy et al., 2021)</cell><cell>88M</cell><cell>73.4</cell><cell>97.8</cell><cell>86.3</cell><cell>85.4</cell><cell>-</cell></row><row><cell></cell><cell>ViT-B/16 (Dosovitskiy et al., 2021)</cell><cell>87M</cell><cell>74.9</cell><cell>98.1</cell><cell>87.1</cell><cell>89.5</cell><cell>-</cell></row><row><cell>Vision</cell><cell>ViT-L/32 (Dosovitskiy et al., 2021)</cell><cell>306M</cell><cell>71.2</cell><cell>97.9</cell><cell>87.1</cell><cell>86.4</cell><cell>-</cell></row><row><cell>Transformers</cell><cell>ViT-L/16 (Dosovitskiy et al., 2021)</cell><cell>306M</cell><cell>76.5</cell><cell>97.9</cell><cell>86.4</cell><cell>89.7</cell><cell>-</cell></row><row><cell></cell><cell>DeiT-B (ViT+regularization) (Touvron et al., 2021)</cell><cell>86M</cell><cell>81.8</cell><cell>99.1</cell><cell>90.8</cell><cell>98.4</cell><cell>92.1</cell></row><row><cell></cell><cell>DeiT-B-384 (ViT+regularization) (Touvron et al., 2021)</cell><cell>86M</cell><cell>83.1</cell><cell>99.1</cell><cell>90.8</cell><cell>98.5</cell><cell>93.3</cell></row><row><cell>ConvNets (ours)</cell><cell>EfficientNetV2-S EfficientNetV2-M EfficientNetV2-L</cell><cell>24M 55M 121M</cell><cell>83.2 85.1 85.7</cell><cell cols="2">98.7±0.04 91.5±0.11 99.0±0.08 92.2±0.08 99.1±0.03 92.3±0.13</cell><cell cols="2">97.9±0.13 93.8±0.11 98.5±0.08 94.6±0.10 98.8±0.05 95.1±0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Transfer learning datasets.</figDesc><table><row><cell></cell><cell cols="3">Train images Eval images Classes</cell></row><row><cell>CIFAR-10 (Krizhevsky &amp; Hinton, 2009)</cell><cell>50,000</cell><cell>10,000</cell><cell>10</cell></row><row><cell>CIFAR-100</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Comparison with the same training settings -Our new EfficientNetV2-M runs faster with less parameters.</figDesc><table><row><cell></cell><cell>Acc. Params</cell><cell>FLOPs</cell><cell cols="2">TrainTime InferTime</cell></row><row><cell></cell><cell>(%) (M)</cell><cell>(B)</cell><cell>(h)</cell><cell>(ms)</cell></row><row><cell>V1-B7</cell><cell>85.0 66</cell><cell>38</cell><cell>54</cell><cell>170</cell></row><row><cell cols="2">V2-M (ours) 85.1 55</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 .</head><label>11</label><figDesc>Scaling down model size -We measure the inference throughput on V100 FP16 GPU with batch size 128.</figDesc><table><row><cell></cell><cell cols="3">Top-1 Acc. Parameters Throughput (imgs/sec)</cell></row><row><cell>V1-B1</cell><cell>79.0%</cell><cell>7.8M</cell><cell>2675</cell></row><row><cell>V2-7M</cell><cell>78.7%</cell><cell>7.4M</cell><cell>(2.1x) 5739</cell></row><row><cell>V1-B2</cell><cell>79.8%</cell><cell>9.1M</cell><cell>2003</cell></row><row><cell>V2-8M</cell><cell>79.8%</cell><cell>8.1M</cell><cell>(2.0x) 3983</cell></row><row><cell>V1-B4</cell><cell>82.9%</cell><cell>19M</cell><cell>628</cell></row><row><cell>V2-14M</cell><cell>82.1%</cell><cell>14M</cell><cell>(2.7x) 1693</cell></row><row><cell>V1-B5</cell><cell>83.7%</cell><cell>30M</cell><cell>291</cell></row><row><cell>V2-S</cell><cell>83.6%</cell><cell>24M</cell><cell>(3.1x) 901</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 .</head><label>12</label><figDesc>Progressive learning for ResNets and EfficientNets -(224) and (380) denote the targeted inference image size. Our progressive training improves both the accuracy and training time for all different networks.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell></cell><cell cols="2">Progressive</cell></row><row><cell></cell><cell cols="3">Acc.(%) TrainTime Acc.(%)</cell><cell>TrainTime</cell></row><row><cell>ResNet50 (224)</cell><cell>78.1</cell><cell>4.9h</cell><cell>78.4</cell><cell>3.5h (-29%)</cell></row><row><cell>ResNet50 (380)</cell><cell>80.0</cell><cell>14.3h</cell><cell>80.3</cell><cell>5.8h (-59%)</cell></row><row><cell>ResNet152 (380)</cell><cell>82.4</cell><cell>15.5h</cell><cell>82.9</cell><cell>7.2h (-54%)</cell></row><row><cell>EfficientNet-B4</cell><cell>82.9</cell><cell>20.8h</cell><cell>83.1</cell><cell>9.4h (-55%)</cell></row><row><cell>EfficientNet-B5</cell><cell>83.7</cell><cell>42.9h</cell><cell cols="2">84.0 15.2h (-65%)</cell></row><row><cell cols="5">As shown in Table 12, our progressive learning generally</cell></row><row><cell cols="5">reduces the training time and meanwhile improves the accu-</cell></row><row><cell cols="5">racy for all different networks. Not surprisingly, when the</cell></row><row><cell cols="5">default image size is very small, such as ResNet50(224) with</cell></row><row><cell cols="5">224x224 size, the training speedup is limited (1.4x speedup);</cell></row><row><cell cols="5">however, when the default image size is larger and the model</cell></row><row><cell cols="5">is more complex, our approach achieves larger gains on ac-</cell></row><row><cell cols="5">curacy and training efficiency: for ResNet152(380), our ap-</cell></row><row><cell cols="5">proach improves speed up the training by 2.1x with slightly</cell></row><row><cell cols="5">better accuracy; for EfficientNet-B4, our approach improves</cell></row><row><cell cols="2">speed up the training by 2.2x.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 .</head><label>13</label><figDesc>Adaptive regularization -We compare ImageNet top-1 accuracy based on the average of three runs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Vanilla</cell><cell>+our adaptive reg</cell></row><row><cell cols="5">Progressive resize (Howard, 2018) 84.3±0.14 85.1±0.07 (+0.8)</cell></row><row><cell cols="5">Random resize (Hoffer et al., 2019) 83.5±0.11 84.2±0.10 (+0.7)</cell></row><row><cell>ImageNet Top-1 Accuracy (%)</cell><cell>0 20 40 60 80</cell><cell></cell><cell cols="2">progressive resize progressive resize + adaptive reg</cell></row><row><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Training epochs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 .</head><label>15</label><figDesc>Throughtput (imgs/sec/gpu) comparison (higher is better). All models have similar ImageNet top-1 accuracy.</figDesc><table><row><cell></cell><cell cols="5">batch=1 batch=4 batch=16 batch=64 batch=256</cell></row><row><cell cols="2">EfficientNetV2-M 23</cell><cell>91</cell><cell>281</cell><cell>350</cell><cell>352</cell></row><row><cell>EfficientNet-B7</cell><cell>24</cell><cell>80</cell><cell>94</cell><cell>108</cell><cell>OOM</cell></row><row><cell>NFNet-F2</cell><cell>16</cell><cell>62</cell><cell>129</cell><cell>189</cell><cell>190</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Google Research, Brain Team. Correspondence to: Mingxing Tan &lt;tanmingxing@google.com&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Ruoming Pang, Sheng Li, Andrew Li, Hanxiao Liu, Zihang Dai, Neil Houlsby, Thang Luong, Daiyi Peng, Yifeng Lu, Da Huang, Chen Liang, Aravind Srinivas, Irwan Bello, Max Moroz, Futang Peng, and the Google Brain team for their help and feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorflow</surname></persName>
		</author>
		<title level="m">A system for large-scale machine learning. OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07579</idno>
		<title level="m">Revisiting resnets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Curriculum learning. ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Highperformance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners. NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proxylessnas</surname></persName>
		</author>
		<title level="m">Direct neural architecture search on target task and hardware. ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detnas</surname></persName>
		</author>
		<title level="m">Neural architecture search on object detection. NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gabrys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autohas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03656</idno>
		<title level="m">Efficient hyperparameter and architecture search</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Accelerator-aware neural network design using automl. On-device Intelligence Workshop in SysML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Akin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet-Edgetpu</surname></persName>
		</author>
		<ptr target="https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html" />
		<title level="m">Creating accelerator-optimized neural networks with automl</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mix &amp; match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08986</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Training imagenet in 3 hours for 25 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<ptr target="https://www.fast.ai/2018/04/30/dawnbench-fastai/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Deep networks with stochastic depth. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpipe</surname></persName>
		</author>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism. NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Collecting a large-scale dataset of fine-grained cars. Second Workshop on Fine-Grained Visual Categorizatio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EfficientNetV2: Smaller Models and Faster Training</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05610</idno>
		<title level="m">Searching for fast model families on datacenter accelerators</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00932</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICVGIP</title>
		<imprint>
			<biblScope unit="page" from="722" to="729" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shortformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15832</idno>
		<title level="m">Better language modeling using shorter inputs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tresnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13630</idno>
		<title level="m">High performance gpudedicated architecture</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis section 6.2</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<title level="m">Rethinking model scaling for convolutional neural networks. ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixconv</surname></persName>
		</author>
		<title level="m">Mixed depthwise convolutional kernels. BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<title level="m">Platform-aware neural architecture search for mobile. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientdet</surname></persName>
		</author>
		<title level="m">Scalable and efficient object detection. CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06423</idno>
		<title level="m">Fixing the train-test resolution discrepancy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237</idno>
		<title level="m">Fixing the train-test resolution discrepancy: Fixefficientnet</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pytorch image model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2021-02-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fbnet: Hardwareaware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Selftraining with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Mobiledets: Searching for object detection architectures for mobile accelerators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14525</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04839</idno>
		<title level="m">Progressive data augmentation for general robustness of deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
		<title level="m">Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnest</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
