<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An All-In-One Convolutional Neural Network for Face Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
							<email>rranjan1@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
							<email>swamiviv@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
							<email>carlos@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An All-In-One Convolutional Neural Network for Face Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a multi-purpose algorithm for simultaneous face detection, face alignment, pose estimation, gender recognition, smile detection, age estimation and face recognition using a single deep convolutional neural network (CNN). The proposed method employs a multi-task learning framework that regularizes the shared parameters of CNN and builds a synergy among different domains and tasks. Extensive experiments show that the network has a better understanding of face and achieves state-of-the-art result for most of these tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Face analysis is a challenging and actively researched problem with applications to face recognition, emotion analysis, biometrics security, etc. Though the performance of few challenging face analysis tasks such as unconstrained face detection and face verification have greatly improved when CNN-based methods are used, other tasks such as face alignment, head-pose estimation, gender and smile recognition are still challenging due to lack of large publicly available training data. Furthermore, all these tasks have been approached as separate problems, which makes their integration into end-to-end systems inefficient. For example, a typical face recognition system needs to detect and align a face from the given image before determining the identity. This results in error accumulation across different modules. Even though the above mentioned tasks are correlated, existing methods do not leverage the synergy among them. It has been shown recently that jointly learning correlated tasks can boost the performance of individual tasks <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this paper, we present a novel CNN model that simultaneously solves the tasks of face detection, landmark localization, pose estimation, gender recognition, smile detection, age estimation and face verification and recognition (see <ref type="figure" target="#fig_0">Fig. 1</ref>). We choose this set of tasks since they span a wide range of applications. We train a CNN jointly in a multi-task learning (MTL) framework (Caruana <ref type="bibr" target="#b2">[3]</ref>), such that parameters from lower layers of CNN are shared among all the tasks. In this way, the lower layers learn general representation common to all the tasks, whereas upper layers are more specific to the given task, which reduces over-fitting in the shared layers. Thus, our model is able to learn robust features for distinct tasks. Employing multiple tasks enables the network to learn the correlations between data from different distributions in an effective way. This approach saves both time and memory in an end-to-end system, since it can simultaneously solve the tasks and requires the storage of a single CNN model instead of separate CNN for each task. To the best of our knowledge, this is the first work which simultaneously solves a diverse set of face analysis tasks using a single CNN in an end-to-end manner.</p><p>We initialize our network with the CNN model trained for face recognition task by Sankaranarayanan et al. <ref type="bibr" target="#b40">[41]</ref>. We argue that a network pre-trained on face recognition task possesses fine-grained information of a face which can be used to train other face-related tasks efficiently. Taskspecific sub-networks are branched out from different layers of this network depending on whether they rely on local or global information of the face. The complete network, when trained end-to-end, significantly improves the face recognition performance as well as other face analysis tasks. This paper makes the following contributions.</p><p>1) We propose a novel CNN architecture that simultaneously performs face detection, landmarks localization, pose estimation, gender recognition, smile detection, age estimation and face identification and verification. 2) We design a MTL framework for training, which regularizes the parameters of the network. 3) We achieve state-of-the-art performances on challenging unconstrained datasets for most of these tasks. Ranjan et al. <ref type="bibr" target="#b35">[36]</ref> recently proposed HyperFace that simultaneously performs the tasks of face detection, landmarks localization, pose estimation and gender recognition. The approach in this paper is different from HyperFace in the following aspects. Firstly, we additionally solve for the tasks of smile detection, age estimation and face recognition. Secondly, our MTL framework utilizes domain-based regularization by training on multiple datasets whereas Hyper-Face trains only on AFLW <ref type="bibr" target="#b23">[24]</ref>. Finally, we initialize our network with weights from face recognition task <ref type="bibr" target="#b40">[41]</ref> which provides a more robust and domain specific initialization while HyperFace network is initialized using the weights from AlexNet <ref type="bibr" target="#b24">[25]</ref>. This paper is organized as follows. Section II reviews closely related works. Section III describes the proposed algorithm in detail. Section IV provides the results of our method on challenging datasets for all the tasks. Finally, Section V concludes the paper with a brief discussion and future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Multitask learning was first analyzed in detail by Caruana <ref type="bibr" target="#b2">[3]</ref>. Since then, several approaches have used MTL for solving many problems in Computer Vision. One of the earlier methods for jointly learning face detection, landmarks localization and pose estimation was proposed in <ref type="bibr" target="#b58">[59]</ref> which was later extended to <ref type="bibr" target="#b59">[60]</ref>. It used a mixture of trees model with shared pool of parts, where a part represents a landmark location. Recently, several methods have incorporated the MTL framework with deep CNNs to train face-related tasks. Levi et al. <ref type="bibr" target="#b26">[27]</ref> proposed a CNN for simultaneous age and gender estimation. HyperFace <ref type="bibr" target="#b35">[36]</ref> trained a MTL network for face detection, landmarks localization, pose and gender estimation by fusing the intermediate layers of CNN for improved feature extraction. Ehrlich et al. <ref type="bibr" target="#b9">[10]</ref> proposed a multi-task restricted Boltzmann machine to learn facial attributes, while Zhang et al. <ref type="bibr" target="#b55">[56]</ref> improved landmarks localization by training it jointly with head-pose estimation and facial attribute inference. Although these methods perform MTL on small set of tasks, they do not allow training a large set of correlated tasks as proposed in this paper.</p><p>Significant research has been undertaken for improving individual face analysis tasks. Recent methods for face detection based on deep CNNs such as DP2MFD <ref type="bibr" target="#b34">[35]</ref>, Faceness <ref type="bibr" target="#b49">[50]</ref>, Hyperface <ref type="bibr" target="#b35">[36]</ref>, Faster-RCNN <ref type="bibr" target="#b19">[20]</ref>, etc., have significantly outperformed traditional approaches like TSM <ref type="bibr" target="#b58">[59]</ref> and NDPFace <ref type="bibr" target="#b29">[30]</ref>.</p><p>Only a handful of methods have used deep CNNs for face alignment tasks <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b35">[36]</ref>, due to lack of sufficient training data. Existing methods for landmark localization have focused mainly on near-frontal faces <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b21">[22]</ref> where all the essential keypoints are visible. Recent methods such as PIFA <ref type="bibr" target="#b20">[21]</ref>, 3DDFA <ref type="bibr" target="#b57">[58]</ref>, HyperFace <ref type="bibr" target="#b35">[36]</ref> and CCL <ref type="bibr" target="#b56">[57]</ref> have explored face alignment over varying pose angles.</p><p>The task of pose estimation is to infer orientation of a person's head relative to the camera. Not much research has been carried out to solve this task for unconstrained images other than TSM <ref type="bibr" target="#b58">[59]</ref>, FaceDPL <ref type="bibr" target="#b59">[60]</ref> and HyperFace <ref type="bibr" target="#b35">[36]</ref>.</p><p>The tasks of gender and smile classification from unconstrained images have been considered as a part of facial attribute inference. Recently, Liu et al. <ref type="bibr" target="#b30">[31]</ref> released CelebA dataset containing about 200, 000 near-frontal images with 40 attributes including gender and smile, which accelerated the research in this field <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Faces of the world <ref type="bibr" target="#b12">[13]</ref> challenge dataset further advanced the research on these tasks for faces with varying scale, illumination and pose <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Age Estimation is the task of finding the real or apparent age of a person based on their face image. Few methods have already surpassed human error for the apparent age estimation challenge <ref type="bibr" target="#b11">[12]</ref> using deep CNNs <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Face Verification is the task of predicting whether a pair of faces belong to the same person. Recent methods such as DeepFace <ref type="bibr" target="#b42">[43]</ref>, Facenet <ref type="bibr" target="#b41">[42]</ref>, VGG-Face <ref type="bibr" target="#b33">[34]</ref> have significantly improved the verification accuracy on the LFW <ref type="bibr" target="#b16">[17]</ref> dataset by training deep CNN models on millions of annotated data. However, it is still a challenging problem for unconstrained faces with large variations in viewpoint and illumination (IJB-A [23] dataset). We address this issue by regularizing the CNN parameters using the MTL framework, with only half-a-million samples (CASIA <ref type="bibr" target="#b50">[51]</ref>) for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>We propose a multi-purpose CNN which can simultaneously detect faces, extract key-points and pose angles, determine smile expression, age and gender from any unconstrained image of a face. Additionally, it assigns an identity descriptor to each face which can be used for face recognition and verification. The proposed algorithm is trained in a MTL framework which builds a synergy among different face related tasks improving the performance for each of them. In this section we discuss the advantages of MTL in the context of face analysis and provide the details of the network design, training and testing procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-task Learning</head><p>Typically, a face analysis task requires a cropped face region as the input. The deep CNN processes the face to obtain a representation and extract meaningful information related to the task. According to <ref type="bibr" target="#b51">[52]</ref>, lower layers of CNN learn features common to a general set of face analysis tasks whereas upper layers are more specific to individual tasks. Therefore, we share the parameters of lower layers of CNN among different tasks to produce a generic face representation which is subsequently processed by the taskspecific layers to generate the required outputs ( <ref type="figure" target="#fig_1">Fig. 2</ref>). Goodfellow et al. <ref type="bibr" target="#b13">[14]</ref> interprets MTL as a regularization methodology for deep CNNs. The MTL approach used in our framework can be explained by following two types of regularization.   <ref type="bibr" target="#b50">[51]</ref> Identification, Gender 490,356 MORPH <ref type="bibr" target="#b38">[39]</ref> Age, Gender 55,608 IMDB+WIKI <ref type="bibr" target="#b39">[40]</ref> Age, Gender 224,840 Adience <ref type="bibr" target="#b26">[27]</ref> Age 19,370 CelebA <ref type="bibr" target="#b30">[31]</ref> Smile, Gender 182,637 AFLW <ref type="bibr" target="#b23">[24]</ref> Detection, Pose, Fiducials 20,342 Total 993,153 1) Task-based Regularization: Let the cost function for a given task t i with shared parameters θ s and task-specific parameters θ ti be J i (θ s , θ ti ; D), where D is the input data. For isolated learning, the optimum network parameters (θ * s , θ * ti ) can be computed using (1)</p><formula xml:id="formula_0">(θ * s , θ * ti ) = arg min (θs,θt i ) J i (θ s , θ ti ; D)<label>(1)</label></formula><p>For MTL, the optimal parameters for the task t i can be obtained by minimizing the weighted sum of loss functions for each task, as shown in <ref type="formula">(2)</ref>. The loss weight for task t i is denoted by α i .</p><formula xml:id="formula_1">(θ * s , θ * ti ) = arg min (θs,θt i ) α i J i (θ s , θ ti ; D) + n j =i α j J j (θ s , θ tj ; D)</formula><p>(2) Since other tasks contribute only to the learning of shared parameters, they can be interpreted as a regularizer R i on θ s with respect to the given task t i , as shown in <ref type="formula">(3)</ref>. Thus, MTL shrinks the solution space of θ s such that the learned parameter vector is in consensus with all the tasks, thus reducing over-fitting and enabling the optimization procedure to find a more robust solution.</p><formula xml:id="formula_2">(θ * s , θ * ti ) = arg min (θs,θt i ) J i (θ s , θ ti ; D) + λR i (θ s ; D) (3)</formula><p>2) Domain-based Regularization: For face analysis tasks, we do not have a large dataset with annotations for face bounding box, fiducial points, pose, gender, age, smile and identity information available. Hence, we adopt the approach of training multiple CNNs with respect to task-related datasets D i , and share the parameters among them. In this way, the shared parameter θ s adapts to the complete set of domains (D 1 , D 2 , ...D d ) instead of fitting to a task-specific domain. Additionally, the total number of training samples increases to roughly one-million, which is advantageous for training deep CNNs. <ref type="table" target="#tab_0">Table I</ref> lists the different datasets used for training our all-in-one CNN, along with their respective tasks and sample sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>The all-in-one CNN architecture is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. We start with the pre-trained face identification network from Sankaranarayanan et al. <ref type="bibr" target="#b40">[41]</ref>. The network consists of seven convolutional layers followed by three fully connected layers. We use it as a backbone network for training the face identification task and sharing the parameters from its first six convolution layers with other face-related tasks. Parametric Rectifier Linear units (PReLUs) are used as the activation function. We argue that a CNN pre-trained on face identification task provides a better initialization for a generic face analysis task, since the filters retain discriminative face information.</p><p>We divide the tasks into two groups: 1) subjectindependent tasks which include face detection, keypoints localization and visibility, pose estimation and smile prediction, and 2) subject-dependent tasks which include age estimation, gender prediction and face recognition. Similar to HyperFace <ref type="bibr" target="#b35">[36]</ref> we fuse the first, third and fifth convolutional layers for training the subject-independent tasks, as they rely more on local information available from the lower layers of the network. We attach two convolution layers and a pooling layer respectively to these layers, to obtain a consistent feature map size of 6 × 6. A dimensionality reduction layer is added to reduce the number of feature maps to 256. It is followed by a fully connected layer of dimension 2048, which forms a generic representation for subject-independent tasks. At this point, the specific tasks are branched into fully connected layers of dimension 512 each, which are followed by the output layers respectively.</p><p>The subject-dependent tasks of age estimation and gender classification are branched out from the sixth convolutional layer of the backbone network after performing the max pooling operation. The global features thus obtained are fed to a 3-layered fully connected network for each of these tasks. We keep the seventh convolutional layer unshared to adapt it specifically to the face recognition task. Task-specific loss functions are used to train the complete network endto-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>The training CNN model contains five sub-networks with parameters shared among them. The tasks of face detection, key-points localization and visibility, and pose estimation are trained in a single sub-network, since all of them use a common dataset (AFLW <ref type="bibr" target="#b23">[24]</ref>) for training. The remaining tasks of smile detection, gender recognition, age estimation and face recognition are trained using separate sub-networks. At test time, these sub-networks are fused together into a single all-in-one CNN <ref type="figure" target="#fig_3">(Fig. 3</ref>). All tasks are trained end-toend simultaneously using Caffe <ref type="bibr" target="#b18">[19]</ref>. Here, we discuss the loss functions and training dataset for each of them.</p><p>1) Face Detection, Key-points Localization and Pose Estimation: These tasks are trained in a similar manner as HyperFace <ref type="bibr" target="#b35">[36]</ref>, using AFLW <ref type="bibr" target="#b23">[24]</ref> dataset. We randomly select 1000 images from the dataset for testing, and use the remaining images for training. We use the Selective Search <ref type="bibr" target="#b44">[45]</ref>   2) Gender Recognition: It is a binary classification problem similar to face detection. The datasets used for training gender are listed in <ref type="table" target="#tab_0">Table I</ref>. The training images are first aligned using facial key-points which are either provided by the dataset or computed using HyperFace <ref type="bibr" target="#b35">[36]</ref>. A crossentropy loss L G is used for training as shown in (4)</p><formula xml:id="formula_3">L G = −(1 − g) · log(1 − p g ) − g · log(p g ),<label>(4)</label></formula><p>where g = 0 for male and 1 for female. p g is the predicted probability that the input face is a female.</p><p>3) Smile Detection: The smile attribute is trained to make the network robust to expression variations for face recognition. We use CelebA <ref type="bibr" target="#b30">[31]</ref> dataset for training. Similar to the gender classification task, the the images are aligned before passing them through the network. The loss function L S is given by (5)</p><formula xml:id="formula_4">L S = −(1 − s) · log(1 − p s ) − s · log(p s ),<label>(5)</label></formula><p>where s = 1 for a smiling face and 0 otherwise. p s is the predicted probability that the input face is a smiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Age Estimation:</head><p>We formulate the age estimation task as a regression problem in which the network learns to predict the age from a face image. We use IMDB+WIKI <ref type="bibr" target="#b39">[40]</ref>, Adience <ref type="bibr" target="#b26">[27]</ref> and MORPH <ref type="bibr" target="#b38">[39]</ref> datasets for training. It has been shown by Ranjan et. al. <ref type="bibr" target="#b36">[37]</ref> that Gaussian loss works better than Euclidean loss for apparent age estimation when the standard deviation of age is given. However, the gradient of Gaussian loss is close to zero when the predicted age is far from the true age ( <ref type="figure" target="#fig_4">Fig. 4)</ref>, which slows the training process. Hence, we use a linear combination of these two loss functions weighted by λ as shown in (6)</p><formula xml:id="formula_5">L A = (1 − λ) 1 2 (y − a) 2 + λ 1 − exp(− (y − a) 2 2σ 2 ) ,<label>(6)</label></formula><p>where L A is the age loss, y is the predicted age, a is the ground-truth age and σ is the standard deviation of the annotated age value. λ is initialized with 0 at the start of the training, and increased to 1 subsequently. In our implementation, we keep λ = 0 initially and switch it to 1 after 20k iterations. σ is fixed at 3 if not provided by the training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Face Recognition:</head><p>We use 10, 548 subjects from CA-SIA <ref type="bibr" target="#b50">[51]</ref> dataset to train the face identification task. The images are aligned using HyperFace <ref type="bibr" target="#b35">[36]</ref> before passing them through the network. We deploy a multi-class crossentropy loss function L R for training as shown in <ref type="formula" target="#formula_6">(7)</ref> L R = 10547 c=0 −y c · log(p c ),</p><p>where y c = 1 if the sample belongs to class c, otherwise 0. The predicted probability that a sample belongs to class c is given by p c .</p><p>The final overall loss L is the weighted sum of individual loss functions, given in (8)</p><formula xml:id="formula_7">L = t=8 t=1 λ t L t ,<label>(8)</label></formula><p>where L t is the loss and λ t is the loss-weight corresponding to task t. The loss-weights are chosen empirically. We assign a higher weight to regression tasks as they tend to have lower loss magnitude than classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Testing</head><p>We deploy a two-stage process during test time as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. In the first stage, we use the Selective Search <ref type="bibr" target="#b44">[45]</ref> to generate region proposals from a test image, which are passed through our all-in-one network to obtain the detection scores, pose estimates, fiducial points and their visibility. We use Iterative Region Proposals and Landmarks-based NMS <ref type="bibr" target="#b35">[36]</ref> to filter out non-faces and improve fiducials and pose estimates. For the second stage, we use the obtained fiducial points to align each detected face to a canonical view using similarity transform. The aligned faces, along with their flipped versions are passed again through the network to get the smile, gender, age and identity information. We use the 512dimensional feature from the penultimate fully connected layer of the identification network as the identity descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The proposed method is evaluated for all the tasks on which it was trained except the key-points visibility, due to the lack of a proper evaluation protocol. We select HyperFace <ref type="bibr" target="#b35">[36]</ref> as a comparison baseline for the tasks of face detection, pose estimation, landmarks localization and gender recognition. For face recognition task, the method from Sankaranarayanan et. al. <ref type="bibr" target="#b40">[41]</ref>, which is used as the initialization, is used as the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Face Detection</head><p>We evaluate the face detection task on Annotated Face in-the-Wild (AFW) <ref type="bibr" target="#b58">[59]</ref>, PASCAL faces <ref type="bibr" target="#b47">[48]</ref> and Face Detection Dataset and Benchmark (FDDB) <ref type="bibr" target="#b17">[18]</ref> datasets. All these datasets contain faces with wide variations in appearance, scale, viewpoint and illumination. To evaluate on AFW and PASCAL datasets, we finetune the face detection branch of the network on FDDB. To evaluate on the FDDB dataset, we finetune according to the 10-fold cross validation experiments <ref type="bibr" target="#b17">[18]</ref>.</p><p>The precision-recall curves for AFW and PASCAL dataset, and Receiver Operating Characteristic (ROC) curve for FDDB dataset are shown in <ref type="figure">Fig. 6</ref>. It can be seen from the figures that our method achieves state-of-the-art performance on AFW and PASCAL dataset with mean average precision (mAP) of 98.5% and 95.01% respectively. On FDDB dataset, our method performs better than most of the reported algorithms. It gets lower recall than Faster-RCNN <ref type="bibr" target="#b19">[20]</ref> and Zhang et al. <ref type="bibr" target="#b53">[54]</ref>, since small faces of FDDB fail to get captured in any of the region proposals. Other recently published methods compared in our detection evaluations include DP2MFD <ref type="bibr" target="#b34">[35]</ref>, Faceness <ref type="bibr" target="#b49">[50]</ref>, Headhunter <ref type="bibr" target="#b32">[33]</ref>, Joint Cascade <ref type="bibr" target="#b4">[5]</ref>, Structured Models <ref type="bibr" target="#b47">[48]</ref>, Cascade CNN <ref type="bibr" target="#b28">[29]</ref>, NDPFace <ref type="bibr" target="#b29">[30]</ref>, TSM <ref type="bibr" target="#b58">[59]</ref>, as well as three commercial systems Face++, Picasa and Face.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Landmarks Localization</head><p>We evaluate our performance on AFW <ref type="bibr" target="#b58">[59]</ref> and AFLW <ref type="bibr" target="#b23">[24]</ref> datasets as they contain large variations in viewpoints of faces. The landmarks location is computed as the mean of the predicted landmarks corresponding to region proposals having IOU&gt;0.5 with the test face. For AFLW <ref type="bibr" target="#b23">[24]</ref> evaluation, we follow the protocol given in <ref type="bibr" target="#b57">[58]</ref>. We randomly create a subset of 450 samples from our test set such that the absolute yaw angles within [0  <ref type="table" target="#tab_0">Table II</ref> compares the Normalized Mean Error (NME) for our method with recent face alignment method adapted to face profoling <ref type="bibr" target="#b57">[58]</ref>, for each of the yaw bins. Our method significantly outperforms the previous best HyperFace <ref type="bibr" target="#b35">[36]</ref>, reducing the error by more than 30%. A low standard deviation of 0.13 suggests that landmarks prediction is consistent as pose angles vary. For AFW <ref type="bibr" target="#b58">[59]</ref> evaluation, we follow the protocol described in <ref type="bibr" target="#b56">[57]</ref>. <ref type="figure">Fig. 7(a)</ref> shows comparisons with recently published methods such as CCL <ref type="bibr" target="#b56">[57]</ref>, HyperFace <ref type="bibr" target="#b35">[36]</ref>, LBF <ref type="bibr" target="#b37">[38]</ref>, SDM <ref type="bibr" target="#b46">[47]</ref>, ERT <ref type="bibr" target="#b21">[22]</ref> and RCPR <ref type="bibr" target="#b0">[1]</ref>. It is evident that the proposed algorithm performs better than existing methods on unconstrained and profile faces since it predicts landmarks with less than 5% NME on more than 95.5% of test faces. However, it lacks in pixel-accurate precise localization of key-points for easy faces, which can be inferred from the lower end of the curve. Most of these algorithms use cascade stage-wise regression to improve the localization, which is slower compared to a single forward pass of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pose Estimation</head><p>We evaluate our metod on AFW <ref type="bibr" target="#b58">[59]</ref> dataset for the pose estimation task. According to the protocol defined in <ref type="bibr" target="#b58">[59]</ref>, we compute the absolute error only for the yaw angles. Since, the ground-truth yaw angles are provided in multiples of 15 • , we round-off our predicted yaw to the nearest 15 • for  <ref type="figure">Fig. 6</ref>: Performance evaluation on (a) the AFW dataset, (b) the PASCAL faces dataset and (c) FDDB dataset. The numbers in the legend are the mean average precision for the corresponding datasets. evaluation. <ref type="figure">Fig. 7(b)</ref> shows the comparison of our method with HyperFace <ref type="bibr" target="#b35">[36]</ref>, Face DPL <ref type="bibr" target="#b59">[60]</ref>, Multiview HoG <ref type="bibr" target="#b58">[59]</ref> and face.com. It is clear that the proposed algorithm performs better than competing methods and is able to predict the yaw in the range of ±15 • for more than 99% of the faces.  <ref type="figure">Fig. 7</ref>: Performance evaluation on AFW dataset for (a) landmarks localization task, (b) pose estimation task. The numbers in the legend are the percentage of test faces with (a) NME less than 5%, (b) absolute yaw error less than or equal to 15 •</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Gender and Smile Recognition</head><p>We evaluate the smile and gender recognition performance on Large-scale CelebFaces Attributes (CelebA) <ref type="bibr" target="#b30">[31]</ref> and ChaLearn Faces of the World <ref type="bibr" target="#b12">[13]</ref> datasets. While CelebA <ref type="bibr" target="#b30">[31]</ref> has wide variety of subjects, they mostly contain frontal faces. Faces of the World <ref type="bibr" target="#b12">[13]</ref> has wide variations in scale and viewpoints of faces. We take the mean of the predicted scores obtained from region proposals having IOU&gt;0.5 with the given face, as our final score for smile and gender attributes. <ref type="table" target="#tab_0">Table III</ref> compares the gender and smile accuracy with recently published methods. On CelebA <ref type="bibr" target="#b30">[31]</ref>, we outperform all the methods for gender accuracy. Our smile accuracy is lower only to Walk and Learn <ref type="bibr" target="#b45">[46]</ref> which uses other contextual information to improve the prediction. The gender and smile branches of the network were finetuned on the training set of Faces of the World <ref type="bibr" target="#b12">[13]</ref> before its evaluation. We achieve state-of-the-art performance for both gender and smile classification on their validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Age Estimation</head><p>We use Chalearn LAP2015 <ref type="bibr" target="#b11">[12]</ref> apparent age estimation challenge dataset and FG-NET <ref type="bibr" target="#b14">[15]</ref> Aging Database for evaluating our age estimation task. We fine-tune the age-task branch of the network on the training set of the challenge dataset, and show the results on the validation set. The error is computed according to the protocol described in <ref type="bibr" target="#b11">[12]</ref>.</p><p>For FG-Net <ref type="bibr" target="#b14">[15]</ref>, we follow the standard Leave-One-Out-Protocol (LOPO). <ref type="table" target="#tab_0">Table IV</ref> lists the evaluation error for both these datasets. We surpass human error of 0.34 and perform comparable to state-of-the-art methods, obtaining an error of 0.293 on Chalearn LAP2015 <ref type="bibr" target="#b11">[12]</ref> dataset. On FG-Net <ref type="bibr" target="#b14">[15]</ref>, we significantly outperform other methods, achieving an average error of 2 years. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Face Identification/Verification</head><p>We evaluate the tasks of face recognition and verification on the IARPA Janus Benchmark-A (IJB-A) <ref type="bibr" target="#b22">[23]</ref> dataset. The dataset contains 500 subjects with a total of 25, 813 images including 5, 399 still images and 20, 414 video frames. It contains faces with extreme viewpoints, resolution and illumination which makes it more challenging than the commonly used LFW <ref type="bibr" target="#b16">[17]</ref> dataset.</p><p>For IJB-A dataset, given a template containing multiple faces, we generate a common vector representation by media pooling the individual face descriptors, as explained in <ref type="bibr" target="#b40">[41]</ref>. A naive way to measure the similarity of a template pair, is by taking cosine distance between their descriptors. A better way is to learn an embedding space where features  corresponding to similar pairs are close to each other while dissimilar pairs are far away. We train a Triplet Probabilistic Embedding (TPE) <ref type="bibr" target="#b40">[41]</ref> using the training splits provided by the dataset. <ref type="table" target="#tab_6">Table V</ref> compares with recently published methods on IJB-A. We achieve state-of-the-results for the face identification task. Although we perform comparable to template-adaptaion learning (Crosswhite et al. <ref type="bibr" target="#b8">[9]</ref>) on verification task, we achieve a significantly faster query time (0.1s after face detection per image pair). We get a consistent improvement of 2% to 3% over the baseline network <ref type="bibr" target="#b40">[41]</ref> for all metrics. We also compare our performance with end-to-end face recognition methods in <ref type="table" target="#tab_0">Table VI</ref>. Our method outperforms existing end-to-end systems which shows that training all the tasks in the pipeline simultaneously, reduces the error. We see a two-fold improvement, i.e., about 80% performance gain is a result of improved identity descriptor and 20% gain is due to improved face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Runtime</head><p>We implemented our all-in-one network on a machine with 8 CPU cores and GTX TITAN-X GPU. It takes an average of 3.5s to process an image. The major bottleneck for speed is the process of generating region proposals and passing each of them through the CNN. The second stage of our method takes merely 0.1s of computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORKS</head><p>In this paper we presented a multi-task CNN-based method for simultaneous face detection, face alignment, pose estimation, gender and smile classification, age estimation and face verification and recognition. Extensive experiments on available unconstrained datasets show that we achieve stateof-the-art results for majority of these tasks. Our method performs significantly better than HyperFace, even though both of them use the MTL framework. This work demonstrates that subject-independent tasks benefit from domainbased regularization and network initialization from face recognition task. Also, the improvement in face verification and recognition performance compared to <ref type="bibr" target="#b40">[41]</ref> clearly suggests that MTL helps in learning robust feature descriptors. In future, we plan to extend this method for other face-related tasks and make the algorithm real time. Several qualitative results of our method are shown in <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head><p>This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. <ref type="figure">Fig. 8</ref>: Qualitative results of our method. The blue boxes denote detected faces. The green dots provide the landmark locations. Pose estimates for each face are shown on top of the boxes in the order of roll, pitch and yaw. The predicted identity, age, gender and smile attributes are shown below the face-boxes. Although the algorithm generates these attributes for all the faces, we show them only for subjects that are present in the IJB-A dataset for better image clarity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The proposed method can simultaneously detect faces, predict their landmarks locations, pose angles, smile expression, gender, age as well as the identity from any unconstrained face image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>A general multitask learning framework for deep CNN architecture. The lower layers are shared among all the tasks and input domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>algorithm to generate region proposals for faces from an image. Regions with Intersection-Over-Union (IOU) overlap of more than 0.5 with the ground truth boundingbox are considered positive examples whereas regions with IOU&lt;0.35 are chosen as negative examples for training the detection task using a softmax loss function. Facial landmarks, key-points visibility and pose estimation tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>CNN Architecture for the proposed method. Each layer is represented by filter kernel size, type of layer, number of feature maps and the filter stride. Orange represents the pre-trained network from Sankaranarayanan et al. [41], while blue represents added layers for MTL. are treated as regression problems and trained with the Euclidean loss. Only regions with IOU&gt;0.35 contribute to back-propagation during their training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Euclidean and Gaussian loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>The end-to-end pipeline for the proposed method during test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(in %) Fraction of Test Faces (468 in total) RCPR (78.87%) ERT (81.70%) SDM (76.75%) LBF (83.54%) CCL (91.22%) HyperFace (84.62%) Ours (95.51%) (in degrees) Fraction of Test Faces face.com (64.3%) Multi. HoG (74.6%) HyperFace (97.7%) FaceDPL (89.4%) Ours (99.1%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Datasets used for training</figDesc><table><row><cell>Dataset</cell><cell>Face Analysis Tasks</cell><cell># training samples</cell></row><row><cell>CASIA</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, 30 • ], [30 • , 60 • ] and [60 • , 90 • ] are 1/3 each.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>The NME(%) of face alignment results on AFLW test set.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">AFLW Dataset (21 pts)</cell><cell></cell></row><row><cell>Method</cell><cell>[0, 30]</cell><cell cols="2">[30, 60] [60, 90]</cell><cell>mean</cell><cell>std</cell></row><row><cell>RCPR [1]</cell><cell>5.43</cell><cell>6.58</cell><cell>11.53</cell><cell>7.85</cell><cell>3.24</cell></row><row><cell>ESR [2]</cell><cell>5.66</cell><cell>7.12</cell><cell>11.94</cell><cell>8.24</cell><cell>3.29</cell></row><row><cell>SDM [47]</cell><cell>4.75</cell><cell>5.55</cell><cell>9.34</cell><cell>6.55</cell><cell>2.45</cell></row><row><cell>3DDFA [58]</cell><cell>5.00</cell><cell>5.06</cell><cell>6.74</cell><cell>5.60</cell><cell>0.99</cell></row><row><cell>3DDFA+SDM</cell><cell>4.75</cell><cell>4.83</cell><cell>6.38</cell><cell>5.32</cell><cell>0.92</cell></row><row><cell>HyperFace [36]</cell><cell>3.93</cell><cell>4.14</cell><cell>4.71</cell><cell>4.26</cell><cell>0.41</cell></row><row><cell>Ours</cell><cell>2.84</cell><cell>2.94</cell><cell>3.09</cell><cell>2.96</cell><cell>0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="3">: Accuray (%) for Gender and Smile classification</cell></row><row><cell cols="3">on CelebA [31] (left) and Faces of the World [13] (right)</cell></row><row><cell>Method PANDA-1 [55] MT-RBM [10] LNets+ANet [31] HyperFace [36] Walk &amp; Learn [46] Ours</cell><cell>Gender Smile 97 92 90 88 98 92 97 -96 98 99 93</cell><cell>Method MT-RBM [10] CMP+ETH [44] DeepBE [28] SIAT MMLAB [53] 91.66 89.34 Gender Smile 71.7 80.8 89.15 79.03 90.44 88.43 Ours 93.12 90.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="4">: Age Estimation error on LAP2015(left) and FG-</cell></row><row><cell>NET(right)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Error</cell><cell>Method</cell><cell>Error</cell></row><row><cell>UMD [37]</cell><cell>0.359</cell><cell>Han2013 [15]</cell><cell>4.6</cell></row><row><cell>Human</cell><cell>0.34</cell><cell>Chao2013 [4]</cell><cell>4.38</cell></row><row><cell cols="2">CascadeAge [6] 0.297</cell><cell>Hong2013 [16]</cell><cell>4.18</cell></row><row><cell>CVL ETHZ</cell><cell>0.295</cell><cell cols="2">El Dib2010 [11] 3.17</cell></row><row><cell>ICT-VIPL</cell><cell>0.292</cell><cell cols="2">CascadeAge [6] 3.49</cell></row><row><cell>Ours</cell><cell>0.293</cell><cell>Ours</cell><cell>2.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Face Identification and Verification Evaluation on IJB-A dataset</figDesc><table><row><cell></cell><cell cols="3">IJB-A Verification (TAR@FAR)</cell><cell></cell><cell cols="2">IJB-A Identification</cell><cell></cell></row><row><cell>Method</cell><cell>0.001</cell><cell>0.01</cell><cell>0.1</cell><cell>FPIR=0.01</cell><cell>FPIR=0.1</cell><cell>Rank=1</cell><cell>Rank=10</cell></row><row><cell>GOTS [23]</cell><cell>0.2(0.008)</cell><cell>0.41(0.014)</cell><cell>0.63(0.023)</cell><cell>0.047(0.02)</cell><cell>0.235(0.03)</cell><cell>0.443(0.02)</cell><cell>-</cell></row><row><cell>VGG-Face [34]</cell><cell>0.604(0.06)</cell><cell>0.805(0.03)</cell><cell>0.937(0.01)</cell><cell>0.46(0.07)</cell><cell>0.67(0.03)</cell><cell>0.913(0.01)</cell><cell>0.981(0.005)</cell></row><row><cell>Chen et al. [7]</cell><cell>-</cell><cell cols="2">0.838(0.042) 0.967(0.009)</cell><cell>-</cell><cell>-</cell><cell cols="2">0.903(0.012) 0.977(0.007)</cell></row><row><cell>Masi et al. [32]</cell><cell>0.725</cell><cell>0.886</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.906</cell><cell>0.977</cell></row><row><cell>NAN [49]</cell><cell>0.785(0.03)</cell><cell>0.897(0.01)</cell><cell>0.959(0.005)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Sankaranarayanan et al. [41] w/o TPE 0.766(0.02)</cell><cell>0.871(0.01)</cell><cell>0.952(0.005)</cell><cell>0.67(0.05)</cell><cell>0.82(0.013)</cell><cell>0.925(0.01)</cell><cell>0.978(0.005)</cell></row><row><cell>Sankaranarayanan et al. [41]</cell><cell>0.813(0.02)</cell><cell>0.90(0.01)</cell><cell>0.964(0.005)</cell><cell>0.753(0.03)</cell><cell>0.863(0.014)</cell><cell>0.932(0.01)</cell><cell>0.977(0.005)</cell></row><row><cell>Crosswhite et al. [9]</cell><cell>-</cell><cell>0.939(0.013)</cell><cell>-</cell><cell cols="2">0.774(0.049) 0.882(0.016)</cell><cell>0.928(0.01)</cell><cell>0.986(0.003)</cell></row><row><cell>Ours</cell><cell>0.787(0.04)</cell><cell>0.893(0.01)</cell><cell>0.968(0.006)</cell><cell>0.704(0.04)</cell><cell cols="3">0.836(0.014) 0.941(0.008) 0.988(0.003)</cell></row><row><cell>Ours + TPE [41]</cell><cell>0.823(0.02)</cell><cell>0.922(0.01)</cell><cell>0.976(0.004)</cell><cell>0.792(0.02)</cell><cell cols="3">0.887(0.014) 0.947(0.008) 0.988(0.003)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison of End-to-End face recognition systems on IJB-A</figDesc><table><row><cell cols="2">Face Detection Face Alignment</cell><cell>Identity Descriptor</cell><cell>Metric Learning</cell><cell cols="2">Verif @FAR=0.01 Ident Rank=1</cell></row><row><cell>DP2MFD [35]</cell><cell>LDDR [26]</cell><cell>Chen et al. [8]</cell><cell>Joint Bayesian [8]</cell><cell>0.776(0.033)</cell><cell>0.834(0.017)</cell></row><row><cell cols="2">HyperFace</cell><cell>Sankaranarayanan et al [41]</cell><cell>cosine</cell><cell>0.871(0.01)</cell><cell>0.925(0.01)</cell></row><row><cell cols="2">HyperFace</cell><cell>Sankaranarayanan et al [41]</cell><cell>TPE [41]</cell><cell>0.90(0.01)</cell><cell>0.932(0.01)</cell></row><row><cell cols="2">HyperFace</cell><cell>Ours</cell><cell>cosine</cell><cell>0.889(0.01)</cell><cell>0.939(0.01)</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>cosine</cell><cell>0.893(0.01)</cell><cell>0.941(0.008)</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>TPE [41]</cell><cell>0.922(0.01)</cell><cell>0.947(0.008)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Facial age estimation based on label-sensitive learning and age-oriented regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="641" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A cascaded convolutional neural network for age estimation of unconstrained faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unconstrained face verification using deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An end-to-end system for unconstrained face verification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03958</idno>
		<title level="m">Template adaptation for face verification and identification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial attributes classification using multi-task representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human age estimation using enhanced bio-inspired features (ebif)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>El Dib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1589" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Chalearn looking at people 2015: Apparent age and cultural event recognition datasets and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Chalearn looking at people and faces of the world: Face analysis workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep learning. Book in preparation for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Age estimation from face images: Human vs. machine performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference on Biometrics (ICB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new biologically inspired active appearance model for face age estimation by using local ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Internet Multimedia Computing and Service</title>
		<meeting>the Fifth International Conference on Internet Multimedia Computing and Service</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="327" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03473</idno>
		<title level="m">Face detection with the faster r-cnn</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pose-invariant 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1931" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Face alignment by local deep descriptor regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1601.07950</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) workshops</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepbe: Learning deep binary encoding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A fast and accurate unconstrained face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Do we really need to collect millions of faces for effective face recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07057</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8692</biblScope>
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A deep pyramid deformable part model for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics Theory, Applications and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multitask learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1603.01249</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unconstrained age estimation with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision Workshop (ICCVW), ICCVW &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision Workshop (ICCVW), ICCVW &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Automatic Face and Gesture Recognition, FGR &apos;06</title>
		<meeting>the 7th International Conference on Automatic Face and Gesture Recognition, FGR &apos;06<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Triplet probabilistic embedding for face verification and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1604.05417</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Structured output svm prediction of apparent age, gender and smile from deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uricár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cvl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Computer Vision, ICCV &apos;11</title>
		<meeting>the 2011 International Conference on Computer Vision, ICCV &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Walk and learn: Facial attribute representation learning from egocentric video and contextual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06433</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supervised descent method and its application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xuehan-Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename><surname>De La</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face detection by structural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<idno>abs/1603.05474</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1311.2901</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gender and smile classification using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Joint face detection and alignment using multi-task cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02878</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07212</idno>
		<title level="m">Face alignment across large poses: A 3d solution</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">FaceDPL: Detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
