<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Ma</surname></persName>
							<email>zhuangma@wharton.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
							<email>mjcollins@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google AI Language and Columbia University †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for loglinear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the rankingbased variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and trade-offs of both methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper considers parameter estimation in conditional models of the form p(y|x; θ) = exp (s(x, y; θ)) Z(x; θ)</p><p>where s(x, y; θ) is the unnormalized score of label y in conjunction with input x under parameters θ, Y is a finite set of possible labels, and Z(x; θ) = y∈Y exp (s(x, y; θ)) is the partition function for input x under parameters θ.</p><p>It is hard to overstate the importance of models of this form in NLP. In log-linear models, including both the original work on maximum-entropy models <ref type="bibr" target="#b1">(Berger et al., 1996)</ref>, and later work on conditional random fields <ref type="bibr">(Lafferty et al., 2001)</ref>, * Part of this work done at Google. † Work done at Google. the scoring function s(x, y; θ) = θ · f (x, y) where f (x, y) ∈ R d is a feature vector, and θ ∈ R d are the parameters of the model. In more recent work on neural networks the function s(x, y; θ) is a nonlinear function. In Word2Vec the scoring function is s(x, y; θ) = θ x · θ y where y is a word in the context of word x, and θ x ∈ R d and θ y ∈ R d are "inside" and "outside" word embeddings x and y. In many NLP applications the set Y is large. Maximum likelihood estimation (MLE) of the parameters θ requires calculation of Z(x; θ) or its derivatives at each training step, thereby requiring a summation over all members of Y, which can be computationally expensive. This has led to many authors considering alternative methods, often referred to as "negative sampling methods", where a modified training objective is used that does not require summation over Y on each example. Instead negative examples are drawn from some distribution, and a objective function is derived based on binary classification or ranking. Prominent examples are the binary objective used in word2vec <ref type="bibr">((Mikolov et al., 2013)</ref>, see also <ref type="bibr">(Levy and Goldberg, 2014)</ref>), and the Noise Contrastive Estimation methods of <ref type="bibr">(Mnih and Teh, 2012;</ref><ref type="bibr">Jozefowicz et al., 2016)</ref> for estimation of language models.</p><p>In spite of the centrality of negative sampling methods, they are arguably not well understood from a theoretical standpoint. There are clear connections to noise contrastive estimation (NCE) (Gutmann and Hyvärinen, 2012), a negative sampling method for parameter estimation in joint models of the form p(y) = exp (s(y; θ)) Z(θ) ; Z(θ) = y∈Y exp (s(y; θ))</p><p>(2) However there has not been a rigorous theoretical analysis of NCE in the estimation of conditional models of the form in Eq. 1, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, the joint model in Eq 2 has a single partition function Z(θ) which is estimated as a param-eter of the model <ref type="bibr">(Gutmann and Hyvärinen, 2012)</ref> whereas the conditional model in Eq 1 has a separate partition function Z(x; θ) for each value of x. This difference is critical.</p><p>We show the following (throughout we define K ≥ 1 to be the number of negative examples sampled per training example):</p><p>• For any K ≥ 1, a binary classification variant of NCE, as used by <ref type="bibr">(Mnih and Teh, 2012;</ref><ref type="bibr">Mikolov et al., 2013)</ref>, gives consistent parameter estimates under the assumption that Z(x; θ) is constant with respect to x (i.e., Z(x; θ) = H(θ) for some function H). Equivalently, the method is consistent under the assumption that the function s(x, y; θ) is powerful enough to incorporate log Z(x; θ).</p><p>• For any K ≥ 1, a ranking-based variant of NCE, as used by <ref type="bibr">(Jozefowicz et al., 2016)</ref>, gives consistent parameter estimates under the much weaker assumption that Z(x; θ) can vary with x. Equivalently, there is no need for s(x, y; θ) to be powerful enough to incorporate log Z(x; θ).</p><p>• We analyze the statistical efficiency of the ranking-based and classification-based NCE variants. Under respective assumptions, both variants achieve Fisher efficiency (the same asymptotic mean square error as the MLE) as K → ∞.</p><p>• We discuss application of our results to approaches of <ref type="bibr">(Mnih and Teh, 2012;</ref><ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr">Levy and Goldberg, 2014;</ref><ref type="bibr">Jozefowicz et al., 2016)</ref> giving a unified account of these methods.</p><p>• We describe experiments on synthetic data and language modeling evaluating the effectiveness of the two NCE variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic Assumptions</head><p>We assume the following setup throughout:</p><p>• We have sets X and Y, where X , Y are finite.</p><p>• There is some unknown joint distribution p X,Y (x, y) where x ∈ X and y ∈ Y. We assume that the marginal distributions satisfy p X (x) &gt; 0 for all x ∈ X and p Y (y) &gt; 0 for all y ∈ Y.</p><p>• We have training examples {x (i) , y (i) } n i=1 drawn I.I.D. from p X,Y (x, y).</p><p>• We have a scoring function s(x, y; θ) where θ are the parameters of the model. For example, s(x, y; θ) may be defined by a neural network.</p><p>• We use Θ to refer to the parameter space. We assume that Θ ⊆ R d for some integer d.</p><p>• We use p N (y) to refer to a distribution from which negative examples are drawn in the NCE approach. We assume that p N satisfies p N (y) &gt; 0 for all y ∈ Y.</p><p>We will consider estimation under the following two assumptions:</p><p>Assumption 2.1 There exists some parameter value θ * ∈ Θ such that for all (x, y) ∈ X × Y,</p><formula xml:id="formula_1">p Y |X (y|x) = exp(s(x, y; θ * )) Z(x; θ * )<label>(3)</label></formula><p>where Z(x; θ * ) = y∈Y exp(s(x, y; θ * )).</p><p>Assumption 2.2 There exists some parameter value θ * ∈ Θ, and a constant γ * ∈ R, such that for all (x, y) ∈ X × Y,</p><formula xml:id="formula_2">p Y |X (y|x) = exp (s(x, y; θ * ) − γ * ) . (4)</formula><p>Assumption 2.2 is stronger than Assumption 2.1. It requires log Z(x; θ * ) ≡ γ * for all x ∈ X , that is, the conditional distribution is perfectly self-normalized. Under Assumption 2.2, it must be the case that ∀x ∈ X</p><formula xml:id="formula_3">y p Y |X (y|x) = y exp{s(x, y; θ * ) − γ * } = 1</formula><p>There are |X | constraints but only d + 1 free parameters. Therefore self-normalization is a nontrivial assumption when |X | d. In the case of language modeling, |X | = |V | k d + 1, where |V | is the vocabulary size and k is the length of the context. The number of constraints grows exponentially fast.</p><p>Given a scoring function s(x, y; θ) that satisfies assumption 2.1, we can derive a scoring function s that satisfies assumption 2.2 by defining</p><formula xml:id="formula_4">s (x, y; θ, {c x : x ∈ X }) = s(x, y; θ) − c x</formula><p>where c x ∈ R is a parameter for history x. Thus we introduce a new parameter c x for each possible history x. This is the most straightforward extension of NCE to the conditional case; it is used by <ref type="bibr">(Mnih and Teh, 2012)</ref>. It has the clear drawback however of introducing a large number of additional parameters to the model. <ref type="figure" target="#fig_1">Figure 1</ref> shows two NCE-based parameter estimation algorithms, based respectively on binary objective and ranking objective. The input to either algorithm is a set of training examples {x (i) , y (i) } n i=1 , a parameter K specifying the number of negative examples per training example, and a distribution p N (·) from which negative examples are sampled. The algorithms differ only in the choice of objective function being optimized: L n B for binary objective, and L n R for ranking objective. Binary objective essentially corresponds to a problem where the scoring function s(x, y; θ) is used to construct a binary classifier that discriminates between positive and negative examples. Ranking objective corresponds to a problem where the scoring function s(x, y; θ) is used to rank the true label y (i) above negative examples y (i,1) . . . y <ref type="bibr">(i,K)</ref> for the input x (i) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two Estimation Algorithms</head><p>Our main result is as follows:</p><p>Theorem 3.1 (Informal: see section 4 for a formal statement.) For any K ≥ 1, the binary classification-based algorithm in figure 1 is consistent under Assumption 2.2, but is not always consistent under the weaker Assumption 2.1. For any K ≥ 1, the ranking-based algorithm in figure 1 is consistent under either Assumption 2.1 or Assumption 2.2. Both algorithms achieve the same statistical efficiency as the maximum-likelihood estimate as K → ∞.</p><p>The remainder of this section gives a sketch of the argument underlying consistency, and discusses use of the two algorithms in previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Sketch of the Consistency Argument for the Ranking-Based Algorithm</head><p>In this section, in order to develop intuition underlying the ranking algorithm, we give a proof sketch of the following theorem:</p><formula xml:id="formula_5">Theorem 3.2 (First part of theorem 4.1 below.) Define L ∞ R (θ) = E[L n R (θ)]. Under Assump- tion 2.1,θ ∈ arg max θ L ∞ R (θ) if and only if, for all (x, y) ∈ X × Y, p Y |X (y|x) = exp(s(x, y;θ))/Z(x,θ).</formula><p>This theorem is key to the consistency argument. Intuitively as n increases L n R (θ) converges to L ∞ R (θ), and the output to the algorithm converges to θ such that p(y|x; θ ) = p Y |X (y|x) for all x, y. Section 4 gives a formal argument.</p><p>We now give a proof sketch for theorem 3.2. Consider the algorithm in figure 1. For convenience defineȳ (i) to be the vector (y (i,0) , y (i,1) , . . . , y (i,K) ).</p><p>Define α(x,ȳ) = Definitions: Defines(x, y; θ) = s(x, y; θ) − log pN (y) Algorithm:</p><formula xml:id="formula_6">• For i = 1 . . . n, k = 1 . . . K, draw y (i,k) I.I.D.</formula><p>from the distribution pN (y). For convenience define y (i,0) = y (i) .</p><p>• If RANKING, define the ranking objective function</p><formula xml:id="formula_7">L n R (θ) = 1 n n i=1 log exp(s(x (i) , y (i,0) ; θ)) K k=0 exp(s(x (i) , y (i,k) ; θ))</formula><p>, and the estimator θR = argmax θ∈Θ L n R (θ).</p><p>• If BINARY, define the binary objective function Figure 1: Two NCE-based estimation algorithms, using ranking objective and binary objective respectively.</p><formula xml:id="formula_8">L n B (θ, γ) = 1 n n i=1 log g(x (i) , y (i,0) ; θ, γ) + K k=1 log 1 − g(x (i) , y (i,k) ; θ, γ) ,</formula><formula xml:id="formula_9">K k=0 p X,Y (x,ȳ k ) j =k p N (ȳ j ), and q(k|x,ȳ; θ) = exp(s(x,ȳ k ; θ)) K k=0 exp(s(x,ȳ k ; θ)) , β(k|x,ȳ) = p X,Y (x,ȳ k ) j =k p N (ȳ j ) α(x,ȳ) = p Y |X (ȳ k |x)/p N (ȳ k ) N k=0 p Y |X (ȳ k |x)/p N (ȳ k ) C(x,ȳ; θ) = − K k=0 β(k|x,ȳ) log q(k|x,ȳ; θ)</formula><p>Intuitively, q(·|x,ȳ; θ) and β(·|x,ȳ) are posterior distributions over the true label k ∈ {0 . . . K} given an input x,ȳ, under the parameters θ and the true distributions p X,Ȳ (x,ȳ) respectively; C(x,ȳ; θ) is the negative cross-entropy between these two distributions. The proof of theorem 3.2 rests on two identities. The first identity states that the objective function is the expectation of the negative crossentropy w.r.t. the density function 1 K+1 α(x,ȳ) (see Section B.1.1 of the supplementary material for derivation):</p><formula xml:id="formula_10">L ∞ R (θ) = x ȳ 1 K + 1 α(x,ȳ)C(x,ȳ; θ). (5)</formula><p>The second identity concerns the relationship between q(·|x,ȳ; θ) and β(·|x,ȳ). Under assumption 2.1, for all x,ȳ, k ∈ {0 . . . K},</p><formula xml:id="formula_11">q(k|x,ȳ; θ * ) = p Y |X (ȳ k |x)Z(x; θ * )/p N (y k ) K k=0 p Y |X (ȳ k |x)Z(x; θ * )/p N (y k ) = β(k|x,ȳ)<label>(6)</label></formula><p>It follows immediately through the properties of negative cross entropy that ∀x,ȳ, θ * ∈ argmax θ C(x,ȳ; θ)</p><p>The remainder of the argument is as follows:</p><p>• Eqs. 7 and 5 imply that θ * ∈ argmax θ L ∞ R (θ). • Assumption 2.1 implies that α(x,ȳ) &gt; 0 for all x,ȳ. It follows that any θ ∈ arg max θ L ∞ R (θ) satisfies for all x,ȳ, k,</p><formula xml:id="formula_13">(8) q(k|x,ȳ; θ ) = q(k|x,ȳ; θ * ) = β(k|x,ȳ)</formula><p>Otherwise there would be some x,ȳ such that C(x,ȳ; θ ) &lt; C(x,ȳ; θ * ).</p><p>• Eq. 8 implies that ∀x, y, p(y|x; θ ) = p(y|x; θ * ). See the proof of lemma B.3 in the supplementary material.</p><p>In summary, the identity in Eq. 5 is key: the objective function in the limit, L ∞ R (θ), is related to a negative cross-entropy between the underlying distribution β(·|x,ȳ) and a distribution under the parameters, q(·|x,ȳ; θ). The parameters θ * maximize this negative cross entropy over the space of all distributions {q(·|x,ȳ; θ), θ ∈ Θ}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Algorithms in Previous Work</head><p>To motivate the importance of the two algorithms, we now discuss their application in previous work.</p><p>Mnih and Teh (2012) consider language modeling, where x = w 1 w 2 . . . w n−1 is a history consisting of the previous n−1 words, and y is a word. The scoring function is defined as</p><formula xml:id="formula_14">s(x, y; θ) = ( n−1 i=1 C i r w i ) · q y + b y − c x</formula><p>where r w i is an embedding (vector of parameters) for history word w i , q y is an embedding (vector of parameters) for word y, each C i for i = 1 . . . n−1 is a matrix of parameters specifying the contribution of r w i to the history representation, b y is a bias term for word y, and c x is a parameter corresponding to the log normalization term for history x. Thus each history x has its own parameter c x . The binary objective function is used in the NCE algorithm. The noise distribution p N (y) is set to be the unigram distribution over words in the vocabulary.</p><p>This method is a direct application of the original NCE method to conditional estimation, through introduction of the parameters c x corresponding to normalization terms for each history. Interestingly, Mnih and Teh (2012) acknowledge the difficulties in maintaining a separate parameter c x for each history, and set c x = 0 for all x, noting that empirically this works well, but without giving justification. <ref type="bibr">Mikolov et al. (2013)</ref> consider an NCE-based method using the binary objective function for estimation of word embeddings. The skip-gram method described in the paper corresponds to a model where x is a word, and y is a word in the context. The vector v x is the embedding for word x, and the vector v y is an embedding for word y (separate embeddings are used for x and y). The method they describe uses</p><formula xml:id="formula_15">s(x, y; θ) = v y · v x or equivalently s(x, y; θ) = v y · v x + log p N (y)</formula><p>The negative-sampling distribution p N (y) was chosen as the unigram distribution p Y (y) raised to the power 3/4. The end goal of the method was to learn useful embeddings v w and v w for each word in the vocabulary; however the method gives a consistent estimate for a model of the form</p><formula xml:id="formula_16">p(y|x) = exp v y · v x + log p N (y) y exp v y · v x + log p N (y) = p N (y) exp v y · v x Z(x; θ)</formula><p>assuming that Assumption 2.2 holds, i.e.</p><formula xml:id="formula_17">Z(x; θ) = y p N (y) exp v y · v x ≡ H(θ) which does not vary with x.</formula><p>Levy and Goldberg (2014) make a connection between the NCE-based method of <ref type="bibr">(Mikolov et al., 2013)</ref>, and factorization of a matrix of pointwise mutual information (PMI) values of (x, y) pairs. Consistency of the NCE-based method under assumption 2.2 implies a similar result, specifically: if we define p N (y) = p Y (y), and define s(x, y; θ) = v y · v x + log p N (y) implyinḡ s(x, y; θ) = v y · v x , then parameters v y and v x converge to values such that</p><formula xml:id="formula_18">p(y|x) = p Y (y) exp v y · v x H(θ) or equivalently PMI(x, y) = log p(y|x) p(y) = v y · v x − log H(θ)</formula><p>That is, following (Levy and Goldberg, 2014), the inner product v y · v x is an estimate of the PMI up to a constant offset H(θ). Finally, Jozefowicz et al. <ref type="formula" target="#formula_0">(2016)</ref> introduce the ranking-based variant of NCE for the language modeling problem. This is the same as the ranking-based algorithm in figure 1. They do not, however, make the connection to assumptions 2.2 and 2.1, or derive the consistency or efficiency results in the current paper. Jozefowicz et al.</p><p>(2016) partially motivate the ranking-based variant throught the importance sampling viewpoint of <ref type="bibr" target="#b0">Bengio and Senécal (2008)</ref>. However there are two critical differences: 1) the algorithm of Bengio and Senécal (2008) does not lead to the same objective L n R in the ranking-based variant of NCE; instead it uses importance sampling to derive an objective that is similar but not identical; 2) the importance sampling method leads to a biased estimate of the gradients of the log-likelihood function, with the bias going to zero only as K → ∞. In contrast the theorems in the current paper show that the NCE-based methods are consistent for any value of K. In summary, while it is tempting to view the ranking variant of NCE as an importance sampling method, the NCE-based view gives stronger guarantees for finite values of K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theory</head><p>This section states the main theorems. The supplementary material contains proofs. Throughout the paper, we use</p><formula xml:id="formula_19">E X [ · ], E Y [ · ], E X,Y [ · ], E Y |X=x [ · ] to represent the expectation w.r.t. p X (·), p Y (·), p X,Y (·, ·), p Y |X (·|x).</formula><p>We use · to denote either the l 2 norm when the operand is a vector or the spectral norm when the operand is a matrix. Finally, we use ⇒ to represent converge in distribution. Recall that we have defined s(x, y; θ) = s(x, y; θ) − log p N (y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ranking</head><p>In this section, we study noise contrastive estimation with ranking objective under Assumption 2.1. First consider the following function:</p><formula xml:id="formula_20">L ∞ R (θ) = x,y 0 ,··· ,y K p X,Y (x, y 0 ) K i=1 p N (y i ) × log exp(s(x, y 0 ; θ)) K k=0 exp(s(x, y k ; θ))</formula><p>.</p><p>By straightforward calculation, one can find that</p><formula xml:id="formula_21">L ∞ R (θ) = E [L n R (θ)] . Under mild conditions, L n R (θ) converges to L ∞ R (θ) as n → ∞. Denote the set of maximiz- ers of L ∞ R (θ) by Θ * R , that is Θ * R = arg max θ∈Θ L ∞ R (θ) .</formula><p>The following theorem shows that any parameter vectorθ ∈ Θ * R if and only if it gives the correct conditional distribution p Y |X (y|x).  In addition, Θ * R is a singleton if and only if Assumption 4.1 holds.</p><p>Next we consider consistency of the estimation algorithm based on the ranking objective under the following regularity assumptions:</p><formula xml:id="formula_22">Assumption 4.2 (Continuity). s(x, y; θ) is con- tinuous w.r.t. θ for all (x, y) ∈ X × Y. Assumption 4.3 Θ * R is contained in the interior of a compact set Θ ⊂ R d .</formula><p>For a given estimate p Y |X of the conditional distribution p Y |X , define the error metric d(·, ·) by</p><formula xml:id="formula_23">d p Y |X , p Y |X = x∈X ,y∈Y p X,Y (x, y) × p Y |X (y|x) − p Y |X (y|x) 2 .</formula><p>For a sequence of IID observations (x (1) , y (1) ), (x (2) , y (2) ), . . . , define the sequences of esti- Remark 4.1 Thoughout the paper, all NCE estimators are defined for some fixed K. We suppress the dependence on K to simplify notation (e.g. θ n R should be interpreted as θ n,K R ).</p><formula xml:id="formula_24">mates ( θ 1 R , p 1 Y |X ), ( θ 2 R , p 2 Y |X ), . . . where the n th estimate ( θ n R , p n Y |X ) is obtained by op- timizing the ranking objective of figure 1 on (x (1) , y (1) ), (x (2) , y (2) ), . . . , (x (n) , y (n) ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification</head><p>Now we turn to the analysis of NCE with binary objective under Assumption 2.2. First consider the following function,</p><formula xml:id="formula_25">L ∞ B (θ, γ) = x,y p X,Y (x, y) log (g(x, y; θ, γ)) + Kp X (x)p N (y) log (1 − g(x, y; θ, γ))</formula><p>One can find that</p><formula xml:id="formula_26">L ∞ B (θ, γ) = E [L n B (θ, γ)] .</formula><p>Denote the set of maximizers of L ∞ B (θ, γ) by Ω * B :</p><formula xml:id="formula_27">Ω * B = arg max θ∈Θ,γ∈Γ L ∞ B (θ, γ) .</formula><p>Parallel results of Theorem 4.1, 4.2 are established as follows.</p><p>Assumption 4.4 (Identifiability). For any θ ∈ Θ, if there exists some constant c such that s(x, y; θ)−s(x, y; θ * ) ≡ c for all (x, y) ∈ X ×Y, then θ = θ * and thus c = 0.</p><formula xml:id="formula_28">Assumption 4.5 Ω * B is in the interior of Θ × Γ where Θ ⊂ R d , Γ ⊂ R are compact sets. Theorem 4.3 Under Assumption 2.2, (θ,γ) ∈ Ω * B if and only if, for all (x, y) ∈ X × Y, p Y |X (y|x) = exp(s(x, y;θ) −γ)</formula><p>for all <ref type="bibr">(x, y)</ref>. Ω * B is a singleton if and only if Assumption 4.4 holds.</p><p>Similarly we can define the sequence of es-  </p><formula xml:id="formula_29">timates ( θ 1 B , γ 1 B , p 1 Y |X ), ( θ 2 B , γ 2 B , p 2 Y |X ), . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Counterexample</head><p>In this section, we give a simple example to demonstrate that the binary classification approach fails to be consistent when assumption 2.1 holds but assumption 2.2 fails (i.e. the partition function depends on the input).</p><formula xml:id="formula_30">Consider X ∈ X = {x 1 , x 2 } with marginal distribution p X (x 1 ) = p X (x 2 ) = 1/2,</formula><p>and Y ∈ Y = {y 1 , y 2 } generated by the conditional model specified in assumption 2.1 with the score function parametrized by θ = (θ 1 , θ 2 ) and s(x 1 , y 1 ; θ) = log θ 1 , s(x 1 , y 2 ; θ) = s(x 2 , y 1 ; θ) = s(x 2 , y 2 ; θ) = log θ 2 .</p><p>Assume the true parameter is θ * = (θ * 1 , θ * 2 ) = (1, 3). By simple calculation,</p><formula xml:id="formula_31">Z(θ * ; x 1 ) = 4, Z(θ * ; x 2 ) = 6, p X,Y (x 1 , y 1 ) = 1/8, p X,Y (x 1 , y 2 ) = 3/8, p X,Y (x 2 , y 1 ) = p X,Y (x 2 , y 2 ) = 1/4.</formula><p>Suppose we choose the negative sampling distribution p N (y 1 ) = p N (y 2 ) = 1/2. For any K ≥ 1, by the Law of Large Numbers, as n goes to infinity, L n B (θ, γ) will converge to L ∞ B (θ, γ). Substitute in the parameters above. One can show that</p><formula xml:id="formula_32">L ∞ B (θ, γ) = 1 8 log 2θ 1 2θ 1 + K exp(γ) + K 4 log K exp(γ) 2θ 1 + K exp(γ) + 7 8 log 2θ 2 2θ 2 + K exp(γ) + 3K 4 log K exp(γ) 2θ 2 + K exp(γ)</formula><p>.</p><p>Setting the derivatives w.r.t. θ 1 , θ 2 to zero, one will obtain</p><formula xml:id="formula_33">θ 1 = 1 4 exp(γ), θ 2 = 7 12 exp(γ).</formula><p>So for any ( θ 1 , θ 2 , γ) ∈ argmax θ,γ L ∞ B (θ, γ), ( θ 1 , θ 2 , γ) will satisfy the equalities above. Then the estimated distribution p Y |X will satisfy</p><formula xml:id="formula_34">p Y |X (y 1 |x 1 ) p Y |X (y 2 |x 1 ) = θ 1 θ 2 = 1/4 7/12 = 3 7 ,</formula><p>which contradicts the fact that</p><formula xml:id="formula_35">p Y |X (y 1 |x 1 ) p Y |X (y 2 |x 1 ) = p X,Y (x 1 , y 1 ) p X,Y (x 1 , y 2 ) = 1 3 .</formula><p>So the binary objective does not give consistent estimation of the conditional distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Asymptotic Normality and Statistical Efficiency</head><p>Noise Contrastive Estimation significantly reduces the computational complexity, especially when the label space |Y| is large. It is natural to ask: does such scalability come at a cost? Classical likelihood theory tells us, under mild conditions, the maximum likelihood estimator (MLE) has nice properties like asymptotic normality and Fisher efficiency. More specifically, as the sample size goes to infinity, the distribution of the MLE will converge to a multivariate normal distribution, and the mean square error of the MLE will achieve the Cramer-Rao lower bound <ref type="bibr">(Ferguson, 1996)</ref>. We have shown the consistency of the NCE estimators in Theorem 4.2 and Theorem 4.4. In this part of the paper, we derive their asymptotic distribution and quantify their statistical efficiency. To this end, we restrict ourselves to the case where θ * is identifiable (i.e. Assumptions 4.1 or 4.4 hold) and the scoring function s(x, y; θ) satisfies the following smoothness condition:</p><formula xml:id="formula_36">Assumption 4.6 (Smoothness). The scoring func- tion s(x, y; θ) is twice continuous differentiable w.r.t. θ for all (x, y) ∈ X × Y.</formula><p>We first introduce the following maximumlikelihood estimator.</p><formula xml:id="formula_37">θ MLE = arg min θ L n MLE (θ) := arg min θ n i=1 log exp(s(x (i) , y (i) ; θ)) y∈Y exp(s(x (i) , y; θ))</formula><p>.</p><p>Define the matrix</p><formula xml:id="formula_38">I θ * = E X Var Y |X=x [∇ θ s(x, y; θ * )] .</formula><p>As shown below, I θ * is essentially the Fisher information matrix under the conditional model. </p><formula xml:id="formula_39">√ n( θ MLE − θ * ) ⇒ N (0, I −1 θ * ).</formula><p>For any given estimator θ, define the scaled asymptotic mean square error by</p><formula xml:id="formula_40">MSE ∞ ( θ) = lim n→∞ E n d θ − θ * 2 ,</formula><p>where d is the dimension of the parameter θ * . Theorem 4.5 implies that,</p><formula xml:id="formula_41">MSE ∞ ( θ MLE ) = Tr(I −1 θ * )/d.</formula><p>where Tr(·) denotes the trace of a matrix. According to classical MLE theory <ref type="bibr">(Ferguson, 1996)</ref>, under certain regularity conditions, this is the best achievable mean square error. So the next question to answer is: can these NCE estimators approach this limit?</p><p>Assumption 4.7 There exist positive constants c, C such that σ min (I θ * ) ≥ c and</p><formula xml:id="formula_42">max (x,y)∈X ×Y |s(x, y; θ * )|, ∇ θ s(x, y; θ * ) , ∇ 2 θ s(x, y; θ * ) ≤ C.</formula><p>where σ min (·) denotes the smallest singular value.</p><p>Theorem 4.6 (Ranking) Under Assumption 2.1, 4.1, 4.3, 4.6, 4.7, there exists an integer K 0 such that for all</p><formula xml:id="formula_43">K ≥ K 0 , as n → ∞ √ n θ R − θ * ⇒ N (0, I −1 R,K ),<label>(9)</label></formula><p>for some matrix I R,K . There exists a constant C such that for all K ≥ K 0 , </p><formula xml:id="formula_44">| MSE ∞ ( θ R ) − MSE ∞ ( θ MLE )| ≤ C/ √ K I −1 R,K − I −1 θ * ≤ C/ √ K<label>Theorem</label></formula><formula xml:id="formula_45">K ≥ K 0 , as n → ∞ √ n θ B − θ * ⇒ N (0, I −1 B,K ),<label>(10)</label></formula><p>for some matrix I B,K . There exists a constant C such that for all K ≥ K 0 ,</p><formula xml:id="formula_46">| MSE ∞ ( θ B ) − MSE ∞ ( θ MLE )| ≤ C/K I −1 B,K − I −1 θ * ≤ C/K.</formula><p>Remark 4.2 Theorem 4.6 and 4.7 reveal that under respective model assumptions, for any given K ≥ K 0 both NCE estimators are asymptotically normal and √ n-consistent. Moreover, both NCE estimators approach Fisher efficiency (statistical optimality) as K grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Simulations</head><p>Suppose we have a feature space X ⊂ R d with |X | = m x , label space Y = {1, · · · , m y }, and parameter θ = (θ 1 , · · · , θ my ) ∈ R my×d . Then for any given sample size n, we can generate observations (x (i) , y (i) ) by first sampling x (i) uniformly from X and then sampling y We first consider the estimation of θ by MLE and NCE-ranking. We fix d = 4, m x = 200, m y = 100 and generate X and the parameter θ from separate mixtures of Gaussians. We try different configurations of (n, K) and report the KL divergence between the estimated distribution and true distribution, as summarized in the left panel of <ref type="figure" target="#fig_7">figure 2</ref>. The observations are:</p><p>• The NCE estimators are consistent for any fixed K. For a fixed sample size, the NCE estimators become comparable to MLE as K increases.</p><p>• The larger the sample size, the less sensitive are the NCE estimators to K. A very small value of K seems to suffice for large sample size.</p><p>Apparently, under the parametrization above, the model is not self-normalized. To use NCEbinary, we add an extra x-dependent bias parameter b x to the score function (i.e. s(x, y; θ) = x θ y + b x ) to make the model self-normalized or else the algorithm will not be consistent. Similar patterns to figure 2 are observed when varying sample size and K (see Section A.1 of the supplementary material). However this makes NCE-binary not directly comparable to NCE-ranking/MLE since its performance will be compromised by estimating extra parameters and the number of extra parameters depends on the richness of the feature space X . To make this clear, we fix n = 16000, d = 4, m y = 100, K = 32 and experiment with m x = 100, 200, 300, 400. The results are summarized on the right panel of figure 2. As |X | increases, the KL divergence will grow while the performance of NCE-ranking/MLE is independent of |X |. Without the x-dependent bias term for NCE-binary, the KL divergence will be much higher due to lack of consistency (0.19, 0.21, 0.24, 0.26 respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Language Modeling</head><p>We evaluate the performance of the two NCE algorithms on a language modeling problem, using the Penn Treebank (PTB) dataset <ref type="bibr">(Marcus et al., 1993)</ref>. We choose <ref type="bibr">(Zaremba et al., 2014)</ref> as the benchmark where the conditional distribution is modeled by two-layer LSTMs and the parameters are estimated by MLE (note that the current state-of-the-art is <ref type="figure" target="#fig_1">(Yang et al., 2018)</ref>). Zaremba et al. (2014) implemented 3 model configurations: "Small" , "Medium" and "Large", which have 200, 650 and 1500 units per layer respectively. We follow their setup (model size, unrolled steps, dropout ratio, etc) but train the model by maximiz-  ing the two NCE objectives. We use the unigram distribution as the negative sampling distribution and consider K = 200, 400, 800, 1600.</p><p>The results on the test set are summarized in table 1. Similar patterns are observed on the validation set (see Section A.2 of the supplementary material). As shown in the table, the performance of NCE-ranking and NCE-binary improves as the number of negative examples increases, and finally outperforms the MLE.</p><p>An interesting observation is, without regularization, the binary classification approach outperforms both ranking and MLE. This suggests the model space (two-layer LSTMs) is rich enough as to approximately incorporate the x-dependent partition function Z(θ; x), thus making the model approximately self-normalized. This motivates us to modify the ranking and MLE objectives by adding the following regularization term:</p><formula xml:id="formula_47">α n n i=1   log   1 m m j=1 exp s(x (i) , y (i,j) ; θ)     2 ≈ α E X (log Z(x; θ)) 2 ,</formula><p>where y (i,j) , 1 ≤ j ≤ m are sampled from the noise distribution p N (·). This regularization term promotes a constant partition function, that is Z(x; θ) ≈ 1 for all x ∈ X . In our experiments, we fix m to be 1/10 of the vocabulary size, K = 1600 and tune the regularization parameter α. As shown in the last three rows of the table, regularization significantly improves the performance of both the ranking approach and the MLE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we have analyzed binary and ranking variants of NCE for estimation of conditional models p(y|x; θ). The ranking-based variant is consistent for a broader class of models than the binary-based algorithm. Both algorithms achieve Fisher efficiency as the number of negative examples increases. Experiments show that both algorithms outperform MLE on a language modeling task. The ranking-based variant of NCE outperforms the binary-based variant once a regularizer is introduced that encourages self-normalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Similation Results for NCE-binary</head><p>See figure 3. <ref type="figure">Figure 3</ref>: KL divergence between the true distribution and the estimated distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Perplexity on Validation Set of Penn Treebank</head><p>See </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs</head><p>Recall we have the following setup:</p><p>• X , Y are finite sets</p><formula xml:id="formula_48">• p X (x), p Y (y), p N (y) &gt; 0, ∀x ∈ X , y ∈ Y.</formula><p>We present the proofs for ranking loss and binary classification loss separately, in the following two subsections.</p><p>The following fact will be frequently used in the consistency proof.</p><p>Lemma B.1 Define the K-simplex by</p><formula xml:id="formula_49">D K = x|x ∈ R K+1 + , K i=0 x i = 1 .</formula><p>Then for any p ∈ D K , p = arg max q∈D K K i=0 q i log p i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proofs for Ranking Loss</head><p>To simplify notations, we use y 0:K to represent the vector (y 0 , · · · , y K ) and y (i,0:K) to represent (y (i,0) , · · · , y (i,K) ) where we define y (i,0) := y (i) .</p><p>For any tuple (x, y 0:K ) and parameter θ, we define q(i|x, y 0:K ; θ) = exp(s(x, y i ; θ)) For any θ 1 , θ 2 , the following three statements are equivalent:</p><p>(1) For all x, y p(y|x; θ 1 ) = p(y|x; θ 2 )</p><p>(2) For all x, y 0:k and 0 ≤ i ≤ K q(i|x, y 0:k ; θ 1 ) = q(i|x, y 0:k ; θ 2 )</p><p>(3) There exists some function c(·) such that for all x, y s(x, y; θ 1 ) − s(x, y; θ 2 ) = c(x).</p><p>Proof: By Lemma B.2,</p><formula xml:id="formula_50">q(i|x, y 0:k ; θ) = p(y i |x; θ)/p N (y i ) K i=0 p(y i |x; θ)/p N (y i )</formula><p>, and hence (1) ⇒ (2). To show (2) ⇒ (3), notice that, for any x, y, y , by setting i = 0 and y 0 = y, y 1 = y 2 . . . = y K = y , q(i|x, y 0:k ; θ 1 ) = q(i|x, y 0:k ; θ 2 ) will imply exp(s(x, y; θ 1 )) exp(s(x, y; θ 1 )) + K exp(s(x, y ; θ 1 )) = exp(s(x, y; θ 2 )) exp(s(x, y; θ 2 )) + K exp(s(x, y ; θ 2 )) ,  or equivalently,</p><formula xml:id="formula_51">1 1 + K exp(s(x, y ; θ 1 ) −s(x, y; θ 1 )) = 1 1 + K exp(s(x, y ; θ 2 ) −s(x, y; θ 2 ))</formula><p>.</p><p>Then it follows that, We then have p(y|x; θ 1 ) = exp(s(x, y; θ 1 )) y exp(s(x, y ; θ 1 )) = 1 y exp(s(x, y ; θ 1 ) − s(x, y; θ 1 )) = 1 y exp(s(x, y ; θ 2 ) − s(x, y; θ 2 )) = p(y|x; θ 2 ).</p><formula xml:id="formula_52">s(x, y ; θ 1 )−s(x, y; θ 1 ) =s(x, y ; θ 2 )−s(x, y ; θ 2 ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Proof of Theorem 4.1</head><p>If we define</p><formula xml:id="formula_53">g i (θ; x, y 0:K ) = p X,Y (x, y i ) j =i p N (y j ) log q(i|x, y 0:K ; θ) then for any i ∈ {0 . . . K}, we have L ∞ R (θ) = x y 0:K g i (x, y 0:K ; θ) It follows that L ∞ R (θ) = 1 K + 1 K i=0 x y 0:K g i (x, y 0:K ; θ) = 1 K + 1 x y 0:K K i=0 g i (x, y 0:K ; θ) = 1 K + 1 x y 0 ,y 1 ,··· ,y K f (θ; x, y 0:K ) where f (θ; x, y 0:K ) = K i=0 g i (x, y 0:K ; θ) = K i=0 p X,Y (x, y i ) j =i p N (y j ) log q(i|x, y 0:K ; θ) =   K i=0 p X,Y (x, y i ) j =i p N (y j )   × K i=0 p X,Y (x, y i ) j =i p N (y j ) K i=0 p X,Y (x, y i ) j =i p N (y j ) log q(i|x, y 0:K ; θ) =   K i=0 p X,Y (x, y i ) j =i p N (y j )   × K i=0 p Y |X (y i |x)/p N (y i ) K j=0 p Y |X (y j |x)/p N (y j )</formula><p>log q(i|x, y 0:K ; θ).</p><p>Next note that K i=0 q(i|x, y 0:K ; θ) = 1, hence by Lemmas B.1 and B.2, θ * ∈ arg max θ f (θ; x, y 0:K ) for all x, y 0:K , hence θ * ∈ arg max θ L ∞ R (θ). It follows that for anyθ ∈ arg maxθ L ∞ R (θ),θ must be in arg max θ f (θ; x, y 0:K ) for all x, y 0:K , from which it follows that So θ ∈ Θ * R . If Θ * R is a singleton, it must be the case thatθ = θ * . This proves the second part of the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Proof of Theorem 4.2</head><p>Under the assumptions in Theorem 4.2, by classical large sample theory (e.g. Theorem 16(a) in <ref type="bibr">(Ferguson, 1996)</ref>), for any compact set K ⊂ Θ,</p><formula xml:id="formula_54">P lim n→∞ sup θ∈K |L n R (θ) − L ∞ R (θ)| = 0 = 1 (11) Since |L n R (θ) − L ∞ R (θ)| ≥ L n R (θ) − L ∞ R (θ), P lim sup n→∞ sup θ∈K (L n R (θ) − L ∞ R (θ)) ≤ 0 = 1.<label>(12)</label></formula><p>For any θ n K ∈ arg max</p><formula xml:id="formula_55">θ∈K L n R (θ), sup θ∈K (L n R (θ) − L ∞ R (θ)) ≥ L n R ( θ n K ) − L ∞ R ( θ n K ) ≥ L n R ( θ n K ) − sup θ∈K L ∞ R (θ) = sup θ∈K L n R (θ) − sup θ∈K L ∞ R (θ).</formula><p>Combining this inequality and <ref type="formula" target="#formula_0">(12)</ref> yields</p><formula xml:id="formula_56">P lim sup n→∞ sup θ∈K L n R (θ) ≤ sup θ∈K L ∞ R (θ) = 1. (13) Recall that Θ * R = θ |θ ∈ arg max θ∈Θ L ∞ R (θ) .</formula><p>For any ρ &gt; 0, define</p><formula xml:id="formula_57">Θ ρ = θ | θ ∈ Θ, arg min θ * ∈Θ * R θ − θ * ≥ ρ .</formula><p>By Assumption 4.2, L ∞ R (θ) is a continuous function of θ and therefore,</p><formula xml:id="formula_58">δ ρ . = sup θ∈Θρ L ∞ R (θ) &lt; δ . = sup θ∈Θ L ∞ R (θ).</formula><p>Apply the uniform convergence results in equation (13) with K = Θ ρ , with probability 1,</p><formula xml:id="formula_59">lim sup n→∞ sup θ∈Θρ L n R (θ) ≤ δ ρ &lt; δ.<label>(14)</label></formula><p>On the other hand, substitute |L n <ref type="formula" target="#formula_0">(11)</ref>, with the same arguments as used to obtain (13), one could also show that, for any compact set K ⊂ Θ,</p><formula xml:id="formula_60">R (θ) − L ∞ R (θ)| ≥ L ∞ R (θ)−L n R (θ) into</formula><formula xml:id="formula_61">P lim inf n→∞ sup θ∈K L n R (θ) ≥ sup θ∈K L ∞ R (θ) = 1.</formula><p>Let K = Θ, by definition,</p><formula xml:id="formula_62">P lim inf n→∞ L n R ( θ R ) ≥ δ = 1.<label>(15)</label></formula><p>Combine equations (14), (15), there exists integer N such that P θ n R ∈ Θ ρ , for all n ≥ N = 1.</p><p>Since this holds for any ρ &gt; 0,</p><formula xml:id="formula_63">P lim n→∞ min θ * ∈Θ * R θ n R − θ * = 0 .<label>(16)</label></formula><p>Define the mapping g from the parameter space Θ to the function space on X ×Y by g(θ) = p(·, · ; θ) where p(x, y; θ) = exp(s(x, y; θ))/Z(x; θ), and Z(x; θ) = y∈Y <ref type="figure">exp(s(x, y; θ)</ref>). By Theorem 4.1,</p><formula xml:id="formula_64">Θ * R = θ | g(θ) = p Y |X , θ ∈ Θ .<label>(17)</label></formula><p>Further by Assumption 4.2, g(θ) is a continuous function of θ under the metric d(·, ·). Since Θ is compact set, g(θ) is uniform continuous. By uniform continuity, for any ε &gt; 0, there exists ∆ such that, for any θ 1 , θ 2 satisfying θ 1 − θ 2 ≤ ∆,</p><formula xml:id="formula_65">d (g(θ 1 ), g(θ 2 )) ≤ ε.</formula><p>Hence, equation <ref type="formula" target="#formula_0">(16)</ref> implies</p><formula xml:id="formula_66">P lim n→∞ d p n Y |X , p Y |X = 0 = 1.</formula><p>When assumption 4.1 holds, by Theorem 4.1, Θ * = {θ * } is a singleton and equation <ref type="formula" target="#formula_0">(16)</ref> is reduced to P lim n→∞ θ n R = θ * = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3 Proof of Theorem 4.6</head><p>Since we have defined y (i,0) := y (i) , the objective function L n R (θ) can be written as the average of the following I.I.D. random variables L (i)</p><formula xml:id="formula_67">R (θ), 1 ≤ i ≤ n: L n R (θ) = 1 n n i=1 log exp(s(x (i) , y (i,0) ; θ)) K j=0 exp(s(x (i) , y (i,j) ; θ) := 1 n n i=1 L (i) R (θ). (18) Define L R (θ) = log exp(s(x, y 0 ; θ)) K j=0 exp(s(x, y j ; θ)))</formula><p>where (x, y 0:K ) ∼ p X,Y (x, y 0 ) K j=1 p N (y j ). Then we have (x (i) , y (i,0:K) ) = d (x, y 0 , · · · , y K )</p><formula xml:id="formula_68">L (i) R (θ) = d L R (θ) for 1 ≤ i ≤ n, where = d represents equal in dis- tribution.</formula><p>Lemma B.4 For any g(x, y 0 , · · · , y K ) : X × Y K+1 → R satisfying g(x, y 0 , · · · , y K ) = g(x, y π(0) , · · · , y π(K) ),</p><p>where π is any permutation on {0, 1, · · · , K}, the following equality holds for any function f (·, ·) :</p><formula xml:id="formula_69">X × Y → R l where l ∈ N + . E   K j=0</formula><p>q(j|x, y 0:K ; θ * )g(x, y 0 , · · · , y K )</p><formula xml:id="formula_70">  = E [g(x, y 0 , · · · , y K )f (x, y 0 )] ,<label>(19)</label></formula><p>where (x, y 0:K ) ∼ p X,Y (x, y 0 ) K j=1 p N (y j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>We use g(x, y 0:K ) to represent g(x, y 0 , · · · , y K ). By definition, </p><formula xml:id="formula_71">  = x,y 0:K p X (x) K j=0 p N (y j ) K + 1 K l=0 p Y |X (y l |x; θ) p N (y l ) × K j=0</formula><p>q(j|x, y 0:K ; θ * )g(x, y 0:K )</p><p>Plug in the definition of q(j|x, y 0:K ; θ * ) and by symmetry,</p><formula xml:id="formula_72">E   K j=0</formula><p>q(j|x, y 0:K ; θ * )g(x, y 0:K )</p><formula xml:id="formula_73">  = x,y 0:K p X (x) K j=0 p N (y j ) K + 1 × K j=0 p Y |X (y j |x) p N (y j ) g(x, y 0:K ) = x,y 0:K p X (x) K j=0 p N (y j ) p Y |X (y 0 |x) p N (y 0 ) × g(x, y 0:K )f (x, y 0 ) = E [g(x, y 0:K )f (x, y 0 )] Lemma B.5 Under the assumptions in Theo- rem 4.6, as n → ∞ √ n θ R − θ * → N (0, V R ) where V R = E ∇ 2 θ L R (θ * ) −1 Var [∇ θ L R (θ * )] E ∇ 2 θ L R (θ * ) −1</formula><p>Proof: By the Mean-Value Theorem,</p><formula xml:id="formula_74">∇ θ L n R ( θ R ) = ∇ θ L n R (θ * )+ 1 0 ∇ 2 θ L n R θ * + t( θ R − θ * ) dt ( θ R − θ * ).</formula><p>Notice that θ R is the maximizer of L n R (θ). By Theorem 4.2, θ R is strongly consistent. Since θ * is in the interior of a compact set Θ, there exists integer N such that for all n &gt; N with probability 1, θ R is also in the interior of Θ. So ∇ θ L n R ( θ R ) = 0 and therefore,</p><formula xml:id="formula_75">θ R − θ * = − 1 0 ∇ 2 θ L n R θ * + t( θ R − θ * ) dt −1 × ∇ θ L n R (θ * )</formula><p>On the one hand, by Strong Law of Large Numbers, for any θ, almost surely,</p><formula xml:id="formula_76">∇ 2 θ L n R (θ) → E[∇ 2 θ L R (θ)].</formula><p>On the other hand, by Theorem 4.2, θ R is strongly consistent. So for any t ∈ [0, 1]</p><formula xml:id="formula_77">∇ 2 θ L n R θ * + t( θ R − θ * ) → E[∇ 2 θ L R (θ * )].</formula><p>Also notice that,</p><formula xml:id="formula_78">∇ θ L R (θ * ) = ∇ θs (x, y 0 ; θ * )− K j=0</formula><p>q(j|x, y 0:K ; θ * )∇ θs (x, y j ; θ * ).</p><p>By Lemma B.4,</p><formula xml:id="formula_80">E[L R (θ * )] = 0.</formula><p>Hence E[L n R (θ * )] = 0 and by Central Limit Theorem,</p><formula xml:id="formula_81">√ n∇ θ L n R (θ * ) → N (0, Var [∇ θ L R (θ * )])</formula><p>. <ref type="formula" target="#formula_0">(21)</ref> Combining these facts together,</p><formula xml:id="formula_82">√ n θ R − θ * → N (0, V R )</formula><p>Define the matrix W R,K by</p><formula xml:id="formula_83">W R,K = E     K j=0 q(j|x, y 0:K ; θ * )∇ θs (x, y j ; θ * )   ×   K j=0 q(j|x, y 0:K ; θ * )∇ θs (x, y j ; θ * )      ,</formula><p>where the expectation is w.r.t. (x, y 0 , · · · , y K ) ∼ p X,Y (x, y 0 ) K j=1 p N (y j ). Lemma B.6</p><formula xml:id="formula_84">W R,K = E K j=0 q(j|x, y 0:K ; θ * ) × ∇ θs (x, y 0 ; θ * )∇ θs (x, y j ; θ * ) = E K j=0 q(j|x, y 0:K ; θ * ) × ∇ θs (x, y j ; θ * )∇ θs (x, y 0 ; θ * )</formula><p>where the expectation is w.r.t. (x, y 0 , · · · , y K ) ∼ p X,Y (x, y 0 ) K j=1 p N (y j ). Notice that,</p><formula xml:id="formula_85">∇ θ L R (θ * ) = ∇ θs (x, y 0 ; θ * )− K j=0 q j ∇ θs (x, y j ; θ * ).</formula><p>We then have </p><formula xml:id="formula_86">∇ 2 θ L R (θ * ) = ∇ 2 θs (x, y 0 ; θ * ) − K j=0 q j ∇ 2 θs (x, y j ; θ * ) − K j=0 q j ∇ θs (x, y j ; θ * )∇ θs (x, y j ; θ * ) +   K j=0 q j ∇ θs (x, y j ; θ * )   ×   K j=0 q j ∇ θs (x,</formula><formula xml:id="formula_87">Var [∇ θ L R (θ * )] = E ∇ θ L R (θ * )∇ θ L R (θ * ) .</formula><p>Observe that,</p><formula xml:id="formula_88">∇ θ L R (θ * )∇ θ L R (θ * ) = ∇ θs (x, y 0 ; θ * )∇ θs (x, y 0 ; θ * ) − K j=0 q j ∇ θs (x, y 0 ; θ * )∇ θs (x, y j ; θ * ) − K j=0 q j ∇ θs (x, y j ; θ * )∇ θs (x, y 0 ; θ * ) +   K j=0 q j ∇ θs (x, y j ; θ * )   ×   K j=0 q j ∇ θs (x, y j ; θ * )   .</formula><p>By Lemma B.6, Then the Lemma follows from Assumption 4.7.</p><formula xml:id="formula_89">Var [∇ θ L R (θ * )] = E ∇ θ L R (θ * )∇ θ L R (θ * ) = E ∇ θs (x, y; θ * )∇ θs (x, y; θ * ) − W R,K .</formula><p>Lemma B.9 Under the assumptions in Theorem 4.6, there exists an integer K 0 such that for some constant C and all K ≥ K 0 ,</p><formula xml:id="formula_90">W R,K = E X E Y |X=x ∇ θs (x, y; θ * ) × E Y |X=x ∇ θs (x, y; θ * ) + ∆ K and ∆ K ≤ C/ √ K Proof:</formula><p>Define the abbreviation q j = q(j|x, y 0:K ; θ * ) and decompose W R,K into two terms,</p><formula xml:id="formula_91">W R,K = E   K j=0 q j ∇ θs (x, y 0 ; θ * )∇ θs (x, y j ; θ * )   = E q 0 ∇ θs (x, y 0 ; θ * )∇ θs (x, y 0 ; θ * ) + E   K j=1 q j ∇ θs (x, y 0 ; θ * )∇ θs (x, y j ; θ * )   := J 1 + J 2 .</formula><p>By Lemma B.8,</p><formula xml:id="formula_92">J 1 ≤ sup x,y ∇ θs (x, y; θ * )∇ θs (x, y; θ * ) × sup x,y q 0 ≤ C 2 × exp(C) K exp(−C) ≤ C 1 K</formula><p>By symmetry,</p><formula xml:id="formula_93">J 2 = K E q 1 ∇ θs (x, y 0 ; θ * )∇ θs (x, y 1 ; θ * )</formula><p>Substitute in the definition of q 1 ,</p><formula xml:id="formula_94">J 2 = x,y 0:K p X (x)p Y |X (y 0 |x) K i=1 p N (y i ) × exp(s(x, y 1 ; θ * )) 1 K K j=0 exp(s(x, y j ; θ * )) × ∇ θs (x, y 0 ; θ * )∇ θs (x, y 1 ; θ * )</formula><p>Notice that exp(s(x, y 1 ; θ * )) = exp(s(x, y 1 ; θ * ))/p N (y 1 ) = p Y |X (y 1 |x)Z(x; θ * )/p N (y 1 ).</p><p>Then we can derive</p><formula xml:id="formula_95">J 2 = x,y 0:K p X (x)p Y |X (y 0 |x)p Y |X (y 1 |x) × K i=2 p N (y i )∇ θs (x, y 0 ; θ * )∇ θs (x, y 1 ; θ * ) × Z(x; θ * ) 1 K K i=0 exp(s(x, y i ; θ * )) := J 3 + δ, where J 3 = x,y 0:K p X (x)p Y |X (y 0 |x)p Y |X (y 1 |x) × K i=2 p N (y i )∇ θs (x, y 0 ; θ * )∇ θs (x, y 1 ; θ * ) = x,y 0 ,y 1 p X (x)p Y |X (y 0 |x)p Y |X (y 1 |x) × ∇ θs (x, y 0 ; θ * )∇ θs (x, y 1 ; θ * ) = x p X (x) y p Y |X (y|x)∇ θs (x, y; θ * ) × y p Y |X (y|x)∇ θs (x, y; θ * ) = E X E Y |X=x ∇ θs (x, y; θ * ) × E Y |X=x ∇ θs (x, y; θ * ) , δ = x,y 0:K p X (x)p Y |X (y 0 |x)p Y |X (y 1 |x) K i=2 p N (y i ) × ∇ θs (x, y 0 ; θ * )∇ θs (x, y 1 ; θ * ) × Z(x; θ * ) − 1 K K i=0 exp(s(x, y i ; θ * )) 1 K K i=0 exp(s(x, y i ; θ * )) = x,y 0:K p X (x)p N (y 0 )p N (y 1 ) K i=2 p N (y i )× × exp(s(x, y 0 ; θ * )) Z(x; θ * ) exp(s(x, y 1 ; θ * )) Z(x; θ * ) × ∇ θs (x, y 0 ; θ * )∇ θs (x, y 1 ; θ * ) 1 K K i=0 exp(s(x, y i ; θ * )) × Z(x; θ * ) − 1 K K i=0</formula><p>exp(s(x, y i ; θ * )) . </p><formula xml:id="formula_96">× ∇ θs (x, y 0 ; θ * )∇ θs (x, y 1 ; θ * ) 1 K K i=0 exp(s(x, y i ; θ * )) ≤ exp(C) exp(−C) × exp(C) exp(−C) × C 2 exp(−C) ≤ C 2 .</formula><p>We then have,</p><formula xml:id="formula_97">δ ≤ C 2 x,y 0:K p X (x) K i=0 p N (y i ) × Z(x; θ * ) − 1 K K i=0 exp (s(x, y i ; θ * )) ≤ C 2 E p X (x) E K i=1 p N (y i ) Z(x; θ * )− 1 K K i=1 exp (s(x, y i ; θ * )) + C 2 K exp(C) ≤ C 2 E p X (x) E K i=1 p N (y i ) Z(x; θ * )− 1 K K i=1 exp(s(x, y i ; θ * )) 2 1/2 + C 2 exp(C) K .</formula><p>Notice that for any given x and y i ∼ p N (y i ), exp(s(x, y i ; θ * ) are I.I.D. random variables with expectation Z(x; θ * ). Therefore, by Lemma B.8,</p><formula xml:id="formula_98">E K i=1 p N (y i ) Z(x; θ * ) − 1 K K i=1 exp(s(x, y i ; θ * )) 2 = 1 K Var Y exp (s(x, y; θ * )) ≤ 1 K E Y exp (2s(x, y; θ * )) ≤ exp(2C) K and for some constant C 3 , δ ≤ C 2 exp(C) √ K + C 2 exp(C) K + 1 ≤ C 3 √ K</formula><p>Combining these facts together,</p><formula xml:id="formula_99">W R,K = E X E Y |X=x ∇ θs (x, y; θ * ) × E Y |X=x ∇ θs (x, y; θ * ) + ∆ K</formula><p>where ∆ K = J 1 + δ and for some constant C 4 ,</p><formula xml:id="formula_100">∆ K ≤ C 4 K .</formula><p>Proof of Theorem 4.6. By Lemma B.5 and B.7,</p><formula xml:id="formula_101">√ n( θ R − θ * ) → N 0, I −1 R,K . = I −1 θ * ∆ K I −1 R,K .</formula><p>Then we have</p><formula xml:id="formula_102">I −1 R,K − I −1 θ * ≤ I −1 θ * ∆ K I −1 R,K ≤ 1 σ min (I θ * ) × C √ K × 2 σ min (I θ * ) ≤ 2C c 2 √ K .</formula><p>Finally, notice that θ R , θ MLE are asymptotically unbiased, </p><formula xml:id="formula_103">| MSE ∞ ( θ R ) − MSE ∞ ( θ MLE )| = Tr(I −1 R,K − I −1 θ * )/d ≤d I −1 R,K − I −1 θ * /d ≤ 2C c 2 √ K B.</formula><formula xml:id="formula_104">g(x, y; θ * , γ * ) = p Y |X (y|x) p Y |X (y|x) + Kp N (y) Proof: Under Assumption 2.2, p Y |X (y|x) = p N (y) exp(s(x, y; θ * ) − γ * ).</formula><p>Hence</p><formula xml:id="formula_105">p Y |X (y|x) p Y |X (y|x) + Kp N (y) = p N (y) exp(s(x, y; θ * ) − γ * ) p N (y) exp(s(x, y; θ * ) − γ * ) + Kp N (y) = exp(s(x, y; θ * ) − γ * ) exp(s(x, y; θ * ) − γ * ) + K Lemma B.11 Define p(y|x; θ, γ) = exp{s(x, y; θ) − γ}</formula><p>For any (θ 1 , γ 1 ), (θ 2 , γ 2 ), the following three statements are equivalent:</p><p>(1) For all x, y, p(y|x; θ 1 , γ 1 ) = p(y|x; θ 2 , γ 2 )</p><p>(2) For all x, y, g(x, y; θ 1 , γ 1 ) = g(x, y; θ 2 , γ 2 )</p><p>(3) For all x, y, s(x, y; θ 1 ) − γ 1 = s(x, y; θ 2 ) − γ 2 . </p><formula xml:id="formula_106">Proof: Since p Y (y) &gt; 0 for all y ∈ Y,</formula><formula xml:id="formula_107">+ Kp X (x)p N (y) log (1 − g(x, y; θ, γ)) = p X,Y (x, y) + Kp X (x)p N (y) p Y |X (y|x) p Y |X (y|x) + Kp N (y) log (g(x, y; θ, γ)) + Kp N (y) p Y |X (y|x) + Kp N (y)</formula><p>log (1 − g(x, y; θ, γ)) .</p><p>By Lemma B.1 and B.10, (θ * , γ * ) ∈ arg max f (θ, γ; x, y) for all x, y.</p><p>Then for any (θ,γ) ∈ arg max L ∞ B (θ, γ), it must be the case that (θ,γ) maximizes f (θ, γ; x, y) for all x, y simultaneously. It follows from Lemma B.1 that It follows from Lemma B.1 and B.11 that (θ,γ) ∈ arg max L ∞ B (θ, γ). So if Ω * B is a singleton, it must be the case thatθ = θ * ,γ = γ * . This proves the second part of the theorem.</p><formula xml:id="formula_108">exp(s(x, y;θ) −γ) exp(s(x, y;θ) −γ) + K = p Y |X (y|x) p Y |X (y|x) + Kp N (y) = exp(s(x, y; θ * ) − γ * ) exp(s(x, y; θ * ) − γ * ) + K .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Proof of Theorem 4.4</head><p>The proof is very similar to the proof of Theorem 4.2 and we leave out the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Proof of Theorem 4.5</head><p>The proof is very similar to the proof of Theorem 4.6 and the proof of Theorem 4.7. We leave out the details. Then the objective function L n B (θ, γ) could be written as the average of n I.I.D. random variables L (i)</p><formula xml:id="formula_109">B (β), 1 ≤ i ≤ n. L n B (θ, γ) = 1 n n i=1 log exp( s(x (i) , y (i,0) ; β)) 1 + exp( s(x (i) , y (i,0) ; β)) + K j=1 log 1 1 + exp( s(x (i) , y (i,j) ; β)) = 1 n n i=1</formula><p>log σ( s(x (i) , y (i) ; β))</p><formula xml:id="formula_110">+ K j=1 log(1 − σ( s(x (i) , y (i,j) ; β))) := 1 n n i=1 L (i) B (β).<label>(24)</label></formula><p>Define L B (β) = log σ( s(x, y 0 ; β))</p><formula xml:id="formula_111">+ K j=1 log(1 − σ( s(x, y j ; β)),</formula><p>where (x, y 0:K ) ∼ p X,Y (x, y 0 ) K j=1 p N (y j ). Then we have (x (i) , y (i,0:K) ) = d (x, y 0 , · · · , y K )</p><formula xml:id="formula_112">L (i) B (β) = d L B (β), 1 ≤ i ≤ n, where = d represents equal in distribution.</formula><p>Throughout the proof, we will repeatedly use the following facts </p><formula xml:id="formula_113">σ (x) = σ(x)(1 − σ(x)),<label>(25)</label></formula><formula xml:id="formula_114">E (1 − σ( s(x, y 0 ; β * ))) f (x, y 0 ) − K j=1 σ( s(x, y 0 ; β * ))f (x, y j ) = 0</formula><p>where (x, y 0:K ) ∼ p X,Y (x, y 0 ) K j=1 p N (y j ). Proof: By definition, where V B is defined as</p><formula xml:id="formula_115">E ∇ 2 β L B (β * ) −1 Var [∇ β L B (β * )] E ∇ 2 β L B (β * ) −1</formula><p>Proof: The proof follows the same arguments as the proof of Lemma B.5 and we leave out the details.</p><p>Define the following quantities, We decompose I 3 into two parts: Notice that (x, y j ) ∼ p X (x)p N (y j ), then again by equation <ref type="formula" target="#formula_11">(26)</ref> </p><formula xml:id="formula_116">I 3 =</formula><formula xml:id="formula_117">= (1 − 1/K) x p X (x) µ x,K ( µ x,K ) = (1 − 1/K) E X [ µ x,K ( µ x,K ) ].</formula><p>where we apply equation <ref type="formula" target="#formula_11">(26)</ref>   </p><formula xml:id="formula_118">V K = W −1 K W K − K + 1 K E X µ x,K ( µ x,K ) W −1 K V = W −1 W − µ µ W −1</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Assumption 4.1 (Identifiability). For any θ ∈ Θ, if there exists a function c(x) such that s(x, y; θ)− s(x, y; θ * ) ≡ c(x) for all (x, y) ∈ X × Y, then θ = θ * and thus c(x) = 0 for all x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 4. 1</head><label>1</label><figDesc>Under Assumption 2.1,θ ∈ Θ * R if and only if, for all (x, y) ∈ X × Y, p Y |X (y|x) = exp(s(x, y;θ))/Z(x,θ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Theorem 4.2 (Consistency) Under Assumptions 2.1, 4.2, 4.3, the estimates based on the ranking objective are strongly consistent in the sense that for any fixed K ≥ 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>based on the binary objective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Theorem 4.4 (Consistency) Under Assumption 2.2, 4.2, 4.5, the estimates defined by the binary objective are strongly consistent in the sense that for any K ≥ 1,P lim n→∞ min (θ * ,γ * )∈Ω * B ( θ n B , γ n B ) − (θ * , γ * ) = 0 = P lim n→∞ d p n Y |X , p Y |X = 0 = 1 If further Assumption 4.4 holds, P lim n→∞ ( θ n B , γ n B ) = (θ * , γ * ) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 4. 5</head><label>5</label><figDesc>Under Assumption 2.1, 4.1, 4.3, and 4.6, if I θ * is non-singular, as n → ∞</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(i) ∈ Y by the condional model p(y|x; θ) = exp(x θ y )/ my y=1 exp(x θ y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>KL divergence between the true distribution and the estimated distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>, y 0:K ; θ * ) = p Y |X (y i |x)/p N (y i ) K j=0 p Y |X (y j |x)/p N (y j ) Proof: The lemma follows Assumption 2.1 that p Y |X (y|x) = exp(s(x, y; θ * )) Z(x; θ * ) = p N (y) exp(s(x, y; θ * )) Z(x; θ * ) Lemma B.3 Define p(y|x; θ) = exp{s(x, y; θ)} y exp{s(x, y; θ)}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>y; θ 1 )−s(x, y; θ 2 ) = s(x, y ; θ 1 )−s(x, y ; θ 2 ). So s(x, y; θ 1 ) − s(x, y; θ 2 ) only depends on x. Finally, we show (3) ⇒ (1). If (3) holds, for all x, y, y s(x, y; θ 1 )−s(x, y; θ 2 ) = s(x, y ; θ 1 )−s(x, y ; θ 2 ), which implies s(x, y; θ 1 )−s(x, y ; θ 1 ) = s(x, y; θ 2 )−s(x, y ; θ 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>q(i|x, y 0:k ;θ) = q(i|x, y 0:k ; θ * ) for all x, y 0:k , i. Then by Lemma B.3, for all x, y p(y|x;θ) = p(y|x; θ * ) = p Y |X (y|x). This proves the first part of the theorem. To prove the second part, for anyθ ∈ Θ * R , by the first part of this theorem, for all x, y p(y|x;θ) = p(y|x; θ * ) = p Y |X (y|x). It follows from Lemma B.3 that there exists function c(x) such that for all x, y s(x, y;θ) − s(x, y; θ * ) = c(x). If Assumption 4.1 holds, θ = θ * , that is Θ * R is a singleton. On the other hand, if there exists function c(x) and parameterθ such that for all x, y s(x, y;θ) − s(x, y; θ * ) = c(x), then by Lemma B.3, for all x, y p(y|x;θ) = p(y|x; θ * ) = p Y |X (y|x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>, y 0:K ; θ * )g(x, y 0:K ) , y 0:K ; θ * )g(x, y 0:K ) = x,y 0:K p X (x) K j=0 p N (y j ) p Y |X (y 0 |x; θ) p N (y 0 ) × K j=0q(j|x, y 0:K ; θ * )g(x, y 0:K ) , y 0:K ; θ * )g(x, y 0:K )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Proof: This follows directly from Lemma B.4. Lemma B.7E[−∇ 2 θ L R (θ * )] = Var[∇ θ L R (θ * )] = E[∇ θs (x, y; θ * )∇ θs (x, y; θ * ) ] − W R,KProof: Define the shorthand, q j = q(j|x, y 0:K ; θ * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>y j ; θ * )   := I 1 − I 2 − I 3 + I 4 By Lemma B.4, E[I 1 − I 2 ] = 0 E[I 3 ] = E[∇ θs (x, y; θ * )s(x, y; θ * ) ]. Hence, E[−∇ 2 θ L R (θ * )] = E[∇ θs (x, y; θ * )∇ θs (x, y; θ * ) ] − W R,K . By Lemma B.4, E[L R (θ * )] = 0. Therefore,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Lemma B. 8</head><label>8</label><figDesc>Under Assumption 4.7, there exists constant C such that sup (x,y)∈X ×Y |s(x, y; θ * )|, ∇ θs (x, y; θ * ) , ∇ 2 θs (x, y; θ * ) ≤ C. Proof: By definition, s(x, y; θ * ) = ∇ θ s(x, y; θ * ) + log p N (y). Then it follows that, ∇ θs (x, y; θ * ) = ∇ θ s(x, y; θ * ), ∇ 2 θs (x, y; θ * ) = ∇ 2 θ s(x, y; θ * ). Since p N (y) &gt; 0 for all y ∈ Y and Y is finite, there exists constant c such that p N (y) &gt; c. Therefore, sup (x,y)∈X ×Y |s(x, y; θ * )| ≤ sup (x,y)∈X ×Y |s(x, y; θ * )| + log(1/c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>By</head><label></label><figDesc>Lemma B.8, there exists constant C 2 such that sup x,y 0:K exp(s(x, y 0 ; θ * )) Z(x; θ * ) exp(s(x, y 1 ; θ * )) Z(x; θ * )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>By</head><label></label><figDesc>Lemma B.11, this is equivalent tō s(x, y;θ) −γ =s(x, y; θ * ) − γ * (22) which implies, s(x, y;θ) −γ = s(x, y; θ * ) − γ * (23) and therefore p(y|x;θ,γ) = p(y|x; θ * , γ * ), which proves the first part of the theorem. Under Assumption 4.4, equation (23) implies (θ,γ) = (θ * , γ * ), that is Ω * B is a singleton. On the other hand, if there exists constant c and parameterθ such that for all x, y s(x, y;θ) − s(x, y; θ * ) = c. We then havē γ = log x,y p X (x) exp(s(x, y; θ * ) + c) = c + γ * and s(x, y;θ) −γ = s(x, y; θ * ) − γ * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Define β = (θ, γ), β = ( θ B , γ B ), β * = (θ * , γ * ) ands(x, y; β) =s(x, y; θ) − γ − log K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>x, y j ; β * ))f (x, y j )p X (x)p N (y)σ( s(x, y; β * ))f (x, y).By equality(26), x, y j ; β * ))f (x, y j ) Y (x, y)(1 − σ( s(x, y; β * )))f (x,y) = E[(1 − σ( s(x, y 0 ; β * )))f (x, y 0 )] Lemma B.13 Under the assumptions in Theorem 4.7, as n → ∞ √ n β B − β * → N (0, V B )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>µ</head><label></label><figDesc>x,K := E Y |X=x (1 − σ( s(x, y; β * ))) ×∇ β s(x, y; β * ) , W K := E p X,Y (1 − σ( s(x, y; β * )))× ∇ β s(x, y; β * )∇ β s(x, y; β * ) .Lemma B.14 E[−∇ 2 β L B (β * )] = W K Var [∇ β L B (β * )] = W K − K + 1 K E X [ µ x,K ( µ x,K ) ]Proof: By straightforward calculation,∇ β L B (β * ) = (1 − σ( s(x, y 0 ; β * ))) ∇ β s(x, y 0 ; β * ) x, y j ; β * ))∇ β s(x, y j ; β * ) ∇ 2 β L B (β * ) = −σ( s(x, y 0 ; β * ))(1 − σ( s(x, y 0 ; β * ))) × ∇ β s(x, y 0 ; β * )∇ β s(x, y 0 ; β * ) + (1 − σ( s(x, y 0 ; β * )))∇ 2 β s(x, y 0 ; β * ) − K j=1 σ( s(x, y j ; β * ))∇ 2 β s(x, y j ; β * ) − K j=1 σ( s(x, y j ; β * ))(1 − σ( s(x, y j ; β * ))) × ∇ β s(x, y j ; β * )∇ β s(x, y j ; β * ) := −I 1 + I 2 − I 3 − I 4 By Lemma B.12, E[I 2 − I 3 ] = 0. Since (x, y 0 ) ∼ p X,Y (x, y 0 ) and (x, y j ) ∼ p X (x)p N (y j ) for all j ≥ 1, E[I 1 ] = x,y p X,Y (x, y)(1 − σ( s(x, y; β * ))) × σ( s(x, y; β * ))∇ β s(x, y; β * )∇ β s(x, y; β * ) E[I 4 ] = K x,y p X (x)p N (y)(1 − σ( s(x, y; β * ))) × σ( s(x, y; β * ))∇ β s(x, y; β * )∇ β s(x, y; β * ) Y (x, y)(1 − σ( s(x, y; β * ))) 2 × ∇ β s(x, y; β * )∇ β s(x, y; β * )Therefore,E[−∇ 2 β L B (β * )] = E [I 1 + I 4 ] = x,y p X,Y (x,y)(1 − σ( s(x, y; β * ))) × ∇ β s(x, y; β * )∇ β s(x, y; β * ) = W K By Lemma B.12, E[L B (β * )] = 0 and henceVar [L B (β * )] = E[∇ β L B (β * )∇ β L B (β * ) ].Observe that,∇ β L B (β * )∇ β L B (β * ) = I 1 − I 2 − I 2 + I 3 where I 1 = (1 − σ( s(x, y 0 ; β * ))) 2 × ∇ β s(x, y 0 ; β * )∇ β s(x, y 0 ; β * ) , x, y j ; β * ))(1 − σ( s(x, y 0 ; β * ))) × ∇ β s(x, y j ; β * )∇ β s(x, y 0 ; β * ) , x, y j ; β * ))σ( s(x, y l ; β * )) × ∇ β s(x, y j ; β * )∇ β s(x, y l ; β * ) .By definition,E[I 1 ] = x,y p X,Y (x, y)(1 − σ( s(x, y; β * ))) 2 × ∇ β s(x, y; β * )∇ β s(x, y; β * ) . Notice that (x, y 0 , y j ) ∼ p X,Y (x, y 0 )p N (y j ),E[I 2 ] = K x,y, y p X,Y (x, y)p N ( y) × σ( s(x, y; β * ))(1 − σ( s(x, y; β * ))) × ∇ β s(x, y; β * )∇ β s(x, y; β * ) By equation (26), E[I 2 ] = x,y, y p X (x)p Y |X (y|x)p Y |X ( y|x) × (1 − σ( s(x, y; β * )))(1 − σ( s(x, y; β * ))) × ∇ β s(x, y; β * )∇ β s(x, y; β * ) = x p X (x) µ x,K µ x,K = E X [ µ x,K ( µ x,K ) ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>s(x, y j ; β * )) × ∇ β s(x, y j ; β * )∇ β s(x, y j ; β * ) + 1≤j =l≤k σ( s(x, y j ; β * ))σ( s(x, y l ; β * )) × ∇ β s(x, y j ; β * )∇ β s(x, y l ; β * ) = I 1 3 + I 2 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>x)p N (y)σ 2 ( s(x, y; β * ))× ∇ β s(x, y; β * )∇ β s(x, y; β * ) = x,y p X,Y (x, y)(1 − σ( s(x, y; β * ))) × σ( s(x, y; β * ))∇ β s(x, y; β * )∇ β s(x, y; β * ) , Y (x, y)(1 − σ( s(x, y; β * ))) × ∇ β s(x, y; β * )∇ β s(x, y; β * ) = W K Also notice that (x, y j , y l ) ∼ p X (x)p N (y j )p N (y l ) for 1 ≤ l = j ≤ K, E[I 2 3 ] = (K 2 − K) x,y, y p X (x)p N (y)p N ( y)× σ( s(x, y; β * ))σ( s(x, y; β * )) × ∇ β s(x, y; β * )∇ β s(x, y; β * )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>W</head><label></label><figDesc>to obtain the second equality. Combining these quantities,Var [∇ β L B (β * )] = W K − (1 + 1/K) E X [ µ x,K ( µ x,K ) ]Define the following quantities,µ = E[∇ θs (x, y; θ * )], µ = µ −1 , µ x = E Y |X=x [∇ θs (x, y; θ * )] = E ∇ θs (x, y; θ * )∇ θs (x, y; θ * ) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Lemma B. 15</head><label>15</label><figDesc>Under Assumption 2.2, the Fisher information matrix can be simplified toI θ * = Var X,Y [∇ θ s(x, y; θ * )]. x) exp(s(x, y; θ * ) − γ * ). x) exp(s(x, y; θ * )).Take derivative w.r.t. θ on both sides, y∈Y exp(s(x, y; θ * ))∇ θ s(x, y; θ * ) = x∈X y∈Y p X (x) exp(s(x, y; θ * ))∇ θ s(x, y; θ * ) which implies E Y |X=x [∇ θ s(x, y; θ * )] =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Training examples {x (i) , y (i) } n i=1 , sampling distribution pN (·) for generating negative examples, an integer K specifying the number of negative examples per training example, a scoring function s(x, y; θ). Flags {BINARY = true, RANKING = false} if binary classification objective is used, {BINARY = false, RANKING = true} if ranking objective is used.</figDesc><table /><note>Inputs:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Perplexity on the test set of Penn Treebank. We show performance for the ranking v.s. binary loss algorithms, with different values for K, and with/without regularization.</figDesc><table><row><cell></cell><cell>Small</cell><cell></cell><cell cols="2">Medium</cell><cell>Large</cell><cell></cell></row><row><cell>MLE</cell><cell>111.5</cell><cell></cell><cell>82.7</cell><cell></cell><cell>78.4</cell><cell></cell></row><row><cell>NCE</cell><cell cols="6">Ranking Binary Ranking Binary Ranking Binary</cell></row><row><cell>K = 200</cell><cell>113.8</cell><cell>106.8</cell><cell>83.2</cell><cell>82.1</cell><cell>79.3</cell><cell>76.0</cell></row><row><cell>K = 400</cell><cell>112.9</cell><cell>105.6</cell><cell>82.3</cell><cell>81.5</cell><cell>77.9</cell><cell>75.6</cell></row><row><cell>K = 800</cell><cell>111.9</cell><cell>105.3</cell><cell>81.4</cell><cell>81.6</cell><cell>77.8</cell><cell>75.7</cell></row><row><cell>K = 1600</cell><cell>110.6</cell><cell>104.8</cell><cell>81.7</cell><cell>81.5</cell><cell>77.5</cell><cell>75.9</cell></row><row><cell>reg-MLE</cell><cell>105.4</cell><cell></cell><cell>79.9</cell><cell></cell><cell>77.0</cell><cell></cell></row><row><cell>reg-Ranking (K = 1600)</cell><cell>105.4</cell><cell></cell><cell>79.8</cell><cell></cell><cell>75.0</cell><cell></cell></row><row><cell>reg-Binary (K = 1600)</cell><cell>104.8</cell><cell></cell><cell>82.5</cell><cell></cell><cell>75.7</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table 2</head><label>2</label><figDesc></figDesc><table /><note>below.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Perplexity on the validation set. We show performance for the Ranking vs. Binary loss algorithms, with different values for K, and with/without regularization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>2 Proofs for Binary Classification LossRecall that for any x, y and parameter (θ, γ), we have defined</figDesc><table><row><cell>g(x, y; θ, γ) =</cell><cell>exp(s(x, y; θ) − γ) exp(s(x, y; θ) − γ) + K</cell><cell>.</cell></row><row><cell cols="2">Lemma B.10 Under Assumption 2.2,</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Notice that L ∞ B (θ, γ) = x,y f (θ, γ; x, y), where f (θ, γ; x, y) =p X,Y (x, y) log (g(x, y; θ, γ))</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>B.2.1 Proof of Theorem 4.3</cell></row><row><cell>g(x, y; θ, γ) =</cell><cell cols="2">p(y|x; θ, γ) p(y|x; θ, γ) + Kp N (y)</cell><cell>,</cell></row><row><cell cols="4">and hence (1) ⇒ (2). Observe that,</cell></row><row><cell>s(x, y; θ)−γ = log</cell><cell>g(x, y; θ, γ) 1 − g(x, y; θ, γ)</cell><cell cols="2">+log(K)</cell></row><row><cell>and by definition</cell><cell></cell><cell></cell></row><row><cell cols="3">s(x, y; θ) =s(x, y; θ) + log p N (y)</cell></row><row><cell cols="4">We then have (2) ⇒ (3). Finally, (3) ⇒</cell></row><row><cell cols="4">(1) is obtained by plugging in the definition of</cell></row><row><cell>g(x, y; θ, γ).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>p X,Y (x, y)(1 − σ( s(x, y; β * ))) =Kp X (x)p N (y)σ( s(x, y; β * )).Lemma B.12For any function f (·, ·) : X × Y → R m , m ∈ N + ,</figDesc><table><row><cell>(26)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Emily Pitler and Ali Elkahky for many useful conversations about the work, and David Weiss for comments on an earlier draft of the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Notice that, W − µ µ = I θ * 0 0 0 .</p><p>We then have,</p><p>Lemma B.17 Under the assumptions in Theorem 4.7, there exists an integer K 0 and positive constant C &gt; 0 such that for all K ≥ K 0 , W K is non-singular and</p><p>Proof: By definition and Lemma B.8,</p><p>Notice that,</p><p>Again by Lemma B.8,</p><p>Similarly, by Lemma B.8, sup x µ x ≤ C + 1. Therefore, there exist constants C 3 , C 4 such that</p><p>On the other hand,</p><p>× ∇ θs (x, y; θ * ) 1 ∇ θs (x, y; θ * ) 1 .</p><p>By the same reasoning,</p><p>Let σ min (·) denote the smallest singular value a matrix. By Lemma B.16, W is non-singular. So there exists K 0 such that σ min W ≥ 2 ∆ W K . Therefore, for any K ≥ K 0 , by Weyl's inequality,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="722" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
