<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Nabati</surname></persName>
							<email>rnabati@utk.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tennessee Knoxville</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tennessee Knoxville</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The perception system in autonomous vehicles is responsible for detecting and tracking the surrounding objects. This is usually done by taking advantage of several sensing modalities to increase robustness and accuracy, which makes sensor fusion a crucial part of the perception system. In this paper, we focus on the problem of radar and camera sensor fusion and propose a middle-fusion approach to exploit both radar and camera data for 3D object detection. Our approach, called CenterFusion, first uses a center point detection network to detect objects by identifying their center points on the image. It then solves the key data association problem using a novel frustum-based method to associate the radar detections to their corresponding object's center point. The associated radar detections are used to generate radar-based feature maps to complement the image features, and regress to object properties such as depth, rotation and velocity. We evaluate CenterFusion on the challenging nuScenes dataset, where it improves the overall nuScenes Detection Score (NDS) of the state-of-the-art camera-based algorithm by more than 12%. We further show that CenterFusion significantly improves the velocity estimation accuracy without using any additional temporal information. The code is available at https://github.com/mrnabati/CenterFusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autonomous vehicles are usually equipped with different types of sensors to take advantage of their complimentary characteristics. Using multiple sensor modalities increases robustness and accuracy, but also introduces new challenges in designing the perception system. Sensor fusion is one of these challenges, which has motivated many studies in 2D and 3D object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">19]</ref>, semantic segmentation <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b15">16]</ref> and object tracking <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> in recent years.</p><p>Most of the recent sensor fusion methods focus on exploiting LiDAR and camera for 3D object detection. Li-DARs use the time of flight of laser light pulses to calcu-late distance to surrounding objects. LiDARs provide accurate 3D measurement at close range, but the resulting point cloud becomes sparse at long range, reducing the system's ability to accurately detect far away objects. Cameras provide rich appearance features, but are not a good source of information for depth estimation. These complementary features have made LiDAR-camera sensor fusion a topic of interest in recent years. This combination has been proven to achieve high accuracy in 3D object detection for many applications including autonomous driving, but it has its limitations. Cameras and LiDARs are both sensitive to adverse weather conditions (e.g. snow, fog, rain), which can significantly reduce their field of view and sensing capabilities. Additionally, LiDARs and cameras are not capable of detecting objects' velocity without using temporal information. Estimating objects' velocity is a crucial requirement for collision avoidance in many scenarios, and relying on the temporal information might not be a feasible solution in time-critical situations. For many years, radars have been used in vehicles for Advanced Driving Assistance System (ADAS) applications such as collision avoidance and Adaptive Cruise Control (ACC). Compared to LiDARs and cameras, radars are very robust to adverse weather conditions and are capable of detecting objects at very long range (up to 200 meters for automotive radars). Radars use the Doppler effect to accurately estimate the velocities of all detected objects, without requiring any temporal information. Additionally, compared to LiDARs, Radar point clouds require less processing before they can be used as object detection results. These features and their lower cost compared to LiDARs, have made radars a popular sensor in autonomous driving applications.</p><p>Despite radar's popularity in the automotive industry, few studies have focused on fusing radar data with other sensors. One reason for this is the fact that there are not many datasets containing radar data for autonomous driving applications, which makes conducting research in this area difficult. Additionally, due to inherent differences between LiDAR and radar point clouds, applying or adapting existing LiDAR-based algorithms to radar point cloud proves to be extremely difficult. <ref type="bibr">Radar</ref>   The frustum association module uses the preliminary boxes to associate radar detections to objects and generate radar feature maps. The image and radar features maps are then concatenated and used to refine the preliminary detections by recalculating depth and rotation as well as estimating objects' velocity and attributes. more sparse than their LiDAR counter parts, making it unfeasible to use for extracting objects' geometry information. Aggregating multiple radar sweeps increases the density of the points, but also introduces delay into the system. Moreover, although radar point clouds are usually represented as points in the 3D coordinate system, the reported vertical measurement of the points are usually not accurate or even non-existent, as most automotive radars only report the distance and azimuth angle to objects.</p><p>In order to effectively combine multiple sensing modalities, a variety of sensor fusion schemes have been developed <ref type="bibr" target="#b7">[8]</ref> taking advantage of the hierarchical feature representation in neural networks. In an early fusion approach, the raw or pre-processed sensory data from different sensor modalities are fused together. With this approach, the network learns a joint representation from the sensing modalities. Early fusion methods are usually sensitive to spatial or temporal misalignment of the data <ref type="bibr" target="#b7">[8]</ref>. On the other hand, a late fusion approach combines the data from different modalities at the decision level, and provides more flexibility for introducing new sensing modalities to the network. However, a late fusion approach does not exploit the full potential of the available sensing modalities, as it does not acquire the intermediate features obtained by learning a joint representation. A compromise between the early and late fusion approaches is referred to as middle fusion. It extracts features from different modalities individually and combines them at an intermediate stage, enabling the network to learn joint representations and creating a balance between sensitivity and flexibility.</p><p>We propose CenterFusion, a middle-fusion approach to exploit radar and camera data for 3D object detection. Cen-terFusion focuses on associating radar detections to preliminary detection results obtained from the image, then generates radar feature maps and uses it in addition to image features to accurately estimate 3D bounding boxes for ob-jects. Particularly, we generate preliminary 3D detections using a key point detection network, and propose a novel frustum-based radar association method to accurately associate radar detections to their corresponding objects in the 3D space. These radar detections are then mapped to the image plane and used to create feature maps to complement the image-based features. Finally, the fused features are used to accurately estimate objects' 3D properties such as depth, rotation and velocity. The network architecture for CenterFusion is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>We evaluate CenterFusion on the challenging nuScenes <ref type="bibr" target="#b1">[2]</ref> dataset, where it outperforms all previous camera-based object detection methods in the 3D object detection benchmark. We also show that exploiting radar information significantly improves velocity estimation for objects without using any temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single-modality Methods</head><p>Monocular 3D object detection methods use a single camera to estimate 3D bounding boxes for objects. Many studies have been reported, taking different approaches to extract the depth information from monocular images. 3D RCNN <ref type="bibr" target="#b10">[11]</ref> uses Fast R-CNN <ref type="bibr" target="#b8">[9]</ref> with an additional head and 3D projection. It also uses a collection of CAD models to learn class-specific shape priors for objects. Deep3DBox <ref type="bibr" target="#b17">[17]</ref> regresses a set of 3D object properties using a convolutional neural network first, then uses the geometric constraints of 2D bounding boxes to produce a 3D bounding box for the object. CenterNet <ref type="bibr" target="#b34">[34]</ref> takes a different approach and uses a keypoint detection network to find objects' center point on the image. Other object properties such as 3D dimension and location are obtained by regression using only the image features at the object's center point. <ref type="figure">Figure 2</ref>. Difference between actual and radial velocity. For target A, velocity in the vehicle coordinate system and the radial velocity are the same (v A ). For target B on the other hand, radial velocity (vr) as reported by the radar is different from the actual velocity of the object (v B ) in the vehicle coordinate system.</p><formula xml:id="formula_0">X Y Z B A ! # ! $ ! %</formula><p>LiDARs have been widely used for 3D object detection and tracking in autonomous driving applications in recent years. The majority of LiDAR-based methods either use 3D voxels <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">35]</ref> or 2D projections <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b31">31]</ref> for point cloud representation. Voxel-based methods are usually slow as a result of the voxel grid's high dimensionality, and projection-based methods might suffer from large variances in object shapes and sizes depending on the projection plane. PointRCNN <ref type="bibr" target="#b25">[25]</ref> directly operates on raw point clouds and generates 3D object proposals in a bottom-up manner using point cloud segmentation. These proposals are refined in the second stage to generate the final detection boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Fusion-based Methods</head><p>Most existing sensor fusion methods focus on the Li-DAR and camera fusion problem. MV3D <ref type="bibr" target="#b3">[4]</ref> extracts features from the front view and Bird's Eye View (BEV) representations of the LiDAR data, in addition to the RGB image. The features obtained from the LiDAR's BEV are then used to generate 3D object proposals, and a deep fusion network is used to combine features from each view and predict the object class and box orientations. PointFusion <ref type="bibr" target="#b28">[28]</ref> processes the image and LiDAR data using a CNN and a PointNet model respectively, and then generate 3D object proposals using the extracted features. Frustum PointNet <ref type="bibr" target="#b23">[23]</ref> directly operates on the raw point clouds obtained from an RGB-D camera and uses the RGB image and a 2D object detector to localize objects in the point cloud.</p><p>Few studies have focused on fusing radars with other sensors for autonomous driving applications. RadarNet <ref type="bibr" target="#b30">[30]</ref> fuses radar and LiDAR data for 3D object detection. It uses an early fusion mechanism to learn joint representations from the two sensors, and a late-fusion mechanism to exploit radar's radial velocity evidence and improve the estimated object velocity. In <ref type="bibr" target="#b2">[3]</ref>, Chadwick et al. project radar detections to the image plane and use them to boost the ob-ject detection accuracy for distant objects. In <ref type="bibr" target="#b20">[20]</ref> authors use radar detections to generate 3D object proposals first, then project them to the image plane to perform joint 2D object detection and depth estimation. CRF-Net <ref type="bibr" target="#b22">[22]</ref> also projects radar detections to the image plane, but represents them as vertical lines where the pixel values correspond to the depth of each detection point. The image data is then augmented with the radar information and used in a convolutional network to perform 2D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Radar Point Cloud</head><p>Radars are active sensors that transmit radio waves to sense the environment and measure the reflected waves to determine the location and velocity of objects. Automotive radars usually report the detected objects as 2D points in BEV, providing the azimuth angle and radial distance to the object. For every detection, the radar also reports the instantaneous velocity of the object in the radial direction. This radial velocity does not necessarily match the object's actual velocity vector in it's direction of movement. <ref type="figure">Fig. 2</ref> illustrates the difference between the radial as reported by the radar, and actual velocity of the object in the vehicle's coordinate system.</p><p>We represent each radar detection as a 3D point in the egocentric coordinate system, and parameterize it as P = (x, y, z, v x , v y ) where (x, y, z) is the position and (v x , v y ) is the reported radial velocity of the object in the x and y directions. The radial velocity is compensated by the ego vehicle's motion. For every scene, we aggregate 3 sweeps of the radar point cloud (detections within the past 0.25 seconds). The nuScenes dataset provides the calibration parameters needed for mapping the radar point clouds from the radar coordinates system to the egocentric and camera coordinate systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CenterNet</head><p>CenterNet <ref type="bibr" target="#b34">[34]</ref> represents the state-of-the-art in 3D object detection using single camera. It takes an image I ∈ R W ×H×3 as input and generates a keypoint heatmap Y ∈ [0, 1]   . Directly mapping the pillars to the image and replacing with radar depth information results in poor association with objects' center and many overlapping depth values (middle image). Frustum association accurately maps the radar detections to the center of objects and minimizes overlapping (bottom image). Radar detections are only associated to objects with a valid ground truth or detection box, and only if all or part of the radar detection pillar is inside the box. Frustum association also prevents associating radar detections caused by background objects such as buildings to foreground objects, as seen in the case of pedestrians on the right hand side of the image.</p><formula xml:id="formula_1">Y qc = max i exp(− (p i − q) 2 2σ 2 i ) (1) ! " # Image Plane ! " ROI Frustum</formula><p>where σ i is a size-adaptive standard deviation, controlling the size of the heatmap for every object based on its size. A fully convolutional encode-decoder network is used to predictŶ .</p><p>To generate 3D bounding boxes, separate network heads are used to regress object's depth, dimensions and orientation directly from the detected center points. Depth is calculated as an additional output channelD ∈ [0, 1]  <ref type="bibr" target="#b18">[18]</ref>. For each center point, a local offset is also predicted to compensate for the discretization error caused by the output strides in the backbone network <ref type="bibr" target="#b34">[34]</ref>.</p><p>Given the annotated objects p 0 , p 1 , ... in an image, the training objective is defined as below based on the focal loss <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_2">L k = 1 N xyc    (1 −Ŷxyc) α log(Ŷxyc) Yxyc = 1 (1 − Yxyc) β (Ŷxyc) α log(1 −Ŷxyc) otherwise ,</formula><p>where N is the number of objects, Y ∈ [0, 1] W R × H R ×C is the annotated objects' ground-truth heatmap and α and β are focal loss hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CenterFusion</head><p>In this section we present our approach to radar and camera sensor fusion for 3D object detection. The overall Cen-terFusion architecture is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We adopt <ref type="bibr" target="#b34">[34]</ref> as our center-based object detection network to detect objects' center points on the image plane, and regress to other object properties such as 3D location, orientation and dimensions. We propose a middle-fusion mechanism that associates radar detections to their corresponding object's center point and exploits both radar and image features to improve the preliminary detections by re-estimating their depth, velocity, rotation and attributes.</p><p>The key in our fusion mechanism is accurate association of radar detections to objects. The center point object detection network generates a heat map for every object category in the image. The peaks in the heat map represent possible center points for objects, and the image features at those locations are used to estimate other object properties. To exploit the radar information in this setting, radar-based features need to be mapped to the center of their corresponding object on the image, which requires an accurate association between the radar detections and objects in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Center Point Detection</head><p>We adopt the CenterNet <ref type="bibr" target="#b34">[34]</ref> detection network for generating preliminary detections on the image. The image features are first extracted using a fully convolutional encoderdecoder backbone network. We follow CenterNet <ref type="bibr" target="#b34">[34]</ref> and use a modified version of the Deep Layer Aggregation (DLA) network <ref type="bibr" target="#b32">[32]</ref> as the backbone. The extracted image features are then used to predict object center points on the image, as well as the object 2D size (width and height), center offset, 3D dimensions, depth and rotation. These values are predicted by the primary regression heads as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Each primary regression head consists of a 3 × 3 convolution layer with 256 channels and a 1 × 1 convolutional layer to generate the desired output. This provides an accurate 2D bounding box as well as a preliminary 3D bounding box for every detected object in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Radar Association</head><p>The center point detection network only uses the image features at the center of each object to regress to all other object properties. To fully exploit radar data in this process, we first need to associate the radar detections to their corresponding object on the image plane. To accomplish this, a naïve approach would be mapping each radar detection point to the image plane and associating it to an object if the point is mapped inside the 2D bounding box of that object. This is not a very robust solution, as there is not a one-to-one mapping between radar detections and objects in the image; Many objects in the scene generate multiple radar detections, and there are also radar detections that do not correspond to any object. Additionally, because the z dimension of the radar detection is not accurate (or does not exist at all), the mapped radar detection might end up outside the 2D bounding box of its corresponding object. Finally, radar detections obtained from occluded objects would map to the same general area in the image, which makes differentiating them in the 2D image plane difficult, if possible at all.</p><p>Frustum Association Mechanism: We develop a frustum association method that uses the object's 2D bounding box as well as its estimated depth and size to create a 3D Region of Interest (RoI) frustum for the object. Having an accurate 2D bounding box for an object, we create a frustum for that object as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. This significantly narrows down the radar detections that need to be checked for association, as any point outside this frustum can be ignored. We then use the estimated object depth, dimension and rotation to create a RoI around the object, to further filter out radar detections that are not associated with this object. If there are multiple radar detections inside this RoI, we take the closest point as the radar detection corresponding to this object.</p><p>In the training phase, we use the object's 3D ground truth bounding box to create a tight RoI frustum and associate radar detections to the object. In the test phase, the RoI frustum is calculated using the object's estimated 3D bounding box as explained before. In this case, we use a parameter δ to control the size of the RoI frustum as shown in <ref type="figure" target="#fig_2">Fig.  3</ref>. This is to account for inaccuracy in the estimated depth values, as the depth of the object at this stage is solely determined using the image-based features. Enlarging the frustum using this parameter increases the chance of including the corresponding radar detections inside the frustum even if the estimated depth is slightly off. The value of δ should be carefully selected, as a large RoI frustum can include radar detections of nearby objects. The RoI frustum approach makes associating overlapping objects effortless, as objects are separated in the 3D space and would have separate RoI frustums. It also eliminates the multi-detection association problem, as only the closest radar detection inside the RoI frustum is associated to the object. It does not, however, help with the inaccurate z dimension problem, as radar detections might be outside the ROI frustum of their corresponding object due to their inaccurate height information.</p><p>Pillar Expansion: To address the inaccurate height information problem, we introduce a radar point cloud preprocessing step called pillar expansion, where each radar point is expanded to a fixed-size pillar, as illustrated in <ref type="figure" target="#fig_3">Fig.  4</ref>. Pillars create a better representation for the physical objects detected by the radar, as these detections are now associated with a dimension in the 3D space. Having this new representation, we simply consider a radar detection to be inside a frustum if all or part of its corresponding pillar is inside the frustum, as shown in <ref type="figure" target="#fig_0">Fig. 1.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Radar Feature Extraction</head><p>After associating radar detections to their corresponding objects, we use the depth and velocity of the radar detections to create complementary features for the image. Particularly, for every radar detection associated to an object, we generate three heat map channels centered at and inside the object's 2D bounding box, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The width and height of the heatmaps are proportional to the object's 2D bounding box, and are controlled by a parameter α. The heatmap values are the normalized object depth (d) and also the x and y components of the radial velocity (v x and v y ) in the egocentric coordinate system:</p><formula xml:id="formula_3">F j x,y,i = 1 M i      f i |x − c j x | ≤ αw j and |y − c i y | ≤ αh j 0 otherwise ,</formula><p>where i ∈ 1, 2, 3 is the feature map channel, M i is a normalizing factor, f i is the feature value (d, v x or v y ), c j x and c j y are the x and y coordinates of the jth object's center point on the image and w j and h j are the width and height of the jth object's 2D bounding box. If two objects have overlapping heatmap areas, the one with a smaller depth value dominates, as only the closest object is fully visible in the image. The generated heat maps are then concatenated to the image features as extra channels. These features are used as inputs to the secondary regression heads to recalculate the object's depth and rotation, as well as velocity and attributes. The velocity regression head estimates the x and y components of the object's actual velocity in the vehicle coordinate system. The attribute regression head estimates different attributes for different object classes, such as moving or parked for the Car class and standing or sitting for the Pedestrian class. The secondary regression heads consist of three convolutional layers with 3 × 3 kernels followed by a 1 × 1 convolutional layer to generate the desired output. The extra convolutional layers compared to the primary regression heads help with learning higher level features from the radar feature maps. The last step is decoding the regression head results into 3D bounding boxes. The box decoder block uses the estimated depth, velocity, rotation, and attributes from the secondary regression heads, and takes the other object properties from the primary heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>We use the pre-trained CenterNet <ref type="bibr" target="#b34">[34]</ref> network with the DLA <ref type="bibr" target="#b32">[32]</ref> backbone as our object detection network. DLA uses iterative deep aggregation layers to increase the resolution of feature maps. CenterNet compares its performance using different backbone architectures, with the Hourglass network <ref type="bibr" target="#b21">[21]</ref> performing better than others. We choose to use the DLA network as it takes significantly less time to train while providing a reasonable performance.</p><p>We directly use the released CenterNet model that is trained for 140 epochs on the nuScenes dataset. This model by default does not provide velocity and attribute predictions. We train the velocity and attribute heads for 30 epochs, and use the resulting model as our baseline. The secondary regression heads in our network are added on top of the CenterNet backbone network, and are trained using the image and radar features for an additional 60 epochs with a batch size of 26 on two Nvidia P5000 GPUs.</p><p>During both training and testing, we reduce the image resolution from the original 1600×900 pixels to 800×450 pixels. Data augmentation is used during training, with random right-left flipping (with a probability of 0.5) and random shifting (from 0 to 20 percent of image size). The same augmentations are also applied to the radar point cloud in reference to the camera coordinate system. We do not apply any scaling augmentation as it changes the 3D measurements. At testing time, we only use flip test augmentation where an image and its flipped version are fed into the net-work and the average of the network outputs is used for decoding the 3D bounding boxes. We do not use the multiscale test augmentation as used by CenterNet. The pillar size is set to [0.2, 0.2, 1.5] meters in the [x, y, z] directions and δ is set to increase the length of the RoI frustum by 20% in the radial direction at test time.</p><p>We use the L1 loss for most of the regression heads, with the exception of the center point heat map head which uses the focal loss and the attributes regression head that uses the Binary Cross Entropy (BCE) loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>We compare our radar and camera fusion network with the state-of-the-art camera-based models on the nuScenes benchmark, and also a LIDAR based method. <ref type="table">Table 1</ref> shows the results on both test and validation splits of the nuScenes dataset. We compare with OFT <ref type="bibr" target="#b24">[24]</ref>, MonoDIS <ref type="bibr" target="#b26">[26]</ref> and CenterNet <ref type="bibr" target="#b34">[34]</ref> which are camera-based 3D object detection networks, as well as InfoFocus <ref type="bibr" target="#b27">[27]</ref> which is a LIDAR-based method. As seen in <ref type="table">Table 1</ref>, CenterFusion outperforms all other methods in the nuScenes NDS score, which is a weighted sum of the mAP and the error metrics. On the test dataset, CenterFusion shows a 12.25% and 16.9% relative increase in the NDS score compared to CenterNet and MonoDIS respectively. The LIDAR-based method InfoFocus shows a better performance in the mAP score compared to other methods, but is significantly outperformed by CenterFusion in the orientation, velocity and attribute error metrics. While CenterNet with the Hourglass <ref type="bibr" target="#b21">[21]</ref> backbone network results in a better mAP score compared to CenterFusion (1.2% difference) on the test split, the results on the validation split show that CenterFusion outperforms CenterNet by 2.6% when both networks use the same DLA <ref type="bibr" target="#b32">[32]</ref> backbone. The validation set results also show CenterFusion improving CenterNet in all the other metrics. CenterFusion shows an absolute gain of 38.1% and 62.1% relative increase in the NDS and velocity error metrics compared to CenterNet, which demonstrates the effectiveness of using radar features. <ref type="table">Table 2</ref> compares the per-class mAP results for both test and validation splits. While CenterNet with an Hourglass backbone has a higher mAP than CenterFusion for most classes in the test set, it is outperformed by CenterFusion on the validation set where the DLA backbone is used for both methods. The most improved classes on the validation set are the motorcycle and car with 5.6% and 4.0% absolute mAP increase respectively. <ref type="figure" target="#fig_5">Fig. 5</ref> demonstrates the 3D object detection results in both camera and BEV. It shows the detection results from CenterFusion (row 1 &amp; 2) and CenterNet (row 3 &amp; 4) for 4 different scenes. The radar point clouds are also shown in the CenterFusion BEV results. Compared to CenterNet, the results from CenterFusion show a better fit for 3D boxes <ref type="table">Table 1</ref>. Performance comparison for 3D object detection on nuScenes dataset. mATE, mASE, mAOE, mAVE and mAAE stand for average translation, scale, orientation, velocity and attribute errors respectively. ↑ indicates that higher is better and ↓ indicates that lower is better. "C", "R" and "L" specify camera, radar and LIDAR modalities respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality</head><p>Error </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Ablation Study</head><p>We validate the effectiveness of our fusion approach by conducting an ablation study on the nuScenes validation set. We use the CenterNet model as our baseline, and study the effectiveness of the pillar expansion, frustum association and flip testing on the detection results. <ref type="table" target="#tab_2">Table 3</ref> shows the overall detection results of the ablation study.</p><p>In the first experiment, we only apply pillar expansion to the radar point clouds, and map the 3D pillars to the image plane and obtain their equivalent 2D bounding boxes. These boxes are then filled with the depth and velocity values of their corresponding radar detections and used as the radar feature maps, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. According to <ref type="table" target="#tab_2">Table  3</ref>, this simple association method results in a 15.4% relative improvement on the NDS score and 1.0% absolute improvement on the mAP compared to the baseline.</p><p>In the next experiment we only use the frustum association method by directly applying it on the radar point clouds without converting them to pillars first. This improves the NDS score by 25.9% relatively and mAP by 2.0%. Applying both pillar expansion and frustum association results in a relative 35.5% and absolute 4.3% improvement on the NDS and mAP scores respectively. Flip testing adds an-other 3.3% improvement on the NDS score and 3.9% on the mAP, resulting in a total of 37.8% and 8.4% improvement on NDS and mAP compared to the baseline method. <ref type="table">Table 4</ref> shows the per-class contribution of each step on the mAP. According to the results, both pillar expansion and frustum association steps have contributed to the improvement of mAP in most object classes. The only class that has not improved from the baseline is the bicycle class, in which the CenterNet mAP score is better than CenterFusion by 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In summary, we proposed a new radar and camera fusion algorithm called CenterFusion, to exploit radar information for robust 3D object detection. CenterFusion accurately associates radar detections to objects on the image using a frustum-based association method, and creates radar-based feature maps to complement the image features in a middlefusion approach. Our frustum association method uses preliminary detection results to generate a RoI frustum around objects in 3D space, and maps the radar detection to the center of objects on the image. We also used a pillar expansion method to compensate for the inaccuracy in radar detections' height information, by converting radar points to fixed-size pillars in the 3D space. We evaluated our proposed method on the challenging nuScenes 3D detection benchmark, where CenterFusion outperformed the state-ofthe-art camera-based object detection methods.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>CenterFusion network architecture. Preliminary 3D boxes are first obtained using the image features extracted by the backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>R</head><label></label><figDesc>×C as output where W and H are the image width and height, R is the downsampling ratio and C is the number of object categories. A prediction ofŶ x,y,c = 1 as the output indicates a detected object of class c centered at position (x, y) on the image. The ground-truth heatmap Y ∈ [0, 1] W R × H R ×C is generated from the ground-truth 2D bounding boxes using a Gaussian kernel. For each bounding box center point p i ∈ R 2 of class c in the image, a Gaussian heatmap is generated on Y :,:,c . The final value of Y for class c at position q ∈ R 2 is defined as [34]:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Frustum association. An object detected using the image features (left), generating the ROI frustum based on object's 3D bounding box (middle), and the BEV of the ROI frustum showing radar detections inside the frustum (right). δ is used to increase the frustum size in the testing phase.d is the ground truth depth in the training phase and the estimated object depth in the testing phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Expanding radar points to 3D pillars (top image)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the inverse sigmoidal transformation used in Eigen et al. [6] to the original depth domain. The object dimensions are directly regressed to their absolute values in meter as three output channelsΓ ∈ [0, 1] W R × H R ×3 . Orientation is encoded as two bins with 4 scalars in each bin, following the orientation representation in Mousavian et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results from CenterFusion (row 1 &amp; 2) and CenterNet (row 3 &amp; 4) in camera view and BEV. In the BEV plots, detection boxes are shown in cyan and ground truth boxes in red. The radar point cloud is shown in green. Red and blue arrows on objects show the ground truth and predicted velocity vectors respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>point clouds are significantly arXiv:2011.04841v1 [cs.CV] 10 Nov 2020</figDesc><table><row><cell>Fully Convolutional</cell><cell cols="2">Primary Regression Heads</cell><cell></cell></row><row><cell>Backbone</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>HM</cell><cell>Off</cell><cell></cell><cell>3D Box</cell></row><row><cell></cell><cell>WH</cell><cell>Dim</cell><cell></cell><cell>Decoder</cell></row><row><cell></cell><cell>Dep</cell><cell>Rot</cell><cell></cell></row><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell>Secondary</cell><cell>Primary Heads</cell><cell>Acronyms</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Img+Rad</cell><cell>Regression</cell><cell>HM: Heat map</cell></row><row><cell></cell><cell cols="2">Frustum Association</cell><cell>Features</cell><cell>Heads</cell><cell>WH: Width,Height Dep: Depth</cell></row><row><cell>Pillar Expansion</cell><cell></cell><cell></cell><cell></cell><cell>Dep Vel</cell><cell>Secondary Heads 3x3 conv 1x1 conv</cell><cell>Rot: Rotation Dim: Dimension Off: Offset</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rot</cell><cell>Vel: Velocity Att: Attributes</cell></row><row><cell>Radar Point Cloud</cell><cell></cell><cell></cell><cell></cell><cell>Att</cell><cell>3x3 conv 1x1 conv</cell><cell>Concatenation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>in most cases, especially objects at a larger distance, such as the far vehicle in the second scene. Additionally, the velocity vectors estimated by CenterFusion show a significant improvement compared to the CenterNet results, as seen in the second and third scenes.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>↓</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Dataset C R L NDS ↑ mAP ↑ mATE mASE mAOE mAVE mAAE</cell></row><row><cell>InfoFocus [27]</cell><cell></cell><cell>test</cell><cell>0.395 0.395 0.363 0.265 1.132 1.000 0.395</cell></row><row><cell>OFT [24]</cell><cell></cell><cell>test</cell><cell>0.212 0.126 0.820 0.360 0.850 1.730 0.480</cell></row><row><cell>MonoDIS [26]</cell><cell></cell><cell>test</cell><cell>0.384 0.304 0.738 0.263 0.546 1.533 0.134</cell></row><row><cell cols="2">CenterNet (HGLS) [34]</cell><cell>test</cell><cell>0.400 0.338 0.658 0.255 0.629 1.629 0.142</cell></row><row><cell>Ours (DLA)</cell><cell></cell><cell>test</cell><cell>0.449 0.326 0.631 0.261 0.516 0.614 0.115</cell></row><row><cell cols="2">CenterNet (DLA) [34]</cell><cell>val</cell><cell>0.328 0.306 0.716 0.264 0.609 1.426 0.658</cell></row><row><cell>Ours (DLA)</cell><cell></cell><cell>val</cell><cell>0.453 0.332 0.649 0.263 0.535 0.540 0.142</cell></row><row><cell cols="4">Table 2. Per-class performance comparison for 3D object detection on nuScenes dataset.</cell></row><row><cell></cell><cell></cell><cell>Modality</cell><cell>mAP ↑</cell></row><row><cell>Method</cell><cell cols="2">Dataset C R L</cell><cell>Car Truck Bus Trailer Const. Pedest. Motor. Bicycle Traff. Barrier</cell></row><row><cell>InfoFocus [27]</cell><cell>test</cell><cell></cell><cell>0.779 0.314 0.448 0.373 0.107 0.634 0.290 0.061 0.465 0.478</cell></row><row><cell>MonoDIS [26]</cell><cell>test</cell><cell></cell><cell>0.478 0.220 0.188 0.176 0.074 0.370 0.290 0.245 0.487 0.511</cell></row><row><cell>CenterNet (HGLS) [34]</cell><cell>test</cell><cell></cell><cell>0.536 0.270 0.248 0.251 0.086 0.375 0.291 0.207 0.583 0.533</cell></row><row><cell>Ours (DLA)</cell><cell>test</cell><cell></cell><cell>0.509 0.258 0.234 0.235 0.077 0.370 0.314 0.201 0.575 0.484</cell></row><row><cell>CenterNet (DLA) [34]</cell><cell>val</cell><cell></cell><cell>0.484 0.231 0.340 0.131 0.035 0.377 0.249 0.234 0.550 0.456</cell></row><row><cell>Ours (DLA)</cell><cell>val</cell><cell></cell><cell>0.524 0.265 0.362 0.154 0.055 0.389 0.305 0.229 0.563 0.470</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Overall ablation study on nuScenes validation set. Improvement percentages in each row are relative to the baseline method. (PE: Pillar Expansion, FA: Frustum Association, FT: Flip Test) Method Cam Rad PE FA FT NDS ↑ mAP ↑ mATE ↓ mASE ↓ mAOE ↓ mAVE ↓ mAAE ↓ 4% +1.0% -2.0% +1.1% -4.4% -13.1% -68.6% Ours --+25.9% +2.0% -2.8% +1.0% -7.4% -48.1% -75.9% Ours -+34.5% +4.3% -5.3% +1.1% -10.0% -61.9% -78.0% Ours +37.8% +8.4% -9.4% -0.5% -11.6% -62.0% -78.3% Table 4. Class-based ablation study results on nuScenes validation set. Method Cam Rad PE FA FT Car Truck Bus Trailer Const. Pedest. Motor. Bicycle Traff. +2.1 -1.2 +1.4 +1.1 +0.1 +3.8 -1.1 +0.4 +0.8 Ours +4.1 +3.4 +2.7 +1.8 +1.8 +1.2 +5.5 -0.7 +1.3 +1.5</figDesc><table><row><cell>Baseline</cell><cell>-</cell><cell></cell><cell>---</cell><cell>0.328 0.306 0.716</cell><cell cols="2">0.264</cell><cell>0.609</cell><cell>1.426</cell><cell>0.658</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell>--</cell><cell cols="5">+15.Barrier</cell></row><row><cell>Baseline</cell><cell>-</cell><cell cols="2">---</cell><cell>48.4 23.1 34.0 13.1</cell><cell>3.5</cell><cell cols="3">37.7 24.9 23.4 55.0 45.6</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell>--</cell><cell cols="5">+0.6 +0.7 -2.1 +0.9 +0.6 +0.9 +1.9 -2.5 +0.1 +0.8</cell></row><row><cell>Ours</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="3">+1.0 +1.0 -2.1 +0.9 +0.9 0.0</cell><cell cols="2">+2.1 -1.9 +0.2 +0.8</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell>-</cell><cell>+2.8</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d object tracking using rgb and lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asvadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Girão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1255" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Distant Vehicle Detection Using Radar and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10951</idno>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-View 3D Object Detection Network for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07759</idno>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Camera and lidar fusion for on-road vehicle tracking with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkun</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1723" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Haase-Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinz</forename><surname>Hertlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudius</forename><surname>Glaeser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Wiesbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07830</idno>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D-RCNN: Instance-Level 3D Object Reconstruction via Render-and-Compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">3D Fully Convolutional Network for Vehicle Detection in Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08069</idno>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07916</idno>
		<title level="m">Vehicle Detection from 3D Lidar Using Fully Convolutional Network</title>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sensor fusion for joint 3d object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Charland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darshan</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00496</idno>
		<title level="m">3D Bounding Box Estimation Using Deep Learning and Geometry</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RRPN: Radar region proposal network for object detection in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Nabati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Nabati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08428</idno>
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06937</idno>
		<imprint>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Nobis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Geisslinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Betz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Lienkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor Data Fusion: Trends, Solutions, Applications (SDF)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08488</idno>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08188</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">PointR-CNN: 3D Object Proposal Generation and Detection from Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04244</idno>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12365</idno>
		<title level="m">Disentangling Monocular 3D Object Detection</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Infofocus: 3d object detection for autonomous driving with dynamic information modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08556</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PointFusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10871</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Sensor Fusion for 3D Bounding Box Estimation</title>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely Embedded Convolutional Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">RadarNet: Exploiting Radar for Robust Perception of Dynamic Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runsheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14366</idno>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">PIXOR: Real-time 3D Object Detection from Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06326</idno>
		<imprint>
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Layer Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sensor fusion for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Candra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1850" to="1857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06396</idno>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
