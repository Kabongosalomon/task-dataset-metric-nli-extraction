<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised GANs via Auxiliary Rotation Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<email>tingchen@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<email>xzhai@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
							<email>marvinritter@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
							<email>lucic@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
							<email>neilhoulsby@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised GANs via Auxiliary Rotation Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labeled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional IMAGENET generation. 1 * Work done at Google.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) are a class of unsupervised generative models <ref type="bibr" target="#b0">[1]</ref>. GANs involve training a generator and discriminator model in an adversarial game, such that the generator learns to produce samples from a desired data distribution. Training GANs is challenging because it involves searching for a Nash equilibrium of a nonconvex game in a high-dimensional parameter space. In practice, GANs are typically trained using alternating stochastic gradient descent which is often unstable and lacks theoretical guarantees <ref type="bibr" target="#b1">[2]</ref>. Consequently, training may exhibit instability, divergence, cyclic behavior, or mode collapse <ref type="bibr" target="#b2">[3]</ref>. As a result, many techniques to stabilize GAN training have been proposed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. A major contributor to training instability is the fact that the generator and discriminator learn in a non-stationary environment. In particular, the discriminator is a classifier for which the distribution of one class (the fake samples) shifts as the generator changes during training. In non-stationary online environments, neural networks forget previous tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. If the discriminator forgets previous classification boundaries, training may become unstable or cyclic. This issue is usually addressed either by reusing old samples or by applying continual learning techniques <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. These issues become more prominent in the context of complex data sets. A key technique in these settings is conditioning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> whereby both the generator and discriminator have access to labeled data. Arguably, augmenting the discriminator with supervised information encourages it to learn more stable representations which opposes catastrophic forgetting. Furthermore, learning the conditional model for each class is easier than learning the joint distribution. The main drawback in this setting is the necessity for labeled data. Even when labeled data is available, it is usually sparse and covers only a limited amount of high level abstractions.</p><p>Motivated by the aforementioned challenges, our goal is to show that one can recover the benefits of conditioning, without requiring labeled data. To ensure that the representations learned by the discriminator are more stable and useful, we add an auxiliary, self-supervised loss to the discriminator. This leads to more stable training because the dependence of the discriminator's representations on the quality of the generator's output is reduced. We introduce a novel modelthe self-supervised GAN -in which the generator and discriminator collaborate on the task of representation learning, and compete on the generative task.</p><p>Our contributions We present an unsupervised generative model that combines adversarial training with selfsupervised learning. Our model recovers the benefits of conditional GANs, but requires no labeled data. In particu-  <ref type="figure">Figure 1</ref>: Discriminator with rotation-based self-supervision. The discriminator, D, performs two tasks: true vs. fake binary classification, and rotation degree classification. Both the fake and real images are rotated by 0, 90, 180, and 270 degrees. The colored arrows indicate that only the upright images are considered for true vs. fake classification loss task. For the rotation loss, all images are classified by the discriminator according to their rotation degree.</p><p>lar, under the same training conditions, the self-supervised GAN closes the gap in natural image synthesis between unconditional and conditional models. Within this setting the quality of discriminator's representations is greatly increased which might be of separate interest in the context of transfer learning. A large-scale implementation of the model leads to promising results on unconditional IMAGENET generation, a task considered daunting by the community. We believe that this work is an important step in the direction of high quality, fully unsupervised, natural image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A Key Issue: Discriminator Forgetting</head><p>The original value function for GAN training is <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_0">V (G, D) =E x∼P data (x) [log P D (S = 1 | x)] + E x∼P G (x) [log(1 − P D (S = 0 | x))]<label>(1)</label></formula><p>where P data is the true data distribution, and P G is the distribution induced by transforming a simple distribution z ∼ P (z) using the deterministic mapping given by the generator, x = G(z), and P D is the discriminator's Bernoulli distribution over the labels (true or fake). In the original minimax setting the generator maximizes Equation 1 with respect to it's parameters, while the discriminator minimizes it. Training is typically performed via alternating stochastic gradient descent. Therefore, at iteration t during training, the discriminator classifies samples as coming from P data or P (t) G . As the parameters of G change, the distribution P (t) G changes, which implies a non-stationary online learning problem for the discriminator.</p><p>This challenge has received a great deal of attention and explicit temporal dependencies have been proposed to improve training in this setting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. Furthermore, in online learning of non-convex functions, neural networks have been shown to forget previous tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. In the context of GANs, learning varying levels of detail, structure, and texture, can be considered different tasks. For example, if the generator first learns the global structure, the discriminator will naturally try to build a representation which allows it to efficiently penalize the generator based only on the differences in global structure, or the lack of local structure. As such, one source of instability in training is that the discriminator is not incentivised to maintain a useful data representation as long as the current representation is useful to discriminate between the classes.</p><p>Further evidence can be gathered by considering the generator and discriminator at convergence. Indeed, Goodfellow et al. <ref type="bibr" target="#b0">[1]</ref> show that the optimal discriminator estimates the likelihood ratio between the generated and real data distributions. Therefore, given a perfect generator, where P G = P data , the optimal discriminator simply outputs 0.5, which is a constant and doesn't depend on the input. Hence, this discriminator would have no requirement to retain meaningful representations. Furthermore, if regularization is applied, the discriminator might ignore all but the minor features which distinguish real and fake data.</p><p>We demonstrate the impact of discriminator forgetting in two settings. (1) A simple scenario shown in <ref type="figure" target="#fig_3">Figure 3</ref> classification tasks on each of the ten classes in CIFAR10. It is trained for 1k iterations on each task before switching to the next. At 10k iterations the training cycle repeats from the first task. <ref type="figure" target="#fig_3">Figure 3</ref>(a) shows substantial forgetting, despite the tasks being similar. Each time the task switches, the classifier accuracy drops substantially. After 10k iterations, the cycle of tasks repeats, and the accuracy is the same as the first cycle. No useful information is carried across tasks. This demonstrates that the model does not retain generalizable representations in this non-stationary environment. In the second setting shown in <ref type="figure" target="#fig_1">Figure 2</ref> we observe a similar effect during GAN training. Every 100k iterations, the discriminator representations are evaluated on IMAGENET classification; the full protocol is described in Section 4.4. During training, classification of the unconditional GAN increases, then decreases, indicating that information about the classes is acquired and later forgotten. This forgetting correlates with training instability. Adding self-supervision, as detailed in the following section, prevents this forgetting of the classes in the discriminator representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Self-Supervised GAN</head><p>Motivated by the main challenge of discriminator forgetting, we aim to imbue the discriminator with a mechanism which allows learning useful representations, independently of the quality of the current generator. To this end, we exploit recent advancements in self-supervised approaches for representation learning. The main idea behind self-supervision is  Left: vanilla classifier. Right: classifier with an additional self-supervised loss. This example demonstrates that a classifier may fail to learn generalizable representations in a non-stationary environment, but self-supervision helps mitigate this problem. to train a model on a pretext task like predicting rotation angle or relative location of an image patch, and then extracting representations from the resulting networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. We propose to add a self-supervised task to our discriminator.</p><p>In particular, we apply the state-of-the-art selfsupervision method based on image rotation <ref type="bibr" target="#b25">[26]</ref>. In this method, the images are rotated, and the angle of rotation becomes the artificial label (cf. <ref type="figure">Figure 1</ref>). The self-supervised task is then to predict the angle of rotation of an image. The effects of this additional loss on the image classification task is evident in <ref type="figure" target="#fig_3">Figure 3</ref>(b): When coupled with the self-supervised loss, the network learns representations that transfer across tasks and the performance continually improves. On the second cycle through the tasks, from 10k iterations onward, performance is improved. Intuitively, this loss encourages the classifier to learn useful image representations to detect the rotation angles, which transfers to the image classification task.</p><p>We augment the discriminator with a rotation-based loss which results in the following loss functions:</p><formula xml:id="formula_1">L G = −V (G, D) − αE x∼P G E r∼R [log Q D (R = r | x r )] , L D = V (G, D) − βE x∼Pdata E r∼R [log Q D (R = r | x r )] ,</formula><p>where V (G, D) is the value function from Equation 1, r ∈ R is a rotation selected from a set of possible rotations. In this work we use R = {0</p><p>• , 90 • , 180 • , 270 • } as in Gidaris et al. <ref type="bibr" target="#b25">[26]</ref>. Image x rotated by r degrees is denoted as x r , and Q(R | x r ) is the discriminator's predictive distribution over the angles of rotation of the sample.</p><p>Collaborative Adversarial Training In our model, the generator and discriminator are adversarial with respect to the true vs. fake prediction loss, V (G, D), however, they are collaborative with respect to the rotation task. First, consider the value function of the generator which biases the generation towards images, that when rotated, the discriminator can detect their rotation angle. Note that the generator is not conditional but only generates "upright" images which are subsequently rotated and fed to the discriminator. On the other hand, the discriminator is trained to detect rotation angles based only on the true data. In other words, the parameters of the discriminator get updated only based on the rotation loss on the true data. This prevents the undesirable collaborative solution whereby the generator generates images whose subsequent rotation is easy to detect. As a result, the generator is encouraged to generate images that are rotation-detectable because they share features with real images that are used for rotation classification. In practice, we use a single discriminator network with two heads to compute P D and Q D . <ref type="figure">Figure 1</ref> depicts the training pipeline. We rotate the real and generated images in four major rotations. The goal of the discriminator on nonrotated images is to predict whether the input is true or fake. On rotated real images, its goal is to detect the rotation angle. The goal of the generator is to generate images matching the observed data, whose representation in the feature space of the discriminator allows detecting rotations. With α &gt; 0 convergence to the true data distribution P G = P data is not guaranteed. However, annealing α towards zero during training will restore the guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We demonstrate empirically that (1) self-supervision improves the representation quality with respect to baseline GAN models, and that (2) it leads to improved unconditional generation for complex datasets, matching the performance of conditional GANs, under equal training conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Datasets We focus primarily on IMAGENET, the largest and most diverse image dataset commonly used to evaluate GANs. Until now, most GANs trained on IMAGENET are conditional. IMAGENET contains 1.3M training images and 50k test images. We resize the images to 128 × 128 × 3 as done in Miyato and Koyama <ref type="bibr" target="#b20">[21]</ref> and Zhang et al. <ref type="bibr" target="#b8">[9]</ref>. We provide additional comparison on three smaller datasets, namely CIFAR10, CELEBA-HQ, LSUN-BEDROOM, for which unconditional GANs can be successfully trained. The LSUN-BEDROOM dataset <ref type="bibr" target="#b26">[27]</ref> contains 3M images. We partition these randomly into a test set containing approximately 30k images and a train set containing the rest. CELEBA-HQ contains 30k images <ref type="bibr" target="#b9">[10]</ref>. We use the 128 × 128 × 3 version  <ref type="table">Table 1</ref>: Best FID attained across three random seeds. In this setting the proposed approach recovers most of the benefits of conditioning.</p><p>obtained by running the code provided by the authors. <ref type="bibr" target="#b1">2</ref>  Models We compare the self-supervised GAN (SS-GAN) to two well-performing baseline models, namely (1) the unconditional GAN with spectral normalization proposed in Miyato et al. <ref type="bibr" target="#b5">[6]</ref>, denoted Uncond-GAN, and (2) the conditional GAN using the label-conditioning strategy and the Projection Conditional GAN (Cond-GAN) <ref type="bibr" target="#b20">[21]</ref>. We chose the latter as it was shown to outperform the AC-GAN <ref type="bibr" target="#b19">[20]</ref>, and is adopted by the best performing conditional GANs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>We use ResNet architectures for the generator and discriminator as in Miyato et al. <ref type="bibr" target="#b5">[6]</ref>. For the conditional generator in Cond-GAN, we apply label-conditional batch normalization. In contrast, SS-GAN does not use conditional batch normalization. However, to have a similar effect on the generator, we consider a variant of SS-GAN where we apply the self-modulated batch normalization which does not require labels <ref type="bibr" target="#b6">[7]</ref> and denote it SS-GAN (sBN). We note that labels are available only for CIFAR10 and IMAGENET, so Cond-GAN is only applied on those data sets.</p><p>We use a batch size of 64 and to implement the rotationloss we rotate 16 images in the batch in all four considered directions. We do not add any new images into the batch to compute the rotation loss. For the true vs. fake task we use the hinge loss from Miyato et al. <ref type="bibr" target="#b5">[6]</ref>. We set β = 1 or the the self-supervised loss. For α we performed a small sweep   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison of Sample Quality</head><p>Metrics To evaluate generated samples from different methods quantitatively, we use the Frechet Inception Distance (FID) <ref type="bibr" target="#b27">[28]</ref>. In FID, the true data and generated samples are first embedded in a specific layer of a pre-trained Inception network. Then, a multivariate Gaussian is fit to the data and the distance computed as FID(</p><formula xml:id="formula_2">x, g) = ||µ x − µ g || 2 2 + Tr(Σ x + Σ g − 2(Σ x Σ g ) 1 2 )</formula><p>, where µ and Σ denote the empirical mean and covariance and subscripts x and g denote the true and generated data respectively. FID is shown to be sensitive to both the addition of spurious modes and to mode dropping <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. An alternative approximate measure of sample quality is Inceptions Score (IS) Salimans et al. <ref type="bibr" target="#b1">[2]</ref>. Since it has some flaws Barratt and Sharma <ref type="bibr" target="#b30">[31]</ref>, we use FID as the main metric in this work.</p><p>Results <ref type="figure" target="#fig_5">Figure 4</ref> shows FID training curves on CIFAR10 and IMAGENET. <ref type="table">Table 1</ref> shows the FID of the best run across three random seeds for each dataset and model combination. The unconditional GAN is unstable on IMAGENET and the training often diverges. The conditional counterpart outperforms it substantially. The proposed method, namely SS-GAN, is stable on IMAGENET, and performs substantially better than the unconditional GAN. When equipped with self-modulation it matches the performance on the conditional GAN. In terms of mean performance ( <ref type="figure" target="#fig_5">Figure 4</ref>) the proposed approach matches the conditional GAN, and in terms of the best models selected across random seeds <ref type="table">(Table 1)</ref>, the performance gap is within 5%. On CIFAR10 and LSUN-BEDROOM we observe a substantial improvement over the unconditional GAN and matching the performance of the conditional GAN. Self-supervision appears not to significantly improve the results on CELEBA-HQ. We posit that this is due to low-diversity in CELEBA-HQ, and also for which rotation task is less informative.</p><p>Robustness across hyperparameters GANs are fragile; changes to the hyperparameter settings have a substantial impact to their performance <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. Therefore, we evaluate different hyperparameter settings to test the stability of SS-GAN. We consider two classes of hyperparameters: First, those controlling the Lipschitz constant of the discriminator, a central quantity analyzed in the GAN literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>. We evaluate two state-of-the-art techniques: gradient penalty <ref type="bibr" target="#b4">[5]</ref>, and spectral normalization <ref type="bibr" target="#b5">[6]</ref>. The gradient penalty introduces a regularization strength parameter, λ. We test two values λ ∈ {1, 10}. Second, we vary the hyperparameters of the Adam optimizer. We test two popular settings (β 1 , β 2 ): (0.5, 0.999) and (0, 0.9). Previous studies find that multiple discriminator steps per generator step help training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, so we try both 1 and 2 discriminator steps per generator step. <ref type="table" target="#tab_3">Table 2</ref> compares the mean FID scores of the unconditional models across penalties and optimization hyperparameters. We observe that the proposed approach yields consistent performance improvements. We observe that in settings where the unconditional GAN collapses (yielding FIDs larger than 100) the self-supervised model does not exhibit such a collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Large Scale Self-Supervised GAN</head><p>We scale up training the SS-GAN to attain the best possible FID for unconditional IMAGENET generation. To do this, we increase the model's capacity to match the model in <ref type="bibr" target="#b21">[22]</ref>. <ref type="bibr" target="#b2">3</ref> We train the model on 128 cores of Google TPU v3 Pod for 500k steps using batch size of 2048. For comparison, we also train the same model without the auxiliary self-supervised loss (Uncond-GAN). We report the FID at 50k to be comparable other literature reporting results on IMAGENET. We repeat each run three times with different random seeds.</p><p>For SS-GAN we obtain the FID of 23.6 ± 0.1 and 71.6 ± 66.3 for Uncond-GAN. Self-supervision stabilizes training; the mean and variance across random seeds is greatly reduced because, unlike for the regular unconditional GAN, SS-GAN never collapsed. We observe improvement in the best model across random seeds, and the best SS-GAN attains an FID of 23.4. To our knowledge, this is the best results attained training unconditionally on IMAGENET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Representation Quality</head><p>We test empirically whether self-supervision encourages the discriminator to learn meaningful representations. For this, we compare the quality of the representations extracted from the intermediate layers of the discriminator's ResNet architecture. We apply a common evaluation method for representation learning, proposed in Zhang et al. <ref type="bibr" target="#b24">[25]</ref>. In particular, we train a logistic regression classifier on the feature maps from each ResNet block to perform the 1000way classification task on IMAGENET or 10-way on CIFAR10 and report top-1 classification accuracy.</p><p>We report results using the Cond-GAN, Uncond-GAN, <ref type="bibr" target="#b2">3</ref> The details can be found at https://github.com/google/ compare_gan. and SS-GAN models. We also ablate the adversarial loss from our SS-GAN which results in a purely rotation-based self-supervised model (Rot-only) which uses the same architecture and hyperparameters as the SS-GAN discriminator. We report the mean accuracy and standard deviation across three independent models with different random seeds. Training details for the logistic classifier are in the appendix.</p><p>Results <ref type="table" target="#tab_6">Table 4</ref> shows the quality of representation at after 1M training steps on IMAGENET. <ref type="figure" target="#fig_10">Figure 9</ref> shows the learning curves for representation quality of the final ResNet block on IMAGENET. The curves for the other blocks are provided in appendix. Note that "training steps" refers to the training iterations of the original GAN, and not to the linear classifier which is always trained to convergence. Overall, the SS-GAN yields the best representations across all blocks and training iterations. We observe similar results on CIFAR10 provided in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>In detail, the IMAGENET ResNet contains six blocks. For Uncond-GAN and Rot-only, Block 3 performs best, for Cond-GAN and SS-GAN, the final Block 5 performs best. The representation quality for Uncond-GAN drops at 500k steps, which is consistent with the FID drop in <ref type="figure" target="#fig_5">Figure 4</ref>. Overall, the SS-GAN and Cond-GAN representations are better than Uncond-GAN, which correlates with their improved sample quality. Surprisingly, the the SS-GAN overtakes Cond-GAN after training for 300k steps. One possibility is that the Cond-GAN is overfitting the training data. We inspect the representation performance of Cond-GAN on the training set and indeed see a very large generalization <ref type="bibr">Uncond</ref>    gap, which indicates overfitting. When we ablate the GAN loss, leaving just the rotation loss, the representation quality substantially decreases. It seems that the adversarial and rotation losses complement each other both in terms of FID and representation quality. We emphasize that our discriminator architecture is optimized for image generation, not representation quality. Rot-only, therefore, is an ablation method, and is not a stateof-the-art self-supervised learning algorithm. We discuss these next. <ref type="table">Table 5</ref> compares the representation quality of SS-GAN to state-of-the-art published self-supervised learning algorithms. Despite the architecture and hyperparameters being optimized for image quality, the SS-GAN model achieves competitive results on IMAGENET. Among those methods, only BiGAN <ref type="bibr" target="#b33">[34]</ref> also uses a GAN to learn representations; but SS-GAN performs substantially (0.073 accuracy points) better. BiGAN learns the representation with an additional encoder network, while SS-GAN is arguably simpler because it extracts the representation directly from the discriminator. The best performing method is the recent DeepClustering algorithm <ref type="bibr" target="#b34">[35]</ref>. This method is just 0.027 accuracy points ahead of SS-GAN and requires expensive offline clustering after every training epoch.</p><p>In summary, the representation quality evaluation highlights the correlation between representation quality and image quality. It also confirms that the SS-GAN does learn relatively powerful image representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>GAN forgetting Catastrophic forgetting was previously considered as a major cause for GAN training instability. The main remedy suggested in the literature is to introduce Method Accuracy Context <ref type="bibr" target="#b23">[24]</ref> 0.317 BiGAN <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref> 0.310 Colorization <ref type="bibr" target="#b24">[25]</ref> 0.326 RotNet <ref type="bibr" target="#b25">[26]</ref> 0.387 DeepClustering <ref type="bibr" target="#b34">[35]</ref> 0.410 SS-GAN (sBN) 0.383 <ref type="table">Table 5</ref>: Comparison with other self-supervised representation learning methods by top-1 accuracy on IMAGENET. For SS-GAN, the mean performance is presented.</p><p>temporal memory into the training algorithm in various ways. For example, Grnarova et al. <ref type="bibr" target="#b18">[19]</ref> induce discriminator memory by replaying previously generated images. An alternative is to instead reuse previous models: Salimans et al. <ref type="bibr" target="#b1">[2]</ref> introduce checkpoint averaging, where a running average of the parameters of each player is kept, and Grnarova et al. <ref type="bibr" target="#b18">[19]</ref> maintain a queue of models that are used at each training iteration. Kim et al. <ref type="bibr" target="#b17">[18]</ref> add memory to retain information about previous samples. Other papers frame GAN training as a continual learning task. Thanh-Tung et al. <ref type="bibr" target="#b13">[14]</ref> study catastrophic forgetting in the discriminator and mode collapse, relating these to training instability. Liang et al. <ref type="bibr" target="#b14">[15]</ref> counter discriminator forgetting by leveraging techniques from continual learning directly (Elastic Weight Sharing <ref type="bibr" target="#b10">[11]</ref> and Intelligent Synapses <ref type="bibr" target="#b36">[37]</ref>).</p><p>Conditional GANs Conditional GANs are currently the best approach for generative modeling of complex data sets, such as ImageNet. The AC-GAN was the first model to introduce an auxiliary classification loss for the discriminator <ref type="bibr" target="#b19">[20]</ref>. The main difference between AC-GAN and the proposed approach is that self-supervised GAN requires no labels. Furthermore, the AC-GAN generator generates images conditioned on the class, whereas our generator is unconditional and the images are subsequently rotated to produce the artificial label. Finally, the self-supervision loss for the discriminator is applied only over real images, whereas the AC-GAN uses both real and fake. More recently, the P-cGAN model proposed by Miyato and Koyama <ref type="bibr" target="#b20">[21]</ref> includes one real/fake head per class <ref type="bibr" target="#b20">[21]</ref>. This architecture improves performance over AC-GAN. The best performing GANs trained on GPUs <ref type="bibr" target="#b8">[9]</ref> and TPUs <ref type="bibr" target="#b21">[22]</ref> use P-cGAN style conditioning in the discriminator. We note that conditional GANs also use labels in the generator, either by concatenating with the latent vector, or via FiLM modulation <ref type="bibr" target="#b37">[38]</ref>.</p><p>Self-supervised learning Self-supervised learning is a family of methods that learn the high level semantic representation by solving a surrogate task. It has been widely used in the video domain <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, the robotics domain <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> and the image domain <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref>. We focused on the image domain in this paper. Gidaris et al. <ref type="bibr" target="#b25">[26]</ref> proposed to rotate the image and predict the rotation angle. This conceptually simple task yields useful representations for downstream image classification tasks. Apart form trying to predict the rotation, one can also make edits to the given image and ask the network to predict the edited part. For example, the network can be trained to solve the context prediction problem, like the relative location of disjoint patches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43]</ref> or the patch permutation of a jigsaw puzzle <ref type="bibr" target="#b43">[44]</ref>. Other surrogate tasks include image inpainting <ref type="bibr" target="#b44">[45]</ref>, predicting the color channels from a grayscale image <ref type="bibr" target="#b24">[25]</ref>, and predicting the unsupervised clustering classes <ref type="bibr" target="#b34">[35]</ref>. Recently, Kolesnikov et al. <ref type="bibr" target="#b45">[46]</ref> conducted a study on self-supervised learning with modern neural architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>Motivated by the desire to counter discriminator forgetting, we propose a deep generative model that combines adversarial and self-supervised learning. The resulting novel model, namely self-supervised GAN when combined with the recently introduced self-modulation, can match equivalent conditional GANs on the task of image synthesis, without having access to labeled data. We then show that this model can be scaled to attain an FID of 23.4 on unconditional ImageNet generation which is an extremely challenging task.</p><p>This line of work opens several avenues for future research. First, it would be interesting to use a state-of-the-art self-supervised architecture for the discriminator, and optimize for best possible representations. Second, the selfsupervised GAN could be used in a semi-supervised setting where a small number of labels could be used to fine-tune the model. Finally, one may exploit several recently introduced techniques, such as self-attention, orthogonal normalization and regularization, and sampling truncation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>, to yield even better performance in unconditional image synthesis.</p><p>We hope that this approach, combining collaborative selfsupervision with adversarial training, can pave the way towards high quality, fully unsupervised, generative modeling of complex data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FID Metric Details</head><p>We compute the FID score using the protocol as described in <ref type="bibr" target="#b27">[28]</ref>. The image embeddings are extracted from an Inception V1 network provided by the TF library <ref type="bibr" target="#b46">[47]</ref>, We use the layer "pool_3". We fit the multivariate Gaussians used to compute the metric to real samples from the test sets and fake samples. We use 3000 samples for CELEBA-HQ and 10000 for the other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SS-GAN Hyper-parameters</head><p>We compare different choices of α, while fixing β = 1 for simplicity. A reasonable value of α helps aqa the generator to train using the self-supervision task, however, an inappropriate value of α could bias the convergence point of the generator. <ref type="table">Table 7</ref> shows the effectiveness of α. In the values compared, the optimal α is 1 for CIFAR10, and 0.2 for IMAGENET. In our main experiments, we used α = 0.2 for all datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Representation Quality</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Implementation Details</head><p>We train the linear evaluation models with batch size 128 and learning rate of 0.1 × batch_size 256 following the linear scaling rule <ref type="bibr" target="#b47">[48]</ref>, for 50 epochs. The learning rate is decayed by a factor of 10 after epoch 30 and epoch 40. For data augmentation we resize the smaller dimension of the image to 146 and preserve the aspect ratio. After that we crop the image to 128 × 128. We apply a random crop for training and a central crop for testing. The model is trained on a single NVIDIA Tesla P100 GPU. <ref type="table">Table 6</ref> shows the top-1 accuracy with on CIFAR10 with standard deviations. The results are stable on CIFAR10 as all the standard deviation is within 0.01. <ref type="table">Table 7</ref> shows the top-1 accuracy with on IMAGENET with standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Additional Results</head><p>Uncond-GAN representation quality shows large variance as we observe that the unconditional GAN collapses in some cases. <ref type="figure" target="#fig_9">Figure 8</ref> shows the representation quality on all 4 blocks on the CIFAR10 dataset. SS-GAN consistently outperforms other models on all 4 blocks. <ref type="figure" target="#fig_10">Figure 9</ref> shows the representation quality on all 6 blocks on the IMAGENET dataset. We observe that all methods perform similarly before 500k steps on block0, which contains low level features. While going from block0 to block6, the conditional GAN and SS-GAN achieve much better representation results. The conditional GAN benefits from the supervised labels in layers closer to the classification head. However, the unconditional GAN attains worse result at the last layer and the rotation only model gets decreasing quality with more training steps. When combining the self-supervised loss and the adversarial loss, SS-GAN representation quality becomes stable and outperforms the other models. <ref type="figure" target="#fig_11">Figure 10</ref> and <ref type="figure">Figure 11</ref> show the correlation between top-1 accuracy and FID score. We report the FID and top-1 accuracy from training steps 10k to 100k on CIFAR10, and 100k to 1M on IMAGENET. We evaluate 10 × 3 models in total, where 10 is the number of training steps at which we evaluate and 3 is the number of random seeds for each run. The collapsed models with FID score larger than 100 are removed from the plot. Overall, the representation quality and the FID score is correlated for all methods on the CI-FAR10 dataset. On IMAGENET, only SS-GAN gets better representation quality with better sample quality on block4 and block5.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a), and, (2) during the training of a GAN shown in Figure 2. In the first case a classifier is trained sequentially on 1-vs.-all Performance of a linear classification model, trained on IMAGENET on representations extracted from the final layer of the discriminator. Uncond-GAN denotes an unconditional GAN. SS-GAN denotes the same model when self-supervision is added. For the Uncond-GAN, the representation gathers information about the class of the image and the accuracy increases. However, after 500k iterations, the representations lose information about the classes and performance decreases. SS-GAN alleviates this problem. More details are presented in Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>With self-supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Image classification accuracy when the underlying class distribution shifts every 1k iterations. The vertical dashed line indicates the end of an entire cycle through the tasks, and return to the original classification task at t = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-GAN SS-GAN UNCOND-GAN SS-GAN GRADIENT PENALTY</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>FID learning curves on CIFAR10 and IMAGENET. The curves show the mean performance across three random seeds. The unconditional GAN (Uncond-GAN) attains significantly poorer performance than the conditional GAN (Cond-GAN). The unconditional GAN is unstable on IMAGENET and the runs often diverge after 500k training iterations. The addition of self-supervision (SS-GAN) stabilizes Uncond-GAN and boosts performance. Finally, when we add the additional self-modulated Batch Norm (sBN)<ref type="bibr" target="#b6">[7]</ref> to SS-GAN, which mimics generator conditioning in the unconditional setting, this unconditional model attains the same mean performance as the conditional GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>A random sample of unconditionally generated images from the self-supervised model. To our knowledge, this is the best results attained training unconditionally on IMAGENET.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>IMAGENET Top 1 accuracy (mean across three seeds) to predict labels from discriminator representations. X-axis gives the number of GAN training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Performance under different α values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Top 1 accuracy on CIFAR10 with training steps from 10k to 100k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Top 1 accuracy on IMAGENET validation set with training steps from 10k to 1M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Correlation between top-1 accuracy and FID score for different numbers of GAN training steps from 10k to 100k on CIFAR10. Overall, the representation quality and the FID score is correlated for all methods. The representation quality varies up to 4% with the same FID score. Correlation between top-1 accuracy and FID score for different numbers of GAN training steps from 100k to 1M on IMAGENET. Representation quality and FID score are not correlated on any of block0 to block4. This indicates that low level features are being extracted, which perform similarly on the IMAGENET dataset. Starting from block4, SS-GAN attains better representation as the FID score improves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We use 3k examples as the test set and the remaining examples as the training set. CIFAR10 contains 70k images (32 × 32 × 3), partitioned into 60k training instances and 10k test instances.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>± 31.44 25.8 ± 0.71 183.36 ± 77.21 80.67 ± 0.43 2 28.11 ± 0.66 26.98 ± 0.54 85.13 ± 2.88 83.08 ± 0.38 ± 17.46 25.22 ± 0.38 242.71 ± 13.62 144.35 ± 91.4</figDesc><table><row><cell>1</cell><cell cols="3">0.0 0.900 121.05 0.5 0.999 1 1 78.54 ± 6.23</cell><cell>25.89 ± 0.33 104.73 ± 2.71</cell><cell>91.63 ± 2.78</cell></row><row><cell>10</cell><cell>0.0 0.900</cell><cell>1 2</cell><cell>188.52 ± 64.54 29.11 ± 0.85</cell><cell>28.48 ± 0.68 227.04 ± 31.45 27.74 ± 0.73 227.74 ± 16.82</cell><cell>85.38 ± 2.7 80.82 ± 0.64</cell></row><row><cell cols="4">0.5 0.999 117.67 SPECTRAL NORM 1 0 1 87.86 ± 3.44 0.0 0.900 2 20.24 ± 0.62</cell><cell>19.65 ± 0.9 17.88 ± 0.64 80.05 ± 1.33 129.96 ± 6.6</cell><cell>86.09 ± 7.66 70.64 ± 0.31</cell></row><row><cell></cell><cell>0.5 0.999</cell><cell>1</cell><cell>86.87 ± 8.03</cell><cell>18.23 ± 0.56 201.94 ± 27.28</cell><cell>99.97 ± 2.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>FID for unconditional GANs under different hyperparameter settings. Mean and standard deviations are computed across three random seeds. Adding the self-supervision loss reduces the sensitivity of GAN training to hyperparameters.</figDesc><table /><note>α ∈ {0.2, 0.5, 1}, and select α = 0.2 for all datasets (see the appendix for details). For all other hyperparameters, we use the values in Miyato et al. [6] and Miyato and Koyama [21]. We train CIFAR10, LSUN-BEDROOM and CELEBA-HQ for 100k steps on a single P100 GPU. For IMAGENET we train for 1M steps. For all datasets we use the Adam optimizer with learning rate 0.0002.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Top-1 accuracy on CIFAR10. Mean score across three training runs of the original model. All standard deviations are smaller than 0.01 and are reported in the appendix.</figDesc><table><row><cell cols="5">Method Uncond. Cond. Rot-only SS-GAN (sBN)</cell></row><row><cell>Block0</cell><cell>0.074</cell><cell>0.156</cell><cell>0.147</cell><cell>0.158</cell></row><row><cell>Block1</cell><cell>0.063</cell><cell>0.187</cell><cell>0.134</cell><cell>0.222</cell></row><row><cell>Block2</cell><cell>0.073</cell><cell>0.217</cell><cell>0.158</cell><cell>0.250</cell></row><row><cell>Block3</cell><cell>0.083</cell><cell>0.272</cell><cell>0.202</cell><cell>0.327</cell></row><row><cell>Block4</cell><cell>0.077</cell><cell>0.253</cell><cell>0.196</cell><cell>0.358</cell></row><row><cell>Block5</cell><cell>0.074</cell><cell>0.337</cell><cell>0.195</cell><cell>0.383</cell></row><row><cell>Best</cell><cell>0.083</cell><cell>0.337</cell><cell>0.202</cell><cell>0.383</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Top-1 accuracy on IMAGENET. Mean score across three training runs of the original model. All standard devia- tions are smaller than 0.01, except for Uncond-GAN whose results exhibit high variance due to training instability. All standard deviations are reported in the appendix.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tkarras/progressive_ growing_of_gans.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would also like to thank Marcin Michalski, Karol Kurach and Anton Raichuk for their help with infustrature, and major contributions to the Compare GAN library. We appreciate useful discussions with Ilya Tolstikhin, Olivier Bachem, Alexander Kolesnikov, Josip Djolonga, and Tiansheng Yao. Finally, we are grateful for the support of other members of the Google Brain team, Zürich.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<title level="m">Spectral normalization for generative adversarial networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On Self Modulation for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On catastrophic forgetting and mode collapse in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Hoang Thanh-Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Theoretical Foundations and Applications of Deep Generative Models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11083</idno>
		<title level="m">Generative Adversarial Network Training is a Continual Learning Problem</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08395</idno>
		<title level="m">Continual learning in Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memorization precedes generation: Learning unsupervised gans with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An online learning approach to generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulina</forename><surname>Grnarova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">cgans with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Are GANs Created Equal? A Largescale Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04720</idno>
		<title level="m">The GAN Landscape: Losses, architectures, regularization, and normalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Understanding the effectiveness of lipschitz constraint in training of gans via gradient analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grasp2vec: Learning object representations from self-supervised grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerrel</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>T Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">TFGAN: A Lightweight Library for Generative Adversarial Networks</title>
		<ptr target="https://ai.googleblog.com/2017/12/tfgan-lightweight-library-for.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training Imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Method</forename><surname>Uncond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Gan Cond-Gan Rot-Only</forename><surname>Ss-Gan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>sBN</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">Top-1 accuracy on CIFAR10 with standard variations</title>
		<meeting><address><addrLine>Method Uncond-GAN Cond-GAN Rot-only SS-GAN</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>sBN</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">Top-1 accuracy on IMAGENET with standard variations</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
