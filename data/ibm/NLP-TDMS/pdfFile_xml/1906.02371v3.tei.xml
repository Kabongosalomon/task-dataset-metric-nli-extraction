<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Omnidirectional Scene Text Detection with Sequential-free Box Discretization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
							<email>liu.yuliang@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
							<email>lianwen.jin@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Lenovo Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Lenovo Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Omnidirectional Scene Text Detection with Sequential-free Box Discretization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text in the wild is commonly presented with high variant characteristics. Using quadrilateral bounding box to localize the text instance is nearly indispensable for detection methods. However, recent researches reveal that introducing quadrilateral bounding box for scene text detection will bring a label confusion issue which is easily overlooked, and this issue may significantly undermine the detection performance. To address this issue, in this paper, we propose a novel method called Sequential-free Box Discretization (SBD) by discretizing the bounding box into key edges (KE) which can further derive more effective methods to improve detection performance. Experiments showed that the proposed method can outperform state-of-the-art methods in many popular scene text benchmarks, including ICDAR 2015, MLT, and MSRA-TD500. Ablation study also showed that simply integrating the SBD into Mask R-CNN framework, the detection performance can be substantially improved. Furthermore, an experiment on the general object dataset HRSC2016 (multioriented ships) showed that our method can outperform recent state-of-the-art methods by a large margin, demonstrating its powerful generalization ability. Source code: https://github.com/Yuliang-Liu/ Box Discretization Network. * Corresponding author: Lianwen Jin. (a) Sensitive to label sequence. (b) Irrelevant to label sequence.</p><p>Figure 1: (a) Previous detecting methods that are sensitive to the label sequence. (b) The proposed SBD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene text presented in real images are often found with multi-oriented, low quality, perspective distortions, and various sizes or scales. To recognize the text content, it is an important prerequisite for detecting methods to localize the scene text tightly.</p><p>Recently, scene text detection methods have achieved significant progress <ref type="bibr" target="#b0">Deng et al., 2018;</ref><ref type="bibr" target="#b2">Liao et al., 2018a]</ref>. One reason for the improvement is that these methods introduce rotated rectangles or quadrangles instead of axis-aligned rectangles to localize the oriented instances, which remarkably improves the detection performance. However, performance of current methods still have a large gap to bridge a commercial application. Recent studies <ref type="bibr" target="#b10">Zhu and Du, 2018]</ref> have found that an underlying problem of introducing quadrilateral bounding box may significantly undermine the detection performance.</p><p>Taking East  as an example: For each pixel of the high-dimensional representation, the method utilizes four feature maps corresponding to the distances from this pixel to the ground truth (GT). It requires preprocessing steps to sort the label sequence of each quadrilateral GT box so that each predicted feature map can well focus on the targets, otherwise the detecting performance may be significantly worse. Such method is called "Sensitive to Label Sequence" (SLS), as shown in <ref type="figure">Figure 1 (a)</ref>. The question is that it is not trivial to find a proper sorting rule that can avoid Learning Confusion (LC) caused by sequence of the points. The rules proposed by <ref type="bibr" target="#b2">Liao et al., 2018a;</ref><ref type="bibr" target="#b0">He et al., 2018]</ref> can alleviate the problem; however, they cannot avoid that a single pixel deviation of a man-made annotation may totally change the corresponding relationships between each feature map and each target of the GT.</p><p>Motivated by this issue, this paper proposes a simple but effective method called Sequential-free Box Discretization (SBD) that can parameterize the bounding boxes into key edges. Basically, to avoid LC issue, the basic idea is to find at least four invariant points (e.g., mean center point, and intersecting point of the diagonals) that are irrelevant to the label sequence and we can use these invariant points to inversely deduce the bounding box coordinates. To simplify parameterization, a novel module called key edge (KE) is proposed to learn the bounding box.</p><p>Experiments on many public scene text benchmarks, including <ref type="bibr">MLT [Nayef et al., 2017]</ref>, MSRA-TD500 <ref type="bibr" target="#b7">[Yao et al., 2012]</ref>, and ICDAR 2015 Robust Reading Competition Challenge 4 "Incidental scene text localization" <ref type="bibr" target="#b1">[Karatzas and Gomez-Bigorda, 2015]</ref>, all demonstrated that our method can outperform previous state-of-the-art methods in terms of Hmean. Moreover, ablation studies showed that by seamlessly integrating SBD in Mask R-CNN framework, the detection result can be substantially improved. On multioriented ship detection dataset HRSC2016 , our method can still perform the best, further showing its promising generalization ability.</p><p>The main contributions of this paper are manifold: 1) We propose an effective SBD method which can not only solve LC issue but also improve the omnidirectional text detection performance; 2) SBD and its derived post-processing methods can further guarantee tighter and more accurate detections; 3) our method can substantially improve Mask R-CNN and achieve the state-of-the-art performance on various benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The mainstream multi-oriented scene text detection methods can be roughly divided into segmentation-based methods and non-segmentation-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Segmentation-based Method</head><p>Most of segmentation-based text detection methods are mainly built and improved from the <ref type="bibr">FCN [Long et al., 2015]</ref> or Mask R-CNN <ref type="bibr" target="#b0">[He et al., 2017a]</ref>. Segmentationbased methods are not SLS methods because the key of segmentation-based method is to conduct pixel-level classification. However, how to accurately separate the adjacent text instances is always a tough issue for segmentation-based methods. Recently, many methods are proposed to solve this issue. For examples, PixelLink <ref type="bibr" target="#b0">[Deng et al., 2018]</ref> additionally learns 8-direction information for each pixel to highlight the text margin; <ref type="bibr" target="#b5">[Lyu et al., 2018]</ref> proposes a corner detection method to produce position-sensitive score map; and [Wu and Natarajan, 2017] defines text border map for effectively distinguishing the instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-segmentation-based Method</head><p>Segmentation-based methods require or post-processing steps to group the positive pixels into final detection results, which may easily be affected by the false positive pixels. Nonsegmentation methods can directly learn the exact bounding box to localize the text instances. For examples, <ref type="bibr" target="#b3">[Liao et al., 2018b</ref>] predicts text location by using different scaled feature;  and <ref type="bibr" target="#b5">[Ma et al., 2018]</ref> utilize quadrilateral and rotated anchors to detect the multi-oriented text; <ref type="bibr" target="#b2">[Liao et al., 2018a]</ref> utilizes carefully-designed anchors to localize text instances; <ref type="bibr" target="#b7">[He et al., 2017b]</ref> directly regress the text sides or vertexes of the text instances. Although non-segmentation methods can also achieve superior performance, most of the non-segmentation methods are SLS methods, and thus they might easily be affected by the label sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe the details of the SBD. SBD is theoretically suitable for any general object detection framework, but in this paper we only build and validate SBD on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequential-free Box Discretization</head><p>The main goal of omnidirectional scene text detection is to accurately predict the compact bounding box which can be rectangular or quadrilateral. As introduced in Section 1, introducing quadrilateral bounding box can also bring the LC issue. Therefore, instead of predicting label-sensitive distances or coordinates, SBD discretizes the quadrilateral GT box into 8 lines that only contain invariant points, which are called key edges (KE). As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, eight KEs in this paper are discretized from the original coordinates: minimum x (x min ) and y (y min ); the second smallest x (x 2 ) and y (y 2 ); the second largest x (x 3 ) and y (y 3 ); maximum x (x max ) and y (y max ).</p><p>As shown in the <ref type="figure" target="#fig_0">Figure 2</ref> and 3, the inputs of SBD are the proposals processed by <ref type="bibr">RoIAlign [He et al., 2017a]</ref>; the feature map is then connected to stacked convolution layers and then upsampled by 2× bilinear upscaling layers, and the resolution of output feature maps F out from deconvolution is restricted to M × M . For each of the x-KEs and y-KEs, we use 1 × M and M × 1 convolution kernels with four output channels to shrink the transverse and longitudinal features, respectively; the number of the output channels are set to the same as the number of x-KEs or y-KEs, respectively. After that, we assign corresponding positions of the GT KEs to each output channel and update the network by minimizing the cross-entropy loss L KE over a M-way softmax output. We found detection in such classification manner instead of regression would be much more accurate.</p><p>Taking t i (t can be x or y, and i can be min, 2, 3, max) as an example, we do not directly learn the t i -th KE; instead, the GT KE is the vertical line t ihalf , and t ihalf = (t i +t mean )/2, where t mean represents the t value of the mean central point of the GT box. Learning t ihalf has two important advantages:</p><p>• Breaking RoI restriction. The original Mask R-CNN only learns to predict inside the RoI, and if parts of the target instances are outside the RoI, it would be impossible to recall these missing pixels. However, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, learning t ihalf can output the real border even if the border is outside the RoI. • Even if the border of the text instance is outside the RoI, in most cases, the t ihalf remains inside the RoI. Therefore, the integration of the text instance can be guaranteed and loss can be well propagated (because if a learning target is outside the RoI, the loss is zero). Formally, a multi-task loss on each foreground RoI is defined as L = L cls +L box +L mask +L ke . The first three terms L cls , L box , and L mask are the same as <ref type="bibr" target="#b0">[He et al., 2017a]</ref>. It is worth mentioning that <ref type="bibr" target="#b0">[He et al., 2017a]</ref> pointed out that the additional keypoint branch reduces the performance of box detection in <ref type="table">Table 5</ref>; however, from our experiments, the proposed SBD is the key component for boosting detection performance, which we think is mainly because: 1) For keypoint learning, there are M 2 classes against each other, while for SBD, the number of competitive pixels is only M ; 2) the keypoint might not be very explicit for a specific point (it could be a small region), while the KEs produced by SBD represent the borders of GT instances, which are absolute and exclusive, and thus the supervision information would not be confused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Match-Type Learning</head><p>Based on the box discretization, we can learn the values of all x and y, but we do not know which y-KEs should be matched to which x-KEs. Intuitively, as shown in the top right of the <ref type="figure" target="#fig_2">Figure 3</ref>, designing a proper matching procedure is a very important issue, otherwise the detection results could be significantly worse.</p><p>To solve this problem, we propose a simple but effective match-type learning (MTL) method. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we concatenate the x-KE and y-KE feature maps followed by 1 × 1, M × M convolutions, and softmax loss is used to learn a total of 24 (A 4 4 ) match-types (because we have 4 x-KEs and y-KEs), including {1234,1243,1324, ..., 4312, 4321}. For example, in the case of the <ref type="figure" target="#fig_0">Figure 2</ref>, the predicted match-type is "2413" which represents the matching results are (x min , y 2 ), (x 2 , y max ), (x 3 , y min ), (x max , y 3 ).</p><p>During training, we find the MTL can be very easy to learn and the loss can quickly converge within ten thousand iterations with 1 image per batch. Moreover, in some cases, the segmentation branch would somehow produce non-positive pixel while both the proposals and SBD predictions are accurate, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Through MTL, SBD can output the final bounding box and improve the detection performance by offsetting the weakness of segmentation branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rescoring and Post Processing</head><p>Based on our observations, some unconsolidated detections could also have virtual high confidence. This is mainly because the confidence outputted from the softmax in Fast R-CNN <ref type="bibr" target="#b0">[Girshick, 2015]</ref> is a classification loss but not for localization. Therefore, the compactness of the bounding box cannot be directly supervised by the score. We thus compute a refined confidence that takes the advantages of SBD prediction which learns the specific position of the final detections. Formally, we refine final instance-level detection score as follow:</p><formula xml:id="formula_0">score( ) = (2 − γ)S box + γS SBD 2 ,<label>(1)</label></formula><p>where, γ is the weighting coefficient, and it satisfies 0 ≤ γ, and γ ≤ 2. S box is the original softmax confidence for the bounding box, and S SBD represents the mean score of all the KEs, which is defined below:</p><formula xml:id="formula_1">S SBD = 1 K K k=1 max xi f k (x i ),<label>(2)</label></formula><p>where, K is the number of the KEs (which is 8, including 4 x-KEs and 4 y-KEs); f is the function to calculate the sum of adjacent 5 scores. We have found that using Equation <ref type="formula" target="#formula_0">(1)</ref> can not only suppress some false positives but also make the results more reliable. Examples are shown in <ref type="figure" target="#fig_5">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implemented Details</head><p>We used synthetic data <ref type="bibr" target="#b0">[Gupta et al., 2016]</ref>  The number of maximum iterations is 40 epochs for each dataset on four NVIDIA 1080ti GPUs. The initial learning rate is 10 −2 and reduces to 10 −3 and 10 −4 on the 25th and 32th epoch, respectively. In order to balance the learning weights of all branches, the weights of KEs and match-type learning are empirically restricted to 0.2 and 0.01, respectively.</p><p>The resolutions of training images were randomly selected from 600 to 920 with the interval of 40, and the maximum size was restricted to 1480. For testing, we only used single scale for all datasets (public methods have numerous settings for multi-scale testing, which is hard to conduct a fair comparison), and the scale and maximum size is (1200, 1600). Polygon non-maximum suppression (PNMS)  with threshold 0.2 is used to suppress the redundant detections. <ref type="bibr">ICDAR 2017</ref><ref type="bibr">MLT. [Nayef et al., 2017</ref> is the largest multi-lingual (9 languages) oriented scene text dataset, including 7.2k training samples, 1.8k validation samples and 9k testing samples. The challenges of this dataset are manifold: 1) Different languages have different annotating styles, e.g., most of the Chinese annotations are long (there is not specific word interval for a Chinese sentence) while most of the English annotations are short, the annotations of Bangla or Arabic may be frequently entwined with each other; 2) more multi-oriented, perspective distortion text on various complexed backgrounds; 3) many images have more than 50 text instances. All instances are well annotated with compact quadrangles. The results of MLT are given in <ref type="table">Table 1</ref>. Our method outperforms previous state-of-the-art methods by a large margin, especially in terms of recall rate. Some of the detection results are visualized in <ref type="figure">Figure 7</ref>. Instead of merely using segmentation predictions to group the rotated rectangular bounding boxes, SBD can directly predict the compact quadrilateral bounding boxes which should be more reasonable. Although there are some text instances missed, most of the text can be robustly recalled. MSRA-TD500. <ref type="bibr" target="#b7">[Yao et al., 2012]</ref> is a text-line based oriented dataset with 300 training images and 200 testing images captured from indoor and outdoor scenes. Although this dataset contains less text per image and most of the text is clean, the major challenge of this dataset is that most of the text in this dataset has the large variance in orientations. The results of MSRA-TD500 are given in  method is slower than some of the previous methods, it has a significant improvement in terms of the Hmean, which demonstrates its robustness in detecting long and strong tilted instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on the Scene Text Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICDAR 2015 Incidental Scene Text. [Karatzas and</head><p>Gomez-Bigorda, 2015] is one of the most popular benchmarks for oriented scene text detection. The images are incidentally captured mainly from streets and shopping malls, and thus the challenges of this dataset rely on the oriented, small, and low resolution text. This dataset contains 1k training samples and 500 testing samples, with about 2k contentrecognizable quadrilateral word-level bounding boxes. The results of ICDAR 2015 are given in <ref type="table" target="#tab_3">Table 3</ref>. From this table, we can observe that our method can still perform the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>In this section, we further conducted ablation studies to validate the effectiveness of SBD, and the results are shown in <ref type="table">Table 5</ref> and <ref type="figure" target="#fig_7">Figure 9</ref>. <ref type="table">Table 5</ref> showed that adding SBD can lead to 2.4% improvement in terms of Hmean. One reason is that the SBD can recall more instances, as discussed in Section 3 and shown in <ref type="figure" target="#fig_4">Figure 5</ref>; the other reason maybe the SBD branch can bring the effect of mutual promotion just like how segmentation branch improves the performance of the Mask R-CNN. In addition, <ref type="figure" target="#fig_7">Figure 9</ref> showed our method can substantially outperform the baseline Mask R-CNN under different confidence thresholds of the detections, which further demonstrated its effectiveness. We also conducted experiments to compare and validate Algorithms R(%) P (%) H(%)  43.0 71.0 54.0 <ref type="bibr" target="#b7">[Tian et al., 2016]</ref> 52.0 74.0 61.0 <ref type="bibr" target="#b7">[Shi et al., 2017b]</ref> 76.8 73.1 75.0  68.2 73.2 70.6  73.5 83.6 78.2  77.0 79.3 78.2 <ref type="bibr" target="#b3">[Liao et al., 2018b]</ref> 79.0 85.6 82.2 <ref type="bibr" target="#b0">[Deng et al., 2018]</ref> 82.0 85.5 83.7 <ref type="bibr" target="#b5">[Ma et al., 2018]</ref> 82.2 73.2 77.4 <ref type="bibr" target="#b5">[Lyu et al., 2018]</ref> 79.7 89.5 84.3 <ref type="bibr" target="#b7">[He et al., 2017b]</ref> 80.0 82.0 81.0 Proposed method 83.8 89.4 86.5   <ref type="table" target="#tab_4">Table 4</ref>, which demonstrated the powerful sequential-free ability of the proposed SBD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments on the Ship Detection Benchmark</head><p>To demonstrate generalization ability of SBD, we further evaluated and compared SBD on Level 1 task of the HRSC2016 dataset  to show our method's performance on multi-directional object detection. The ship </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Algorithms Hmean</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICDAR2015</head><p>Mask R-CNN baseline 83.5% Baseline + SBD 85.9% (↑ 2.4%) Baseline + SBD + Rescoring 86.5% (↑ 0.6%) <ref type="table">Table 5</ref>: Ablation studies to show the effectiveness of the proposed method. The γ of rescoring is set to 1.4 (best practice). instances in this dataset might appear in various orientations, and annotating bounding box is based on rotated rectangles. There are 436, 181, and 444 images for training, validating, and testing set, respectively. The evaluating metric is the same as <ref type="bibr" target="#b1">[Karatzas and Gomez-Bigorda, 2015]</ref>. Only the training and validation sets are used for training, and because of the small amount of the training data, the whole training procedure took us only about two hours.</p><p>The result showed that our method can easily surpass previous methods by a large margin (as shown in <ref type="table" target="#tab_5">Table 6</ref>), 7.7% higher than recent state-of-the-art RRD <ref type="bibr" target="#b3">[Liao et al., 2018b]</ref> in mAP score. Some of the detection results are presented in <ref type="figure" target="#fig_6">Figure 8</ref>. Both the quantitative and qualitative results all show Algorithms mAP <ref type="bibr" target="#b0">[Girshick, 2015;</ref><ref type="bibr" target="#b3">Liao et al., 2018b]</ref> 55.7 <ref type="bibr" target="#b0">[Girshick, 2015;</ref><ref type="bibr" target="#b3">Liao et al., 2018b]</ref> 69.6 <ref type="bibr" target="#b0">[Girshick, 2015;</ref><ref type="bibr" target="#b3">Liao et al., 2018b]</ref> 75.7 <ref type="bibr" target="#b3">[Liao et al., 2018b]</ref> 84.3 Proposed method 93.7 that the proposed method can perform well on common oriented object detections even with very limited training data, further demonstrating its powerful generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposed SBD -a novel method that uses discretization methodology for oriented scene text detection. SBD solves the LC issue by discretizing the point-wise prediction into sequential-free KEs that only contain invariant points, and using a novel match-type learning method to guide the compound mode. Benefiting from SBD, we can improve the reliability of the confidence of the bounding box and adopt more effective post-processing methods to improve performance.</p><p>Experiments on various oriented scene text benchmarks (MLT, ICDAR 2015, MSRA-TD500) all demonstrate the outstanding performance of the SBD. To test generalization ability, we further conducted an experiment on oriented general object dataset HRSC2016, and the results showed that our method can outperform recent state-of-the-art methods with a large margin. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overall framework. SBD is connected to the Mask R-CNN as an additional branch. The backbone is ResNet-50-FPN in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Mask R-CNN. The overall framework is illustrated in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of SBD. The resolution M in this paper is simply set to 56.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Detection examples that the results of SBD can break the restriction of proposal (RoI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Examples that SBD can recall many instances that segmentation branch fails to recall. Green bounding boxes and scores represent RoIs. Rotated cyan bounding box and transparent pixels represent the result from segmentation branch. Transparent pixels are predicted by mask branch. Purple quadrangles are final detection results from SBD. KEs are simplify by colorful points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Example of the effect of rescoring. Original confidence is mainly for classification, while our refined score further considers the localization possibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Experimental results on HSRC 2016. The detections are highlighted with red bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Ablation study on ICDAR 2015 benchmark. X-axis represents confidence threshold and Y-axis represents Hmean result. Baseline represents Mask R-CNN. By integrating with proposed SBD, the detection results can be substantially better than the results of Mask R-CNN baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Examples of KE score results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Although our</figDesc><table><row><cell>Algorithms</cell><cell cols="3">R(%) P (%) H(%)</cell><cell></cell></row><row><cell>[Nayef et al., 2017]</cell><cell>25.59</cell><cell>44.48</cell><cell>32.49</cell><cell></cell></row><row><cell>[Nayef et al., 2017]</cell><cell>34.78</cell><cell>67.75</cell><cell>45.97</cell><cell></cell></row><row><cell>[Ma et al., 2018]</cell><cell>67.0</cell><cell>55.0</cell><cell>61.0</cell><cell></cell></row><row><cell>[Ma et al., 2018]</cell><cell>55.5</cell><cell>71.17</cell><cell>62.37</cell><cell></cell></row><row><cell>[Nayef et al., 2017]</cell><cell>69.0</cell><cell>67.75</cell><cell>45.97</cell><cell></cell></row><row><cell>[Nayef et al., 2017]</cell><cell>62.3</cell><cell>80.28</cell><cell>64.96</cell><cell></cell></row><row><cell>[Zhong et al., 2018]</cell><cell>66.0</cell><cell>75.0</cell><cell>70.0</cell><cell></cell></row><row><cell>[Lyu et al., 2018] (SS)</cell><cell>55.6</cell><cell>83.8</cell><cell>66.8</cell><cell></cell></row><row><cell>[Liu et al., 2018] (SS)</cell><cell>62.3</cell><cell>81.86</cell><cell>70.75</cell><cell></cell></row><row><cell>Proposed method</cell><cell>70.1</cell><cell>83.6</cell><cell>76.3</cell><cell></cell></row><row><cell cols="5">Table 1: Experimental results on MLT dataset. SS represents single</cell></row><row><cell cols="5">scale. R: Recall rate. P: Precision. H: Harmonic mean of R and P.</cell></row><row><cell cols="4">Note that we only use single scale for all experiments.</cell><cell></cell></row><row><cell>Algorithms</cell><cell cols="4">R(%) P (%) H(%) FPS</cell></row><row><cell>[Kang et al., 2014]</cell><cell>62.0</cell><cell>71.0</cell><cell>66.0</cell><cell>-</cell></row><row><cell>[Zhang et al., 2016]</cell><cell>67.0</cell><cell>83.0</cell><cell>74.0</cell><cell>0.48</cell></row><row><cell>[Yao et al., 2016]</cell><cell>75.3</cell><cell>76.5</cell><cell>75.9</cell><cell>1.61</cell></row><row><cell>[Zhou et al., 2017]</cell><cell>67.4</cell><cell>87.3</cell><cell>76.1</cell><cell>13.2</cell></row><row><cell>[Shi et al., 2017b]</cell><cell>70.0</cell><cell>86.0</cell><cell>77.0</cell><cell>8.9</cell></row><row><cell>[He et al., 2017b]</cell><cell>70.0</cell><cell>77.0</cell><cell>74.0</cell><cell>1.1</cell></row><row><cell>[Wu and Natarajan, 2017]</cell><cell>78.0</cell><cell>77.0</cell><cell>77.0</cell><cell>-</cell></row><row><cell>[Deng et al., 2018]</cell><cell>73.2</cell><cell>83.0</cell><cell>77.8</cell><cell>-</cell></row><row><cell>[Lyu et al., 2018]</cell><cell>76.5</cell><cell>87.6</cell><cell>81.5</cell><cell>5.7</cell></row><row><cell>[Liao et al., 2018b]</cell><cell>73.0</cell><cell>87.0</cell><cell>79.0</cell><cell>10</cell></row><row><cell>Proposed method</cell><cell>80.5</cell><cell>89.6</cell><cell>84.8</cell><cell>3.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on MSRA-TD500 benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Experimental results on ICDAR 2015 dataset. For fair com-</cell></row><row><cell cols="5">parison, this table only listed the single scale results without recog-</cell></row><row><cell>nition supervision.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 7: Examples of detection results. Purple detections are the</cell></row><row><cell cols="5">final detection results of SBD. The transparent regions are the seg-</cell></row><row><cell cols="5">mentation results from the segmentation branch, and rotated rectan-</cell></row><row><cell cols="5">gles are the minimum area bounding boxes grouped by the trans-</cell></row><row><cell cols="5">parent regions. Horizontal thin green bounding boxes are the rois.</cell></row><row><cell cols="2">Zoom in for better visualization.</cell><cell></cell><cell></cell></row><row><cell cols="2">Textboxes++</cell><cell>East</cell><cell>CTD</cell><cell>Ours</cell></row><row><cell>∆ Hmean</cell><cell>↓ 9.7%</cell><cell cols="3">↓ 13.7% ↓ 24.6% ↑ 0.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison on ICDAR 2015 dataset showing different methods' ability of resistant to the LC issue (by adding rotated pseudo samples). East and CTD are both SLS methods.different methods' ability of resistant to the LC issue. Specifically, we first trained the East, CTD, and proposed method with original 1k training images of ICDAR 2015 dataset. Then, we randomly rotated the training images among [0 • , 15 • , 30 • , ..., 360 • ] and randomly picked up additional 2k images from the rotated dataset to finetune on the these three methods. The results are given in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Experimental results on HRS 2016 dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Some additional data and figures are provided here for better understanding our method. Our method is built on MaskRCNN-benchmark 1 , which is based on pytorch framework. KE score. <ref type="figure">Figure 10</ref> shows some example results of the ke scores. Normally, detection results will produce the similar shapes as the normal case in <ref type="figure">Figure 10</ref>. Note that even if in the normal case, the highest score may still obviously below 1.0, and that explains why we use the sum of adjacent 5 score in the rescoring operation. False positive suppression. Match type can also be used for false positives. Because for some false positives, there is not clear edge, and in such case the match type learning may predict an abnormal result as shown in <ref type="figure">Figure 11</ref>. These abnormal results can be easily removed by judging if the quadrangle is valid (sides should only have two intersections on the head and tail). By doing so, we can further eliminate some false positives that might cheat mask branch, as shown in <ref type="figure">Figure 12</ref>. <ref type="bibr">OKS-NMS. [Papandreou et al., 2017]</ref> adopted object keypoint similarity non-maximum suppression (NMS-OKS) that can be effective to suppress some unnecessary box-in-box. We can follow similar implement on our KEs detections except removing σ 2 , which is because all KEs should weight the same. The formula is given as follows:  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01315</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<editor>Zheqi He, Yafeng Zhou, Yongtao Wang, Siwei Wang, Xiaoqing Lu, Zhi Tang, and Ling Cai</editor>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="887" to="895" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gomez-Bigorda, Lluis. Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
	<note>14th IAPR International Conference on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09585</idno>
		<title level="m">Icdar2017 competition on reading chinese text in the wild (rctw-17)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-organized text detection with minimal postprocessing via border learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<idno type="arXiv">arXiv:1606.09002</idno>
		<idno>arXiv:1804.09003</idno>
	</analytic>
	<monogr>
		<title level="m">Lei Sun, and Qiang Huo. An anchor-free region proposal network for faster r-cnn based text detection approaches</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">East: An efficient and accurate scene text detector</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sliding line point regression for shape robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du ; Yixing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09969</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
