<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Deep Detection Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<email>hbilen@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Deep Detection Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised learning of object detection is an important problem in image understanding that still does not have a satisfactory solution. In this paper, we address this problem by exploiting the power of deep convolutional neural networks pre-trained on large-scale image-level classification tasks. We propose a weakly supervised deep detection architecture that modifies one such network to operate at the level of image regions, performing simultaneously region selection and classification. Trained as an image classifier, the architecture implicitly learns object detectors that are better than alternative weakly supervised detection systems on the PASCAL VOC data. The model, which is a simple and elegant end-to-end architecture, outperforms standard data augmentation and fine-tuning techniques for the task of image-level classification as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, Convolutional Neural Networks (CNN) <ref type="bibr" target="#b19">[20]</ref> have emerged as the new state-of-the-art learning framework for image recognition. Key to their success is the ability to learn from large quantities of labelled data the complex appearance of real-world objects. One of the most striking aspects of CNNs is their ability to learn generic visual features that generalise to many tasks. In particular, CNNs pre-trained on datasets such as ImageNet ILSVRC have been shown to obtain excellent results in recognition in other domains <ref type="bibr" target="#b7">[8]</ref>, in object detection <ref type="bibr" target="#b11">[12]</ref>, in semantic segmentation <ref type="bibr" target="#b12">[13]</ref>, in human pose estimation <ref type="bibr" target="#b30">[31]</ref>, and in many other tasks.</p><p>In this paper we look at how the power of CNNs can be leveraged in weakly supervised detection (WSD), which is the problem of learning object detectors using only imagelevel labels. The ability of learning from weak annotations is very important for two reasons: first, image understanding aims at learning an growing body of complex visual concepts (e.g. hundred thousands object categories in Im-ageNet). Second, CNN training is data-hungry. Therefore, being able to learn complex concepts using only light super- <ref type="bibr">Figure 1</ref>. Weakly Supervised Deep Detection Network. Our method starts from a CNN pre-trained for image classification on a large dataset, e.g. ImageNet. It then modifies to reason efficiently about regions, branching off a recognition and a detection data streams. The resulting architecture can be fine-tuned on a target dataset to achieve state-of-the-art weakly supervised object detection using only image-level annotations. vision can reduce significantly the cost of data annotation in tasks such as image segmentation, image captioning, or object detection.</p><p>We are motivated in our research by the hypothesis that, since pre-trained CNNs generalise so well to a large number of tasks, they should contain meaningful representations of the data. For example, there exists evidence that CNNs trained for image classification learn proxies to objects and objects parts <ref type="bibr" target="#b35">[36]</ref>. Remarkably, these concepts are acquired implicitly, without ever providing the network with information about the location of such structures in images. Hence, CNNs trained for image classification may already contain implicitly most of the information required to perform object detection.</p><p>We are not the first to address the problem of WSD with CNNs. The method of Wang et al. <ref type="bibr" target="#b33">[34]</ref>, for example, uses a pre-trained CNN to describe image regions and then learn object categories as corresponding visual topics. While this method is currently state-of-the-art in weakly supervised object detection, it comprises several components beyond the CNN and requires signifiant tuning.</p><p>In this paper we contribute a novel end-to-end method for weakly supervised object detection using pre-trained CNNs which we call a weakly supervised deep detection network (WSDDN) ( <ref type="figure">fig. 1</ref>). Our method (section 3) starts from an existing network, such as AlexNet pre-trained on ImageNet data, and extends it to reason explicitly and efficiently about image regions R. In order to do so, given an image x, the first step is to efficiently extract region-level descriptors φ(x; R) by inserting a spatial pyramid pooling layer on top of the convolutional layers of the CNN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref>. Next, the network is branched to extract two data streams from the pooled region-level features. The first stream associates a class score φ c (x; R) to each region individually, performing recognition. The second stream, instead, compares regions by computing a probability distribution φ d (x; R) over them; the latter represents the belief that, among all the candidate regions in the image, R is the one that contains the most salient image structure, and is therefore a proxy to detection. The recognition and detection scores computed for all the image regions are finally aggregated in order to predict the class of the image as a whole, which is then used to inject image-level supervision in learning.</p><p>It is interesting to compare our method to the most common weakly supervised object detection technique, namely multiple instance learning (MIL) <ref type="bibr" target="#b6">[7]</ref>. MIL alternates between selecting which regions in images look like the object of interest and estimating an appearance model of the object using the selected regions. Hence, MIL uses the appearance model itself to perform region selection. Our technique differs from MIL in a fundamental way as regions are selected by a dedicated parallel detection branch in the network, which is independent of the recognition branch. In this manner, our approach helps avoiding one of the pitfalls of MIL, namely the tendency of the method to get stuck in local optima.</p><p>Our two-stream CNN is also weakly related to the recent work of Lin et al. <ref type="bibr" target="#b20">[21]</ref>. They propose a "bilinear" architecture where the output of two parallel network streams are combined by taking the outer product of feature vectors at corresponding spatial locations. The authors state that this construction is inspired by the ventral and dorsal streams of the human visual system, one focusing on recognition and the other one on localisation. While our architecture contains two such streams, the similarity is only superficial.</p><p>A key difference is that in Lin et al. the two streams are perfectly symmetric, and therefore there is no reason to believe that one should perform classification and the other detection; in our scheme, instead, the detection branch is explicitly designed to compare regions, breaking the symmetry. Note also that Lin et al. <ref type="bibr" target="#b20">[21]</ref> do not perform WSD nor evaluate object detection performance.</p><p>Once the modifications have been applied, the network is ready to be fine-tuned on a target dataset, using only image-level labels, region proposals and back-propagation.</p><p>In section 4 we show that, when fine-tuned on the PASCAL VOC training set, this architecture achieves state-of-the-art weakly supervised object detection on the PASCAL data, achieving superior results to the current state-of-the-art <ref type="bibr" target="#b33">[34]</ref> but using only CNN machinery. Since the system can be trained end-to-end using standard CNN packages, it is also as efficient as the recent fully-supervised Fast R-CNN detector of Girshick et al. <ref type="bibr" target="#b10">[11]</ref>, both in training and in testing. Finally, as a byproduct of our construction we also obtain a powerful image classifier that performs better than standard fine-tuning techniques on the target data. Our findings are summarised in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The majority of existing approaches to WSD formulate this task as MIL. In this formulation an image is interpreted as a bag of regions. If the image is labeled as positive, then one of the regions is assume to tightly contain the object of interest. If the image is labeled as negative, then no region contains the object. Learning alternates between estimating a model of the object appearance and selecting which regions in the positive bags correspond to the object using the appearance model.</p><p>The MIL strategy results in a non-convex optimization problem; in practice, solvers tend to get stuck in local optima such that the quality of the solution strongly depends on the initialization. Several papers have focused on developing various initialization strategies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref> and on regularizing the optimization problem <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1]</ref>. <ref type="bibr">Kumar et al. [18]</ref> propose a self-paced learning strategy that progressively includes harder samples to a small set of initial ones at training. Deselaers et al. <ref type="bibr" target="#b4">[5]</ref> initialize object locations based on the objectness score. Cinbis et al. <ref type="bibr" target="#b3">[4]</ref> propose a multi-fold split of the training data to escape local optima. Song et al. <ref type="bibr" target="#b28">[29]</ref> apply Nesterov's smoothing technique <ref type="bibr" target="#b21">[22]</ref> to the latent SVM formulation <ref type="bibr" target="#b9">[10]</ref> to be more robust against poor initializations. Bilen et al. <ref type="bibr" target="#b0">[1]</ref> propose a smoothed version of MIL that softly labels object instances instead of choosing the highest scoring ones. Additionally, their method regularizes the latent object locations by penalizing unlikely configurations based on symmetry and mutual exclusion principles.</p><p>Another line of research in WSD <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref> is based on the idea of identifying the similarity between image parts. Song et al. <ref type="bibr" target="#b28">[29]</ref> propose a discriminative graph-based algorithm that selects a subset of windows such that each window is connected to its nearest neighbors in positive images. In <ref type="bibr" target="#b29">[30]</ref>, the same authors extend this method to discover multiple co-occurring part configurations. Wang et al. <ref type="bibr" target="#b33">[34]</ref> propose an iterative technique that applies a latent semantic clustering via latent Semantic Analysis (pLSA) on the windows of positive samples and selects the most discriminative cluster for each class based on its classification per-formance. Bilen et al. <ref type="bibr" target="#b1">[2]</ref> propose a formulation that jointly learns a discriminative model and enforces the similarity of the selected object regions via a discriminative convex clustering algorithm.</p><p>Recently a number of researchers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> have proposed weakly supervised localization principles to improve classification performance of CNNs without providing any annotation for the location of objects in images. Oquab et al.</p><p>[23] employ a pre-trained CNN to compute a mid-level image representation for images of PASCAL VOC. In their follow-up work, Oquab et al. <ref type="bibr" target="#b23">[24]</ref> modify a CNN architecture to coarsely localize object instances in image while predicting its label.</p><p>Jaderberg et al. <ref type="bibr" target="#b14">[15]</ref> proposed a CNN architecture in which a subnetwork automatically pre-transforms an image in order to optimize the classification accuracy of a second subnetwork. This "transformer network", which is trained in an end-to-end fashion from image-level labels, is shown to align objects to a common reference frame, which is a proxy to detection. Our architecture contains a mechanism that pre-select image regions that are likely to contain the object, also trained in an end-to-end fashion; while this may seem very different, this mechanism can also be thought as learning transformations (as the ones that map the detected regions to a canonical reference frame). However, the nature of the selection process in in our and their networks are very different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we introduce our weakly supervised deep detection network (WSDDN) method. The overall idea consists of three steps. First, we obtain a CNN pre-trained on a large-scale image classification task (section 3.1). Second, we construct the WSDDN as an architectural modification of this CNN (section 3.2). Third, we train/fine-tune the WS-DDN on a target dataset, once more using only image-level annotations (section 3.3). The remainder of this section discusses these three steps in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-trained network</head><p>We build our method on a pre-trained CNN that has been pre-trained on the ImageNet ILSVRC 2012 data <ref type="bibr" target="#b25">[26]</ref> with only image-level supervision (i.e. no bounding box annotations). We give the details of the used CNN architectures in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Weakly supervised deep detection network</head><p>Given the pre-trained CNN, we transform it into a WS-DDN by introducing three modifications (see also section 3). First, we replace the last pooling layer immediately following the ReLU layer in the last convolutional block (also known as relu5 and pool5, respectively) with a layer implementing spatial pyramid pooling (SPP) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14]</ref>. This results in a function that takes as input an image x and a region (bounding box) R and produces as output a feature vector or representation φ(x; R). Importantly, the function decomposes as</p><formula xml:id="formula_0">φ(x; R) = φ SPP (·; R) • φ relu5 (x)</formula><p>where φ relu5 (x) needs to be computed only once for the whole image and φ SPP (·; R) is fast to compute for any given region R. In practice, SPP is configured to be compatible to the first fully connected layers of networks (i.e. fc6). Note that SPP is implemented as a network layer as in <ref type="bibr" target="#b10">[11]</ref> to allow to train the system end-to-end (and for efficiency).</p><p>Given an image x, a shortlist of candidate object regions R = (R 1 , . . . , R n ) are obtained by a region proposal mechanism. Here we experiment with two methods, Selective Search Windows (SSW) <ref type="bibr" target="#b31">[32]</ref> and Edge Boxes (EB) <ref type="bibr" target="#b36">[37]</ref>. As in <ref type="bibr" target="#b10">[11]</ref>, we then modify the SPP layer to take as input not a single region, but rather the full list</p><formula xml:id="formula_1">R; in particular, φ(x; R) is defined as the concatenation of φ(x; R 1 ), . . . , φ(x; R n ) along the fourth dimension (since each individual φ(x; R) is a 3D tensor).</formula><p>At this point in the architecture, region-level features are further processed by two fully connected layers φ fc6 and φ fc7 , each comprising a linear map followed by a ReLU. Out of the output of the last such layer, we branch off two data streams, described next.</p><p>Classification data stream. The first data stream performs classification of the individual regions, by mapping each of them to a C-dimensional vector of class scores, assuming that the system is trained to detect C different classes. This is achieved by evaluating a linear map φ f c8c and results in a matrix of data x c ∈ R C×|R| , containing the class prediction scores for each region. The latter is then passed through a softmax operator, defined as follows:</p><formula xml:id="formula_2">[σ class (x c )] ij = e x c ij C k=1 e x c kj .<label>(1)</label></formula><p>Detection data stream. The second data stream performs instead detection, by scoring regions relative to one another. This is done on a class-specific basis by using a second linear map φ f c8d , also resulting in a matrix of scores x d ∈ R C×|R| . It is then passed through another softmax operator, but this time defined as follows:</p><formula xml:id="formula_3">[σ det (x d )] ij = e x d ij |R| k=1 e x d ik .<label>(2)</label></formula><p>While the two streams are remarkably similar, the introduction of the σ class and σ det non-linearities in the classification and detection streams is a key difference which allows to interpret them as performing classification and detection, respectively. In the first case, in fact, the softmax operator compares, for each region independently, class scores, whereas in the second case the softmax operator compares, for each class independently, the scores of different regions. Hence, the first branch predicts which class to associate to a region, whereas the second branch selects which regions are more likely to contain an informative image fragment.</p><formula xml:id="formula_4">x φ pool5 φ SPP φ fc6 φ fc7 φ fc8c SSW/EB φ fc8d σ class σ det Σ y x c R x d x R</formula><p>Combined region scores and detection. The final score of each region is obtained by taking the element-wise (Hadamard) product x R = σ class (x c ) σ det (x d ) of the two scoring matrices. The region scores are then used to rank image regions by likelihood of centring an object (for each class independently); standard non-maxima suppression is then performed (by iteratively removing regions with Intersection over Union (IoU) larger than 40% with regions already selected) to obtain the final list of class-specific detections in an image. The way the two streams' scores are combined is reminiscent of the bilinear networks of <ref type="bibr" target="#b20">[21]</ref>, but there are three key differences. The first difference is that the introduction of the different softmax operators explicitly breaks the symmetry of the two streams. The second one is that, instead of computing the outer product of the two feature vectors σ class (x c r ) ⊗ σ det (x d r ), we compute the element-wise product σ class (x c r ) σ det (x d r ) (generating quadratically less parameters). The third difference is that scores σ class (x c r )⊗σ det (x d r ) are computed for specific image regions r rather than a fixed set of image locations on a grid. Together, these three differences mean that we can interpret σ det (x d ) as a term that ranks regions, whereas σ class (x c ) ranks classes. It is more difficult to clearly assess the nature of the two streams in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Image-level classification scores. So far, WSDDN has computed region-level scores x R . This is transformed in an image-level class prediction score by summation over regions:</p><formula xml:id="formula_5">y c = |R| r=1 x R cr .</formula><p>Note that both y c is a sum of element-wise product of softmax normalised scores over |R| regions and thus it is in the range of (0, 1). Softmax is not performed at this stage as images are allowed to contain more than one object class (whereas regions should contain a single class).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training WSDDN</head><p>Having discussed the WSDDN architecture in the previous section, here we explain how the model is trained. The data is a collection of images x i , i = 1, . . . , n with image level labels y i ∈ {−1, 1} C . We denote by φ y (x|w) the complete architecture, mapping an image x to a vector of class scores y ∈ R C . The parameters w of the model lump together the coefficients of all the filters and biases in the convolutional and fully-connected layers. Then, stochastic gradient descent with momentum is used to optimise the energy function</p><formula xml:id="formula_6">E(w) = λ 2 w 2 + n i=1 C k=1 log(y ki (φ y k (x i |w) − 1 2 ) + 1 2 ),<label>(3)</label></formula><p>hence optimising a sum of C binary-log-loss terms, one per class. As φ y k (x i |w) is in range of (0, 1), it can be considered as a probability of class k being present in image x i , i.e. p(y ki = 1). When the ground-truth label is positive, the binary log loss becomes log(p(y ki = 1)), log(1 − p(y ki = 1)) otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Spatial Regulariser</head><p>As WSDDN is optimised for image-level class labels, it does not guarantee any spatial smoothness such that if a region obtains a high score for an object class, the neighbouring regions with high overlap will also have high scores. In the supervised detection case, Fast-RCNN <ref type="bibr" target="#b10">[11]</ref> takes the region proposals that have at least 50% IoU with a ground truth box as positive samples and learns to regress them into their corresponding ground truth bounding box. As our method does not have access to ground truth boxes, we follow a soft regularisation strategy that penalises the feature map discrepancies at the second fully connected layer fc7 between the highest scoring region and the regions with at least 60% IoU (i.e. r ∈ |R|) during training:</p><formula xml:id="formula_7">1 nC C k=1 N + k i=1 |R| r=1 1 2 (φ y k * i ) 2 (φ fc7 k * i − φ fc7 kri ) T (φ fc7 k * i − φ fc7 kri )</formula><p>where N + k is the number of positive images for the class k and * = arg max r φ y kri is the highest scoring region in image i for the class k. We add this regularisation term to the cost function in eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we conduct a thorough investigation of WSDDN and its components on weakly supervised detection and image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark data.</head><p>We evaluate our method on the PASCAL VOC 2007 and 2010 datasets <ref type="bibr" target="#b8">[9]</ref>, as they are the most widely-used benchmark in weakly supervised object detection. While the VOC 2007 dataset consists of 2501 training, 2510 validation, and 5011 test images containing bounding box annotations for 20 object categories, VOC 2010 dataset contains 4998 training, 5105 validation, and 9637 test images for the same number of categories. We use the suggested training and validation splits and report results evaluated on test split. We report performance of our method on both the object detection and the image classification tasks of PASCAL VOC.</p><p>For detection, we use two performance measures. The first one follows the standard PASCAL VOC protocol and reports average precision (AP) at 50% intersection-overunion (IoU) of the detected boxes with the ground truth ones. We also report CorLoc, a commonly-used weakly supervised detection measure <ref type="bibr" target="#b5">[6]</ref>. CorLoc is the percentage of images that contain at least one instance of the target object class for which the most confident detected bounding box overlaps by at least 50% with one of these instances. Differently from AP, which is measured on the PASCAL test set, CorLoc is evaluated on the union of the training and validation subset of PASCAL. For classification, we use the standard PASCAL VOC protocol and report AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental setup.</head><p>We comprehensively evaluate our method with three pretrained CNN models in our experiments as in <ref type="bibr" target="#b10">[11]</ref>. The first network is the VGG-CNN-F <ref type="bibr" target="#b2">[3]</ref> which is similar to AlexNet <ref type="bibr" target="#b16">[17]</ref> but has reduced number of convolutional filters. We refer to this network as S, for small. The second one is VGG-CNN-M-1024 which has the same depth as S but has smaller stride in the first convolutional layer. We name this network M for medium. The last network is the deep VGG-VD16 model <ref type="bibr" target="#b27">[28]</ref> and we call this network L for large. These models, which are pre-trained on the ImageNet ILSVRC 2012 challenge data <ref type="bibr" target="#b25">[26]</ref>, attain 18.8%, 16.1% and 9.9% top-5 accuracy respectively (using a single centre-crop) on ILSVRC (importantly no bounding box information is provided during pre-training). As explained in section 3.1, we apply the following modifications to the network. First, we replace the last pooling layer pool5 with a SPP layer <ref type="bibr" target="#b13">[14]</ref> which is configured to be compatible with the network's first fully connected layer. Second, we add a parallel detection branch to the classification one that contains a fully-connected layer followed by a soft-max layer. Third, we combine the classification and detection streams by element-wise product followed by summing scores across regions, and feed the latter to a binary log-loss layer. Note that this layer assesses the classification performance for the 20 classes together, but each of them is treated as a different binary classification problem; the reason is that classes can co-occur in the PASCAL VOC, such that the softmax log loss used in AlexNet is not appropriate.</p><p>The WSDDNs are trained on the PASCAL VOC training and validation data by using fine-tuning on all layers, a widely-adopted technique to improve the performance of a CNN on a target domain <ref type="bibr" target="#b2">[3]</ref>. Here, however, fine tuning performs the essential function of learning the classification and detection streams, effectively causing the network to learn to detect objects, but using only weak image-level supervision. The experiments are run for 20 epochs and all the layers are fine-tuned with the learning rate 10 −5 for the first ten epochs and 10 −6 for the last ten epochs. Each minibatch contains all region proposals from a single image.</p><p>In order to generate candidate regions to use with our networks, we evaluate two proposal methods, Selective Search Windows (SSW) <ref type="bibr" target="#b31">[32]</ref> using its fast setting, and EdgeBoxes (EB) <ref type="bibr" target="#b36">[37]</ref>. In addition to region proposals, EB provides an objectness score for each region based on the number of contours wholly encloses. We exploit this additional information by multiplying the feature map φ SPP proportional to its score via a scaling layer in WSDDN and denote this setting as Box Sc. Since we use a SPP layer to aggregate descriptors for each region, images do not need to be resized to a particular size as in the original pre-trained model. Instead, we keep the original aspect ratio of images fixed and resize them to five different scales (setting their maximum of width or height to {480, 576, 688, 864, 1200} respectively) as in <ref type="bibr" target="#b13">[14]</ref>. During training, we apply random horizontal flips to the images and select a scale at random as a form of jittering or data augmentation. At test time we average the outputs of 10 images (i.e. the 5 scales and their flips). We use the publicly available CNN toolbox MatCon-vNet <ref type="bibr" target="#b32">[33]</ref> to conduct our experiments and share our code, models and data 1 .</p><p>When evaluated on an image, WSDDN produces, for each target class c and image x, a score x R r = S c (x; r) for each region r and an aggregated score y c = S c (x) for each image. Non-maxima suppression (with 40 % IoU threshold) is applied to the regions and then the scored regions and images are pooled together to compute detection AP and CorLoc.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPP [14]</head><p>- </p><formula xml:id="formula_8">- - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG-VD16 [28]</head><p>-  </p><formula xml:id="formula_9">- - - - - - - - - - - - - - - - - - - 89.3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detection results</head><p>Baseline method. First we design a single stream classification-detection network as an alternative baseline to WSDDN. Part of the construction is similar to WSDDN, as we replace pool5 layer of VGG-CNN-F model with an SPP. However, we do not branch off two streams, but simply append to the last fully connected layer (φ fc8c ) the following loss layer</p><formula xml:id="formula_10">1 nC n i=1 C k=1 max{0, 1 − y ki log |R| r=1 exp(x R cr )}.</formula><p>The term log |R| r=1 exp(x R cr ) is a soft approximation of the max operator max r x R cr and was found to yield better performance than using the max scoring region. This observation is also reported in <ref type="bibr" target="#b0">[1]</ref>. Note that the non-linearity is necessary as otherwise aggregating region-based scores would sum over the scores of a majority of regions that are uninformative. The loss function is once more a sum of C binary hinge-losses, one for each class. This baseline obtains 21.6% mAP detection score on the PASCAL VOC test set, which is well below the state-of-the-art (31.6% in <ref type="bibr" target="#b33">[34]</ref>).</p><p>Pre-trained CNN architectures. We evaluate our method with the models S, M and L and also report the results for the ensemble of these models by simply averaging their scores. <ref type="table">Table 1</ref> shows that WSDDN with individual models S and M are already on par with the state-ofthe-art method <ref type="bibr" target="#b33">[34]</ref> and the ensemble outperforms the best previous score in the VOC 2007 dataset. Differently from supervised detection methods (e.g. <ref type="bibr" target="#b10">[11]</ref>), detection performance of WSDDN does not improve with use of wider or deeper networks. In contrast, model L performs significantly worse than models S and M (see <ref type="table">table 1</ref>). This can be explained with the fact that model L frequently focuses on parts of objects, instead of whole instances, and is still able to associate these parts with object categories due to its smaller convolution strides, higher resolution and deeper architecture.</p><p>Object proposals. Next, we compare the detection performances with two popular object proposal methods, SSW <ref type="bibr" target="#b31">[32]</ref> and EB <ref type="bibr" target="#b36">[37]</ref>. While both the region proposals provides comparable quality region proposals, using box scores of EB (denoted as Box Sc in table 1) leads to a 2% improvement for models S and M and boosts the detection performance of model L 5%.</p><p>Spatial regulariser. We denote the setting where WS-DDN is trained with the additional spatial regularisation term (denoted as Sp. Reg. in table 1). Finally the introduction of the regularisation improves the detection performance 1, 2 and 4 mAP points for models S, M and L respectively. The improvements show that larger network benefits more from introduction of the spatial invariance around high confidence regions.</p><p>Comparison with the state of the art. After evaluating the design decisions, we follow the best setting (last row in table 1) and compare WSDDN to the state of the art in weakly supervised detection literature in <ref type="table" target="#tab_0">table 2 and table 3  for the VOC 2007 dataset and in table 5 and table 6</ref> for the VOC 2010 dataset. The results show that our method already achieves overall significantly better performance than these alternatives with a single model and ensemble models further boost the performance. The majority of previous work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2]</ref> use the Caffe reference CNN model <ref type="bibr" target="#b15">[16]</ref>, which is comparable to model S in this paper, as a black box to extract features over SSW proposals. In addition to CNN features, Cinbis et al. <ref type="bibr" target="#b3">[4]</ref> use Fisher Vectors <ref type="bibr" target="#b24">[25]</ref> and EB objectness measure of Zitnick and Dollar <ref type="bibr" target="#b36">[37]</ref> as well. Differently from the previous work, WSDDN is based on a simple modification of the original CNN architecture fine-tuned on the target data using back-propagation.</p><p>Next, we investigate the results in more detail. While our method significantly outperforms the alternatives in majority of categories, is not as strong in chair, person and pottedplant categories. Failure and success case are illustrated in <ref type="figure" target="#fig_1">fig. 3</ref>. It can be noted that, by far, the most important failure modality for our system is that an object part (e.g. person face) is detected instead as the object as a whole. This can be explained by the fact that parts such as "face" are very often much more discriminative and with a less variable appearance than the rest of the object. Note that the root cause for this failure modality is that we, as many other authors, define objects as image regions that are most predictive for a given object class, and these may not include the object as a whole. Addressing this issue will therefore require incorporating additional cue in the model to try to learn the "whole object".</p><p>The output of our model could also be used as input to one of the existing methods for weakly-supervised detection that use a CNN as a black-box for feature extraction. Investigating this option is left to future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Classification Results</head><p>While WSDDN is primarily designed for weaklysupervised object detection, ultimately it is trained to perform image classification. Hence, it is interesting to evaluate its performance on this task as well. To this end, we use the PASCAL VOC 2007 benchmark and contrast it to standard fine-tuning techniques that are often used in combination with CNNs and show the results in table 6. These techniques have been thoroughly investigated in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>. Chatfield et al. <ref type="bibr" target="#b2">[3]</ref>, in particular, analyse many variants of fine-tuning, including extensive data augmentation, on the PASCAL VOC. They experiment with three architectures, VGG-F, VGG-M, and VGG-S. While VGG-F is their fastest model, the other two networks are slower but more accurate. As explained in 4.2, we initialise WSDDN S and M with the pre-trained VGG-F and VGG-M-1024 respectively and thus they should be considered as right baselines. WS-DDN S and M improves 8 and 7 points over VGG-F and VGG-M-1024 respectively.</p><p>We also compare WSDDN to the SPP-net <ref type="bibr" target="#b13">[14]</ref> which uses the Overfeat-7 <ref type="bibr" target="#b26">[27]</ref> with a 4-level spatial pyramid pooling layer {6 × 6, 3 × 3, 2 × 2, 1 × 1} for supervised object detection. While they do not perform fine-tuning, they include a spatial pooling layer. Applied to image classification, their best performance on the PASCAL VOC 2007 is 82.4%. Finally we compare WSDDN L to the competitive VGG-VD16 <ref type="bibr" target="#b27">[28]</ref>. Interestingly, this method also exploits coarse local information by aggregating the activations of the last fully connected layer over multiple locations and scales. WSDDN L outperforms this very competitive baseline with a margin of 0.4 point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have presented WSDDN, a simple modification of a pre-trained CNN for image classification that allows it to perform weakly supervised detection. It achieves significantly better performance than existing methods on weakly supervised detection, while requiring only fine-tuning on a target dataset using back-propagation, region proposals and image-level labels. Since it works on top of a SPP layer, it is also efficient at training and test time. WSDDN is also shown to perform better than traditional fine-tuning techniques to improve the performance of a pre-trained CNN on the problem of image classification.</p><p>We have identified the detection of object parts as a failure modality of the method, damaging its performance in selected object categories, and imputed that to the main criterion used to identify objects, namely the selection of highly-distinctive image regions. We are currently exploring complementary cues that would favour detecting complete objects instead.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Weakly-supervised deep detection network. The figure illustrates the architecture of WSDDN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>This figure depicts success (in green) and failure cases (in red) of our detector in randomly picked images. Majority of false detections contains two kinds of error: i) group multiple object instances with a single bounding box, ii) focus on (discriminative) parts (e.g. "faces") rather than whole object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean WSDDN S 42.9 56.0 32.0 17.6 10.2 61.8 50.2 29.0 3.8 36.2 18.5 31.1 45.8 54.5 10.2 15.4 36.3 45.2 50.1 43.8 34.5 WSDDN M 43.6 50.4 32.2 26.0 9.8 58.5 50.4 30.9 7.9 36.1 18.2 31.7 41.4 52.6 8.8 14.0 37.8 46.9 53.4 47.9 34.9 WSDDN L 39.4 50.1 31.5 16.3 12.6 64.5 42.8 42.6 10.1 35.7 24.9 38.2 34.4 55.6 9.4 14.7 30.2 40.7 54.7 46.9 34.8 WSDDN Ensemble 46.4 58.3 35.5 25.9 14.0 66.7 53.0 39.2 8.9 41.8 26.6 38.6 44.7 59.0 10.8 17.3 40.7 49.6 56.9 50.8 39.3 Wang et al. [34] 48.8 41.0 23.6 12.1 11.1 42.7 40.9 35.5 11.1 36.6 18.4 35.3 34.8 51.3 17.2 17.4 26.8 32.8 35.1 45.6 30.9 Wang et al. [34]+context 48.9 42.3 26.1 11.3 11.9 41.3 40.9 34.7 10.8 34.7 18.8 34.4 35.4 52.7 19.1 17.4 35.9 33.3 34.8 46.5 31.6 VOC 2007 test detection average precision (%). Comparison of our WSDDN on PASCAL VOC 2007 to the state-of-the-art in terms of AP.</figDesc><table><row><cell>Bilen et al. [1]</cell><cell>42.2 43.9 23.1 9.2 12.5 44.9 45.1 24.9 8.3 24.0 13.9 18.6 31.6 43.6</cell><cell>7.6 20.9 26.6 20.6 35.9 29.6 26.4</cell></row><row><cell>Bilen et al. [2]</cell><cell>46.2 46.9 24.1 16.4 12.2 42.2 47.1 35.2 7.8 28.3 12.7 21.5 30.1 42.4</cell><cell>7.8 20.0 26.8 20.8 35.8 29.6 27.7</cell></row><row><cell>Cinbis et al. [4]</cell><cell cols="2">39.3 43.0 28.8 20.4 8.0 45.5 47.9 22.1 8.4 33.5 23.6 29.2 38.5 47.9 20.3 20.0 35.8 30.8 41.0 20.1 30.2</cell></row><row><cell>method</cell><cell cols="2">aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean</cell></row><row><cell>WSDDN S</cell><cell cols="2">68.5 67.5 56.7 34.3 32.8 69.9 75.0 45.7 17.1 68.1 30.5 40.6 67.2 82.9 28.8 43.7 71.9 62.0 62.8 58.2 54.2</cell></row><row><cell>WSDDN M</cell><cell cols="2">65.1 63.4 59.7 45.9 38.5 69.4 77.0 50.7 30.1 68.8 34.0 37.3 61.0 82.9 25.1 42.9 79.2 59.4 68.2 64.1 56.1</cell></row><row><cell>WSDDN L</cell><cell cols="2">65.1 58.8 58.5 33.1 39.8 68.3 60.2 59.6 34.8 64.5 30.5 43.0 56.8 82.4 25.5 41.6 61.5 55.9 65.9 63.7 53.5</cell></row><row><cell>WSDDN Ensemble</cell><cell cols="2">68.9 68.7 65.2 42.5 40.6 72.6 75.2 53.7 29.7 68.1 33.5 45.6 65.9 86.1 27.5 44.9 76.0 62.4 66.3 66.8 58.0</cell></row><row><cell>Bilen et al. [2]</cell><cell cols="2">66.4 59.3 42.7 20.4 21.3 63.4 74.3 59.6 21.1 58.2 14.0 38.5 49.5 60.0 19.8 39.2 41.7 30.1 50.2 44.1 43.7</cell></row><row><cell>Cinbis et al. [4]</cell><cell cols="2">65.3 55.0 52.4 48.3 18.2 66.4 77.8 35.6 26.5 67.0 46.9 48.4 70.5 69.1 35.2 35.2 69.6 43.4 64.6 43.7 52.0</cell></row><row><cell>Wang et al. [34]</cell><cell cols="2">80.1 63.9 51.5 14.9 21.0 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3 69.5 14.1 38.3 58.8 47.2 49.1 60.9 48.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>VOC 2007 trainval correct localization (CorLoc [6]) on positive trainval images (%). method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean WSDDN S</figDesc><table><row><cell></cell><cell>92.5 89.9 89.5 88.3 66.5 83.6 92.1 90.3 73.0 85.7 72.6 91.4 90.1 89.0 94.4 78.1 86.0 76.1 91.1 85.5 85.3</cell></row><row><cell>WSDDN M</cell><cell>93.9 91.0 90.4 89.3 72.7 86.4 91.9 91.5 73.8 85.6 74.9 91.9 91.5 89.9 94.5 78.6 85.0 78.6 91.5 85.7 86.4</cell></row><row><cell>WSDDN L</cell><cell>93.3 93.9 91.6 90.8 82.5 91.4 92.9 93.0 78.1 90.5 82.3 95.4 92.7 92.4 95.1 83.4 90.5 80.1 94.5 89.6 89.7</cell></row><row><cell>WSDDN Ensemble</cell><cell>95.0 92.6 91.2 90.4 79.0 89.2 92.8 92.4 78.5 90.5 80.4 95.1 91.6 92.5 94.7 82.2 89.9 80.3 93.1 89.1 89.0</cell></row><row><cell>Oquab et al. [23]</cell><cell>88.5 81.5 87.9 82.0 47.5 75.5 90.1 87.2 61.6 75.7 67.3 85.5 83.5 80.0 95.6 60.8 76.8 58.0 90.4 77.9 77.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>VOC 2007 test classification average precision (%). method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean WSDDN Ensemble 57.4 51.8 41.2 16.4 22.8 57.3 41.8 34.8 13.1 37.6 10.8 37.0 45.2 64.9 14.1 22.3 33.8 27.6 49.1 44.8 36.2 Cinbis et al. [4] 44.6 42.3 25.5 14.1 11.0 44.1 36.3 23.2 12.2 26.1 14.0 29.2 36.0 54.3 20.7 12.4 26.5 20.3 31.2 23.7 27.4</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .Table 1 .</head><label>561</label><figDesc>VOC 2010 test detection average precision (%). http://host.robots.ox.ac.uk:8080/anonymous/3QGEGM.html method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean WSDDN Ensemble 77.4 73.2 61.9 39.6 50.8 84.4 67.5 49.6 38.6 73.4 30.4 53.2 72.9 84.1 30.3 53.1 76.6 48.5 61.6 66.7 59.7 Cinbis et al. [4] 61.1 65.0 59.2 44.3 28.3 80.6 69.7 31.2 42.8 73.3 38.3 50.2 74.9 70.9 37.3 37.1 65.3 55.3 61.7 58.2 55.2 VOC 2010 trainval correct localization (CorLoc [6]) on positive trainval images (%). Box Sc. + Sp. Reg. 34.5 34.9 34.8 39.3 VOC 2007 test detection average precision (%). The ensemble network is denoted as Ens.</figDesc><table><row><cell></cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>Ens.</cell></row><row><cell>SSW</cell><cell cols="4">31.1 30.9 24.3 33.3</cell></row><row><cell>EB</cell><cell cols="4">31.5 30.9 25.5 34.2</cell></row><row><cell>EB + Box Sc.</cell><cell cols="4">33.4 32.7 30.4 36.7</cell></row><row><cell>EB +</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hbilen/WSDDN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work acknowledges the support of the EPSRC EP/L024683/1, EPSRC Seebibyte EP/M013774/1 and the ERC Starting Grant IDIU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00949</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="452" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grishick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smooth minimization of non-smooth functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="127" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1611" to="1619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeppose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4659</idno>
		<title level="m">Human pose estimation via deep neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
