<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPU-based Self-Organizing Maps for Post-Labeled Few-Shot Unsupervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyes</forename><surname>Khacef</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LEAT</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LEAT</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electronics Dept</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Miramond</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LEAT</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GPU-based Self-Organizing Maps for Post-Labeled Few-Shot Unsupervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>brain-inspired computing · self-organizing map · few-shot classification · post-labeled unsupervised learning · transfer learning · feature extraction</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot classification is a challenge in machine learning where the goal is to train a classifier using a very limited number of labeled examples. This scenario is likely to occur frequently in real life, for example when data acquisition or labeling is expensive. In this work, we consider the problem of post-labeled few-shot unsupervised learning, a classification task where representations are learned in an unsupervised fashion, to be later labeled using very few annotated examples. We argue that this problem is very likely to occur on the edge, when the embedded device directly acquires the data, and the expert needed to perform labeling cannot be prompted often. To address this problem, we consider an algorithm consisting of the concatenation of transfer learning with clustering using Self-Organizing Maps (SOMs). We introduce a TensorFlow-based implementation to speed-up the process in multicore CPUs and GPUs. Finally, we demonstrate the effectiveness of the method using standard off-the-shelf few-shot classification benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last decade, Deep Learning (DL) techniques have achieved state-of-theart performance in many classification problems. However, DL heavily relies on supervised learning with abundant labeled data. With the fast expansion of Internet of Things (IoT) devices, a huge amount of unlabeled data is gathered everyday, but labeling these data is a very difficult task because of the human annotation cost as well as the scarcity of data in some classes <ref type="bibr" target="#b3">[4]</ref>. Finding methods to learn to generalize to new classes with a limited amount of labeled examples for each class is therefore a very active topic of research. This is the main motivation for few-shot learning. Recently, three main approaches have been proposed in the literature:</p><p>-Hallucination methods where the aim is to augment the training sets by learning a generator that can create novel data using data-augmentation arXiv:2009.03665v1 [cs.NE] 4 Sep 2020</p><p>techniques <ref type="bibr" target="#b3">[4]</ref>. However, these methods lack precision which results in coarse and low-quality synthesized data that can sometimes lead to very poor gains in performance <ref type="bibr" target="#b30">[29]</ref>. -Meta-learning where the goal is to train an optimizer that initializes the network parameters using a first generic dataset, so that the model can reach good performance with only a few more steps on the new dataset <ref type="bibr" target="#b27">[26]</ref>. This type of solution suffers from the domain shift problem <ref type="bibr" target="#b3">[4]</ref> as well as the sensitivity of hyper-parameters. -Transfer learning where a model developed for a given task is reused as the starting point for a model on a different task. In real-world problems, it happens that we have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest. Therefore, knowledge transfer would greatly improve the performance of learning by avoiding much expensive data-gathering and data-labeling efforts <ref type="bibr" target="#b20">[19]</ref>. Hence, transfer learning has emerged as the new learning framework for the few-shot classification task.</p><p>The problem becomes even harder when facing technical limitations, such as using embedded implementations for real-time processing on the edge. As a matter of fact, in many real-world scenarios, the training data is acquired using the same device that will later be used for training and inference, and labels could be given at any time of the process. To encompass for this added difficulty, we consider in this work the problem of post-labeled few-shot unsupervised learning. In this problem, learning algorithms can be deployed using no annotated data, for example to learn representations using the data acquired by the considered device. These algorithms can later be adjusted using a few labeled samples so that they become able to make predictions, at the condition that this adjustment comes with almost no added complexity to the process, so that it can be performed on the edge.</p><p>To address this problem, we propose a solution that combines transfer learning with a recently introduced algorithm [9] using Self-Organizing Maps (SOM). On the one hand, transfer learning is used to exploit a Deep Neural Network (DNN) trained on a large collection of labeled data as a "universal" feature extractor. On the other hand, a post-labeled clustering algorithm is used to leverage the obtained features and make predictions. This algorithm works in two steps: in a first step, clusters prototypes are learned using no annotated data, then the prototypes are named (labeled) using a few available annotated samples.</p><p>The motivation for using the SOM, initially proposed in <ref type="bibr" target="#b11">[11]</ref>, comes from the fact they are known to be a very effective clustering method. Indeed, it has been shown that SOMs perform better in representing overlapping structures compared to classical clustering techniques such as partitive clustering or K-means <ref type="bibr" target="#b1">[2]</ref>. In addition, SOMs are well suited to hardware implementation based on cellular neuromorphic architectures <ref type="bibr" target="#b26">[25]</ref> [10] <ref type="bibr" target="#b22">[21]</ref>. Thanks to a fully distributed architecture with local connectivity amongst hardware neurons, the energy-efficiency of the SOM is highly improved since there is no communication between a centralized controller and a shared memory unit, as it is the case in classical Von-Neumann architectures. Moreover, the connectivity and time complexities of the SOM become scalable with respect to the number of neurons <ref type="bibr" target="#b22">[21]</ref>. SOMs are used in a large range of applications <ref type="bibr" target="#b12">[12]</ref> going from high-dimensional data analysis to more recent developments such as identification of social media trends <ref type="bibr" target="#b24">[23]</ref>, incremental change detection <ref type="bibr" target="#b19">[18]</ref> and energy consumption minimization on sensor networks <ref type="bibr" target="#b13">[13]</ref>.</p><p>This work is an extension of <ref type="bibr" target="#b8">[9]</ref>, where we used the SOM for MNIST <ref type="bibr" target="#b14">[14]</ref> classification with unsupervised learning, and compared different training and labeling techniques. Here, we focus on the case of few-shot learning, and demonstrate the ability of the proposed method in reaching top performance with the challenging benchmark of mini-ImageNet classification task. We introduce a Ten-sorFlow (TF) software implementation for the proposed method, and compare execution times when using multi-core CPUs or GPUs.</p><p>The outline of the paper is as follows. Section 2 details the SOM training and labeling algorithms and describes the transfer learning methods. Then, Section 3 presents the mini-ImageNet few-labels classification problem. Next, Section 4 presents the TF-based SOM implementation and shows the multi-core CPU and GPU speed-ups. Afterwards, Section 5 presents the experiments and results on transfer learning with few labels using a SOM classifier. Finally, Section 6 and Section 7 discuss and conclude our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed methodology</head><p>In this section, we review the proposed methodology. We begin with the transfer learning part, then how to train the SOM, and we finally explain the labeling procedure.</p><p>Let us consider that we are given a dataset X = {x, x ∈ X}, that we initially consider to be unlabeled. Our first step consists in extracting relevant features from these inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transfer learning</head><p>In this work, we follow the approach proposed by <ref type="bibr" target="#b7">[8]</ref> and train a supervised feature extractor f ϕ that we call a backbone on a large annotated dataset. The parameters of the backbone are then fixed and used to obtain generic features from any input. In our case, we therefore transform</p><formula xml:id="formula_0">X into V = f ϕ (X) = {f ϕ (x), x ∈ X}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Organizing Maps learning procedure</head><p>The next step consists in training a SOM using the transformed representations in V , i.e. the extracted features. To this end, we use a two-dimensional array of k neurons, that are randomly initialized and updated thanks to the following algorithm, based on the one in <ref type="bibr" target="#b11">[11]</ref>:</p><p>Initialize the network as a two-dimensional array of k neurons, where each neuron n with m inputs is defined by a two-dimensional position p n and a randomly initialized m-dimensional weight vector w n . for t from 0 to t f do for every input vector v do for every neuron n in the network do Compute the afferent activity a n from the distance d:</p><formula xml:id="formula_1">d = v − w n (1) a n = e − d α<label>(2)</label></formula><p>end for Compute the winner s such that:</p><formula xml:id="formula_2">a s = k−1 max n=0 (a n )<label>(3)</label></formula><p>for every neuron n in the network do Compute the neighborhood function h σ (t, n, s):</p><formula xml:id="formula_3">h σ (t, n, s) = e − pn−ps 2 2σ(t) 2<label>(4)</label></formula><p>Update the weight w n of the neuron n:</p><formula xml:id="formula_4">w n = w n + (t) × h σ (t, n, s) × (v − w n )<label>(5)</label></formula><p>end for end for Update the learning rate (t):</p><formula xml:id="formula_5">(t) = i f i t/t f (6)</formula><p>Update the width of the neighborhood σ(t):</p><formula xml:id="formula_6">σ(t) = σ i σ f σ i t/t f<label>(7)</label></formula><p>end for It is to note that t f is the number of epochs, i.e. the number of times the whole training dataset is presented. The α hyper-parameter is the width of the Gaussian kernel. Its value in Equation 2 is fixed to 1 in the SOM training, but it does not have any impact in the training phase since it does not change the neuron with the maximum activity. Its value becomes critical though in the labeling process. The SOM hyper-parameters are reported in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>At the end of the training process, each neuron of the SOM corresponds to a cluster prototype in the considered problem. At this stage, these prototypes are anonymous and cannot be directly used to perform predictions. The next step explains the neurons labeling process for transforming the SOM into a classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SOM labeling</head><p>The labeling is the step between training and test where we assign each neuron the class it represents in the training dataset. We proposed in <ref type="bibr" target="#b8">[9]</ref> a labeling algorithm based on very few labels. The idea is the following: we randomly considered a labeled subset of the training dataset, and we tried to minimize its size while keeping the best classification accuracy. Our study showed that we only need 1% of randomly taken labeled samples from the training dataset for MNIST classification. The labeling algorithm detailed in <ref type="bibr" target="#b8">[9]</ref> can be summarized in five steps:</p><p>-First, we calculate the neurons activations based on the labeled input samples from the euclidean distance following Equation 2, where v is the input vector, w n and a n are respectively the weights vector and the activity of the neuron n. The parameter α is the width of the Gaussian kernel that becomes a hyper-parameter for the method. The complete GPU-based source code for the SOM training, labeling and test is available in https://github.com/lyes-khacef/GPU-SOM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets and implementation details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">mini-ImageNet few-shot learning</head><p>In this work, we perform experiments using the mini-ImageNet <ref type="bibr" target="#b29">[28]</ref> benchmark. mini-ImageNet is a subset of ImageNet <ref type="bibr" target="#b23">[22]</ref> that contains 60,000 images divided into 100 classes of 600 images, each image has 84 × 84 pixels. Following the standard approach <ref type="bibr" target="#b21">[20]</ref>, we use 64 base classes with labels to train the backbone and 20 novel classes to draw the novel datasets from. For each run, 5 classes are drawn uniformly at random among these 20 classes, then q unlabeled inputs and s labelled inputs per class are chosen uniformly at random among the 5 drawn classes. The features of the (q + s) × 5 samples are used to train the SOM, then the s labeled samples are used to label the SOM neurons. Finally, the Q = q × 5 unlabeled samples are classified and produce a classification accuracy for each run. We run 10,000 random draws to obtain a mean accuracy score and indicate confidence scores (95%) when relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">WRN training</head><p>The feature extractor we use is the same as in <ref type="bibr" target="#b7">[8]</ref>. It is mostly based on Wide Residual Networks (WRN) <ref type="bibr" target="#b31">[30]</ref> as a backbone extractor, with 28 convolutional layers and a widening factor of 10. As a result, the output feature size (the dimension of a vector v ∈ V ) is 640. Let us insist on the fact the backbone is trained on a completely disjoint dataset with the tasks we consider thereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cosine distance</head><p>In transfer learning, the backbone feature extractor is trained with 80 classes that are different from the 20 classes we classify using the SOM. Hence, the features amplitude is not relevant, and the Euclidean distance of the SOM does not provide the best performance. Therefore, we replace the Euclidean distance in Equation 1 with the Cosine distance in Equation <ref type="bibr" target="#b7">8</ref>.</p><formula xml:id="formula_7">d = 1 − cos(v, w n ) = 1 − v.w n v × w n<label>(8)</label></formula><p>The Cosine distance is also used in the labeling and test phases. The comparison to Euclidean distance is discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SOM software implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TensorFlow-based SOM</head><p>The SOM was implemented using TF [1] 2.1, an end-to-end open source platform for machine learning that uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across multiple computational devices including multi-core CPUs, generalpurpose GPUs and custom-designed ASICs known as Tensor Processing Units (TPUs) <ref type="bibr" target="#b0">[1]</ref>. TF facilitates the design of many machine learning models providing built-in functionalities such as convolution, pooling and dense (i.e. fully connected) layers. However, TF does not provide computational neuroscience models, and to the best of our knowledge, there is no efficient implementation for SOMs using TF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CPU and GPU speedups</head><p>The SOMs of different sizes were trained for 10 epochs on MNIST database, i.e. 600,000 samples of 784 dimensions. The CPU mono-core implementation is based on NumPy <ref type="bibr" target="#b28">[27]</ref> and run on an Intel Core i9-9880H CPU (16 cores), while the GPU implementation is based on TF 2.1 <ref type="bibr" target="#b0">[1]</ref> and run on two different GPUs: Nvidia GeForce RTX 2080 and Nvidia Tesla K80 freely available on Google Colab cloud service <ref type="bibr" target="#b2">[3]</ref>. Interestingly, the TF-based SOM can also run on the multiple cores of the CPU, providing a speed-up even without access to GPU.  As shown in <ref type="figure">Figure 2</ref>, we achieved a minimum speedup of 12× (22×) and a maximum speedup of 161× (138×) with the TF-GPU Tesla (TF-GPU GeForce) implementation, with an increasing speedup with respect to the number of neurons. Our GPU implementation is therefore scalable in simulation time with respect to the SOM size, which is an important aspect to accelerate the simulations and hyper-parameters exploration. In addition, we achieved a minimum speedup of 11× times and a maximum speedup of 29× times with the TF-CPU implementation, which runs the 16 cores of the CPU. Nevertheless, the gap between the GPU and CPU speed-ups increases with the number of neurons, which is expected due to the highly parallel computation of the GPU hardware.</p><p>Recent works have tried an other approach using CUDA acceleration on Nvidia GPUs. They showed relative gains to CPU of 44× <ref type="bibr" target="#b17">[17]</ref>, 47× <ref type="bibr" target="#b5">[6]</ref> and 67× <ref type="bibr" target="#b16">[16]</ref>. Our implementation reaches an average gain of 19× in a multi-core Intel Core i9 CPU, 100× in a Nvidia Tesla GPU and 102× in a Nvidia GeForce GPU. A fair comparison is difficult since we do not target the same hardware, but the order of magnitude is comparable and our results are in the state of the art. Another advantage of our TF-based approach is the easy integration of the SOM layer into Keras <ref type="bibr" target="#b4">[5]</ref>, a high-level neural networks API capable of running on top of TF with a focus on enabling fast experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and results</head><p>The SOM training hyper-parameters for the different settings were found with a grid search and are reported in <ref type="table" target="#tab_0">Table 1</ref>.  First, we investigated the impact of the SOM size on the classification accuracy for the commonly used number of unlabeled samples q = 15 and labeled samples s = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>  <ref type="bibr" target="#b7">[8]</ref>. <ref type="figure">Figure 3</ref> shows that there is an optimal point at 25 neurons for s = 1 and 100 neurons for s = 3 and s = 5. There is a tradeoff between the number of neurons that learn different prototypes and the quality of the learning/labeling of these neurons. The more neurons we have, the more potential to learn different prototypes of the data but the more fuzzy the prototypes become, which makes the labeling part more difficult. For example, a neuron may be assigned a class "A" with respect to the labeled subset, but will be more active for a class "B" with respect to the test set. When we only have one labeled sample per class, i.e. s = 1, then a SOM of only 25 neurons achieves the best accuracy because more neurons will not converge as well. Next, we varied the number of unlabeled data Q = q × 5 with the above mentioned SOM sizes. <ref type="figure">Figure 4</ref> shows that even though the labels are only used for the neurons class assignment and not in the training process, they still have a large impact on the accuracy. Naturally, the more labeled data we have, the better accuracy we get. A second remark is that the more unlabeled data we have, the better accuracy we get too. This is not intuitive, because the unlabeled data are the queries, i.e. the samples to classify, so the more we have the harder the classification task becomes. However, since the SOM is trained on these data, its adaptation capabilities makes the accuracy increase with the number of unlabeled data for the same number of labels. The only exception is when s = 1, where there is a small decrease in accuracy between Q = 250 (71.74% ± 0.21) and Q = 500 (71.27% ± 0.21). A third remark is that the SOM reaches the same accuracy for [s = 5, Q = 25] and [s = 3, Q = 250], which means that the lack of labeled data can be compensated by more unlabeled data. In fact, it is a very interesting property since unlabeled data can be gathered much more easily, and no extra-effort for labeling these data is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Classification accuracy (%) The choice of using the Cosine distance in the SOM computation (training, labeling and test) was inspired from the work of <ref type="bibr" target="#b7">[8]</ref>. In fact, <ref type="figure" target="#fig_6">Figure 5</ref> shows that replacing the Euclidean distance by the Cosine distance significantly improves the SOM classification accuracy, with a gain of +5.9%, +4.96% and +4.68% for s = 1, s = 3 and s = 5, respectively. It validates our hypothesis about the non-effectiveness of the Euclidean distance when using transfer learning.</p><p>Finally, <ref type="table">Table 2</ref> reports the recent works that proposed solutions to the mini-ImageNet few labels classification problem using transfer learning with the WRN backbone feature extractor. The SOM reaches top-2 accuracy for s = 1 and top- <ref type="table">Table 2</ref>. mini-ImageNet few labels transfer learning with q = 15 (Q = 75): state of the art reported from <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Classifier 1-shot (%) 5-shot (%) wDAE-GNN <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and further works</head><p>We introduced in this work the problem of post-labeled few-shot unsupervised learning and proposed a solution that combines transfer learning and SOMs. Transfer learning was used to exploit a WRN backbone trained on a base dataset as a feature extractor, and the SOM was used to classify the obtained features from the target dataset. The SOM is trained with no label, then labeled with the few available annotated samples. We show that we reach a good performance on the mini-ImageNet few shot classification benchmark with an unsupervised learning method. Furthermore, the SOM is suitable for hardware implementations based on a cellular neuromorphic architecture, which enables its application on the edge. Finally, to speed-up the SOM simulation process, we proposed a novel TF-based GPU implementation which is about 100× faster than the classical CPU implementation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>-Second, the Best Matching Unit (BMU), i.e. the neuron with the maximum activity is elected.-Third, each neuron accumulates its normalized activation (simple division) with respect to the BMU activity in the corresponding class accumulator, and the three steps are repeated for every sample of the labeling subset. -Fourth, each class accumulator is normalized over the number of samples per class. -Fifth and finally, the label of each neuron is chosen according to the class accumulator that has the maximum activity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>SOM training speed on MNIST database for 10 epochs (i.e. 600,000 samples of 784 dimensions) VS. number of SOM neurons: (a) CPU (mono-core) implementation; (b) TF-CPU (muti-core) implementation; (c) TF-GPU GeForce implementation; (d) TF-GPU Tesla implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figures 1 -</head><label>1</label><figDesc>a , 1-b, 1-c and 1-d show that the time complexities of the CPU, TF-CPU and TF-GPU implementations are all linear. It is to note that the time complexity slope of the TF-CPU, TF-GPU GeForce and TF-GPU Tesla implementations changes at 1600 neurons, 400 and 1024 neurons respectively, which is due to their different degrees of parallelism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Tesla speed-up TF-GPU GeForce speed-up TF-CPU speed-up Fig. 2. TF-CPU and TF-GPU speed-ups compared to CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Dataset i f ηi η f Epochs α mini-ImageNet 1 0.01 10 0.1 10 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>SOM classification accuracy on mini-ImageNet transfer learning for different numbers of labeled samples s vs. number of SOM neurons. SOM classification accuracy on mini-ImageNet transfer learning for different numbers of labeled samples s vs. number of unlabeled samples to classify Q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>SOM classification accuracy on mini-ImageNet transfer learning with few labels using Euclidean distance and Cosine distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>SOM training hyper-parameters.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">accuracy for s = 5, which is a good result that proves the SOM ability to handle complex datasets. Nevertheless, one has to keep in mind that while the other works use the few labels in the training process, we only use them for neurons labeling phase. Our accuracy performance is therefore obtained with fully unsupervised learning followed by post-labeling, which we believe is the right approach for the few-shot classification problem in the context of embedded systems on the edge.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work has been supported by the French government, through the UCAJEDI Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR-15-IDEX-01.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation. p. 265283. OSDI16, USENIX Association</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation. p. 265283. OSDI16, USENIX Association<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing the performance of traditional cluster analysis, self-organizing maps and fuzzy c-means method for strategic grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Budayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Birgonul</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2009.04.022</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.eswa.2009.04.022" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="11772" to="11781" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Performance analysis of google colaboratory as a tool for accelerating deep learning applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V M D</forename><surname>Nobrega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nepomuceno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H C</forename><surname>De Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P R</forename><surname>Filho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="61677" to="61685" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cuda-self-organizing feature map based visual sentiment analysis of bank customer complaints for analytical crm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gavval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Harshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Ravi</surname></persName>
		</author>
		<idno>ArXiv abs/1905.09598</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploiting unsupervised inputs for accurate fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-organizing neurons: toward brain-inspired unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khacef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrientos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Upegui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/IJCNN.2019.8852098</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2019.8852098" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neuromorphic hardware as a self-organizing computing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khacef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Rougier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Upegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN 2018 Neuromorphic Hardware In Practice and Use workshop</title>
		<meeting><address><addrLine>Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Rio de Janeiro</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The self-organizing map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.58325</idno>
		<ptr target="https://doi.org/10.1109/5.58325" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1464" to="1480" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Engineering applications of the self-organizing map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Simula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kangas</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.537105</idno>
		<ptr target="https://doi.org/10.1109/5.537105" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1358" to="1384" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Energy consumption minimization on lorawan sensor network by using an artificial neural network based application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kromes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Verdier</surname></persName>
		</author>
		<idno type="DOI">10.1109/SAS.2019.8705992</idno>
		<ptr target="https://doi.org/10.1109/SAS.2019.8705992" />
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Applications Symposium (SAS). pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<title level="m">Prototype rectification for few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalability of self-organizing maps on a GPU cluster using OpenCL and CUDA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sturgeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hurley</surname></persName>
		</author>
		<idno type="DOI">10.1088/1742-6596/341/1/012018</idno>
		<ptr target="https://doi.org/10.1088" />
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">341</biblScope>
			<biblScope unit="page">12018</biblScope>
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel high dimensional self organizing maps using cuda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Botelho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F O</forename><surname>Gaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Brazilian Robotics Symposium and Latin American Robotics Symposium</title>
		<imprint>
			<date type="published" when="2012-10" />
			<biblScope unit="page" from="302" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/SBR-LARS.2012.56</idno>
		<ptr target="https://doi.org/10.1109/SBR-LARS.2012.56" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Intelligent detection of driver behavior changes for effective coordination between autonomous and human driven vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nallaperuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IECON 2018 -44th Annual Conference of the IEEE Industrial Electronics Society</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3120" to="3125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A distributed cellular approach of large scale SOM models for hardware implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khacef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Image Processing and Signals</title>
		<meeting><address><addrLine>Sophia-Antipolis, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">211252</biblScope>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Machine learning to support social media empowered patients in cancer care and cancer treatment decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K B</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Bandaragoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adikari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Iddamalgoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Lawrentschuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Persad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Osipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An fpga distributed implementation model for embedded som with on-line learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>De Abreu De Sousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Del-Moral-Hernandez</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2017.7966351</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2017.7966351" />
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<title level="m">Learning to Learn</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The numpy array: A structure for efficient numerical computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2011.37</idno>
		<ptr target="https://doi.org/10.1109/MCSE.2011.37" />
	</analytic>
	<monogr>
		<title level="j">Computing in Science Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<title level="m">Generalizing from a few examples: A survey on few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
