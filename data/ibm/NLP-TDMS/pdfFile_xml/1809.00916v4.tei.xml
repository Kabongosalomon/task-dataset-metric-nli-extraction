<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OCNet: Object Context for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Jianyuan</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Chao Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Xilin</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OCNet: Object Context for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>IJCV manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic segmentation</term>
					<term>Context</term>
					<term>Self-attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the semantic segmentation task with a new context aggregation scheme named object context, which focuses on enhancing the role of object information. Motivated by the fact that the category of each pixel is inherited from the object it belongs to, we define the object context for each pixel as the set of pixels that belong to the same category as the given pixel in the image. We use a binary relation matrix to represent the relationship between all pixels, where the value one indicates the two selected pixels belong to the same category and zero otherwise.</p><p>We propose to use a dense relation matrix to serve as a surrogate for the binary relation matrix. The dense relation matrix is capable to emphasize the contribution of object information as the relation scores tend to be larger on the object pixels than the other pixels. Considering that the dense relation matrix estimation requires quadratic computation overhead and memory consumption w.r.t. the input size, we propose an efficient interlaced sparse self-attention scheme to model the dense relations between any two of all pixels via the combination of two sparse relation matrices.</p><p>To capture richer context information, we further combine our interlaced sparse self-attention scheme with the conventional multi-scale context schemes including pyramid pooling  and atrous spatial pyramid pooling . We empirically show the advantages of our approach with competitive performances on five challenging benchmarks including: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a fundamental topic in computer vision and is critical for various scene understanding problems. It is typically formulated as a task of predicting the category of each pixel, i.e., the category of the object that the pixel belongs to. We are mainly interested in improving the pixel classification accuracy through explicitly identifying the object region that the pixel belongs to.</p><p>Extensive efforts based on deep convolutional neural networks have been made to address the semantic segmentation since the pioneering approach of the fully convolutional network (FCN) <ref type="bibr" target="#b80">(Long et al., 2015)</ref>. The original FCN approach suffers from two main drawbacks including the reduced feature resolution that loses the detailed spatial information and the small effective receptive field that fails to capture long-range dependencies. There exist two main paths to tackle the above drawbacks: (i) raising the resolution of feature maps for improving the spatial precision or maintaining a high-resolution response map through all stages, e.g., through dilated convolutions <ref type="bibr" target="#b94">Yu and Koltun, 2016)</ref>, decoder network <ref type="bibr" target="#b56">(Badrinarayanan et al., 2017;</ref><ref type="bibr" target="#b84">Ronneberger et al., 2015)</ref> or high-resolution networks <ref type="bibr">(Sun et al., 2019a,b)</ref>. (ii) exploiting the global context to capture long-range dependencies, e.g., ParseNet <ref type="bibr" target="#b79">(Liu et al., 2015)</ref>, DeepLabv3 , and PSP-Net . In this work, we focus on the second path and propose a more efficient context scheme. We define the context of a pixel as a set of selected pixels and its context representation as an aggregation of all selected pixels' representations if not specified.</p><p>Most previous representative studies mainly exploit the multi-scale context formed from spatially nearby or sampled pixels. For instance, the pyramid pooling module (PPM) in PSPNet  divides all pixels into multiple regions and selects all pixels lying in the same region with arXiv:1809.00916v4 [cs.CV] 15 Mar 2021 <ref type="figure">Fig. 1</ref>: Illustrating the predicted dense relation matrices. The first column illustrates example images sampled from the Cityscapes val, and we mark three pixels on object car, person and road with respectively. The second column illustrates ground truth segmentation maps. The third column illustrates the dense relation matrices (or approximated object context maps) of the three pixels. We can see that the relation values corresponding to the pixels belonging to the same category as the selected pixel tend to be larger. a pixel as its context. The atrous spatial pyramid pooling module (ASPP) in DeepLabv3  selects the surrounding pixels of a pixel with different dilation rates as its context. Therefore, the selected pixels of both PPM context and ASPP context tend to be the mixture of object pixels, relevant background pixels and irrelevant background pixels. Motivated by the fact that category of each pixel is essentially the category of the object that it belongs to, we should enhance the object pixels that constitute the object.</p><p>To explicitly emphasize the contribution of the object pixels, we present an object context that aims at only gathering the pixels that belong to the same category as a given pixel as its context. Compared to the conventional multiscale context schemes, our object context pays more attention to the necessary object information. Although estimating the accurate object context is not an easy task, we empirically find that a coarse estimation of the object context already outperforms both PPM and ASPP schemes on various benchmarks.</p><p>For a given pixel, we can use a binary vector to record pixels that belong to the same category as it with 1 and 0 otherwise. Thus, a binary relation matrix of N × N can be used to record the pair-wise relations between any two of N pixels. Since computing the binary relation matrix is intractable, we use a dense relation matrix to serve as a surrogate of it, in which each relation value is computed based on the high-level features' inner-product similarities. Therefore, the relation value of the semantically similar pixels tend to be larger. In our implementation, we use the conventional self-attention scheme <ref type="bibr" target="#b88">(Vaswani et al., 2017)</ref> to predict the dense relation matrix, which requires O(N 2 ) computation complexity. To address the efficiency problem, we propose a new interlaced sparse self-attention scheme that significantly improves the efficiency while maintaining the performance via two sparse relation matrices to approximate the dense relation matrix. To illustrate that our approach is capable of enhancing the object pixels, we show some examples of the predicted dense relation matrices in <ref type="figure">Fig. 1</ref>, where the relation values on the object pixels are larger than the relation values on the background pixels.</p><p>We further illustrate two extensions that capture richer context information: (i) pyramid object context, which estimates the object context within each sub-region generated by the spatial pyramid partitions following the PPM . (ii) atrous spatial pyramid object context, which combines ASPP  with the object context. We summarize our main contributions as following:</p><p>-We present a new object context scheme that explicitly enhances the object information. -We propose to instantiate the object context scheme with an efficient interlaced sparse self-attention that significantly decreases the complexity compared to the conventional self-attention scheme. -We construct the OCNet based on three kinds of object context modules and achieve competitive performance on five challenging semantic segmentation benchmarks including Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Resolution. Earlier studies based on conventional FCN <ref type="bibr" target="#b80">(Long et al., 2015)</ref> apply the consecutive convolution striding and pooling operations to extract low-resolution feature map with high-level semantic information. For example, the output feature map size of ResNet-101 is 1 32 of the input image, and such significant loss of the spatial information is one of the main challenges towards accurate semantic segmentation. To generate high-resolution feature map without much loss of the semantic information, many efforts <ref type="bibr" target="#b56">(Badrinarayanan et al., 2017;</ref><ref type="bibr" target="#b58">Chen et al., 2017;</ref><ref type="bibr" target="#b84">Ronneberger et al., 2015;</ref><ref type="bibr" target="#b86">Sun et al., 2019a;</ref><ref type="bibr" target="#b94">Yu and Koltun, 2016)</ref> have proposed various efficient mechanisms. In this paper, we adopt the dilated convolution <ref type="bibr" target="#b94">Yu and Koltun, 2016)</ref> on ResNet-101 to increase the output stride from 32 to 8 by following the same settings of PSPNet . Besides, we also conduct experiments based on the recent HRNet <ref type="bibr" target="#b86">(Sun et al., 2019a)</ref> with output stride 4. We empirically verify that our approach is more efficient than the conventional multi-scale context mechanism, PPM and ASPP, with high resolution output feature map. More detailed comparisons are summarized in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Context. Context plays an important role in various computer vision tasks and it is of various forms such as global scene context, geometric context, relative location, 3D layout and so on. Context has been investigated for both object detection <ref type="bibr" target="#b64">(Divvala et al., 2009</ref>) and part detection <ref type="bibr" target="#b67">(Gonzalez-Garcia et al., 2018)</ref>. The importance of context for semantic segmentation is also verified in the recent works <ref type="bibr" target="#b79">Liu et al., 2015;</ref><ref type="bibr" target="#b85">Shetty et al., 2019;</ref><ref type="bibr" target="#b101">Zhao et al., 2017)</ref>. It is common to define the context as a set of pixels in the literature of semantic segmentation. Especially, we can divide most of the existing context mechanisms into two kinds: (i) nearby spatial context: ParseNet <ref type="bibr" target="#b79">(Liu et al., 2015)</ref> treats all pixels over the whole image as the context, and PSPNet  performs pyramid pooling over sub-regions of four pyramid scales and all pixels within the same sub-region are treated as the context for the pixels belonging to the sub-region. (ii) sampled spatial context: DeepLabv3  applies multiple atrous convolutions with different atrous rates to capture spatial pyramid context information and regards these spatially regularly sampled pixels as the context.</p><p>These two kinds of context are defined over regular rectangle regions and might carry pixels belonging to the background categories. Different from them, our object context is defined as the set of pixels belonging to the same object category, emphasizing the object pixels that are essential for labeling the pixel. There also exist some con-current efforts <ref type="bibr" target="#b98">Zhang et al., 2019b;</ref><ref type="bibr" target="#b101">Zhao et al., 2018)</ref> that exploit the semantic relations between pixels to construct the context, and our approach is different from most of them as we propose a simple yet effective interlaced sparse mechanism to model the relational context with smaller computation cost.</p><p>Attention. Self-attention <ref type="bibr" target="#b88">(Vaswani et al., 2017)</ref> and nonlocal neural network  have achieved great success on various tasks with its efficiency on modeling long-range contextual information. The self-attention scheme <ref type="bibr" target="#b88">(Vaswani et al., 2017)</ref> calculates the context at one position as a aggregation of all positions in a sentence (at the encoder stage). Wang et al. further proposed the non-local neural network  for vision tasks such as video classification, object detection and instance segmentation based on self-attention scheme.</p><p>Our implementation is inspired by the self-attention scheme. We first apply the self-attention scheme to predict the dense relation matrix and verify its capability to approximate the object context, and there also exist some concurrent studies <ref type="bibr" target="#b98">Zhang et al., 2019b</ref>) that apply the self-attention scheme for semantic segmentation. Some recent efforts <ref type="bibr" target="#b96">Yue et al., 2018;</ref><ref type="bibr" target="#b102">Zhu et al., 2019)</ref> propose different mechanisms to decrease the computation complexity and memory consumption of selfattention scheme. For example, CGNL <ref type="bibr" target="#b96">(Yue et al., 2018)</ref> (Compact Generalized Non-local) applies the Taylor series of the RBF kernel function to approximate the pair-wise similarities, RCCA (Huang et al., 2019) (Recurrent Criss-Cross Attention) applies two consecutive criss-cross attention to approximate the original self-attention scheme.</p><p>Our interlaced sparse self-attention scheme is different from both CGNL and RCCA through factorizing the dense relation matrix to two sparse relation matrices, and we find that similar mechanisms have been applied in the previous studies on network architecture design including <ref type="bibr">Shuf-fleNet (Ma et al., 2018)</ref> and Interleaved Group Convolution <ref type="bibr" target="#b100">Zhang et al., 2017b)</ref>. The concurrent sparse transformer <ref type="bibr" target="#b61">(Child et al., 2019)</ref> also apply the similar mechanism on one dimensional text/audio related tasks that require sequential masked inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We introduce our approach with four subsections. First, we introduce the general mathematical formulation of the context representation and the definition of object context (Sec. 3.1). Second, we instantiate the object context with the conventional self-attention (SA) <ref type="bibr" target="#b88">(Vaswani et al., 2017)</ref> and our interlaced sparse self-attention (ISA) (Sec. 3.2). Third, we present the pyramid extensions of object context (Sec. 3.3). Last, we illustrate the overall pipeline and the implementation details of OCNet (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>Preliminary. We define the general mathematical formulation of the context representation as:</p><formula xml:id="formula_0">z i = ρ( 1 |I i | j∈Ii δ(x j )).</formula><p>(1)</p><p>We use X and Z to represent the input representation and the context representation respectively. x j is the j-th element of X and z i is the i-th element of Z. δ(·) and ρ(·) are two different transform functions. I = {1, · · · , N } represents a set of N pixels. We use I i to represent a subset of I, in other words, I i is the set of context pixels for pixel i. We show how I i selects pixels in the following discussions. Intuitively, the above formula of context representation is to describe a pixel with the weighted average representations of a set of relevant pixels.</p><p>The above mathematical formulations are based on the one-dimensional case for convenience and can be generalized to the higher dimensional cases easily. One of the main differences between the existing representative context methods is the formulation of the context I i .</p><p>Multi-scale context. <ref type="bibr" target="#b101">Zhao et al. (2017)</ref> proposes the pyramid pooling (PPM) context that constructs the I i as a set of spatially-close pixels around pixel i within the regular regions of different scales. <ref type="bibr" target="#b58">Chen et al. (2017)</ref> introduces the atrous spatial pyramid pooling (ASPP) context that estimates I i as a set of sparsely sampled pixels with different dilation rates around pixel i.</p><p>We take the representative multi-scale context scheme ASPP as an example to illustrate the formulation of I i :</p><formula xml:id="formula_1">I i = r∈{12,24,36} {j ∈ I | |i − j| = r},<label>(2)</label></formula><p>where r ∈ {12, 24, 36} is the dilation rate, |i − j| represents the spatial distances between pixel i and j, and I i is defined over one-dimensional input. Therefore, the ASPP context of pixel i is a set of sampled pixels that have the predefined spatial distances with i. Besides, we illustrate the formulation of I i based on PPM scheme in Appendix A.</p><p>Both kinds of context tend to be a mixture of object pixels and background pixels. Therefore, they have not explicitly enhanced the contribution from the object pixels. Motivated by the fact that the category of each pixel is essentially inherited from the category of the object that it lies in, we propose a new scheme named object context to explicitly enhance the information of object pixels.</p><p>Object context. We define the object context for pixel i as:</p><formula xml:id="formula_2">I i = {j ∈ I|l j = l i },<label>(3)</label></formula><p>where l i and l j are the label of pixel i and j respectively. We can see the object context for pixel i is essentially a set of pixels that belong to the same object category as i. We can represent the pairwise relations between any two of N pixels (encoded in the ground-truth object context) with a binary relation matrix of N × N , where the i-th row records all pixels belonging to the same category with pixel i with 1 and 0 otherwise. Especially, the binary relation matrix only encodes partial information of the ground-truth object context, i.e., the (same category) label co-occurring relations. In other words, all classes could be permuted and the binary relation matrix would be unchanged.</p><p>Considering it is intractable to estimate the binary relation matrix, we propose to use a dense relation matrix to serve as a surrogate of the binary relation matrix. We expect that the relation values between the pixels belonging to the same object category are larger than the ones belonging to different categories, thus, the contributions of the object pixels are enhanced.</p><p>In the following discussions, we first illustrate the formulation of the dense relation scheme that directly estimates the dense relation matrix W of size N × N . Second, to improve efficiency, we propose a sparse relation scheme that factorizes the dense relation matrix as the combination of two sparse relation matrices including W l and W g , where both sparse relation matrices are of size N ×N . More details are illustrated as follows.</p><p>Dense relation. The dense relation scheme estimates the relations between each pixel i and all pixels in I. We illustrate the context representation based on dense relation:</p><formula xml:id="formula_3">z i = ρ( j∈I w ij δ(x j )),<label>(4)</label></formula><p>where w ij is the relation value between pixel i and j, i.e., the element of W at coordinates (i, j). As we need to estimate the relations between i and all pixels in I directly, the computational complexity of estimating W is quadratic to the input size: O(N 2 ).</p><p>Sparse relation. The sparse relation scheme only estimates the relations between the pixel i and two subsets of selected pixels following the "interlacing (a.k.a. interleaving) method" <ref type="bibr" target="#b68">(Greenspun, 1999;</ref><ref type="bibr" target="#b83">Roelofs and Koman, 1999)</ref>. We illustrate the context representation based on sparse relation:</p><formula xml:id="formula_4">z g i = ρ( j∈I g i w g ij δ(x j )),<label>(5)</label></formula><formula xml:id="formula_5">z l i = ρ( j∈I l i w l ij δ(z g j )).<label>(6)</label></formula><p>We use the superscript g / l to mark the operators and operations associated with the global / local relation stage respectively. For example, z g i / z l i represents the context representation after the global / local relation stage of the i-th pixel. Refer to Sec. 3.2 for more details. w g ij / w l ij is the relation between pixel i and pixel j that belongs to I g i / I l i respectively. I g i and I l i are the selected context pixels:</p><formula xml:id="formula_6">I g i = {j ∈ I : j ≡ i (mod P )},<label>(7)</label></formula><p>I l i = {j ∈ I :</p><formula xml:id="formula_7">j − 1 P = i − 1 P },<label>(8)</label></formula><p>where I g i / I l i is a subset of pixels with the same remainder / quotient as the pixel i when divided by P respectively. Both i and j in the above illustrations represent the spatial positions of pixel i and j in the one-dimensional case. P represents the group number in the global relation stage (Sec. 3.2) and it determines the selection of context pixels. The main advantage of the sparse relation scheme lies at we only need to estimate the relation values between pixel i and I g i ∪ I l i instead of I, thus, saves a lot of computation cost.</p><p>Considering the pixels with equal remainder / quotient share the same I g * / I l * , thus, we ignore the subscript and use I g / I l to represent all groups of pixels that share equal remainder / quotient respectively. We compute (part of) the sparse relation matrices W g / W l within each group of pixels in I g / I l respectively. Especially, both W g and W l are <ref type="figure">Fig. 2</ref>: Illustrating the Interlaced Sparse Self-Attention. Our approach is consisted of a global relation module and a local relation module. The feature map in the left-most/right-most is the input/output. First, we color the input feature map X with four different colors. We can see that there are 4 local groups and each group is consisted of four different colors. For the global relation module, we permute and group (divide) all positions with the same color having long spatial interval distances together in X, which outputs X g . Then, we divide the X g into 4 groups and apply the self-attention on each group independently. We merge all the updated feature map of each group together as the output Z g . For the local relation module, we permute the Z g to group the originally nearby positions together and get X l . Then we divide and apply self-attention following the same manner as global relation, obtain the final feature map Z l . We can propagate the information from all input positions to each output position with the combination of the global relation module and the local relation module. The positions with the same saturation of color mark the value of the feature maps are kept unchanged. We only increase the saturation the color when we update the feature maps with self-attention operation.</p><formula xml:id="formula_8">Permute Divide Merge Permute Divide Merge SA SA SA SA SA SA SA SA Global Relation Local Relation X X g Z g X l Z l</formula><p>sparse block matrices and we can approximate the original dense relation matrix as the product of these two sparse relation matrices:</p><formula xml:id="formula_9">W = W l P W g P,<label>(9)</label></formula><p>where we use P to represent a permutation matrix of size N ×N that ensures the pixel orderings of the two sparse relation matrices are matched and P is the transpose of P. We illustrate the definition of each value p i,j in P in Appendix B and why the sparse relation scheme is more efficient than the dense relation scheme in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instantiations</head><p>We explain the specific instantiations of dense relation and sparse relation based on the self-attention and the interlaced sparse self-attention respectively. Self-attention. The implementation of dense relation scheme based on self-attention is illustrated as following,</p><formula xml:id="formula_10">W = Softmax( θ(X)φ(X) √ d ),<label>(10)</label></formula><formula xml:id="formula_11">Z = ρ(Wδ(X)),<label>(11)</label></formula><p>X ∈ R N ×Cin is the input representation, W ∈ R N ×N is the dense relation matrix, and Z ∈ R N ×Cout is the output representation. We assume C in = C out = C in the following discussion for convenience. θ and φ are two different functions that transform the input to lower dimensional space and θ(X), φ(X) ∈ R N × C 2 . The inner product in the lower dimensional space is used to compute the dense relation matrix W. The scaling factor d is used to to solve the small gradient problem of softmax function according to <ref type="bibr" target="#b88">Vaswani et al. (2017)</ref> and we set d = C 2 . Self-attention uses the function ρ and δ to learn a better embedding and we have ρ(·) ∈ R N ×C and δ(·) ∈ R N × C 2 . According to the original description of self-attention in <ref type="bibr" target="#b88">Vaswani et al. (2017)</ref>, we can also call θ, φ and δ as query-, key-, and valuetransform function respectively.</p><p>We implement both θ(·) and φ(·) with two consecutive groups of 1 × 1 conv → BN → ReLU. BN is the abbreviation for batch normalization (Ioffe and Szegedy, 2015) that synchronize the statistics. We implement δ(·) and ρ(·) with 1 × 1 conv. Specifically, θ(·), φ(·) and δ(·) halve the input channels while ρ(·) doubles the input channels.</p><p>Interlaced sparse self-attention. The implementation of the sparse relation scheme, i.e., the interlaced sparse selfattention, first divides all pixels into multiple subgroups and then applies the self-attention on each subgroup to compute the sparse relation matrices, i.e., W g and W l , and the context representations.</p><p>We illustrate the overall pipeline of the interlaced sparse self-attention scheme with a two dimensional example in <ref type="figure">Fig. 2</ref>, where we estimate W g with the global relation module and W l with the local relation module. With the combination of these two sparse relation matrices, we can approximate the dense relations between any two of all pixels, which is explained with an example in Appendix D.</p><p>Global relation. We divide all positions into multiple groups with each group consists of a subset of sampled positions according to the definition of I g . Considering that the pixels within each group are sampled based on the remainder divided by the number of groups P and they are distributed across the global image range, thus, we call it global relation.</p><p>We first permute the input feature map X:</p><formula xml:id="formula_12">X g = Permute(X) = PX,<label>(12)</label></formula><p>Second, we divide X g into P groups with each group containing Q neighboring positions (N = P × Q):</p><formula xml:id="formula_13">X g Divide − −−− → {X g 1 , X g 2 , · · · , X g P },<label>(13)</label></formula><p>where each X g p ∈ R Q×C is a subset of X g and we have X g = [X g 1 , X g 2 , · · · , X g P ] . Third, we apply the selfattention on each X g p independently:</p><formula xml:id="formula_14">W g p = Softmax( θ(X g p )φ(X g p ) √ d ),<label>(14)</label></formula><formula xml:id="formula_15">Z g p = ρ(W g p δ(X g p )),<label>(15)</label></formula><p>where W g p ∈ R Q×Q is a small dense relation matrix based on all positions from X g p , Z g p ∈ R Q×C is the updated representation based on X g p , and d takes the same value as in the previous Equation 10. We apply the same implementation for all transform functions including θ(·), φ(·), δ(·) and ρ(·) following the implementation of self-attention. We illustrate the overall sparse relation matrix W g in the global relation stage:</p><formula xml:id="formula_16">W g =      W g 1 0 · · · 0 0 W g 2 · · · 0 . . . . . . . . . . . . 0 0 · · · W g P      ,<label>(16)</label></formula><p>where only the relation values in the diagonal blocks are non-zero. Therefore, we only need to estimate the relation values between pixel pairs belonging to the same group and ignore the relations between pixel pairs from different groups. We concatenate all Z g p from different groups and get the output representation Z g = [Z g 1 , Z g 2 , · · · , Z g P ] after the global relation stage:</p><formula xml:id="formula_17">{Z g 1 , Z g 2 , · · · , Z g P } Merge − −−− → Z g ,<label>(17)</label></formula><p>Local relation. In the local relation stage, we divide the positions into multiple groups according to the definition of I l , where each group of pixels are sampled based on the quotient and they are distributed within the local neighboring range, thus, we call it local relation. We apply another permutation on the output feature map from the global relation module following:</p><formula xml:id="formula_18">X l = Permute(Z g ) = P Z g ,<label>(18)</label></formula><p>Then, we divide X l into Q groups with each group containing P neighboring positions:</p><formula xml:id="formula_19">X l Divide − −−− → [X l 1 , X l 2 , · · · , X l Q ],<label>(19)</label></formula><p>where each X l q ∈ R P ×C and we have X l = [X l 1 , X l 2 , · · · , X l Q ] . We apply the self-attention on each X l q independently, which is similar with the Equation 14 and Equation 15 in the global relation module. Accordingly, we can get W l q and Z l q , where W l q ∈ R P ×P is a small dense relation matrix based on X l q , Z l q ∈ R P ×C is the updated representation based on X l q . We illustrate the sparse relation matrix computed based on the local relation:</p><formula xml:id="formula_20">W l =      W l 1 0 · · · 0 0 W l 2 · · · 0 . . . . . . . . . . . . 0 0 · · · W l Q      ,<label>(20)</label></formula><p>where the above relation matrix W l based on the local relation is also very sparse and most of the relation values are zero.</p><p>We concatenate all Z l q from different groups and get out-</p><formula xml:id="formula_21">put representation Z l = [Z l 1 , Z l 2 , · · · , Z l Q ]</formula><p>after the local relation stage:</p><formula xml:id="formula_22">{Z l 1 , Z l 2 , · · · , Z l Q } Merge − −−− → Z l ,<label>(21)</label></formula><p>where Z l is also the final output representation of interlaced sparse self-attention scheme.</p><p>Complexity. Given an input feature map of size H ×W ×C, we analyze the computation/memory cost of both the selfattention mechanism and our interlaced sparse self-attention scheme as follows.</p><p>The computation complexity of self-attention mechanism is O(HW C 2 + (HW ) 2 C), and the complexity of our interlaced sparse self-attention mechanism is O(HW C 2 + (HW ) 2 C( 1 P h Pw + 1 Q h Qw )), where we divide the height dimension into P h groups and the width dimension to P w groups in the global relation stage and Q h and Q w groups during the local relation stage. We have H = P h Q h , W = P w Q w . The complexity of our approach can be reduced to O(HW C 2 + (HW )</p><formula xml:id="formula_23">3 2 C) when P h P w = √</formula><p>HW . Detailed formulations and proof of the computation complexity are provided in Appendix E. We compare the theoretical GFLOPs of the interlaced sparse self-attention scheme and the conventional self-attention scheme in <ref type="figure">Fig. 3</ref>, where we can see that our interlaced sparse self-attention is much more efficient than the conventional self-attention when processing inputs of higher resolution. We further report the actual GPU memory cost (measured by MB), computation cost (measured by GFLOPs), and inference time (measured by ms) of both mechanisms in <ref type="figure" target="#fig_0">Fig. 4</ref> to illustrate the advantage of our method.</p><p>We present the PyTorch code of the proposed interlaced sparse self-attention in Algorithm 1. We explain the rough )/23V* 6$ ,6$ <ref type="figure">Fig. 3</ref>: FLOPs vs. input size. The x-axis represents the height or width of the input feature map (we assume the height is equal to the width for convenience) and the y-axis represents the computation cost measured with GFLOPs. We can see that the GFLOPs of self-attention (SA) mechanism increases much faster than our interlaced sparse selfattention (ISA) mechanism with inputs of higher resolution. Object context pooling. We use self-attention or interlaced sparse self-attention to implement the object context pooling module (OCP). The object context pooling estimates the context representation of each pixel i by aggregating the representations of the selected subset of pixels based on the estimated dense relation matrix or the two sparse relation matrices. We first apply the object context pooling module based on either self-attention scheme or the proposed inter-Algorithm 1: Interlaced Sparse Self-Attention.</p><p>laced sparse self-attention scheme to compute the context representation, and then we concatenate the input representation with the context representation as the output representation, resulting in a baseline method named as Base-OC, and we illustrate the details in <ref type="figure" target="#fig_3">Fig. 5 (b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pyramid Extensions</head><p>To handle objects of multiple scales 1 , we further combine our approach with the conventional multi-scale context schemes including PPM and ASPP.</p><p>Combination with PPM. Inspired by the previous pyramid pooling module , we divide the input image into regions of four pyramid scales: 1 × 1 region, 2 × 2 regions, 3 × 3 regions and 6 × 6 regions, and we update the feature maps for each scale by feeding the feature map of each region into the object context pooling module respectively, then we combine the four updated feature maps together. Finally, we concatenate the multiple pyramid object context representations with the input feature map. We call the resulting method as Pyramid-OC. More details are illustrated in <ref type="figure" target="#fig_3">Fig. 5 (c)</ref>.</p><p>Combination with ASPP. The conventional atrous spatial pyramid pooling  consists of 5 branches including: an image-level pooling branch, a 1 × 1 convolution branch and three 3×3 dilated convolution branches with dilation rates being 12, 24 and 36, respectively. We replace the image-level pooling branch with the object context pooling to exploit the relation-based object context information, resulting in a method which we name as ASP-OC. More details are illustrated in <ref type="figure" target="#fig_3">Fig. 5 (d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Architecture</head><p>We illustrate the overall pipeline of our OCNet in <ref type="figure" target="#fig_3">Fig. 5 (a)</ref>. More details are illustrated as follows.</p><p>Backbone. We use the ResNet-101 <ref type="bibr" target="#b69">(He et al., 2016)</ref> or HRNetV2-W48  pretrained over the Im-ageNet dataset as the backbone. For the ResNet-101, we make some modifications by following PSPNet : replace the convolutions within the last two blocks by dilated convolutions with dilation rates being 2 and 4, respectively, so that the output stride becomes 8. For the HRNetV2-W48, we directly apply our approach on the final concatenated feature map with output stride 4.</p><p>Base-OC. Before feeding the feature map into the OCP, we apply a dimension reduction module (a 3 × 3 convolution) to reduce the channels of the feature maps output from the backbone to 512 for both ResNet-101 and HRNetV2-W48. Then we feed the updated feature map into the OCP and concatenate the output feature map of the OCP with the input feature map to the OCP. We further perform a 1 × 1 convolution to decrease the channels of the concatenated feature map from 1024 to 512, which is not included in <ref type="figure" target="#fig_3">Fig. 5 (b)</ref>.</p><p>Pyramid-OC. We first apply a 3 × 3 convolution to reduce the channels to 512 in advance, then we feed the dimension reduced feature map to the Pyramid-OC and perform four different pyramid partitions (1×1 region, 2×2 regions, 3×3 regions, and 6 × 6 regions) on the input feature map, and we concatenate the four different output object context feature maps output by the four parallel OCPs. Each one of the four object context feature maps has 512 channels. We apply a 1 × 1 convolution to increase the channel of the input feature map from 512 to 2048 and concatenate it with all four object context feature maps. Lastly, we use a 1 × 1 convolution on the concatenated feature map with 4096 channels and produce the final feature map with 512 channels , which is not included in <ref type="figure" target="#fig_3">Fig. 5 (c)</ref>.</p><p>ASP-OC. We only perform the dimension reduction within the object context pooling branch, where we use a 3 × 3 convolution to reduce the channel to 256. The output feature map from object context pooling module has 256 channels. For the other four branches, we exactly follow the original ASPP module and apply a 1 × 1 convolution within the second above branch and 3 × 3 dilated convolution with different dilation rates <ref type="bibr">(12,</ref><ref type="bibr">24,</ref><ref type="bibr">36)</ref> in the three remaining parallel branches. We set the output channel as 256 in all these four branches following the original settings . Lastly, we concatenate these five parallel output fea-ture maps and use a 1×1 convolution to decrease the channel of the concatenated feature map from 1280 to 256, which is not included in <ref type="figure" target="#fig_3">Fig. 5 (d)</ref>.</p><p>Discussion. The concept of object context is also discussed in our another work: object contextual representations (OCR) . The main difference is that this work is focused on efficiently modeling the dense relations between pixel and pixel while OCR mainly exploits the coarse segmentation maps to construct a set of object region representations and models the dense relations between pixel and object regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate our approach on five challenging semantic segmentation benchmarks. First, we study various components within our approach and compare our approach to some closely related mechanisms (Sec 4.3). Second, we compare our approach to the recent state-of-the-art methods to verify that we achieve competitive performance <ref type="bibr">(Sec 4.4)</ref>. Last, we apply our approach on the conventional Mask-RCNN to verify that our method generalizes well (Sec 4.5). Besides, we also illustrate the quantitative improvements along the boundary <ref type="table" target="#tab_6">(Table 6</ref>) and the qualitative improvements on various benchmarks <ref type="figure">(Fig. 9</ref>) based on our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Cityscapes 2 . Cityscapes <ref type="bibr" target="#b62">(Cordts et al., 2016)</ref> contains 5, 000 finely annotated images with 19 semantic classes. The images are in 2048 × 1024 resolution and captured from 50 different cities. The training, validation, and test sets consist of 2, 975, 500, 1, 525 images, respectively.    given an input image, we use a backbone to extract the feature map, then we apply an object context module on the feature map and output the contextual feature map. Based on the contextual feature map, we apply a classifier to predict the final segmentation map. (b) Base-OC: we perform an object context pooling (OCP) on the input feature map, then we concatenate the output of OCP and the input feature map as the final output. (c) Pyramid-OC: we apply four parallel OCPs independently. Each branch divides the input to different pyramid scales, and the object context pooling is shared within each branch, then we concatenate the four output feature maps with a new feature map that is generated by increasing the channels of the input feature map by 4× (512 → 2048). (d) ASP-OC: we apply an OCP and four dilated convolutions (these four branches are the same with the original ASPP and the rate represents the dilation rate), then we concatenate the five output feature maps as the output. All the convolutions are followed by a group of BN → ReLU operation.</p><p>59 semantic classes and 1 background class. The training set and test set consist of 4, 998 and 5, 105 images, respectively.</p><p>COCO-Stuff 6 . COCO-Stuff <ref type="bibr" target="#b57">(Caesar et al., 2018</ref>) is a challenging scene parsing dataset that contains 171 semantic classes. The training set and test set consist of 9K and 1K images, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Training setting. We initialize the parameters within the object context pooling module and the classification head randomly. We perform the polynomial learning rate policy with factor (1 − ( iter itermax ) 0.9 ). We set the weight on the final loss as 1 and the weight on the auxiliary loss as 0.4 following PSPNet . The auxiliary loss is applied on the representation output from stage-3 of ResNet-101 or the final representation of HRNetV2-W48. We all use the INPLACE-ABN sync (Rota Bulò et al., 2018) to synchronize the mean and standard-deviation of batch normalization across multiple GPUs. For the data augmentation, we perform random flipping horizontally, random scaling in the range of [0.5, 2] and random brightness jittering within the range of [−10, 10]. More details are illustrated as following.</p><p>For the experiments on Cityscapes: we set the initial learning rate as 0.01, weight decay as 0.0005, crop size as 512 × 1024 and batch size as 8. For the experiments evalu-6 https://github.com/nightrome/cocostuff ated on val/test, we set training iterations as 60K/100K on train/train+val respectively.</p><p>For the experiments on ADE20K: we set the initial learning rate as 0.02, weight decay as 0.0001, crop size as 520 × 520, batch size as 16 and training iterations as 150K if not specified.</p><p>For the experiments on LIP: we set the initial learning rate as 0.007, weight decay as 0.0005, crop size as 473 × 473, batch size as 32 and training iterations as 100K if not specified.</p><p>For the experiments on PASCAL-Context: we set the initial learning rate as 0.001, weight decay as 0.0001, crop size as 520 × 520, batch size as 16 and training iterations as 30K if not specified.</p><p>For the experiments on COCO-Stuff : we set the initial learning rate as 0.001, weight decay as 0.0001, crop size as 520 × 520, batch size as 16 and training iterations as 60K if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We choose the dilated ResNet-101 as our backbone to conduct all ablation experiments, and we also use ResNet-101 alternatively for convenience. We choose the OCNet with Base-OC (ISA) as our default setting if not specified.</p><p>Group numbers. In order to study the influence of group numbers within the Base-OC (ISA) scheme, we train the Base-OC (ISA) method by varying P h and P w , where we can determine the value of Q h and Q w according to H =    P h × Q h and W = P w × Q w . In <ref type="table" target="#tab_1">Table 1</ref>, we illustrate the results on Cityscapes val. We can see that our approach with different group numbers consistently improves over the baseline and we get the best result with P h = P w = 8, there- fore, we set P h = P w = 8 in all experiments by default setting if not specified.</p><p>Global+Local vs. Local+Global. We study the influence of the order of global relation and local relation within Base-OC (ISA) module. We report the results in the 6 th row and 10 th row of <ref type="table" target="#tab_1">Table 1</ref>. We can see that both mechanisms improve over the baseline by a large margin. Applying the global relation first seems to be favorable. We apply the global relation first unless otherwise specified for all our experiments. Besides, we also compare the results with only sparse global attention or only local relation. We report the results (measured by mIoU): only global relation: 78.9% and only local relation: 77.2%, which verifies that the global relation is more important and the local relation is complementary with the global relation.</p><p>Comparison to multi-scale context. We compare the proposed relational context scheme to two conventional multiscale context schemes including: PPM  and ASPP .</p><p>We conduct the comparison experiments under the same training/testing settings, e.g., the same training iterations and batch size. We report the related results in <ref type="table" target="#tab_2">Table 2</ref>. Our reproduced PPM outperforms the original reported performance (Ours: 78.5% vs. Paper : 77.6%). Our approach consistently outperforms both PPM and ASPP on the evaluated benchmarks including Cityscapes and ADE20K. For example, Base-OC (ISA) outperforms the PPM by 0.99%/0.61% on Cityscapes/ADE20K, respectively measured by mIoU. Compared to the ASPP, our approach is more efficient according to the complexity comparison reported in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Besides, we also compare the performance based on the conventional self-attention (SA) and the proposed interlaced sparse self-attention (ISA) in <ref type="table" target="#tab_2">Table 2</ref>, and we can see that our ISA achieves comparable performance while being much more efficient.</p><p>Comparison to RCCA/CGNL/Efficient-Attention. We compare our approach with several existing mechanisms that focus on addressing the efficiency problem of selfattention/non-local, such as SA-2× ,  <ref type="bibr" target="#b96">(Yue et al., 2018)</ref> and Efficient Attention . For SA-2×, we directly down-sample the feature map for 2× before computing the dense relation matrix. We evaluate all these mechanisms on the Cityscapes val and report the results in <ref type="table" target="#tab_3">Table 3</ref>. We can see that our approach consistently outperforms all these three mechanisms, which verifies that our approach is more reliable. We all report the average performance for fairness considering the mIoU variance of RCCA is large.</p><p>Complexity. We compare the complexity of our approach with PPM , DANet , <ref type="bibr">RCCA (Huang et al., 2019)</ref>, CGNL <ref type="bibr" target="#b96">(Yue et al., 2018)</ref> and Efficient Attention  in this section. We report the GPU memory, GFLOPs and inference time when processing input feature map of size 2048 × 128 × 128 under the same setting in <ref type="table" target="#tab_4">Table 4</ref>. We can see that our approach based on ISA is much more efficient than most of the other approaches except the Efficient Attention scheme. For example, the proposed Base-OC (ISA) is nearly 3× faster and saves more than 88% GPU memory when compared with the DANet. Besides, our approach also requires less GPU memory/inference time than both RCCA and CGNL.</p><p>Pyramid extensions. We study the performance with the two pyramid extensions including the Pyramid-OC and ASP-OC. We choose the dilated ResNet-101 as our baseline and summarize all related results in <ref type="table" target="#tab_5">Table 5</ref>. We can find that the ASP-OC consistently improves the performance for both the SA scheme and ISA scheme while the Pyramid-OC slightly degrades the performance compared to the Base-OC mechanism. Accordingly, we only report the performance with Base-OC (ISA) module and ASP-OC (ISA) module for the following experiments if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to State-of-the-arts</head><p>We choose the object context pooling module based on ISA, e.g., Base-OC (ISA) and ASP-OC (ISA), by default. We evaluate the performance of OCNet (w/ Base-OC) and OCNet (w/ ASP-OC) on 5 benchmarks and illustrate the related results as follows.  <ref type="bibr" target="#b101">Zhao et al., 2018)</ref> ResNet-101 78.6 AAF <ref type="bibr" target="#b70">(Ke et al., 2018)</ref> ResNet-101 79.1 RefineNet  ResNet-101 73.6 DUC-HDC  ResNet-101 77.6 DSSPN  ResNet-101 77.8 SAC  ResNet-101 78.1 DepthSeg <ref type="bibr" target="#b71">(Kong and Fowlkes, 2018)</ref> ResNet-101 78.2 BiSeNet <ref type="bibr" target="#b94">(Yu et al., 2018a)</ref> ResNet-101 78.9 DFN <ref type="bibr" target="#b94">(Yu et al., 2018b)</ref> ResNet-101 79.3 TKCN <ref type="bibr" target="#b90">(Wu et al., 2018)</ref> ResNet-101 79.5 PSANet  ResNet-101 80.1 DenseASPP  DenseNet-161 80.6 CFNet  ResNet-101 79.6 SVCNet  ResNet-101 81.0 DANet  ResNet-101 81.5 BFP  ResNet Cityscapes. We report the comparison to the state-of-theart methods on Cityscapes test in <ref type="table" target="#tab_7">Table 7</ref>, and we apply OHEM, the multi-scale testing and flip testing following the previous work. We apply our approach on both ResNet-101 and HRNetV2-W48, and our approach achieves competitive performance with both backbones. For example, we improve the performance of HRNetV2-W48 from 81.6% to 82.5% with the ASP-OC module, which also outperforms the recent ACNet .</p><p>ADE20K. In <ref type="table" target="#tab_9">Table 8</ref>, we compare our approach to the state-of-the-arts on the ADE20K val. Our approach also achieves competitive performance, e.g., OCNet (w/ ASP-OC) based on ResNet-101 and HRNetV2-W48 achieve 45.40% and 45.50% respectively, both are slightly worse PASCAL-Context. As illustrated in <ref type="table" target="#tab_10">Table 9</ref>, we compare our approach with the previous state-of-the-arts on the PASCAL-Context test. We can find that our approach significantly improves the performance of HRNet <ref type="bibr" target="#b86">(Sun et al., 2019a)</ref> and achieves 56.2% with OCNet (w/ Base-OC), which also outperforms most of the other previous approaches.</p><p>LIP. We compare our approach to the previous state-of-thearts on LIP val and illustrate the results in <ref type="table" target="#tab_1">Table 10</ref>. The OCNet (w/ ASP-OC) based on HRNetV2-48 achieves competitive performance 56.35%, which is slightly worse than the recent CNIF . <ref type="table" target="#tab_1">Table 11</ref>, we can see that our method also achieves competitive performance 40.0% on COCO-Stuff test, which is comparable with the very recent stateof-the-art method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO-Stuff. From</head><p>Visualization. We visualize some examples of the global relation, local relation and dense relation predicted with our approach, e.g., OCNet based on dilated ResNet-101 + Base-OC (ISA), on different benchmarks in <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref>.</p><p>For all examples, we down-sample the the ground-truth label map to match the size of the dense relation map, which is 1 8 of the input size. We choose the same group numbers P h = P w = 8 for all datasets, thus, the global relation matrix and the local relation matrix are of various shapes. For example, the dense relation matrix / global relation ma- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone mIoU (%) trix / local relation matrix is of size 256 × 128 / 32 × 16 / 8 × 8 respectively for Cityscapes images. We generate the dense relation matrix by multiplying the local relation with the global relation, and we can see that the estimated dense relation matrix puts its most relation weights on the pixels belonging to the same category as the chosen pixel, which well approximates the ground-truth object context. We compare the segmentation maps predicted with our approach and the baseline (dilated ResNet-101) to illustrate the qualitative improvements, and we visualize the results in <ref type="figure">Fig. 9</ref>. We can find that our method produces better segmentation maps compared with the baseline. We mark all of the improved regions with white dashed boxes.</p><p>Boundary Analysis. We report the boundary improvements within 3, 5, 9 and 12 pixels width based on our approach </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone mIoU (%)</p><p>FCN <ref type="bibr" target="#b80">(Long et al., 2015)</ref> VGG-16 22.7 DAG- <ref type="bibr">RNN (Shuai et al., 2017)</ref> VGG-16 31.2 RefineNet  ResNet-101 33.6 CCL  ResNet-101 35.7 SVCNet  ResNet-101 39.6 DANet  ResNet-101 39.7 EMANet  ResNet-101 39.9 ACNet  ResNet-101 on Cityscapes val in <ref type="table" target="#tab_6">Table 6</ref>, and we can find that our approach significantly improves the boundary quality for several object categories including wall, truck, bus, train and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Application to Mask-RCNN</head><p>Dataset. We use COCO <ref type="bibr" target="#b78">(Lin et al., 2014)</ref> dataset to evaluate our approach. The dataset is one of the most challenging datasets for object detection and instance segmentation, which contains 140K images annotated with object bounding boxes and masks of 80 categories. We follow the COCO2017 split as in <ref type="bibr" target="#b69">(He et al., 2017)</ref>, where the training, validation and test sets contains 115K, 5K, 20K images, respectively. We report the standard COCO metrics including Average Precision (AP), AP 50 and AP 75 for both bounding boxes and masks.</p><p>Training settings. We use Mask-RCNN <ref type="bibr" target="#b69">(He et al., 2017)</ref> as baseline to conduct our experiments. Similar to , we insert 1 non-local block or object context pooling module based on interlaced sparse self-attention before the last block of res-4 stage of the ResNet-50 FPN <ref type="bibr" target="#b78">(Lin et al., 2017b)</ref> backbone. All models are initialized with Im-ageNet pretrained weights and built upon open source toolbox <ref type="bibr">(Massa and Girshick, 2018)</ref>. We train the models using SGD with batch size of 16 and weight decay of 0.0001. We conduct experiments using training schedules including "1× schedule" and "2× schedule" <ref type="bibr">(Massa and Girshick, 2018)</ref>. The 1× schedule starts at a learning rate of 0.02 and is decreased by a factor of 10 after 60K and 80K iterations and finally terminates at 90K iterations. We train for 180K iterations for 2× schedule and decreases the learning rate proportionally. The other training and inference strategies keep the same with the default settings in the (Massa and Girshick, 2018). Results. We report the results on COCO dataset in <ref type="table" target="#tab_1">Table 12</ref>.</p><p>We can see that adding one non-local block   Last, we visualize the object detection and instance segmentation results of our approach and the Mask-RCNN on the validation set of COCO in <ref type="figure">Fig. 8</ref>. We can find that our approach improves the Mask-RCNN consistently on all the examples. For example, the Mask-RCNN fails to detect multiple cars in the last example while our approach achieves better detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present the object context that is capable of enhancing the object information via exploiting the semantic relations between pixels. Our object context is more in line with the definition of the semantic segmentation that defines the category of each pixel as the category of the object that it belongs to. We propose two different kinds of implementations including: (i) dense relation based on the conventional self-attention scheme and (ii) sparse relation based on the proposed interlaced sparse self-attention scheme. We demonstrate that the effectiveness of our method on five challenging semantic segmentation benchmarks, e.g., Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff. We also extend our approach on Mask-RCNN to verify the advantage and we believe our approach might benefit various vision tasks through replacing the original self-attention or non-local scheme with our interlaced sparse self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future work</head><p>Although our object context scheme achieves competitive results on various benchmarks, there still exist many other important paths to construct richer context information. We illustrate three potential candidates:</p><p>use the co-occurring relations between different object categories to refine the coarse segmentation map, e.g., the "rider" tends to co-occur with the "bicycle", thus, we can refine the "rider" pixels being misclassified as "person". use the shape structure information to regularize the segmentation, e.g., the shape of "bus" tends to be quadrilateral, pentagon, or hexagon under various views, thus, we might use a set of prior shape masks to refine the predictions like the recent ShapeMask <ref type="bibr" target="#b73">(Kuo et al., 2019)</ref>.</p><p>the spatial location relation information, e.g., the "keyboard" is typically lying under the "monitor", thus, we can use the prior knowledge on the spatial relations between different objects to refine the predictions. Besides, there also exist some efforts <ref type="bibr" target="#b72">(Krishna et al., 2017</ref>) that focused on predicting the "relationship" between different objects from the input image directly. </p><formula xml:id="formula_24">I i = {j ∈ I | (j − 1)k N = (i − 1)k N },<label>(22)</label></formula><p>where k ∈ {2, 3, 6} represents different pyramid region partitions. Such context is a aggregation of the pixels the the same quotient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Formulation of Permutation Matrix.</head><p>We illustrate the definition of each value p i,j in the permutation matrix P:</p><formula xml:id="formula_25">p i,j = 1, j = ((i − 1) mod P ) × P + i−1 P + 1; 0, otherwise,<label>(23)</label></formula><p>where, according to W = W l P W g P, we permute the i-th column of W g / W l to the j-th column if p i,j = 1 / p j,i = 1 when multiplying permutation matrix P / P on the right side of W g / W l respectively.  <ref type="figure">Fig. 10</ref>: Effect of background context on semantic segmentation and the context explanation provided by grid saliency for an erroneous prediction, the image is taken from MS <ref type="bibr">COCO Lin et al. (2014)</ref>. The grid saliency (a) shows the responsible context for misclassifying the cow (green) as horse (purple) in the semantic segmentation (b). It shows the training bias that horses are more likely on road than cows. Removing the background context context (c) yields a correctly classified cow (d).</p><p>D. Why the sparse relation is more efficient?</p><p>For the convenience of analysis, we rewrite the mathematical formulation of computing the context representations (w/o considering the transform functions δ(·) and ρ(·)) based on dense relation scheme and sparse relation scheme as following. The formulation of dense relation scheme is Z = WX and the formulation of sparse relation scheme is Z = (W l P W g P)X. We can see that the formulation of the sparse relation scheme still requires O(N 2 ) GPU memory to store the reconstructed dense relation matrix W l P W g P. To avoid such expensive GPU memory consumption, we rewrite the formulation of the sparse relation scheme as Z = W l (P (W g (PX))) according to the associative laws. Because both W l and W g are sparse block matrices and each block is independent from the other blocks, we compute the multiple block matrices concurrently via transforming these block matrices to align on the batch dimension. Besides, we also implement the permutation matrix via the combination of permute and reshape operation provided in PyTorch. More details are illustrated in the discussion in Sec. 3.2 and Algorithm 1 E. Intuitive example of the sparse relation scheme.</p><p>We use an one-dimensional example in <ref type="figure">Fig. 11</ref> to explain why the combination of two sparse relation matrices are capable to approximate the dense relation matrix. In other words, both dense relation and sparse relation ensure that each output position is connected with all input positions. Specifically, in <ref type="figure">Fig. 11 (b)</ref>, the output position A 1 has direct relations with {A 2 , A 3 , B 1 } and indirect relations with {B 2 , B 3 } via B 1 .</p><p>F. Complexity Analysis.</p><p>We illustrate the proof of the complexity of interlaced sparse self-attention scheme:</p><formula xml:id="formula_26">(a) Dense Relation (b) Sparse Relation Input Output Input A 1 B 1 A 2 B 2 A 3 B 3 A 1 B 1 A 2 B 2 A 3 B 3 A 1 B 1 A 2 B 2 A 3 B 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Relation Local Relation</head><formula xml:id="formula_27">A 1 A 2 A 3 B 1 B 2 B 3 permute A 1 A 2 A 3 B 1 B 2 B 3 A 1 B 1 A 2 B 2 A 3 B 3 A 1 B 1 A 2 B 2 A 3 B 3</formula><p>Output permute W W W <ref type="figure">Fig. 11</ref>: Illustrating how the sparse relation approximates the dense relation. We use A 1 , B 1 , ..., B 3 to represent the different input positions. The gray arrows represent the information propagation path from one input position to one output position. In (a) Dense Relation, each output position connects with all input positions directly, thus, the relation matrix is fully dense. We use the dense relation matrix W to record the weights on all connections. In (b) Sparse Relation, we have two relation matrices and each relation matrix only contains the sparse connections to a small set of selected pixels, and the combination of the two sparse connections ensures that each output position has direct or indirect relations with all input positions. We use the two sparse relation matrices W g and W l to record the weights on all sparse connections.</p><p>Proof . The shapes of the input &amp; output are: X is of shape HW × C, θ(X), φ(X), δ(X) ∈ R HW × C 2 , ρ(X) ∈ R HW ×C . In the formulation of the ISA's global relation stage, the overall complexity of θ(·), φ(·), δ(·), and ρ(·) are O(HW C 2 ). The overall complexity of θ(X g p )φ(X g p ) and W g p δ(X g p ) are O(( HW P h Pw ) 2 C) within each group of positions. There exist P h P w groups in the global relation stage and P h P w . Thus, the overall complexity of the global relation stage of ISA is:</p><p>T (ISA/global) = T (θ(·)) + T (φ(·)) + T (δ(·)) + T (ρ(·)) +P h P w T (θ(X g p )φ(X g p ) ) = O(HW C 2 + (HW ) 2 C 1 P h P w ),</p><p>Similarly, we can get the complexity of the lobal relation stage in ISA:</p><formula xml:id="formula_29">T (ISA/local) = O(HW C 2 + (HW ) 2 C 1 Q h Q w ),<label>(25)</label></formula><p>In summary, we can compute the final complexity of ISA via adding T (ISA/global) and T (ISA/local),</p><formula xml:id="formula_30">T (ISA) = O(HW C 2 + (HW ) 2 C( 1 P h P w + 1 Q h Q w ))<label>(26)</label></formula><p>where we can achieve the minimized computation complexity of O(HW C 2 + (HW ) 3 2 C) when P h P w = Q h Q w is satisfied (according to arithmetic mean ≥ geometric mean).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Illustrating the Permutation Scheme of ISA.</head><p>To help the readers to understand how we select and permute the indices within Interlaced Sparse Self-Attention, we use an example in <ref type="figure">Fig. 12</ref> to explain the details.</p><p>H. More details of Pyramid-OC.</p><p>We explain the details of Pyramid-OC as following: Given an input feature map X of shape H × W × C, we first divide it into k × k groups (k ∈ {1, 2, 3, 6}) following the pyramid partitions of PPM :</p><formula xml:id="formula_31">X →        </formula><p>X 1,1 X 1,2 · · · X 1,k X 2,1 X 2,2 · · · X 2,k . . . . . . . . . . . .</p><formula xml:id="formula_32">X k,1 X k,2 · · · X k,k         ,<label>(27)</label></formula><p>where each X i,j is of shape H k × W k × C, ∀ i, j ∈ {1, 2, .., k}. We apply the object context pooling (OCP) on each group X i,j (note that the parameters of OCP are shared across the groups within the same partition) to compute the context representations, then, we concatenate the context representations to obtain the output feature Z k of shape H × W × C:         OCP(X 1,1 ) OCP(X 1,2 ) · · · OCP(X 1,k ) OCP(X 2,1 ) OCP(X 2,2 ) · · · OCP(X 2,k )</p><p>. . . . . . . . . . . .</p><p>OCP(X k,1 ) OCP(X k,2 ) · · · OCP(X k,k )</p><formula xml:id="formula_33">        → Z k .<label>(28)</label></formula><p>We compute four different context feature maps {Z 1 , Z 2 , Z 3 , Z 6 } based on four different pyramid partitions. Last, we concatenate these context feature maps: Z = concate(Z 1 , Z 2 , Z 3 , Z 6 ).</p><p>(29)</p><p>I. Checkered artefact with ISA.</p><p>We can observe checkered artefact in the <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref>, which is caused by our implementation on the visualization of the global relation, local relation and dense relation. We illustrate the related pseudo-code in Algorithm 2. Specifically speaking, for a selected pixel, we multiply a set of global relation matrices (associates with the pixels that belong to the same group as the selected pixel in the local relation stage) with the its local relation matrix. Therefore, the shape of the checkerboard is exactly the same as the shape of the global relation map. For example, we zoom in the dense relation and global relation of some examples in the <ref type="figure">Fig. 13</ref>.</p><p>Algorithm 2: Python code of visualizing the relation matrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 :</head><label>4</label><figDesc>GPU memory/FLOPs/Running time comparison between SA and ISA. All numbers are tested on a single Titan XP GPU with CUDA8.0 and an input feature map of 1 × 512 × 128 × 128 during inference stage. The lower, the better for all metrics. We can see that the proposed interlaced sparse self-attention (ISA) only uses 10.2% GPU memory and 24.6% FLOPs while being nearly 2× faster when compared with the self-attention (SA). correspondence betweenFig. 2and Algorithm 1. For example, in global relation stage, the combination of reshape in line-9 and permute in line-10 of Algorithm 1 corresponds to the Permute inFig. 2, andthe reshape in line-11 of Algorithm 1 corresponds to the Divide inFig. 2. Our implementation is optimized for the efficiency as we are applying the interlacing operations on the tensors of high dimension. Especially, the permute function (in both line-10 and line-16 of Algorithm 1) does not correspond to the Permute (for one-dimensional situation) illustrated inFig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ADE20K 3 . ADE20K (Zhou et al., 2017) is very challenging and it contains 22K densely annotated images with 150 finegrained semantic concepts. The training and validation sets consist of 20K, 2K images, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>LIP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Illustrating the overall framework of OCNet. (a) The overall network pipeline of OCNet:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b104">4</ref> . LIP is a large-scale dataset that focuses on semantic understanding of human bodies. It contains 50K images with 19 semantic human part labels and 1 background label for human parsing. The training, validation, and test sets consist of 30K, 10K, 10K images, respectively.</figDesc><table><row><cell>(a) OCNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">BackBone</cell><cell>Object Context Module</cell><cell></cell><cell>Classifier</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b) Base-OC</cell><cell></cell><cell></cell><cell>(c) Pyramid-OC</cell><cell></cell><cell></cell><cell></cell><cell>(d) ASP-OC</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1x1 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H × W × 2048</cell><cell></cell><cell>1x1 Conv</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>OCP</cell><cell></cell><cell></cell><cell>3x3 Conv rate 12</cell><cell></cell></row><row><cell>3x3 Conv</cell><cell>OCP</cell><cell></cell><cell>3x3 Conv</cell><cell></cell><cell>OCP</cell><cell></cell><cell></cell><cell>3x3 Conv rate 24</cell><cell></cell></row><row><cell>H × W × 2048</cell><cell>H × W × 512</cell><cell>H × W × 1024</cell><cell>H × W × 2048</cell><cell>H × W × 512</cell><cell>OCP</cell><cell>H × W × 4096</cell><cell>H × W × 2048</cell><cell>3x3 Conv rate 36</cell><cell>H × W × 1280</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>H × W × 512</cell><cell>OCP</cell><cell>H × W × 512</cell><cell></cell><cell>3x3 Conv → OCP</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H × W × 256</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">PASCAL-Context 5 . PASCAL-Context (Mottaghi et al.,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">2014) is a challenging scene parsing dataset that contains</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Influence of P h and Pw, the order of global relation and local relation within the interlaced sparse self-attention on Cityscapes val.</figDesc><table><row><cell>Method</cell><cell>P h</cell><cell>Pw</cell><cell cols="2">Pixel Acc (%) mIoU (%)</cell></row><row><cell>Dilated ResNet-101</cell><cell>-</cell><cell>-</cell><cell>96.08</cell><cell>75.90</cell></row><row><cell></cell><cell>4</cell><cell>4</cell><cell>96.30</cell><cell>78.97</cell></row><row><cell></cell><cell>4</cell><cell>8</cell><cell>96.31</cell><cell>78.95</cell></row><row><cell></cell><cell>8</cell><cell>4</cell><cell>96.32</cell><cell>79.31</cell></row><row><cell>+ Base-OC (ISA, Global+Local)</cell><cell>8</cell><cell>8</cell><cell>96.33</cell><cell>79.49</cell></row><row><cell></cell><cell>8</cell><cell>16</cell><cell>96.29</cell><cell>79.04</cell></row><row><cell></cell><cell>16</cell><cell>8</cell><cell>96.19</cell><cell>78.90</cell></row><row><cell></cell><cell>16</cell><cell>16</cell><cell>96.32</cell><cell>79.40</cell></row><row><cell>+ Base-OC (ISA, Local+Global)</cell><cell>8</cell><cell>8</cell><cell>96.26</cell><cell>79.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with multi-scale context scheme including PPM and ASPP on Cityscapes val and ADE20K val.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Pixel Acc (%) mIoU (%)</cell></row><row><cell></cell><cell>Dilated ResNet-101</cell><cell>96.08</cell><cell>75.90</cell></row><row><cell></cell><cell>+ PPM</cell><cell>96.20</cell><cell>78.50</cell></row><row><cell>Cityscapes</cell><cell>+ ASPP</cell><cell>96.29</cell><cell>79.10</cell></row><row><cell></cell><cell>+ Base-OC (SA)</cell><cell>96.32</cell><cell>79.40</cell></row><row><cell></cell><cell>+ Base-OC (ISA)</cell><cell>96.33</cell><cell>79.49</cell></row><row><cell></cell><cell>Dilated ResNet-50</cell><cell>76.41</cell><cell>34.35</cell></row><row><cell></cell><cell>+ PPM</cell><cell>80.17</cell><cell>41.50</cell></row><row><cell>ADE20K</cell><cell>+ ASPP</cell><cell>80.23</cell><cell>42.00</cell></row><row><cell></cell><cell>+ Base-OC (SA)</cell><cell>80.25</cell><cell>42.05</cell></row><row><cell></cell><cell>+ Base-OC (ISA)</cell><cell>80.27</cell><cell>42.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with SA with 2× downsampling, RCCA and CGNL on Cityscapes val.</figDesc><table><row><cell>Method</cell><cell>SA-2×</cell><cell>RCCA</cell><cell>CGNL</cell><cell cols="2">Efficient Attention Base-OC (ISA)</cell></row><row><cell>mIoU</cell><cell>76.49±0.35</cell><cell>78.64±0.12</cell><cell>77.06±0.04</cell><cell>78.25±0.15</cell><cell>79.49±0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Efficiency comparison given input feature map of size [2048 × 128 × 128] during inference stage. All results are based on Pytorch 0.4.1 with a single Tesla V100 with CUDA 9.0.</figDesc><table><row><cell>Method</cell><cell>Memory</cell><cell>GFLOPs</cell><cell>Time</cell></row><row><cell>PPM (Zhao et al., 2017)</cell><cell>792MB</cell><cell>619</cell><cell>78ms</cell></row><row><cell>ASPP (Chen et al., 2017)</cell><cell>331MB</cell><cell>492</cell><cell>52ms</cell></row><row><cell>DANet (Fu et al., 2019a)</cell><cell>2339MB</cell><cell>1110</cell><cell>106ms</cell></row><row><cell>RCCA (Huang et al., 2019)</cell><cell>427MB</cell><cell>804</cell><cell>84ms</cell></row><row><cell>CGNL (Yue et al., 2018)</cell><cell>266MB</cell><cell>412</cell><cell>43ms</cell></row><row><cell>Efficient Attention (Shen et al., 2018)</cell><cell>214MB</cell><cell>331</cell><cell>35ms</cell></row><row><cell>Base-OC (SA)</cell><cell>2168MB</cell><cell>619</cell><cell>66ms</cell></row><row><cell>Base-OC (ISA)</cell><cell>301MB</cell><cell>386</cell><cell>42ms</cell></row><row><cell>Pyramid-OC (SA)</cell><cell>2206MB</cell><cell>880</cell><cell>112ms</cell></row><row><cell>Pyramid-OC (ISA)</cell><cell>935MB</cell><cell>595</cell><cell>80ms</cell></row><row><cell>ASP-OC (SA)</cell><cell>348MB</cell><cell>656</cell><cell>67ms</cell></row><row><cell>ASP-OC (ISA)</cell><cell>349MB</cell><cell>651</cell><cell>65ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison to the pyramid extensions of object context pooling on Cityscapes val.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell></row><row><cell>Dilated ResNet-101</cell><cell>75.90</cell></row><row><cell>+ Base-OC (SA)</cell><cell>79.40</cell></row><row><cell>+ Base-OC (ISA)</cell><cell>79.49</cell></row><row><cell>+ Pyramid-OC (SA)</cell><cell>78.80</cell></row><row><cell>+ Pyramid-OC (ISA)</cell><cell>79.02</cell></row><row><cell>+ ASP-OC (SA)</cell><cell>79.76</cell></row><row><cell>+ ASP-OC (ISA)</cell><cell>80.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Category-wise improvements over the baseline based on Base-OC (ISA) in terms of boundary F-score on Cityscapes val.</figDesc><table><row><cell>thrs</cell><cell>method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrian</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell><cell>mean</cell></row><row><cell>12px</cell><cell cols="20">Dilated ResNet-101 92.6 80.7 87.8 57.1 57.3 83.6 77.2 82.3 91.0 62.3 90.2 79.7 81.7 92.2 78.5 89.8 90.4 82.9 80.3 + Base-OC (ISA) 92.8 80.7 88.6 64.9 61.6 83.7 76.3 81.8 91.0 65.1 90.8 78.8 81.8 92.4 86.5 92.7 95.6 82.3 79.1</cell><cell>80.9 82.4</cell></row><row><cell>9px</cell><cell cols="20">Dilated ResNet-101 91.7 78.9 85.7 56.0 55.9 82.4 76.1 81.0 88.9 61.0 89.3 78.1 80.3 91.0 78.0 89.4 90.2 82.4 78.1 + Base-OC (ISA) 91.9 78.9 86.5 63.6 60.5 82.5 75.3 80.4 88.9 63.8 89.9 77.2 80.4 91.1 86.0 92.3 95.5 81.7 76.8</cell><cell>79.7 81.2</cell></row><row><cell>5px</cell><cell cols="20">Dilated ResNet-101 88.9 73.6 79.5 53.1 52.7 78.9 72.1 76.7 82.2 57.4 85.9 73.1 75.9 86.9 77.0 88.5 90.0 80.9 71.7 + Base-OC (ISA) 89.0 73.6 80.3 60.8 57.4 79.0 71.3 76.2 82.3 60.2 86.5 72.3 76.2 87.0 85.0 91.3 95.2 80.1 70.5</cell><cell>76.1 77.6</cell></row><row><cell>3px</cell><cell cols="20">Dilated ResNet-101 84.3 65.8 70.7 50.0 49.6 72.1 65.7 69.1 72.1 53.4 79.1 64.9 70.3 79.9 75.8 87.1 89.6 79.0 63.6 + Base-OC (ISA) 84.4 65.5 71.7 57.7 54.4 72.8 64.7 68.8 72.6 56.2 80.0 64.6 70.7 80.1 83.7 89.7 94.8 78.5 62.5</cell><cell>70.7 72.3</cell></row><row><cell cols="5">RCCA (Huang et al., 2019), CGNL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison with state-of-the-arts on Cityscapes test.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Validation mIoU (%)</cell></row><row><cell>PSPNet (Zhao et al., 2017)</cell><cell>ResNet-101</cell><cell>78.4</cell></row><row><cell>PSANet (</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison with state-of-the-arts on ADE20K val.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mIoU (%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison with state-of-the-arts on PASCAL-Context test.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Comparison with state-of-the-arts on COCO-Stuff test.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Comparison with non-local (NL) on the validation set of COCO. We use Mask-RCNN<ref type="bibr" target="#b69">(He et al., 2017)</ref> as baseline and choose ResNet-50 FPN backbone for all models.</figDesc><table><row><cell>Method</cell><cell cols="3">Schedule AP box AP box 50</cell><cell>AP box 75</cell><cell cols="2">AP mask AP mask 50</cell><cell>AP mask 75</cell></row><row><cell>Mask-RCNN</cell><cell>1×</cell><cell>37.7</cell><cell>59.2</cell><cell>41.0</cell><cell>34.2</cell><cell>56.0</cell><cell>36.2</cell></row><row><cell>+ NL</cell><cell>1×</cell><cell>38.8</cell><cell>60.6</cell><cell>42.3</cell><cell>35.1</cell><cell>57.4</cell><cell>37.3</cell></row><row><cell>+ ISA</cell><cell>1×</cell><cell>38.8</cell><cell>60.7</cell><cell>42.5</cell><cell>35.2</cell><cell>57.3</cell><cell>37.6</cell></row><row><cell>Mask-RCNN</cell><cell>2×</cell><cell>38.7</cell><cell>59.9</cell><cell>42.1</cell><cell>34.9</cell><cell>56.8</cell><cell>37.0</cell></row><row><cell>+ NL</cell><cell>2×</cell><cell>39.7</cell><cell>61.3</cell><cell>43.4</cell><cell>35.9</cell><cell>58.3</cell><cell>38.2</cell></row><row><cell>+ ISA</cell><cell>2×</cell><cell>39.7</cell><cell>61.1</cell><cell>43.3</cell><cell>35.7</cell><cell>57.8</cell><cell>38.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Visualization of the predicted dense relation matrices by OCNet on LIP val and COCO-Stuff test. We apply the dilated ResNet-101 + Base-OC (ISA) to generate these relation matrices. Visualization of the object detection and instance segmentation results of Mask-RCNN<ref type="bibr" target="#b69">(He et al., 2017)</ref> and our approach on the validation set of COCO (Best viewed in color). Qualitative comparison on Cityscapes val, ADE20K val, PASCAL-Context test, COCO-Stuff test and LIP val. We choose dilated ResNet-101 as the baseline and further apply the Base-OC (ISA) on dilated ResNet-101 as our approach.</figDesc><table><row><cell></cell><cell></cell><cell>Image</cell><cell>Ground Truth</cell><cell>Dense Relation</cell><cell>Global Relation</cell><cell>Local Relation</cell></row><row><cell>Fig. 7: Mask-RCNN Ours Image Baseline Ours Ground Truth Fig. 8: Image Image Fig. 9:</cell><cell>Image Image Image</cell><cell cols="2">Ground Truth Ground Truth Ground Truth Ground Truth Ours (a) Cityscapes Baseline Image Baseline Ours (c) PASCAL-Context</cell><cell cols="2">Dense Relation (a) Cityscapes Dense Relation (b) ADE20K Dense Relation (a) LIP Dense Relation (b) COCO-Stuff Ground Truth Image Ground Truth Image (e) LIP</cell><cell>Global Relation Global Relation Global Relation Global Relation Local Relation Local Relation Local Relation Local Relation Baseline Ours Ground Truth (b) ADE20K Baseline Ours Ground Truth (d) COCO-Stuff</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) PASCAL-Context</cell></row></table><note>Fig. 6: Visualization of the predicted dense relation matrices by OCNet on Cityscapes val, ADE20K val and PASCAL-Context test. We apply the dilated ResNet-101 + Base-OC (ISA) to generate these relation matrices. We visualize both the global relation and the local relation for each selected pixel and we compute the dense relation as the product of the global relation and the local relation.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">def InterlacedSparseSelfAttention(x, P h, P w): 2 # x: input features with shape [N,C,H,W] 3 # P h, P w: Number of groups along H and W dimension</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">There exist two kinds of multiple scales problem: (i) objects of different categories have multiple scales given their distances to the camera are the same, e.g., the "car" is larger than the "person". (ii) the objects of the same category have multiple scales given their distances to the camera are different, e.g., the closer "person" is larger than the distance "person".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.cityscapes-dataset.com/ 3 https://groups.csail.mit.edu/vision/ datasets/ADE20K/ 4 http://sysu-hcp.net/lip/ 5 https://cs.stanford.edu/˜roozbeh/ pascal-context/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. More discussions on the benefits of object context.</p><p>We give a further explanation on why we believe OC is superior to the previous two representation methods including PPM  and ASPP  as following:</p><p>-In theory, enhancing the object information in the context can decrease the variance of the context information, in other words, the context of PPM and ASPP suffers from larger variance than the OC context. Because the OC context only contains the variance of the {object in-formation} while the context of PPM/ASPP further contains the variance of {object information, useful background information, irrelevant background information }. The recent study <ref type="bibr" target="#b69">(Hoyer et al., 2019;</ref><ref type="bibr" target="#b85">Shetty et al., 2019)</ref> has verified that the overuse of the noisy context information based on PPM suffers from poor generalization ability. For example, the "cow" pixels might be mis-classified as "horse" pixels when the "cow" appears on the road. We directly use the <ref type="figure">Fig. 10</ref>    <ref type="figure">Fig. 12</ref>: Illustrating the Interlaced Sparse Self-Attention with Indices Permutation. We mark the positions in the input feature map with the indices from a to p, e.g., the index a represents the spatial position (1, 1) and the index p represents the spatial position <ref type="bibr" target="#b104">(4,</ref><ref type="bibr" target="#b104">4)</ref>. We illustrate how we permute the indices in all stages as following: (i) For the Permute in the global relation stage, we permute the positions according to the remainder of the indices divided by the group numbers, e.g., 2 for both height and width dimension. For example, all positions including {(1, 1), (1, 3), (3, 1), (2, 2)} share the same remainder (1, 1) when we divide the indices by 2 for both dimensions, thus, we group the positions {a, c, i, k} together. Similarly, we get the other 3 groups of positions: {b, d, j, l} (share the same remainder (1, 0)), {e, g, m, o} (share the same remainder (0, 1)) and {f, h, n, p} (share the same remainder (0, 0)). (ii) For the Permute in the local relation stage, we permute the positions according to the quotient of the indices divided by the group numbers (2, 2). Similarly, we get 4 groups of positions: {a, b, e, f} (share the same quotient (0, 0)), {c, d, g, h} (share the same quotient (0, 1)), {i, j, m, n} (share the same quotient (1, 0)) and {k, l, o, p} (share the same quotient (1, 1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Relation Global Relation</head><p>Dense Relation Global Relation <ref type="figure">Fig. 13</ref>: Illustrating that the shape of the checkerboard in dense relation (based on interlaced sparse self-attention) is the same as the shape of the global relation map. The left / right two columns present the example from the 2-rd / 1-st row of <ref type="figure">Fig. 6</ref> (b) / (c) separately. .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>size(</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>X = X.Reshape(n</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>X = X.Reshape(n * P H * P W</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>X = X.Reshape(n, P H, P W</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>X = X.Reshape(n * Q H * Q W</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Selfattention</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>X = X.Reshape(n, Q H, Q W</surname></persName>
		</author>
		<imprint>
			<pubPlace>N, C, H, W)</pubPlace>
		</imprint>
	</monogr>
	<note>P w) 21 return x.permute(0, 3, 1, 4, 2, 5).reshape</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Refinenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno>ResNet-101 40.20</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Refinenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno>ResNet-152 40.70</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Upernet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<idno>ResNet-101 42.66</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Pspnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno>ResNet-101 43.29</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Pspnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno>ResNet-152 43.51</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dsspn (liang</surname></persName>
		</author>
		<idno>ResNet-101 43.68</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Psanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno>ResNet-101 43.77</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sac (zhang</surname></persName>
		</author>
		<idno>ResNet-101 44.30</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgr (liang</surname></persName>
		</author>
		<idno>ResNet-101 44.32</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Encnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>ResNet-101 44.65</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupta</forename><surname>Gcu (li</surname></persName>
		</author>
		<idno>ResNet-101 44.81</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Cfnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>ResNet-101 44.89</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Apcnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno>ResNet-101 45.38</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Ccnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno>ResNet-101 45.22</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nl (</forename><surname>Asymmetric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno>ResNet-101 45.24</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Danet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<idno>ResNet-101 45.22</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Acnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<idno>OC) ResNet-101 45.04</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<idno>OC) ResNet-101 45.40</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<idno>OC) HRNetV2-48 45.10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<title level="m">than the recent ACNet</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>that exploits rich global context and local context</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Deeplabv2</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno>ResNet-101 45.7</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Unet++</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>ResNet-101 47.7</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Pspnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno>ResNet-101 47.8</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ccl (ding</surname></persName>
		</author>
		<idno>ResNet-101 51.6</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Encnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>ResNet-101 51.7</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgr (liang</surname></persName>
		</author>
		<idno>ResNet-101 52.5</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Danet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<idno>ResNet-101 52.6</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Svcnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Cfnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>ResNet-101 54.0</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dupsampling (tian</surname></persName>
		</author>
		<idno>Xception-71 52.5</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Hrnetv2</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno>54.0</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Apcnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno>ResNet-101 54.7</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Emanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<idno>OC) HRNetV2-48 56.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Comparison with state-of-the-arts on LIP val. Method Backbone mIoU (%)</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Attention+ssl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
		<idno>ResNet-101 44.73</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mman (luo</surname></persName>
		</author>
		<idno>ResNet-101 46.81</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ss-Nan (</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ResNet-101 47.92</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Mula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie</surname></persName>
		</author>
		<idno>ResNet-101 49.30</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Jppnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno>ResNet-101 51.37</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ce2p (liu</surname></persName>
		</author>
		<idno>ResNet-101 53.10</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Hrnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno>55.90</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cnif (wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<idno>OC) ResNet-101 55.07</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<idno>OC) ResNet-101 55.20</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<idno>OC) HRNetV2-48 56.20</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>arXiv:170605587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spgnet: Semantic prediction guidance for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5218" to="5228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>arXiv:190410509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G ; Cvpr</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6819" to="6829" />
		</imprint>
	</monogr>
	<note>Boundaryaware feature propagation for scene segmentation</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Cvpr Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>An empirical study of context in object detection</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Adaptive context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6748" to="6757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L ; Cvpr</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>Objects as context for detecting their semantic parts</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Philip and Alex&apos;s guide to Web publishing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Greenspun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Grid saliency for context explanations of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cvpr He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cvpr He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rb ; Iccv</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V ; Nips</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv:150203167</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV Ioffe S, Szegedy C (2015) Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Ccnet: Criss-cross attention for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Shapemask: Learning to segment novel objects by refining shape priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty ; Iccv</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9167" to="9176" />
		</imprint>
	</monogr>
	<note>Expectationmaximization attention networks for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Look into person: Joint body parsing &amp; pose estimation network and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Dynamic-structured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Id ; Cvpr Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sj ; Cvpr</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maire</forename><forename type="middle">M</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl ; Eccv</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno>arXiv:180905996</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context. Devil in the details: Towards accurate single and multiple human parsing</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>arXiv:150604579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T ; Cvpr</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Junqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y ; Eccv</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
	</analytic>
	<monogr>
		<title level="j">ECCV Massa F</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Shufflenet v2: Practical guidelines for efficient cnn architecture design</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Mutual learning to adapt for joint human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S ; Eccv</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4230" to="4239" />
		</imprint>
	</monogr>
	<note>Towards bridging semantic gap to improve semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">PNG: the definitive guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Reilly &amp; Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T ; Miccai Rota</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P ; Cvpr</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>arXiv:181201243</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>U-net: Convolutional networks for biomedical image segmentation. Efficient attention: Attention with linear complexities</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Not using the car to see the sidewalk-quantifying and controlling the effects of context in classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M ; Cvpr</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
	<note>Scene segmentation with dag-recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J ; Cvpr</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv:190404514</idno>
	</analytic>
	<monogr>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3126" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning compositional neural information fusion for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I ; Nips</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G ; Wacv</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5703" to="5713" />
		</imprint>
	</monogr>
	<note>Understanding convolution for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Tree-structured kronecker convolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno>arXiv:181204945</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<idno>arXiv:180406202</idno>
		<title level="m">Interleaved structured sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N ; Eccv</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N ; Cvpr</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Multi-scale context aggregation by dilated convolutions</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv:190911065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6798" to="6807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A ; Cvpr</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>Context encoding for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Interleaved group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J ; Cvpr</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jianping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Dahua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J ; Eccv</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A ; Cvpr</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mmr</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Psanet: Point-wise spatial attention network for scene parsing</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai X ; I, J</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wl</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 1 def VisualizeRelation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>2 # i, j: indices of the pixel to be visualized</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">W: the height and width of the input of ISA module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">#</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Number of groups along H and W dimension 5 Q h, Q w = H // P h, W // P w 6 # Wg: global relation with shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname># P H</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>P h * P w, Q h * Q w, Q h * Q w</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">#</forename><surname>Wl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Q H * Q W, P H * P W, P H * P W</surname></persName>
		</author>
		<title level="m">local relation with shape</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">local w idx = j % P w, j // P w 12 global idx = gobal h idx * P w + global w idx 13 local idx = local h idx * Q w + local w idx 14 15 # global relation for current pixel 16 global rel = Wg[global idx, local idx] # [Q h * Q w] 17 18 # local relation for current pixel 19 local rel = Wl[local idx, global idx] # [P h * P w] 20 21 # dense relation for current pixel 22 multi global rel = Wg[:, local idx] # [P h * P w, Q h * Q w] 23 dense rel = multi global rel * local rel.reshape(P h * P w, 1) 24 dense rel = dense rel.reshape(P h, P w, Q h, Q w) 25 dense rel = dense rel</title>
	</analytic>
	<monogr>
		<title level="m"># obtain the indices of current pixel 10 global h idx, local h idx = i % P h, i // P h 11 global w idx</title>
		<imprint/>
	</monogr>
	<note>permute(0, 2, 1, 3).reshape(H, W) 26 27 return global rel, local rel, dense rel</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
