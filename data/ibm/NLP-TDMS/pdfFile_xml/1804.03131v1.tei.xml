<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
							<email>malberto@student.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ESAT/PSI</orgName>
								<orgName type="institution">VISICS</orgName>
								<address>
									<settlement>Leuven</settlement>
									<region>KU</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper tackles the problem of video object segmentation, given some user annotation which indicates the object of interest. The problem is formulated as pixel-wise retrieval in a learned embedding space: we embed pixels of the same object instance into the vicinity of each other, using a fully convolutional network trained by a modified triplet loss as the embedding model. Then the annotated pixels are set as reference and the rest of the pixels are classified using a nearest-neighbor approach. The proposed method supports different kinds of user input such as segmentation mask in the first frame (semi-supervised scenario), or a sparse set of clicked points (interactive scenario). In the semi-supervised scenario, we achieve results competitive with the state of the art but at a fraction of computation cost (275 milliseconds per frame). In the interactive scenario where the user is able to refine their input iteratively, the proposed method provides instant response to each input, and reaches comparable quality to competing methods with much less interaction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Immeasurable amount of multimedia data is recorded and shared in the current era of the Internet. Among it, video is one of the most common and rich modalities, albeit it is also one of the most expensive to process. Algorithms for fast and accurate video processing thus become crucially important for real-world applications. Video object segmentation, i.e. classifying the set of pixels of a video sequence into the object(s) of interest and background, is among the tasks that despite having numerous and attractive applications, cannot currently be performed in a satisfactory quality level and at an acceptable speed. The main objective of this paper is to fill in this gap: we perform video object segmentation at the accuracy level comparable to the state of the art while keeping the processing time at a speed that even allows for real-time human interaction.</p><p>Towards this goal, we model the problem in a simple and intuitive, yet powerful and unexplored way: we formu- <ref type="bibr">Figure 1</ref>. Interactive segmentation using our method: The white circles represent the clicks where the user has provided an annotation, the colored masks show the resulting segmentation in a subset of the sequence's frames. late video object segmentation as pixel-wise retrieval in a learned embedding space. Ideally, in the embedding space, pixels belonging to the same object instance are close together and pixels from other objects are further apart. We build such embedding space by learning a Fully Convolutional Network (FCN) as the embedding model, using a modified triplet loss tailored for video object segmentation, where no clear correspondence between pixels is given. Once the embedding model is learned, the inference at testtime only needs to compute the embedding vectors with a forward pass for each frame, and then perform a per-pixel nearest neighbor search in the embedding space to find the most similar annotated pixel. The object, defined by the user annotation, can therefore be segmented throughout the video sequence.</p><p>There are several main advantages of our formulation: Firstly, the proposed method is highly efficient as there is no fine-tuning in test time, and it only requires a single forward pass through the embedding network and a nearest-neighbor search to process each frame. Secondly, our method provides the flexibility to support different types of user input (i.e. clicked points, scribbles, segmentation masks, etc.) in an unified framework. Moreover, the embedding process is independent of user input, thus the embedding vectors do not need to be recomputed when the user input changes, which makes our method ideal for the interactive scenario.</p><p>We show an example in <ref type="figure">Figure 1</ref>, where the user aims to segment several objects in the video: The user can iteratively refine the segmentation result by gradually adding more clicks on the video, and get feedback immediately after each click.</p><p>The proposed method is evaluated on the DAVIS 2016 <ref type="bibr" target="#b25">[26]</ref> and DAVIS 2017 <ref type="bibr" target="#b28">[29]</ref> datasets, both in the semisupervised and interactive scenario. In the context of semisupervised Video Object Segmentation (VOS), where the full annotated mask in the first frame is provided as input, we show that our algorithm presents the best trade-off between speed and accuracy, with 275 milliseconds per frame and J &amp;F=77.5% on DAVIS 2016. In contrast, better performing algorithms start at 8 seconds per frame, and similarly fast algorithms reach only 60% accuracy. Where our algorithm shines best is in the field of interactive segmentation, with only 10 clicks on the whole video we can reach an outstanding 74.5% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semi-Supervised and Unsupervised Video Object Segmentation:</p><p>The aim of video object segmentation is to segment a specific object throughout an input video sequence. Driven by the surge of deep learning, many approaches have been developed and performance has improved dramatically. Dependent on the amount of supervision, methods can be roughly categorized into two groups: semi-supervised and unsupervised.</p><p>Semi-supervised video object segmentation methods take the segmentation mask in the first frame as input. MaskTrack <ref type="bibr" target="#b24">[25]</ref> propagates the segmentation from the previous frame to the current one, with optical flow as input. OSVOS <ref type="bibr" target="#b2">[3]</ref> learns the appearance of the first frame by a FCN, and then segments the remaining frames in parallel. Follow-up works extend the idea with various techniques, such as online adaptation <ref type="bibr" target="#b38">[39]</ref>, semantic instance segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. Other recent techniques obtain segmentation and flow simultaneously <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref>, train a trident network to improve upon the errors of optical flow propagation <ref type="bibr" target="#b17">[18]</ref>, or use a CNN in the bilateral space <ref type="bibr" target="#b16">[17]</ref>.</p><p>Unsupervised video object segmentation, on the other hand, uses only video as input. These methods typically aim to segment the most salient object from cues such as motion and appearance. The current leading technique <ref type="bibr" target="#b18">[19]</ref> use region augmentation and reduction to refine object proposals to estimate the primary object in a video. <ref type="bibr" target="#b15">[16]</ref> proposes to combine motion and appearance cues with a two-stream network. Similarly, <ref type="bibr" target="#b36">[37]</ref> learns a two-stream network to encode spatial and temporal features, and a memory module to capture the evolution over time.</p><p>In this work, we focus on improving the efficiency of video object segmentation to make it suitable for real-world applications where rapid inference is needed. We do so by, in contrast to previous techniques using deep learning, not performing test-time network fine-tuning and not relying on optical flow or previous frames as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interactive Video Object Segmentation:</head><p>Interactive Video Object Segmentation relies on iterative user interaction to segment the object of interest. Many techniques have been proposed for the task. Video Cutout <ref type="bibr" target="#b39">[40]</ref> solves a min-cut labeling problem over a hierarchical mean-shift segmentation of the set of video frames, from user-generated foreground and background scribbles. The pre-processing plus post-processing time is in the order of an hour, while the time between interactions is in the order of tens of seconds. A more local strategy is LIVEcut <ref type="bibr" target="#b29">[30]</ref>, where the user iteratively corrects the propagated mask frame to frame and the algorithm learns from it. The interaction response time is reduced significantly (seconds per interaction), but the overall processing time is comparable. TouchCut <ref type="bibr" target="#b40">[41]</ref> simplifies the interaction to a single point in the first frame, and then propagates the results using optical flow. Click carving <ref type="bibr" target="#b14">[15]</ref> uses point clicks on the boundary of the objects to fit object proposals to them. A few strokes <ref type="bibr" target="#b22">[23]</ref> are used to segment videos based on point trajectories, where the interaction time is around tens of seconds per video. A click-and-drag technique <ref type="bibr" target="#b27">[28]</ref> is used to label per-frame regions in a hierarchy and then propagated and corrected.</p><p>In contrast to most previous approaches, our method response time is almost immediate, and the pre-processing time is 275 milliseconds per frame, making it suitable to real-world use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Metric Learning:</head><p>Metric learning is a classical topic and has been widely studied in the learning community <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b3">4]</ref>. Following the recent success of deep learning, deep metric learning has gained increasing popularity <ref type="bibr" target="#b35">[36]</ref>, and has become the cornerstone of many computer vision tasks such as person reidentification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b43">44]</ref>, face recognition <ref type="bibr" target="#b32">[33]</ref>, or unsupervised representation learning <ref type="bibr" target="#b41">[42]</ref>. The key idea of deep metric learning is usually to transform the raw features by a network and then compare the samples in the embedding space directly. Usually metric learning is performed to learn the similarity between images or patches, and methods based on pixel-wise metric learning are limited. Recently, <ref type="bibr" target="#b10">[11]</ref> exploits metric learning at the pixel level for the task of instance segmentation.</p><p>In this work, we learn an embedding where pixels of the same instance are aimed to be close to each other, and we formulate video object segmentation as a pixel-wise retrieval problem. The formulation is inspired also by works in image retrieval <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31]</ref>.  <ref type="figure">Figure 2</ref>. Overview of the proposed approach: Here we assume the user input is provided in the form of full segmentation mask for the reference frame, but interactions of other kind are supported as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>In this work, we formulate video object segmentation as a pixel-wise retrieval problem, that is, for each pixel in the video, we look for the most similar reference pixel in the embedding space and assign the same label to it. The proposed method is sketched in <ref type="figure">Figure 2</ref>. Our method consists of two stages when processing a new video: we first embed each pixel into a d-dimensional embedding space using the proposed embedding network. Then the second step is to perform per-pixel retrieval in this space to transfer labels to each pixel according to its nearest reference pixel.</p><p>A key aspect of our approach, which allows for a fast user interaction, is our way of incorporating the user input. Alternative approaches have been exploited to inject user input into deep learning systems:</p><p>User input to fine-tune the model: The first way is to fine-tune the network to the specific object based on the user input. For example, techniques such as OSVOS <ref type="bibr" target="#b2">[3]</ref> or MaskTrack <ref type="bibr" target="#b24">[25]</ref> fine-tune the network at test time based on the user input. When processing a new video, they require many iterations of training to adapt the model to the specific target object. This approach can be time-consuming (seconds per sequence) and therefore impractical for realtime applications, especially with a human in the loop.</p><p>User input as the network input: Another way of injecting user interaction is to use it as an additional input to the network. In this way, no training is performed at test time. Such methods typically either directly concatenate the user input with the image <ref type="bibr" target="#b44">[45]</ref>, or use a sub-network to encode the user input <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">46]</ref>. A drawback of these methods is that the network has to be recomputed once the user input changes. This can still be a considerable amount of time, especially for video, considering the large number of frames.</p><p>In contrast to previous methods, in this work user input is disentangled from the network computation, thus the forward pass of the network needs to be computed only once. The only computation after user input is then a nearestneighbor search, which is very fast and enables rapid response to the user input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Segmentation as Pixel-wise Retrieval</head><p>For clarity, here we assume a single-object segmentation scenario, and the segmentation mask of first frame is used as user input. The discussion is, however, applicable for multiple objects and for other types of inputs as well.</p><p>The task of semi-supervised video object segmentation is defined as follows: segmenting an object in a video given the object mask of the first frame. Formally, let us denote the i-th pixel in the j-th frame of the input video as x j,i . The user provides the annotation for the first frame: (x 1,i , l 1,i ), where l ∈ {0, 1}, and l 1,i = 0, 1 indicates x 1,i belongs to background and foreground, respectively. We refer to these annotated pixels as reference pixels. The goal is then to infer the labels of all the unlabeled pixels in other frames l j,i with j &gt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Model:</head><p>We build an embedding model f and each pixel x j,i is represented as a d-dimensional embedding vector e j,i = f (x j,i ). Ideally, pixels belonging to the same object are close to each other in the embedding space, and pixels belonging to different objects are distant to each other. In more detail, our embedding model is build on DeepLab-v2 <ref type="bibr" target="#b4">[5]</ref> based on the ResNet101 <ref type="bibr" target="#b13">[14]</ref> backbone architecture. First, we pre-train the network for semantic segmentation on COCO <ref type="bibr" target="#b19">[20]</ref> using the same procedure presented in <ref type="bibr" target="#b4">[5]</ref> and then we remove the final classification layer and replace it with a new convolutional layer with d output channels. We fine-tune the network to learn the embedding for video object segmentation, which will be detailed in Section 3.3. To avoid confusion, we refer to the the original DeepLab-v2 architecture as base feature extractor and to the two convolutional layers as embedding head. The resulting network is fully convolutional, thus the embedding vector of all pixels in a frame can be obtained in a single forward pass. For an image of size h × w pixels the output is a tensor [h/8, w/8, d], where d is the dimension of the embedding space. We use d = 128 unless otherwise specified. The tensor is 8 times smaller due to that the network has a stride length of 8 pixels.</p><p>Since an FCN is deployed as the embedding model, spatial and temporal information are not kept due to the translation invariance nature of the convolution operation. However, such information is obviously important for video and should not be ignored when performing segmentation. We circumvent this problem with a simple approach: we add the spatial coordinates and frame number as additional inputs to the embedding head, thus making it aware of spatial and temporal information. Formally, the embedding function can be represented as e j,i = f (x j,i , i, j), where i and j refer to the ith pixel in frame j. This way, spatial information i and temporal information j can also be encoded in the embedding vector e j,i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval with Online Adaptation:</head><p>During inference, video object segmentation is simply performed by retrieving the closer reference pixels in the embedded space. We deploy a k-Nearest Neighbors (kNN) classifier which finds the set of reference pixels whose feature vector e j i is closer to the feature vector of the pixels to be segmented. In the experiments, we set k = 5 for the semi-supervised case, and k = 1 for the interactive segmentation case. Then, the identity of the pixel is computed by a majority voting of the set of closer reference pixels. Since our embedding model operates with a stride of 8, we upsample our results to the original image resolution by the bilateral solver <ref type="bibr" target="#b0">[1]</ref>.</p><p>A major challenge for semi-supervised video object segmentation is that the appearance changes as the video progresses. The appearance change causes severe difficulty for a fixed model learned in the first frame. As observed in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6]</ref>, such appearance shift usually leads to a decrease in performance for FCNs. To cope with this issue, OnAVOS <ref type="bibr" target="#b38">[39]</ref> proposes to update the model using later frames where their prediction is very confident. In order to update their model online, however, they have to run a few iterations of the fine-tuning algortihm using highly confident samples, which makes their method even slower than the original OSVOS.</p><p>This issue can also be understood as the sample distribution shifts in the embedding space over time. In this work, we can easily update the model online to capture the appearance change, a process that is nearly effortless. In particular we initialize the pool of reference samples with the samples that the user have annotated. As the video progresses, we gradually add samples with high confidence to the pool of reference samples. We add the samples into our reference pool if all of its k = 5 near neighbors agree with the label.</p><p>Generalization to different user input modes and multiple objects: So far we focused on single-object scenarios where user interaction is provided as the full object mask in the first frame. However, multiple object might be present in the video, and the user input might be in an arbitrary form other than the full mask of the first frame. Our method can be straightforwardly applicable to such cases.</p><p>In a general case, the input from user can be represented as a set of pixels and its corresponding label: {x i,j , l i,j } without need for all inputs to be on the first frame (j = 1) or the samples to be exhaustive (covering all pixels of one frame). Please note that the latter is in contrast to the majority of semi-supervised video object segmentation techniques, which assume a full annotated frame to segment the object from the video.</p><p>In our case, the input x i,j can be in the form of clicked points, drawn scribbles, or others possibilities. The label l i,j can also be an integer l j i ∈ {1...K} representing an identifier of an object within a set of K objects, thus generalizing our algorithm to multiple-object video segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>The basic idea of metric learning is to pull similar samples close together and push dissimilar points far apart in the embedding space. A proper training loss and sampling strategy are usually of critical importance to learn a robust embedding. Below we present our training loss and sampling strategy specifically designed for video object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training loss:</head><p>In the metric learning literature, contrastive loss <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>, triplet loss <ref type="bibr" target="#b3">[4]</ref>, and their variants are widely used for metric learning. We argue, however, and verify in our experiments, that the standard losses are not suitable for the task at hand, i.e. video object segmentation, arguably due to the intra-object variation present in a video. In other words, the triplet loss is designed for the situation where the identity of the sample is clear, which is not the case for video object segmentation as an object can be composed of several parts, and each part might have very different appearance. Pulling these samples close to each other, therefore, is an extra constraint that can be harmful for learning a robust metric. We illustrate this effect with an example in <ref type="figure">Figure 3</ref>.</p><p>Keeping this in mind, we modify the standard triplet loss to adapt it to our application. Formally, let us refer to anchor sample as x a . x p ∈ P is a positive sample from a positive  <ref type="figure">Figure 3</ref>. Illustration of pixel-wise feature distribution: Green denotes pixels from motorbike, blue represents person, and black background. The object of interest in this video and the annotation is the human and the motorbike. However, features from motorbike and person lie in two clusters in the feature space. Pulling these two cluster close might be harmful for the metric learning. Visualization is done by t-SNE <ref type="bibr" target="#b20">[21]</ref>. sample pool P. Similarly, x n denotes a negative sample and N denotes the negative pool. The standard triplet loss pushes the negative points further away than the distance between anchor and positive points. Since we do not want to pull every pair of positive points close (different parts of an object that look different), we modify the loss to only push the smallest negative points further than the smallest positive points, the loss can thus be represented as:</p><formula xml:id="formula_0">x a ∈A { min x p ∈P f (x a )−f (x p ) 2 2 − min x n ∈N f (x a )−f (x n ) 2 2 +α}</formula><p>(1) where α is the slack variable to control the margin between positive and negative samples, as in the standard formulation, and we denote the set of anchors as A.</p><p>For each anchor sample x a we have two pools of samples: one pool of positive samples P, whose labels are consistent with the anchor and another pool of negative examples N , whose labels are different from the anchor sample. We take the closest sample to the anchor in each pool, and we compare the positive distance and negative distance. Intuitively, the loss pushes only the closest negative away, while keeping the closest positive closer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Strategy:</head><p>During training, we have fully annotated videos available (object segmentation on each frame). To form a valid triplet to train from, to be used in the aforementioned loss, we need to sample an anchor point x a , a positive sample pool P and a negative sample pool N . For this purpose, three frames are randomly sampled from the training video: from one we sample anchor points and the pixels from the other two frames are joined together. From those, the pixels that have the same label than the anchor form the positive pool P, and the rest form the negative pool N . Note that the pools are sampled from two different frames to have temporal variety, which is needed for the embedding head to learn to weight the temporal information from the feature vector. Also, we do not use pixels from the the anchor frame in the pools to avoid too easy samples.</p><p>In each iteration, a forward pass is performed on three randomly selected frames with one frame as the anchor. Then the anchor frame is used to sample 256 anchor samples, and the positive and negative pools are all foreground and background pixels in the other two frames. We compute the loss according to Equation 1 and the network is trained in an end to end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Validation</head><p>We evaluate the proposed method mainly on DAVIS 2016 <ref type="bibr" target="#b25">[26]</ref>, a dataset containing 50 full high-definition videos annotated with pixel-level accurate object masks (one per sequence) densely on all the frames. We train our model on the 30 training videos and report the results on the validation set, consisting of 20 videos. We perform experiments with multiple objects in DAVIS 2017 <ref type="bibr" target="#b28">[29]</ref>, an extension of the former to 150 sequences and multiple objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semi-supervised VOS on DAVIS</head><p>We first consider the semi-supervised scenario defined in DAVIS 2016, where the methods are given the full segmentation of the object in the first frame and the goal is to segment the rest of the frames.</p><p>We compare against an exhaustive set of very recent techniques: OnAVOS <ref type="bibr" target="#b38">[39]</ref>, OSVOS <ref type="bibr" target="#b2">[3]</ref>, MSK <ref type="bibr" target="#b24">[25]</ref>, SFL <ref type="bibr" target="#b7">[8]</ref>, CTN <ref type="bibr" target="#b17">[18]</ref>, VPN <ref type="bibr" target="#b16">[17]</ref>, OFL <ref type="bibr" target="#b37">[38]</ref>, BVS <ref type="bibr" target="#b23">[24]</ref>, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PML (Ours)</head><p>OnAVOS <ref type="bibr" target="#b38">[39]</ref> OSVOS <ref type="bibr" target="#b2">[3]</ref> MSK <ref type="bibr" target="#b24">[25]</ref> SFL <ref type="bibr" target="#b7">[8]</ref> CTN <ref type="bibr" target="#b17">[18]</ref> VPN <ref type="bibr" target="#b16">[17]</ref> OFL <ref type="bibr" target="#b37">[38]</ref> BVS <ref type="bibr" target="#b23">[24]</ref>  <ref type="figure">Figure 4</ref>. : Per-sequence results of mean region similarity (J ) and contour accuracy (F). The rest of the state-of-the-art techniques are shown using bars, ours is shown using a line. Sequences are sorted by our performance.  <ref type="figure">Figure 5</ref>. Quality versus timing in DAVIS 2016: J &amp;F of all techniques with respect to their time to process one frame. The timing is taken from each paper. OnAVOS and MSK do not report their timings with the post-processing steps that lead to the most accurate results, so we compare to the version with reported times.</p><p>FCP <ref type="bibr" target="#b26">[27]</ref>, JMP <ref type="bibr" target="#b9">[10]</ref>, HVS <ref type="bibr" target="#b11">[12]</ref>, and SEA <ref type="bibr" target="#b31">[32]</ref>; using the pre-computed results available on the DAVIS website and the metrics proposed in DAVIS (J Jaccard index or IoU, F boundary accuracy, T temporal stability). Readers are referred to each paper for more details. <ref type="table" target="#tab_1">Table 1</ref> shows the comparison to the rest of the state of the art, i.e. at the best-performing regime (and slowest) of all techniques. In global terms (J &amp;F), PML (Ours) is comparable to MSK and only behind OSVOS and On-AVOS, which are significantly slower, as we will show in the next experiment. Our technique is especially competitive in terms of boundary accuracy (F), despite there is no refinement or smoothing step explicitly tackling this feature as in other methods.</p><p>To analyze the trade off between quality and performance, <ref type="figure">Figure 5</ref> plots the quality of each technique with respect to their mean time to process one frame (in 480p resolution). Our technique presents a significantly better trade off than the rest of techniques. Compared to the fastest one (BVS), we perform +18 points better while still being 100 milliseconds faster. Compared to the technique with more accurate results (OnAVOS), we lose 5 points but we process each frame 43× faster. <ref type="figure">Figure 4</ref> breaks the performance into each of the 20 sequences of DAVIS validation. We can observe that we are close to the best performance in the majority of the sequences, we obtain the best result in some of them, and our worst performance is 0.5, which shows the robustness of the embedding over various challenges and scenarios. <ref type="figure" target="#fig_1">Figure 6</ref> displays the qualitative results of our technique on a homogeneous set of sequences, from the ones in which we perform the best to those more challenging. Please note that in sequences Bmx-Trees (last row) and Libby (third row), our method is very robust to heavy occlusions, which is logical since we do not perform any type of temporally-neighboring propagation. Results also show that our method is robust to drastic changes in foreground scale and appearance (Motocross-Jump -fourth row) and to background appearance changes (Parkour -second row). Sequences Motocross-Jump, and BMX-Trees (fourth, and last row) show a typical failure mode (which is also observed in other techniques such as OSVOS) in which foreground objects that were not seen in the first frames are classified as foreground when they appear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>In this section we analyze the relative importance of each proposed component, by evaluating ablated versions of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Losses for Metric Learning:</head><p>As discussed in Section 3.3, our embedding model is optimized using a modified version of the triplet loss. To verify the design, we compare our model with two others trained with the original contrastive loss and triplet loss, respectively; while keeping the other settings unchanged. First, we briefly describe the different losses tested: The contrastive loss operates on pairs of samples and can be written as:</p><formula xml:id="formula_1">L contra = N i (y)d 2 + (1 − y) max(α−d, 0) 2</formula><p>where y is the label of the pair (y = 0 indicates that the pairs have different identities and y = 1 otherwise), d = x i − x j is the distance between two points, and α is a slack variable to avoid negative points being overly penalized. The loss minimizes the distance between samples if y = 1, and maximizes it if y = 0.</p><p>The triplet loss shares a similar spirit with contrastive loss, but using three samples as a unit. Each triplet is composed of three samples: one as anchor x a , one positive x p , and one negative x n . The positive (negative) sample has the same (different) label than the anchor. The loss is then defined as:</p><formula xml:id="formula_2">L = N i f (x a ) − f (x p ) 2 2 − f (x a ) − f (x n ) 2 2 + α</formula><p>where again α is a slack variable to control the margin.  <ref type="table">Table 3</ref>. Ablation study on online adaptation and spatio-temporal embedding Spatio-Temporal-Aware Embedding and Online Adaptation:</p><p>We proceed with our ablation analysis by studying the individual impact of two major sub-components: online adaptation and spatial and temporal awareness, as presented in Section 3.2. <ref type="table">Table 3</ref> presents our ablation study on each component: online adaptation provides a slight boost of +1.2% in J . Bringing in spatial and temporal information gives +2.3% improvement in J and +4.5% in F which validates the importance of spatial and temporal information for video object segmentation. Combining both results gives the best performance of 75.5% in overlap, which is overall +3.5% higher at nearly no extra cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Interactive Video Object Segmentation</head><p>Getting dense annotations in the first frame is a laborious and expensive process. It is therefore highly desirable that a system can interact with users in a more realistic way and reach the target quality with as little effort as possible. Our system allows users to interact with the system in real time, and see the result immediately after their input. In this section we consider the scenario of interactive video object segmentation, where the users are allowed to annotate any frame. The process is iterated and the user decides how to annotate based on the result up to the given point.</p><p>For the sake of simplicity, we limit the interaction to clicks: users can click the object of interest or the background. This way, the amount of interaction can easily be quantified as number of clicks. Please note though, that other types of interactions such as scribbles are also naturally supported by our system, although more difficult to evaluate in this experiment.</p><p>We first simulate the user behavior by a robot. The robot randomly selects one pixel from the foreground and one pixel from the background as the first annotations, thus the nearest neighbor search can be performed. After having the initial result, the robot iteratively refines the segmentation result by randomly selecting from the pixels where the predicted label is wrong, and correcting its label based on the ground-truth.</p><p>The left side of <ref type="figure" target="#fig_2">Figure 7</ref> ( ) shows the evolution of the quality of the result as more clicks are provided. We achieve an overlap of J = 80% with only 0.55 clicks per frame, and the performance goes up to J = 83% with 2 clicks per frame. Our method achieves the same result as when providing the full mask on the first frame (J = 75.5%) using only 0.15 clicks per frame. Due to the randomness of our experiment, each experiment is repeated for 5 times and we report the average overlap. We find the variance to be only 0.1 at 1 click per frame, which suggests that our method is reasonably robust to the selection of points.</p><p>To verify that the simulated clicks are realistic, we carry out a user study on real users, where we ask them to click freely until they are happy with the segmentation. The results are shown as points ( ) in <ref type="figure" target="#fig_2">Figure 7</ref>. We can see that the real-user results are slightly better than the simulated ones, which we attribute to the fact that a real user can choose which point to click based on a global view (for instance, select the worst frame) instead of the random sampling that the robot performs.</p><p>On average, the user did 0.17 clicks per frame to achieve an overall result of J = 77.7%. This equals to 11 clicks per video, which takes around 24 seconds. In contrast, a user takes 79 seconds to segment an object at the MS COCO quality <ref type="bibr" target="#b19">[20]</ref>, so the full mask of the first frame at the quality of DAVIS can safely be estimated to take over 3 minutes. The quality achieved in these 24 seconds is comparable with most state-of-the-art semi-supervised methods, but at a fraction of the annotation and running cost. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Extension to Multiple Objects</head><p>As discussed in Section 3.2, our method can naturally extend to the segmentation of multiple objects. To validate the effectiveness of our method in such scenario, we carry out experiments on DAVIS 2017 <ref type="bibr" target="#b28">[29]</ref>, where each video has multiple objects, usually interacting with and occluding each other.</p><p>We summarize our results in the right side of <ref type="figure" target="#fig_2">Figure 7</ref>: our method generalizes well to multiple objects and the results are comparable with most state-of-the-art methods. For instance, OSVOS achieves 57% in J . We match their results by only 0.5 clicks per frame, which leads to a fraction of the processing time of the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This work presents a conceptually simple yet highly effective method for video object segmentation. The problem is casted as a pixel-wise retrieval in an embedding space learned via a modification of the triplet loss specifically designed for video object segmentation. This way, the annotated pixels on the video (via scribbles, segmentation on the first mask, clicks, etc.) are the reference samples, and the rest of pixels are classified via a simple and fast nearestneighbor approach. We obtain results comparable to the state of the art in the semi-supervised scenario, but significantly faster. Since the computed embedding vectors do not depend on the user input, the method is especially well suited for interactive segmentation: the response to the input feedback can be provided almost instantly. In this setup, we reach the same quality than in the semi-supervised case with only 0.15 clicks per frame. The method also naturally generalizes to the multiple objects scenario.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>B</head><label></label><figDesc>la ck sw an C ow s D og Pa rk ou r H or se ju m p-H ig h G oa t C ar -R ou nd ab ou t C ar -S ha do w C am el L ib by So ap bo x M ot oc ro ss -J um p D ri ft -S tr ai gh t D ri ft -C hi ca ne B re ak da nc e D an ce -T w ir l B m x-T re es K ite -S ur f Sc oo te r-B la ck Pa ra gl id in g-L au nc</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results: Homogeneous sample of DAVIS sequences with our result overlaid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Interactive Segmentation Results: Achieved quality with respect to the number of clicks provided in the single-object (left) on DAVIS 2016 and multiple-object (right) scenarios on DAVIS 2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>MeasureOnAVOS OSVOS MSK PML (Ours) SFL CTN VPN OFL BVS FCP JMP HVS SEA Evaluation results on DAVIS 2016 validation set set: We compare the proposed method with an exhaustive set of very recent techniques.</figDesc><table><row><cell cols="2">J &amp;F Mean M ↑</cell><cell>85.5</cell><cell>80.2</cell><cell>77.5</cell><cell>77.4</cell><cell>76.0 71.4 67.8 65.7 59.4 53.8 55.1 53.8 49.2</cell></row><row><cell></cell><cell>Mean M ↑</cell><cell>86.1</cell><cell>79.8</cell><cell>79.7</cell><cell>75.5</cell><cell>76.1 73.5 70.2 68.0 60.0 58.4 57.0 54.6 50.4</cell></row><row><cell>J</cell><cell>Recall O ↑</cell><cell>96.1</cell><cell>93.6</cell><cell>93.1</cell><cell>89.6</cell><cell>90.6 87.4 82.3 75.6 66.9 71.5 62.6 61.4 53.1</cell></row><row><cell></cell><cell>Decay D ↓</cell><cell>5.2</cell><cell>14.9</cell><cell>8.9</cell><cell>8.5</cell><cell>12.1 15.6 12.4 26.4 28.9 -2.0 39.4 23.6 36.4</cell></row><row><cell></cell><cell>Mean M ↑</cell><cell>84.9</cell><cell>80.6</cell><cell>75.4</cell><cell>79.3</cell><cell>76.0 69.3 65.5 63.4 58.8 49.2 53.1 52.9 48.0</cell></row><row><cell>F</cell><cell>Recall O ↑</cell><cell>89.7</cell><cell>92.6</cell><cell>87.1</cell><cell>93.4</cell><cell>85.5 79.6 69.0 70.4 67.9 49.5 54.2 61.0 46.3</cell></row><row><cell></cell><cell>Decay D ↓</cell><cell>5.8</cell><cell>15.0</cell><cell>9.0</cell><cell>7.8</cell><cell>10.4 12.9 14.4 27.2 21.3 -1.1 38.4 22.7 34.5</cell></row><row><cell>T</cell><cell>Mean M ↓</cell><cell>19.0</cell><cell>37.8</cell><cell>21.8</cell><cell>47.0</cell><cell>18.9 22.0 32.4 22.2 34.7 30.6 15.9 36.0 15.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Contrastive Loss Triplet Loss Proposed Loss</cell></row><row><cell>Mean J</cell><cell>66.1</cell><cell>69.5</cell><cell>75.5</cell></row><row><cell>Mean F</cell><cell>68.5</cell><cell>73.5</cell><cell>79.3</cell></row><row><cell>Mean J &amp;F</cell><cell>67.3</cell><cell>71.5</cell><cell>77.4</cell></row></table><note>. Ablation Study on Different Losses</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>compares our embedding model with the models trained with the alternative losses. The results clearly show that the proposed loss achieves better performance than the alternatives.Spat.-Temp. Online Adapt. Mean J Mean F Mean J &amp;F</figDesc><table><row><cell>72.0</cell><cell>73.6</cell><cell>72.8</cell></row><row><cell>73.2</cell><cell>75.0</cell><cell>74.1</cell></row><row><cell>74.3</cell><cell>78.1</cell><cell>76.2</cell></row><row><cell>75.5</cell><cell>79.3</cell><cell>77.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This project is supported by armasuisse.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The fast bilateral solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01926</idno>
		<title level="m">Semantically-guided video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jumpcut: Non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Click Carving: Segmenting Objects in Video with Point Clicks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HCOMP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05384</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06031</idno>
		<title level="m">Video object segmentation without temporal information</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Nicolas</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-automatic video object segmentation by advanced manipulation of segmentation hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Content-Based Multimedia Indexing (CBMI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Livecut: Learningbased interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to rank bag-of-word histograms for large-scale object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Oneshot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03410</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Content-based image retrieval at the end of the early years. T-PAMI</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cohen. Interactive video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">585</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">TouchCut: Fast image and video segmentation using single-touch interaction. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="14" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01218</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
