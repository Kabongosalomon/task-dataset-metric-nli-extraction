<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Technology Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Technology Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories. The first category treats scene text as a type of general objects and follows general object detection paradigm to localize scene text by regressing the text box locations, but troubled by the arbitrary-orientation and large aspect ratios of scene text. The second one segments text regions directly, but mostly needs complex post processing. In this paper, we present a method that combines the ideas of the two types of methods while avoiding their shortcomings. We propose to detect scene text by localizing corner points of text bounding boxes and segmenting text regions in relative positions. In inference stage, candidate boxes are generated by sampling and grouping corner points, which are further scored by segmentation maps and suppressed by NMS. Compared with previous methods, our method can handle long oriented text naturally and doesn't need complex post processing. The experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text demonstrate that the proposed algorithm achieves better or comparable results in both accuracy and efficiency. Based on VGG16, it achieves an F-measure of 84.3% on ICDAR2015 and 81.5% on MSRA-TD500.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, extracting textual information from natural scene images has become increasingly popular, due to the growing demands of real-world applications (e.g., product search <ref type="bibr" target="#b2">[4]</ref>, image retrieval <ref type="bibr" target="#b17">[19]</ref>, and autonomous driving). Scene text detection, which aims at locating text in natural images, plays an important role in various text reading systems <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b23">25]</ref>.</p><p>Scene text detection is challenging due to both external and internal factors. The external factors come from the en- vironment, such as noise, blur and occlusion, which are also major problems disturbing general object detection. The internal factors are caused by properties and variations of scene text. Compared with general object detection, scene text detection is more complicated because: 1) Scene text may exist in natural images with arbitrary orientation, so the bounding boxes can also be rotated rectangles or quadrangles; 2) The aspect ratios of bounding boxes of scene text vary significantly; 3) Since scene text can be in the form of characters, words, or text lines, algorithms might be confused when locating the boundaries.</p><p>In the past few years, scene text detection has been widely studied <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b40">42]</ref> and has achieved obvious progresses recently, with the rapid development of general object detection and semantic segmentation. Based on general object detection and semantic segmentation models, several well-designed modifications are made to detect text more accurately. Those scene text detectors can be split into two branches. The first branch is based on general object detectors (SSD <ref type="bibr" target="#b28">[30]</ref>, YOLO <ref type="bibr" target="#b35">[37]</ref> and DenseBox <ref type="bibr" target="#b16">[18]</ref>), such as TextBoxes <ref type="bibr" target="#b25">[27]</ref>, FCRN <ref type="bibr" target="#b12">[14]</ref> and EAST <ref type="bibr" target="#b51">[53]</ref> etc., which predict candidate bounding boxes directly. The second branch is based on semantic segmentation, such as <ref type="bibr" target="#b50">[52]</ref> and <ref type="bibr" target="#b48">[50]</ref>, which generate segmentation maps and produce the final text bounding boxes by postprocessing.</p><p>Different from previous methods, in this paper we combine the ideas of object detection and semantic segmentation and apply them in an alternative way. Our motivations mainly come from two observations: 1) a rectangle can be <ref type="bibr">Figure 2</ref>. Overview of our method. Given an image, the network outputs corner points and segmentation maps by corner detection and position-sensitive segmentation. Then candidate boxes are generated by sampling and grouping corner points. Finally, those candidate boxes are scored by segmentation maps and suppressed by NMS. determined by corner points, regardless of the size, aspect ratio or orientation of the rectangle; 2) region segmentation maps can provide effective location information of text. Thus, we first detect the corner points (top-left, top-right, bottom-right, bottom-left, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>) of text region rather than text boxes directly. Besides, we predict positionsensitive segmentation maps (shown in <ref type="figure" target="#fig_0">Fig. 1</ref>) instead of a text/non-text map as in <ref type="bibr" target="#b50">[52]</ref> and <ref type="bibr" target="#b48">[50]</ref>. Finally, we generate candidate bounding boxes by sampling and grouping the detected corner points and then eliminate unreasonable boxes by segmentation information. The pipeline of our proposed method is depicted in <ref type="figure">Fig. 2</ref>.</p><p>The key advantages of the proposed method are as follows: 1) Since we detect scene text by sampling and grouping corner points, our approach can naturally handle arbitrary-oriented text; 2) As we detect corner points rather than text bounding boxes, our method can spontaneously avoid the problem of large variation in aspect ratio; 3) With position-sensitive segmentation, it can segment text instances well, no matter the instances are characters, words, or text lines; 4) In our method, the boundaries of candidate boxes are determined by corner points. Compared with regressing text bounding box from anchors ( <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b30">32]</ref>) or from text regions ( <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b14">16]</ref>), the yielded bounding boxes are more accurate, particularly for long text.</p><p>We validate the effectiveness of our method on horizontal, oriented, long and oriented text as well as multi-lingual text from public benchmarks. The results show the advantages of the proposed algorithm in accuracy and speed. Specifically, the F-Measures of our method on ICDAR2015 <ref type="bibr" target="#b20">[22]</ref>, MSRA-TD500 <ref type="bibr" target="#b47">[49]</ref> and MLT <ref type="bibr" target="#b0">[2]</ref> are 84.3%, 81.5% and 72.4% respectively, which outperform previous stateof-the-art methods significantly. Besides, our method is also competitive in efficiency. It can process more than 10.4 images (512x512 in size) per second.</p><p>The contributions of this paper are four-fold: (1) We propose a new scene text detector that combines the ideas of object detection and segmentation, which can be trained and evaluated end-to-end. (2) Based on position-sensitive ROI pooling <ref type="bibr" target="#b7">[9]</ref>, we propose a rotated position-sensitive ROI average pooling layer that can handle arbitrary-oriented proposals. (3) Our method can simultaneously handle the challenges (such as rotation, varying aspect ratios, very close instances) in multi-oriented scene text, which are suffered by previous methods. (4) Our method achieves better or competitive results in both accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Regression Based Text Detection</head><p>Regression based text detection has become the mainstream of scene text detection in the past two years. Based on general object detectors, several text detection methods were proposed and achieved substantial progress. Originating from SSD <ref type="bibr" target="#b28">[30]</ref>, TextBoxes <ref type="bibr" target="#b25">[27]</ref> use "long" default boxes and "long" convolutional filters to cope with the extreme aspect ratios. Similarly, in <ref type="bibr" target="#b30">[32]</ref> Ma et al. utilize the architecture of Faster-RCNN <ref type="bibr" target="#b36">[38]</ref> and add rotated anchors in RPN to detect arbitrary-oriented scene text. SegLink <ref type="bibr" target="#b37">[39]</ref> predicts text segments and the linkage of them in a SSD style network and links the segments to text boxes, in order to handle long oriented text in natural scene. Based on Dense-Box <ref type="bibr" target="#b16">[18]</ref>, EAST <ref type="bibr" target="#b51">[53]</ref> regresses text boxes directly.</p><p>Our method is also adapted from a general object detector DSSD <ref type="bibr" target="#b9">[11]</ref>. But unlike the above methods that regress text boxes or segments directly, we propose to localize the positions of corner points, and then generate text boxes by sampling and grouping the detected corners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Segmentation Based Text Detection</head><p>Segmentation based text detection is another direction of text detection. Inspired by FCN <ref type="bibr" target="#b29">[31]</ref>, some methods are proposed to detect scene text by using segmentation maps. In <ref type="bibr" target="#b50">[52]</ref>, Zhang et al. first attempt to extract text blocks from a segmentation map by a FCN. Then they detect characters in those text blocks with MSER <ref type="bibr" target="#b32">[34]</ref> and group the characters to words or text lines by some priori rules. In <ref type="bibr" target="#b48">[50]</ref>, Yao et al. use a FCN to predict three types of maps (text re-gions, characters, and linking orientations) of the input images. Then some post-processings are conducted to obtain text bounding boxes with the segmentation maps.</p><p>Different from the previous segmentation based text detection methods, which usually need complex postprocessing, our method is simpler and clearer. In inference stage, the position-sensitive segmentation maps are used to score the candidate boxes by our proposed Rotated Position-Sensitive Average ROI Pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Corner Point Based General Object Detection</head><p>Corner point based general object detection is a new stream of general object detection methods. In DeNet <ref type="bibr" target="#b43">[45]</ref>, Tychsen-Smith et al. propose a corner detect layer and a sparse sample layer to replace RPN in a Faster-RCNN style two-stage model. In <ref type="bibr" target="#b46">[48]</ref>, Wang et al. propose PLN (Point Linking Network) which regresses the corner/center points of bounding-box and their links using a fully convolutional network. Then the bounding boxes of objects are formed using the corner/center points and their links.</p><p>Our method is inspired by those corner point based object detection methods, but there are key differences. First, the corner detector of our method is different. Second, we use segmentation map to score candidate boxes. Third, it can produce arbitrary-oriented boxes for objects (text).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Position-Sensitive Segmentation</head><p>Recently, instance-aware semantic segmentation methods are proposed with position-sensitive maps. In <ref type="bibr" target="#b6">[8]</ref>, Dai et al. first introduce relative position to segmentation and propose InstanceFCN for instance segment proposal. In FCIS <ref type="bibr" target="#b24">[26]</ref>, with the assistance of position-sensitive inside/outside score maps, Li et al. propose an end-to-end network for instance-aware semantic segmentation.</p><p>We also adopt position-sensitive segmentation maps to predict text regions. Compared with the above-mentioned methods, there are three key differences: 1) We optimize the network with position-sensitive ground truth directly (detailed in Sec 4.1.1); 2) Our position-sensitive maps can be used to predict text regions and score proposals simultaneously (detailed in Sec 4.2.2), different from FCIS which uses two types of position-sensitive maps (inside and outside); 3) Our proposed Rotated Position-Sensitive ROI Average Pooling can handle arbitrary-oriented proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network</head><p>The network of our method is a fully convolutional network that plays the roles of feature extraction, corner detection and position-sensitive segmentation. The network architecture is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. Given an image, the network produces candidate corner points and segmentation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>The backbone of our model is adapted from a pre-trained VGG16 <ref type="bibr" target="#b39">[41]</ref> network and designed with the following considerations: 1) the size of scene text varies hugely, so the backbone must has enough capacity to handle this problem well; 2) backgrounds in natural scenes are complex, so the features should better contain more context. Inspired by the good performance achieved on those problem by FPN <ref type="bibr" target="#b26">[28]</ref> and DSSD <ref type="bibr" target="#b9">[11]</ref>, we adopt the backbone in FPN/DSSD architecture to extract features.</p><p>In detail, we convert the fc6 and fc7 in the VGG16 to convolutional layers and name them conv6 and conv7 respectively. Then several extra convolutional layers (conv8, conv9, conv10, conv11) are stacked above conv7 to enlarge the receptive fields of extracted features. After that, a few deconvolution modules proposed in DSSD <ref type="bibr" target="#b9">[11]</ref> are used in a top-down pathway ( <ref type="figure" target="#fig_3">Fig. 3</ref>). Particularly, to detect text with different sizes well, we cascade deconvolution modules with 256 channels from conv11 to conv3 (the features from conv10, conv9, conv8, conv7, conv4, conv3 are reused), and 6 deconvolution modules are built in total. Including the features of conv11, we name those output features F 3 , F 4 , F 7 , F 8 , F 9 , F 10 and F 11 for convenience. In the end, the feature extracted by conv11 and deconvolution modules which have richer feature representations are used to detect corner points and predict position-sensitive maps. Following SSD and DSSD, we detect corner points with default boxes. Different from the manner in SSD or DSSD where each default box outputs the classification scores and offsets of the corresponding candidate box, corner point detection is more complex because there might be more than one corner points in the same location (such as a location can be the bottom-left corner and top-right corner of two boxes simultaneously). So in our case, a default box should output classification scores and offsets for 4 candidate boxes corresponding to the 4 types of corner points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Corner Detection</head><p>We adapt the prediction module proposed in <ref type="bibr" target="#b9">[11]</ref> to predict scores and offsets in two branches in a convolutional manner. In order to reduce the computational complexity, the filters of all convolutions are set to 256. For an m × n   feature map with k default boxes in each cell, the "score" branch and "offset" branch output 2 scores and 4 offsets respectively for each type of corner point of each default box. Here, 2 for "score" branch means whether a corner point exists in this position. In total, the output channels of the "score" branch and the "offset" branch are k × q × 2 and k × q × 4, where q means the type of corner points. By default, q is equal to 4.</p><formula xml:id="formula_0">) 2 ( ' '     q k h w ) 4 ( ' '     q k h w x 16 x 8 x 4 x 2 F9 F8 F7 F4 F3 x 2 x 2 g g    h w</formula><p>In the training stage, we follow the matching strategy of default boxes and ground truth ones in SSD. To detect scene text with different sizes, we use default boxes of multiple sizes on multiple layer features. The scales of all default boxes are listed in <ref type="table">Table 1</ref>. The aspect ratios of default boxes are set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Position-Sensitive Segmentation</head><p>In the previous segmentation based text detection methods <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b48">50]</ref>, a segmentation map is generated to represent the probability of each pixel belonging to text regions. However those text regions in score map always can not be separated from each other, as a result of the overlapping of text regions and inaccurate predictions of text pixels. To get the text bounding boxes from the segmentation map, complex post-processing are conducted in <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b48">50]</ref>.</p><p>Inspired by InstanceFCN <ref type="bibr" target="#b6">[8]</ref>, we use position-sensitive segmentation to generate text segmentation maps. Compared with previous text segmentation methods, relative positions are generated. In detail, for a text bounding box R, a g × g regular grid is used to divide the text bounding box into multiple bins (i.e., for a 2 × 2 grid, a text region can be split into 4 bins, that is top-left, top-right, bottom-right, bottom-left). For each bin, a segmentation map is used to determine whether the pixels in this map belong to this bin.</p><p>We build position-sensitive segmentation with corner point detection in a unified network. We reuse the features of F 3 , F 4 , F 7 , F 8 , F 9 and build some convolutional blocks on them follow the residual block architecture of corner point detection branch (Shown in <ref type="figure" target="#fig_3">Fig. 3</ref>). All outputs of those blocks are resized to the scale of F 3 by bilinear upsampling with the scale factors set to 1, 2, 4, 8, 16. Then all those outputs with the same scale are added together to generate richer features. We further enlarge the resolution of fused features by two continuous Conv1x1-BN-ReLU-Deconv2x2 blocks and set the kernels of the last deconvolution layer to g × g. So, the final position-sensitive segmentation maps have g × g channels and the same size as the input images. In this work, we set g to 2 in default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Label Generation</head><p>For an input training sample, we first convert each text box in ground truth into a rectangle that covers the text box region with minimal area and then determine the relative position of 4 corner points.</p><p>We determine the relative position of a rotated rectangle by the following rules    We generate the label of corner point detection and position-sensitive segmentation using R. For corner point detection, we first compute the short side of R and represent the 4 corner points by horizontal squares as shown in <ref type="figure" target="#fig_7">Fig. 5 (a)</ref>. For position-sensitive segmentation, we generate pixel-wise masks of text/non-text with R. We first initialize 4 masks with the same scale as the input image and set all pixel value to 0. Then we divide R into four bins with a 2 × 2 regular grid and assign each bin to a mask, such as top-left bin to the first mask. After that, we set the value of all pixels in those bins to 1, as shown in <ref type="figure" target="#fig_7">Fig. 5 (b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Optimization</head><p>We train the corner detection and position-sensitive segmentation simultaneously. The loss function is defined as:</p><formula xml:id="formula_1">L = 1 N c L conf + λ 1 N c L loc + λ 2 N s L seg<label>(1)</label></formula><p>Where L conf and L loc are the loss functions of the score branch for predicting confidence score and the offset branch for localization in the module of corner point detection. L seg is the loss function of position-sensitive segmentation. N c is the number of positive default boxes, N s is the number of pixels in segmentation maps. N c and N s are used to normalize the losses of corner point detection and segmentation. λ 1 and λ 2 are the balancing factors of the three tasks.</p><p>In default, we set the λ 1 to 1 and λ 2 to 10. We follow the matching strategy of SSD and train the score branch using Cross Entropy loss:</p><formula xml:id="formula_2">L conf = CrossEntropy(y c , p c )<label>(2)</label></formula><p>Where y c is the ground truth of all default boxes, 1 for positive and 0 otherwise. p c is the predicted scores. In consideration of the extreme imbalance between positive and negative samples, the category homogenization is necessary. We use the online hard negative mining proposed in <ref type="bibr" target="#b38">[40]</ref> to balance training samples and set the ratio of positives to negatives to 1 : 3.</p><p>For the offset branch, we regress the offsets relative to default boxes as Fast RCNN <ref type="bibr" target="#b10">[12]</ref> and optimize them with Smooth L1 loss:</p><formula xml:id="formula_3">L loc = SmoothL1(y l , p l )<label>(3)</label></formula><p>Where y l = ( x, y, s s , s s ) is the ground truth of offset branch and p l = ( x, ỹ, s s , s s ) is the predicted offsets. The y l can be calculated by a default box B = (x b , y b , ss b , ss b ) and a corner point box C = (x c , y c , ss c , ss c ):</p><formula xml:id="formula_4">x = x b − x c ss b (4) y = y b − y c ss b (5) ss = log( ss b ss c )<label>(6)</label></formula><p>We train position-sensitive segmentation by minimizing the Dice loss <ref type="bibr" target="#b31">[33]</ref>:</p><formula xml:id="formula_5">L seg = 1 − 2y s p s y s + p s<label>(7)</label></formula><p>Where y s is the label of position-sensitive segmentation and p s is the prediction of our segmentation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sampling and Grouping</head><p>In inference stage, many corner points are yielded with the predicted location, short side and confidence score. Points with high score (great than 0.5 in default) are kept. After NMS, 4 corner point sets are composed based on relative position information. We generate the candidate bounding boxes by sampling and grouping the predicted corner points. In theory, a rotated rectangle can be constructed by two points and a side perpendicular to the line segment made up by the two points. For a predicted point, the short side is known, so 2) the shortest side of the constructed rotated rectangle must be greater than a threshold (the default is 5); 3) the predicted short sides ss 1 and ss 2 of the two points in a pair must satisfy: max(ss 1 , ss 2 ) min(ss 1 , ss 2 ) ≤ 1.5 (8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Scoring</head><p>A large number of candidate bounding boxes can be generated after sampling and grouping corner points. Inspired by InstanceFCN <ref type="bibr" target="#b6">[8]</ref> and RFCN <ref type="bibr" target="#b7">[9]</ref>, we score the candidate bounding boxes by the position-sensitive segmentation maps. The processes are shown in <ref type="figure" target="#fig_7">Fig. 5</ref>.</p><p>To handle the rotated text bounding boxes, we adapt the Position-Sensitive ROI pooling layer in <ref type="bibr" target="#b7">[9]</ref> and propose Rotated Position-Sensitive ROI Average pooling layer. Specifically, for a rotated box, we first split the box into g × g bins. Then we generate a rectangle for each bin with the minimum area to cover the bin. We loop over all pixels in the minimum rectangle and calculate mean value of all pixels which in the bin. In the end, the score of a rotated bounding box is obtained by averaging the means of g × g bins. The specific processes are shown in Algorithm 1.</p><p>The candidate boxes with low score will be filtered out. We set the threshold τ to 0.6 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Rotated Position-Sensitive ROI Average Pooling</head><p>Input: rotated bounding box B, g × g regular grid G, Segmentation maps S 1: Generating Bins by spitting B with G. 2: M ← 0, i ← 0 3: for i in range(g × g) do <ref type="bibr">4:</ref> bin ← Bins[i], C ← 0, P ← 0,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>R ← M iniRect(bin) <ref type="bibr">6:</ref> for pixel in R do <ref type="bibr">7:</ref> if pixel in bin then </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To validate the effectiveness of the proposed method, we conduct experiments on five public datasets: ICDAR2015, ICDAR2013, MSRA-TD500, MLT, COCO-Text, and compare with other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>SynthText <ref type="bibr" target="#b12">[14]</ref> is a synthetically generated dataset which consists of about 800000 synthetic images. We use the dataset with word level labels to pre-train our model.</p><p>ICDAR2015 is a dataset proposed in the Challenge 4 of the 2015 Robust Reading Competition <ref type="bibr" target="#b20">[22]</ref> for incidental scene text detection. There are 1000 images for training and 500 images for testing with annotations labeled as word level quadrangles.</p><p>ICDAR2013 is a dataset proposed in the Challenge 2 of the 2013 Robust Reading Competition <ref type="bibr" target="#b21">[23]</ref> focuses on horizontal text in scene. It contains 229 images for training and 233 images for testing.</p><p>MSRA-TD500 <ref type="bibr" target="#b47">[49]</ref> is a dataset collected for detecting arbitrary-oriented long text lines. It consists of 300 training images and 200 test images with text line level annotations.</p><p>MLT is a dataset that proposed on ICDAR2017 Competition <ref type="bibr" target="#b0">[2]</ref> and focuses on the multi-oriented, multi-script and multi-lingual aspects of scene text. It consists of 7200 training images, 2000 validation images and 9000 test images.</p><p>COCO-Text <ref type="bibr" target="#b44">[46]</ref> is a large scale scene text dataset which comes from the MS COCO dataset <ref type="bibr" target="#b27">[29]</ref>. There are 63686 images are annotated and two versions of annotations and splits (V1.1 and V1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Training Our model is pre-trained on SynthText then finetuned on other datasets (except COCO-Text). We use Adam <ref type="bibr" target="#b22">[24]</ref> to optimize our model with the learning rate fixed to 1e − 4. In pre-train stage, we train our model on SynthText for one epoch. During finetuning stage, the number of iterations are decided by the sizes of datasets.</p><p>Data Augmentation We use the same way of data augmentation as SSD. We randomly sample a patch from the input image in the manner of SSD, then resize the sampled patch to 512 × 512.</p><p>Post Processing NMS is the only post processing step of our method. We set the threshold of NMS to 0.3.</p><p>Our method is implemented in PyTorch <ref type="bibr" target="#b1">[3]</ref>. All the experiments are conducted on a regular workstation (CPU: Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz; GPU:Titan Pascal; RAM: 64GB). We train our model with the batch size of 24 on 4 GPUs in parallel and evaluate our model on 1 GPU with batch size set as 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Detecting Oriented Text</head><p>We evaluate our model on the ICDAR2015 dataset to test its ability of arbitrarily oriented text detection. We finetune our model another 500 epochs on the datasets of IC-DAR2015 and ICDAR2013. Note that, to detect vertical text better, in the last 15 epochs, we randomly rotate the sampled patches by 90 degree or −90 degree with the probability of 0.2. In testing, we set τ to 0.7 and resize the input images to 768 × 1280. Following <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b14">16]</ref>, we also evaluate our model on ICDAR2015 with multi-scale inputs, {512×512, 768×768, 768×1280, 1280×1280} in default.</p><p>We compare our method with other state-of-the-art methods and list all the results in <ref type="table">Table 2</ref>. Our method outperforms the previous methods by a large margin. When tested at single scale, our method achieves the F-measure of 80.7%, which surpasses all competitors <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b13">15]</ref> . Our method achieves 84.3% in F-measure with multi-scale inputs, higher than the current best one <ref type="bibr" target="#b14">[16]</ref> by 3.3%.</p><p>To explore the gain between our method which detects corner points and the method which regresses text boxes directly, we train a network named "baseline" in <ref type="table">Table.</ref> 2 using the same settings as our method. The baseline model consists of the same backbone as our method and the prediction module in SSD/DSSD. With slight time cost, our method boost the accuracy greatly (53.3% vs 80.7%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Detecting Horizontal Text</head><p>We evaluate the ability of our model to detect horizontal text on ICDAR2013 dataset. We further train our model on ICDAR2013 dataset for 60 epochs on the basis of the finetuned ICDAR2015 model. In testing, the input images are resized to 512 × 512. We also use multi-scale inputs to evaluate our model. The results are listed in <ref type="table">Table 3</ref> and mostly are reported with the "Deteval" evaluation protocol. Our method achieves very competitive results. When tested at single scale, our method achieves the F-measure of 85.8%, which is slightly lower than the highest result. Besides, our method can run at 10.4 FPS, faster than most methods. For multi-scale evaluation, our method achieves the F-measure of 88.0%, which is also competitive compared with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Detecting Long Oriented Text Line</head><p>On MSRA-TD500, we evaluate the performance of our method for detecting long and multi-lingual text lines. HUST-TR400 is also used as training data as the MSRA-TD500 only contains 300 training images. The model is initialized with the model pre-trained on SynthText and then </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Precision Recall F-measure FPS finetuned another 240 epochs. In test stage, we input the images with the size 768 × 768 and set τ to 0.65.</p><p>As shown in <ref type="table" target="#tab_3">Table 4</ref>, our method surpasses all the previous methods by a large margin. Our method achieves state-of-the-art performances both in recall, precision and F-measure (87.6%, 76.2% and 81.5%), and much better than the previous best result (81.5% vs. 77.0%). That means our method is more capable than other methods of detecting arbitrarily oriented long text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Detecting Multi-Lingual Text</head><p>We verify the ability of our method to detect multilingual text on MLT. We finetune about 120 epochs on the model pre-trained on SynthText. When testing in single scale, the sizes of images are set as 768 × 768. We evaluate our method online and compare with some public results on the leaderboard <ref type="bibr" target="#b0">[2]</ref>. As shown in <ref type="table">Table 5</ref>, our method outperforms all competing methods by at least 3.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Generalization Ability</head><p>To evaluate the generalization ability of our model, we test it on COCO-Text using the model finetuned on IC-DAR2015. We set the test image size as 768 × 768. We use the annotations (V1.1) to compare with other methods, for the sake of fairness. The results are shown in <ref type="table">Table 6</ref>. Without training, on COCO-Text, our method achieves an F-measure of 42.5%, better than competitors.</p><p>Besides, we also evaluate our model on the ICDAR2017 Robust Reading Challenge on COCO-Text [1] with the annotations V1.4. The results are reported in <ref type="table">Table 6</ref>. Among all the public results in leaderboard [1], our method ranks the first. Especially when the threshold of iou is set to 0.75, the result that our method exceeds others in a large margin shows it can detect text more accurately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Limitations</head><p>One limitation of the proposed method is that when two text instances are extremely close, it may predict the two instances as one ( <ref type="figure" target="#fig_11">Fig. 7)</ref>, since the position-sensitive segmentation might fail. Besides, the method is not good at detecting curved text ( <ref type="figure" target="#fig_11">Fig. 7)</ref>, as there are few curved samples in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have presented a scene text detector that localize text by corner point detection and positionsensitive segmentation. We evaluated it on several public benchmarks focusing on oriented, horizontal, long oriented and multi-lingual text. The superior performances demonstrate the effectiveness and robustness of our method. In the future, we are interested in constructing an end-to-end OCR system based on the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The images in top row and bottom row are the predicted corner points and position-sensitive maps in top-left, top-right, bottom-right, bottom-left order, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For a given</head><label></label><figDesc>rotated rectangular bounding box R = (x, y, w, h, θ), there are 4 corner points (top-left, top-right, bottom-right, bottom-left) and can be represented as two-dimensional coordinates {(x 1 , y 1 ), (x 2 , y 2 ), (x 3 , y 3 ), (x 4 , y 4 )} in a clockwise direction. To expediently detect corner points, here we redefine and represent a corner point by a horizontal square C = (x c , y c , ss, ss), where x c , y c are the coordinate of a corner point (such as x 1 , y 1 for top-left point) as well as the center of the horizontal square. ss is the length short side of the rotated rectangular bounding box R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Network Architecture. The network contains three parts: backbone, conner point detector and position-sensitive segmentation predictor. The backbone is adapted from DSSD<ref type="bibr" target="#b9">[11]</ref>. Conner point detectors are built on multiple feature layers (blocks in pink). positionsensitive segmentation predictor shares some features (pink blocks) with corner point detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>: 1) the x-coordinates of topleft and bottom-left corner points must less than the xcoordinates of top-right and bottom-right corner points; 2) the y-coordinates of top-left and top-right corner points must less than the y-coordinates of bottom-left and bottomright corner points. After that, the original ground truth can be represented as a rotated rectangle with relative position layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Label generation for corner points detection and position-sensitive segmentation. (a) Corner points are redefined and represented by squares (boxes in white, red, green, blue) with the side length set as the short side of text bounding box R (yellow box). (b) Corresponding ground truth of R in (a) for positionsensitive segmentation. of corner points. For convenience, we term the rotated rectangle R = {P i |i ∈ {1, 2, 3, 4}}, where P i = (x i , y i ) are the corner points of the rotated rectangle in top-left, topright, bottom-right, bottom-left order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Overview of the scoring process. The yellow boxes in (a) are candidate boxes. (b) are predicted segmentation maps. We generate instance segment (c) of candidate boxes by assembling the segmentation maps as [8]. Scores are calculated by averaging the instance segment regions. we can form a rotated rectangle by sampling and grouping two corner points in corner point sets arbitrarily, such as (top-left, top-right), (top-right, bottom-right), (bottom-left, bottom-right) and (top-left, bottom-left) pairs. Several priori rules are used to filter unsuitable pairs: 1) the relative positional relations can not be violated, such as the x-coordinate of top-left point must less than that of topright point in (top-left, top-right) pair;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>8 :C</head><label>8</label><figDesc>← C + 1, P ← P + G[i][pixel].value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>4) are released by the official. Previous methods are all evaluated on V1.1 and the new V1.4 are used on ICDAR2017 Competition [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Examples of detection results. From left to right in columns: ICDAR2015, ICDAR2013, MSRA-TD500, MLT, COCO-Text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Failure cases of our method. The boxes in green are ground truth. The red boxes are our predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, 6, 10, 12, 16 20, 24, 28, 32 36, 40, 44, 48 56, 64, 72, 80 88, 96, 104, 112 124, 136, 148, 160 184, 208, 232, 256 Table 1. Scales of default boxes on different layers.</figDesc><table><row><cell>F3</cell><cell>F4</cell><cell>F7</cell><cell>F8</cell><cell>F9</cell><cell>F10</cell><cell>F11</cell></row><row><cell>scales 4, 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Results on ICDAR2015. * means multi-scale, † stands for the base net of the model is not VGG16. Results on ICDAR2013. * means multi-scale, † stands for the base net of the model is not VGG16. Note that, the methods of the top three lines are evaluated under the "ICDAR2013" evaluation protocol.</figDesc><table><row><cell>Method</cell><cell cols="4">Precision Recall F-measure FPS</cell></row><row><cell>Zhang et al. [52]</cell><cell>70.8</cell><cell>43.0</cell><cell>53.6</cell><cell>0.48</cell></row><row><cell>CTPN [44]</cell><cell>74.2</cell><cell>51.6</cell><cell>60.9</cell><cell>7.1</cell></row><row><cell>Yao et al. [50]</cell><cell>72.3</cell><cell>58.7</cell><cell>64.8</cell><cell>1.61</cell></row><row><cell>SegLink [39]</cell><cell>73.1</cell><cell>76.8</cell><cell>75.0</cell><cell>-</cell></row><row><cell>EAST [53]</cell><cell>80.5</cell><cell>72.8</cell><cell>76.4</cell><cell>6.52</cell></row><row><cell>SSTD [15]</cell><cell>80.0</cell><cell>73.0</cell><cell>77.0</cell><cell>7.7</cell></row><row><cell>baseline</cell><cell>66.0</cell><cell>44.7</cell><cell>53.3</cell><cell>4.5</cell></row><row><cell>ours</cell><cell>94.1</cell><cell>70.7</cell><cell>80.7</cell><cell>3.6</cell></row><row><cell>EAST  *   † [53]</cell><cell>83.3</cell><cell>78.3</cell><cell>80.7</cell><cell>-</cell></row><row><cell>WordSup  *  [17]</cell><cell>79.3</cell><cell>77.0</cell><cell>78.2</cell><cell>2</cell></row><row><cell>He et al.  *   † [16]</cell><cell>82.0</cell><cell>80.0</cell><cell>81.0</cell><cell>1.1</cell></row><row><cell>ours  *</cell><cell>89.5</cell><cell>79.7</cell><cell>84.3</cell><cell>1</cell></row><row><cell>Method</cell><cell cols="3">Precision Recall F-measure</cell><cell>FPS</cell></row><row><cell>Neumann et al. [35]</cell><cell>81.8</cell><cell>72.4</cell><cell>77.1</cell><cell>3</cell></row><row><cell>Neumann et al. [36]</cell><cell>82.1</cell><cell>71.3</cell><cell>76.3</cell><cell>3</cell></row><row><cell>Fastext [6]</cell><cell>84.0</cell><cell>69.3</cell><cell>76.8</cell><cell>6</cell></row><row><cell>Zhang et al. [51]</cell><cell>88.0</cell><cell>74.0</cell><cell>80.0</cell><cell>0.02</cell></row><row><cell>Zhang et al. [52]</cell><cell>88.0</cell><cell>78.0</cell><cell>83.0</cell><cell>0.5</cell></row><row><cell>Yao et al. [50]</cell><cell>88.9</cell><cell>80.2</cell><cell>84.3</cell><cell>1.61</cell></row><row><cell>CTPN [44]</cell><cell>93.0</cell><cell>83.0</cell><cell>88.0</cell><cell>7.1</cell></row><row><cell>TextBoxes [27]</cell><cell>88.0</cell><cell>74.0</cell><cell>81.0</cell><cell>11</cell></row><row><cell>SegLink [39]</cell><cell>87.7</cell><cell>83.0</cell><cell>85.3</cell><cell>20.6</cell></row><row><cell>SSTD [15]</cell><cell>89.0</cell><cell>86.0</cell><cell>88.0</cell><cell>7.7</cell></row><row><cell>ours</cell><cell>93.3</cell><cell>79.4</cell><cell>85.8</cell><cell>10.4</cell></row><row><cell>FCRN  *  [14]</cell><cell>92.0</cell><cell>75.5</cell><cell>83.0</cell><cell>0.8</cell></row><row><cell>TextBoxes  *  [27]</cell><cell>89.0</cell><cell>83.0</cell><cell>86.0</cell><cell>1.3</cell></row><row><cell>He et al.  *   † [16]</cell><cell>92.0</cell><cell>81.0</cell><cell>86.0</cell><cell>1.1</cell></row><row><cell>WordSup  *  [17]</cell><cell>93.3</cell><cell>87.5</cell><cell>90.3</cell><cell>2</cell></row><row><cell>ours  *</cell><cell>92.0</cell><cell>84.4</cell><cell>88.0</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results on MSRA-TD500. † stands for the base net of the model is not VGG16.</figDesc><table><row><cell>TD-ICDAR [49]</cell><cell>53.0</cell><cell>52.0</cell><cell>50.0</cell><cell>-</cell></row><row><cell>TD-Mixture [49]</cell><cell>63.0</cell><cell>63.0</cell><cell>60.0</cell><cell>-</cell></row><row><cell>Kang et al. [21]</cell><cell>71.0</cell><cell>62.0</cell><cell>66.0</cell><cell>-</cell></row><row><cell>Zhang et al. [52]</cell><cell>83.0</cell><cell>67.0</cell><cell>74.0</cell><cell>0.48</cell></row><row><cell>Yao et al. [50]</cell><cell>76.5</cell><cell>75.3</cell><cell>75.9</cell><cell>1.61</cell></row><row><cell>EAST [53]</cell><cell>81.7</cell><cell>61.6</cell><cell>70.2</cell><cell>6.52</cell></row><row><cell>EAST  † [53]</cell><cell>87.3</cell><cell>67.4</cell><cell>76.1</cell><cell>13.2</cell></row><row><cell>SegLink [39]</cell><cell>86.0</cell><cell>70.0</cell><cell>77.0</cell><cell>8.9</cell></row><row><cell>He et al.  † [16]</cell><cell>77.0</cell><cell>70.0</cell><cell>74.0</cell><cell>1.1</cell></row><row><cell>ours</cell><cell>87.6</cell><cell>76.2</cell><cell>81.5</cell><cell>5.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Results on MLT. * means multi-scale. Results on COCO-Text.</figDesc><table><row><cell>Method</cell><cell cols="3">Precision Recall F-measure</cell></row><row><cell>TH-DL [2]</cell><cell>67.8</cell><cell>34.8</cell><cell>46.0</cell></row><row><cell>SARI FDU RRPN V1 [2]</cell><cell>71.2</cell><cell>55.5</cell><cell>62.4</cell></row><row><cell>Sensetime OCR [2]</cell><cell>56.9</cell><cell>69.4</cell><cell>62.6</cell></row><row><cell>SCUT DLVClab1 [2]</cell><cell>80.3</cell><cell>54.5</cell><cell>65.0</cell></row><row><cell>e2e ctc01 multi scale [2]</cell><cell>79.8</cell><cell>61.2</cell><cell>69.3</cell></row><row><cell>ours</cell><cell>83.8</cell><cell>55.6</cell><cell>66.8</cell></row><row><cell>ours  *</cell><cell>74.3</cell><cell>70.6</cell><cell>72.4</cell></row><row><cell>Method</cell><cell cols="3">Precision Recall F-measure</cell></row><row><cell>Baseline A [46]</cell><cell>83.8</cell><cell>23.3</cell><cell>36.5</cell></row><row><cell>Baseline B [46]</cell><cell>89.7</cell><cell>10.7</cell><cell>19.1</cell></row><row><cell>Baseline C [46]</cell><cell>18.6</cell><cell>4.7</cell><cell>7.5</cell></row><row><cell>Yao et al. [50]</cell><cell>43.2</cell><cell>27.1</cell><cell>33.3</cell></row><row><cell>EAST [53]</cell><cell>50.4</cell><cell>32.4</cell><cell>39.5</cell></row><row><cell>WordSup [17]</cell><cell>45.2</cell><cell>30.9</cell><cell>36.8</cell></row><row><cell>SSTD [15]</cell><cell>46.0</cell><cell>31.0</cell><cell>37.0</cell></row><row><cell>ours</cell><cell>69.9</cell><cell>26.2</cell><cell>38.1</cell></row><row><cell>ours  *</cell><cell>61.9</cell><cell>32.4</cell><cell>42.5</cell></row><row><cell cols="3">COCO-Text Challenge (IOU 0.5)</cell><cell></cell></row><row><cell>UM [1]</cell><cell>47.6</cell><cell>65.5</cell><cell>55.1</cell></row><row><cell>TDN SJTU v2 [1]</cell><cell>62.4</cell><cell>54.3</cell><cell>58.1</cell></row><row><cell>Text Detection DL [1]</cell><cell>60.1</cell><cell>61.8</cell><cell>61.4</cell></row><row><cell>ours</cell><cell>72.5</cell><cell>52.9</cell><cell>61.1</cell></row><row><cell>ours  *</cell><cell>62.9</cell><cell>62.2</cell><cell>62.6</cell></row><row><cell cols="3">COCO-Text Challenge (IOU 0.75)</cell><cell></cell></row><row><cell>Text Detection DL [1]</cell><cell>25.2</cell><cell>25.5</cell><cell>25.4</cell></row><row><cell>UM [1]</cell><cell>22.7</cell><cell>31.2</cell><cell>26.3</cell></row><row><cell>TDN SJTU v2 [1]</cell><cell>31.8</cell><cell>27.7</cell><cell>29.6</cell></row><row><cell>ours</cell><cell>40.0</cell><cell>30.0</cell><cell>34.6</cell></row><row><cell>ours  *</cell><cell>35.1</cell><cell>34.8</cell><cell>34.9</cell></row></table><note>* means multi-scale.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mlt-Challenge</surname></persName>
		</author>
		<ptr target="http://rrc.cvc.uab.es/?ch=8&amp;com=evaluation&amp;task=1&amp;gtv=1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://pytorch.org/" />
		<title level="m">Pytorch</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Integrating scene text and visual appearance for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04613</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fastext: Efficient unconstrained scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1206" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep textspotter: An end-to-end trainable scene text localization and recognition framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Textproposals: a text-specific selective search algorithm for word spotting in the wild. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="60" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Orientation robust text line detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4034" to="4041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (IC-DAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>13th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
	<note>12th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards end-to-end text spotting with convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01086</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="770" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient scene text localization and recognition with local character refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="746" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-time lexicon-free scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1872" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Wetext: Scene text detection under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04826</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Denet: Scalable realtime object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<title level="m">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Point linking network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03646</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09002</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Symmetry-based text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2558" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
