<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 INCORPORATING BERT INTO NEURAL MACHINE TRANSLATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
							<email>3wulijun3@mail2.sysu.edu.cn4dihe@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tyliu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 INCORPORATING BERT INTO NEURAL MACHINE TRANSLATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently proposed BERT (Devlin et al., 2019)  has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. We propose a new algorithm named BERT-fused model, in which we first use BERT to extract representations for an input sequence, and then the representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms. We conduct experiments on supervised (including sentence-level and document-level translations), semi-supervised and unsupervised machine translation, and achieve state-of-the-art results on seven benchmark datasets. Our code is available at https://github.com/bert-nmt/bert-nmt. * This work is conducted at Microsoft Research Asia. The first two authors contributed equally to this work.</p><p>Published as a conference paper at ICLR 2020 Given that there is limited work leveraging BERT for NMT, our first attempt is to try two previous strategies: (1) using BERT to initialize downstream models and then fine-tuning the models, and (2) using BERT as context-aware embeddings for downstream models. For the first strategy, following Devlin et al. <ref type="formula">(2019)</ref>, we initialize the encoder of an NMT model with a pre-trained BERT model, and then finetune the NMT model on the downstream datasets. Unfortunately, we did not observe significant improvement. Using a pre-trained XLM (Lample &amp; Conneau, 2019) model, a variant of BERT for machine translation, to warm up an NMT model is another choice. XLM has been verified to be helpful for WMT'16 Romanian-to-English translation. But when applied to a language domain beyond the corpus for training XLM (such as IWSLT dataset <ref type="bibr" target="#b2">(Cettolo et al., 2014)</ref>, which is about spoken languages) or when large bilingual data is available for downstream tasks, no significant improvement is observed neither. For the second strategy, following the practice of (Peters et al., 2018), we use BERT to provide context-aware embeddings for the NMT model. We find that this strategy outperforms the first one (please refer to Section 3 for more details). This motivates us to go along this direction and design more effective algorithms.</p><p>We propose a new algorithm, BERT-fused model, in which we exploit the representation from BERT by feeding it into all layers rather than served as input embeddings only. We use the attention mechanism to adaptively control how each layer interacts with the representations, and deal with the case that BERT module and NMT module might use different word segmentation rules, resulting in different sequence (i.e., representation) lengths. Compared to standard NMT, in addition to BERT, there are two extra attention modules, the BERT-encoder attention and BERT-decoder attention. An input sequence is first transformed into representations processed by BERT. Then, by the BERTencoder attention module, each NMT encoder layer interacts with the representations obtained from BERT and eventually outputs fused representations leveraging both BERT and the NMT encoder. The decoder works similarly and fuses BERT representations and NMT encoder representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, pre-training techniques, like <ref type="bibr">ELMo (Peters et al., 2018)</ref>, GPT/GPT-2 <ref type="bibr" target="#b24">(Radford et al., 2018;</ref>, BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, cross-lingual language model (briefly, XLM) <ref type="bibr" target="#b14">(Lample &amp; Conneau, 2019)</ref>, XLNet <ref type="bibr" target="#b41">(Yang et al., 2019b)</ref> and RoBERTa  have attracted more and more attention in machine learning and natural language processing communities. The models are first pre-trained on large amount of unlabeled data to capture rich representations of the input, and then applied to the downstream tasks by either providing context-aware embeddings of an input sequence <ref type="bibr" target="#b23">(Peters et al., 2018)</ref>, or initializing the parameters of the downstream model <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> for fine-tuning. Such pre-training approaches lead to significant improvements on natural language understanding tasks. Among them, BERT is one of the most powerful techniques that inspires lots of variants like XLNet, XLM, RoBERTa and achieves state-of-the-art results for many language understanding tasks including reading comprehension, text classification, etc <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>.</p><p>Neural Machine Translation (NMT) aims to translate an input sequence from a source language to a target language. An NMT model usually consists of an encoder to map an input sequence to hidden representations, and a decoder to decode hidden representations to generate a sentence in the target language. Given that BERT has achieved great success in language understanding tasks, a question worthy studying is how to incorporate BERT to improve NMT. Due to the computation resource limitation, training a BERT model from scratch is unaffordable for many researchers. Thus, we focus on the setting of leveraging a pre-trained BERT model (instead of training a BERT model from scratch) for NMT.</p><p>We conduct 14 experiments on various NMT tasks to verify our approach, including supervised, semi-supervised and unsupervised settings. For supervised NMT, we work on five tasks of IWSLT datasets and two WMT datasets. Specifically, we achieve 36.11 BLEU score on IWSLT'14 Germanto-English translation, setting a new record on this task. We also work on two document-level translations of IWSLT, and further boost the BLEU score of German-to-English translation to 36.69. On WMT'14 datasets, we achieve 30.75 BLEU score on English-to-German translation and 43.78 on English-to-French translation, significantly better over the baselines. For semi-supervised NMT, we boost BLEU scores of WMT'16 Romanian-to-English translation with back translation <ref type="bibr" target="#b27">(Sennrich et al., 2016b)</ref>, a classic semi-supervised algorithm, from 37.73 to 39.10, achieving the best result on this task. Finally, we verify our algorithm on unsupervised English↔French and unsupervised English↔Romanian translations and also achieve state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>We briefly introduce the background of NMT and review current pre-training techniques.</p><p>NMT aims to translate an input sentence from the source language to the target one. An NMT model usually consists of an encoder, a decoder and an attention module. The encoder maps the input sequence to hidden representations and the decoder maps the hidden representations to the target sequence. The attention module is first introduced by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, which is used to better align source words and target words. The encoder and decoder can be specialized as LSTM <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b38">Wu et al., 2016)</ref>, CNN <ref type="bibr" target="#b10">(Gehring et al., 2017)</ref> and Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>. A Transformer layer consists of three sublayers, a self-attention layer that processes sequential data taking the context of each timestep into consideration, an optional encoder-decoder attention layer that bridges the input sequence and target sequence which exists in decoder only, and a feed-forward layer for non-linear transformation. Transformer achieves the state-of-the-art results for NMT <ref type="bibr" target="#b1">(Barrault et al., 2019)</ref>. In this work, we will use Transformer as the basic architecture of our model.</p><p>Pre-training has a long history in machine learning and natural language processing <ref type="bibr" target="#b8">(Erhan et al., 2009;</ref><ref type="bibr" target="#b9">2010)</ref>. <ref type="bibr" target="#b19">Mikolov et al. (2013)</ref> and <ref type="bibr" target="#b22">Pennington et al. (2014)</ref> proposed to use distributional representations (i.e., word embeddings) for individual words. <ref type="bibr" target="#b4">Dai &amp; Le (2015)</ref> proposed to train a language model or an auto-encoder with unlabeled data and then leveraged the obtained model to finetune downstream tasks. Pre-training has attracted more and more attention in recent years and achieved great improvements when the data scale becomes large and deep neural networks are employed. ELMo was proposed in <ref type="bibr" target="#b23">Peters et al. (2018)</ref> based on bidirectional LSTMs and its pre-trained models are fed into downstream tasks as context-aware inputs. In GPT <ref type="bibr" target="#b24">(Radford et al., 2018)</ref>, a Transformer based language model is pre-trained on unlabeled dataset and then finetuned on downstream tasks. BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> is one of the widely adopted pre-training approach for model initialization. The architecture of BERT is the encoder of Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>. Two kinds of objective functions are used in BERT training: (1) Masked language modeling (MLM), where 15% words in a sentence are masked and BERT is trained to predict them with their surrounding words. (2) Next sentence prediction (NSP): Another task of pre-training BERT is to predict whether two input sequences are adjacent. For this purpose, the training corpus consists of tuples <ref type="bibr">([cls]</ref>, input 1, [sep], input 2, [sep]), with learnable special tokens [cls] to classify whether input 1 and input 2 are adjacent and [sep] to segment two sentences, and with probability 50%, the second input is replaced with a random input. Variants of BERT have been proposed: In XLM <ref type="bibr" target="#b14">(Lample &amp; Conneau, 2019)</ref>, the model is pre-trained based on multiple languages and NSP task is removed; in RoBERTa , more unlabeled data is leveraged without NSP task neither; in XLNet <ref type="bibr" target="#b41">(Yang et al., 2019b)</ref>, a permutation based modeling is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A PRELIMINARY EXPLORATION</head><p>While a few pieces of work <ref type="bibr" target="#b14">(Lample &amp; Conneau, 2019;</ref><ref type="bibr" target="#b30">Song et al., 2019)</ref> design specific pretraining methods for NMT, they are time and resource consuming given that they need to pre-train large models from scratch using large-scale data, and even one model for each language pair. In this work, we focus on the setting of using a pre-trained BERT model. Detailed model download links can be found in Appendix D.</p><p>Considering that pre-trained models have been utilized in two different ways for other natural language tasks, it is straightforward to try them for NMT. Following previous practice, we make the following attempts.</p><p>(I) Use pre-trained models to initialize the NMT model. There are different implementations for this approach. (1) Following <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, we initialize the encoder of an NMT model with a pretrained BERT. (2) Following <ref type="bibr" target="#b14">(Lample &amp; Conneau, 2019)</ref>, we initialize the encoder and/or decoder of an NMT model with XLM.</p><p>(II) Use pre-trained models as inputs to the NMT model. Inspired from <ref type="bibr" target="#b23">(Peters et al., 2018)</ref>, we feed the outputs of the last layer of BERT to an NMT model as its inputs.</p><p>We conduct experiments on the IWSLT'14 English→German translation, a widely adopted dataset for machine translation consisting of 160k labeled sentence pairs. We choose Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> as the basic model architecture with transformer iwslt de en configuration (a six-layer model with 36.7M parameters). The translation quality is evaluated by BLEU <ref type="bibr" target="#b21">(Papineni et al., 2002)</ref> score; the larger, the better. Both BERT base and XLM models are pre-trained and we get them from the Web. More details about the experimental settings are included in Appendix A.2. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. We have several observations: (1) Using BERT to initialize the encoder of NMT can only achieve 27.14 BLEU score, which is even worse than standard Transformer without using BERT. That is, simply using BERT to warm up an NMT model is not a good choice.</p><p>(2) Using XLM to initialize the encoder or decoder respectively, we get 28.22 or 26.13 BLEU score, which does not outperform the baseline. If both modules are initialized with XLM, the BLEU score is boosted to 28.99, slightly outperforming the baseline. Although XLM achieved great success on WMT'16 Romanian-to-English, we get limited improvement here. Our conjecture is that the XLM model is pre-trained on news data, which is out-of-domain for IWSLT dataset mainly about spoken languages and thus, leading to limited improvement. (3) When using the output of BERT as context-aware embeddings of the encoder, we achieve 29.67 BLEU, much better than using pretrained models for initialization. This shows that leveraging BERT as a feature provider is more effective in NMT. This motivates us to take one step further and study how to fully exploit such features provided by pre-trained BERT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ALGORITHM</head><p>In this section, we first define the necessary notations, then introduce our proposed BERT-fused model and finally provide discussions with existing works.</p><p>Notations Let X and Y denote the source language domain and target language domain respectively, which are the collections of sentences with the corresponding languages. For any sentence x ∈ X and y ∈ Y, let l x and l y denote the number of units (e.g., words or sub-words) in x and y. The i-th unit in x/y is denoted as x i /y i . Denote the encoder, decoder and BERT as Enc, Dec and BERT respectively. For ease of reference, we call the encoder and decoder in our work as the NMT module. W.l.o.g., we assume both the encoder and decoder consists of L layers. Let attn(q, K, V ) denote the attention layer, where q, K and V indicate query, key and value respectively <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>. We use the same feed-forward layer as that used in <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> and denote it as FFN. Mathematical formulations of the above layers are left at Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BERT-FUSED MODEL</head><p>An illustration of our algorithm is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Any input x ∈ X is progressively processed by the BERT, encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-Enc Attention</head><p>Add &amp; Norm Step-1: Given any input x ∈ X , BERT first encodes it into representation H B = BERT(x). H B is the output of the last layer in BERT. The h B,i ∈ H B is the representation of the i-th wordpiece in x.</p><p>Step-2: Let H l E denote the hidden representation of l-th layer in the encoder, and let H 0 E denote word embedding of sequence x. Denote the i-th element in H l E as h l i for any i ∈ [l x ]. In the l-th</p><formula xml:id="formula_0">layer, l ∈ [L], h l i = 1 2 attn S (h l−1 i , H l−1 E , H l−1 E ) + attn B (h l−1 i , H B , H B ) , ∀i ∈ [l x ],<label>(1)</label></formula><p>where attn S and attn B are attention models (see Eqn. <ref type="formula">(6)</ref>) with different parameters. Then each h l i is further processed by FFN(·) defined in Eqn. <ref type="formula">(7)</ref> and we get the output of the l-th layer:</p><formula xml:id="formula_1">H l E = (FFN(h l 1 ), · · · , FFN(h l lx ))</formula><p>. The encoder will eventually output H L E from the last layer.</p><p>Step-3: Let S l &lt;t denote the hidden state of l-th layer in the decoder preceding time step t, i.e., S l &lt;t = (s l 1 , · · · , s l t−1 ). Note s 0 1 is a special token indicating the start of a sequence, and s 0 t is the embedding of the predicted word at time-step t − 1. At the l-th layer, we havê</p><formula xml:id="formula_2">s l t = attn S (s l−1 t , S l−1 &lt;t+1 , S l−1 &lt;t+1 ); s l t = 1 2 attn B (ŝ l t , H B , H B ) + attn E (ŝ l t , H L E , H L E ) , s l t = FFN(s l t ).<label>(2)</label></formula><p>The attn S , attn B and attn E represent self-attention model, BERT-decoder attention model and encoder-decoder attention model respectively. Eqn. <ref type="formula" target="#formula_2">(2)</ref> iterates over layers and we can eventually obtain s L t . Finally s L t is mapped via a linear transformation and softmax to get the t-th predicted wordŷ t . The decoding process continues until meeting the end-of-sentence token.</p><p>In our framework, the output of BERT serves as an external sequence representation, and we use an attention model to incorporate it into the NMT model. This is a general way to leverage the pre-trained model regardless of the tokenization way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DROP-NET TRICK</head><p>Inspired by dropout <ref type="bibr" target="#b31">(Srivastava et al., 2014)</ref> and drop-path <ref type="bibr">(Larsson et al., 2017)</ref>, which can regularize the network training, we propose a drop-net trick to ensure that the features output by BERT and the conventional encoder are fully utilized. The drop-net will effect Eqn.(1) and Eqn. <ref type="bibr" target="#b43">(2)</ref>. Denote the drop-net rate as p net ∈ [0, 1]. At each training iteration, for any layer l, we uniformly sample a random variable U l from [0, 1], then all theh l i in Eqn. <ref type="bibr" target="#b42">(1)</ref> are calculated in the following way:</p><formula xml:id="formula_3">h l i,drop-net = I U l &lt; pnet 2 · attnS(h l−1 i , H l−1 E , H l−1 E ) + I U l &gt; 1 − pnet 2 · attnB(h l−1 i</formula><p>, HB, HB)</p><formula xml:id="formula_4">+ 1 2 I pnet 2 ≤ U l ≤ 1 − pnet 2 · attnS(h l−1 i , H l−1 E , H l−1 E ) + attnB(h l−1 i , HB, HB) ,<label>(3)</label></formula><p>where I(·) is the indicator function. For any layer, with probability p net /2, either the BERT-encoder attention or self-attention is used only; w.p. (1 − p net ), both the two attention models are used. For example, at a specific iteration, the first layer might uses attn S only while the second layer uses attn B only. During inference time, the expected output of each attention model is used, which is</p><formula xml:id="formula_5">E U ∼uniform[0,1] (h l i,drop-net ).</formula><p>The expectation is exactly Eqn. <ref type="bibr" target="#b42">(1)</ref>. Similarly, for training of the decoder, with the drop-net trick, we havẽ</p><formula xml:id="formula_6">s l t,drop-net = I(U l &lt; pnet 2 ) · attnB(ŝ l t , HB, HB) + I(U l &gt; 1 − pnet 2 ) · attnE(ŝ l t , H L E , H L E ) + 1 2 I( pnet 2 ≤ U l ≤ 1 − pnet 2 ) · (attnB(ŝ l t , HB, HB) + attnE(ŝ l t , H L E , H L E )).<label>(4)</label></formula><p>For inference, it is calculated in the same way as Eqn. <ref type="bibr" target="#b43">(2)</ref>. Using this technique can prevent network from overfitting (see the second part of Section 6 for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DISCUSSION</head><p>Comparison with ELMo As introduced in Section 2, ELMo (Peters et al., 2018) provides a contextaware embeddings for the encoder in order to capture richer information of the input sequence. Our approach is a more effective way of leveraging the features from the pre-trained model: <ref type="formula" target="#formula_0">(1)</ref> The output features of the pre-trained model are fused in all layers of the NMT module, ensuring the well-pre-trained features are fully exploited; <ref type="formula" target="#formula_2">(2)</ref> We use the attention model to bridge the NMT module and the pre-trained features of BERT, in which the NMT module can adaptively determine how to leverage the features from BERT.</p><p>Limitations We are aware that our approach has several limitations. (1) Additional storage cost: our approach leverages a BERT model, which results in additional storage cost. However, considering the BLEU improvement and the fact that we do not need additional training of BERT, we believe that the additional storage is acceptable. (2) Additional inference time: We use BERT to encode the input sequence, which takes about 45% additional time (see Appendix C for details). We will leave the improvement of the above two limitations as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATION TO SUPERVISED NMT AND SEMI-SUPERVISED NMT</head><p>We first verify our BERT-fused model on the supervised setting, including low-resource and richresource scenarios. Then we conduct experiments on document-level translation to verify our approach. Finally, we combine BERT-fused model with back translation <ref type="bibr" target="#b27">(Sennrich et al., 2016b)</ref> to verify the effectiveness of our method on semi-supervised NMT. We choose BERT base for IWSLT tasks and BERT large for WMT tasks, which can ensure that the dimension of the BERT and NMT model almost match. The BERT models are fixed during training. Detailed BERT information for each task is in Appendix D. The drop-net rate p net is set as 1.0.</p><p>Training Strategy We first train an NMT model until convergence, then initialize the encoder and decoder of the BERT-fused model with the obtained model. The BERT-encoder attention and BERTdecoder attention are randomly initialized. Experiments on IWSLT and WMT tasks are conducted on 1 and 8 M40 GPUs respectively. The batchsize is 4k tokens per GPU. Following , for WMT tasks, we accumulate the gradient for 16 iterations and then update to simulate a 128-GPU environment. It takes 1, 8 and 14 days to obtain the pre-trained NMT models, and additional 1, 7 and 10 days to finish the whole training process. The optimization algorithm is Adam (Kingma &amp; Ba, 2014) with initial learning rate 0.0005 and inverse sqrt learning rate scheduler <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>. For WMT'14 En→De, we use beam search with width 4 and length penalty 0.6 for inference following <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>. For other tasks, we use width 5 and length penalty 1.0.</p><p>Evaluation We use multi-bleu.perl to evaluate IWSLT'14 En↔De and WMT translation tasks for fair comparison with previous work. For the remaining tasks, we use a more advance implementation of BLEU score, sacreBLEU for evaluation. Script urls are in Appendix A.1. The results of IWSLT translation tasks are reported in <ref type="table" target="#tab_2">Table 2</ref>. We implemented standard Transformer as baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RESULTS</head><p>Our proposed BERT-fused model can improve the BLEU scores of the five tasks by 1.88, 1.47, 2.4, 1.9 and 2.8 points respectively, demonstrating the effectiveness of our method. The consistent improvements on various tasks shows that our method works well for low-resource translations. We achieved state-of-the-art results on IWSLT'14 De→En translation, a widely investigated baseline in ma-chine translation. The comparison with previous methods are shown in Appendix B.4 due to space limitation.</p><p>The results of WMT'14 En→De and En→Fr are shown in <ref type="table" target="#tab_3">Table 3</ref>. Our reproduced Transformer matches the results reported in , and we can see that our BERT-fused model can improve these two numbers to 30.75 and 43.78, achieving 1.63 and 0.82 points improvement. Our approach also outperforms the well-designed model DynamicConv ) and a model obtained through neural architecture search <ref type="bibr" target="#b29">(So et al., 2019)</ref>. BERT is able to capture the relation between two sentences, since the next sentence prediction (NSP) task is to predict whether two sentences are adjacent. We can leverage this property to improve translation with document-level contextual information <ref type="bibr" target="#b18">(Miculicich et al., 2018)</ref>, which is briefly denoted as document-level translation. The inputs are a couple of sentences extracted from a paragraph/document,</p><formula xml:id="formula_7">x d 1 , x d 2 , · · · , x d T ,</formula><p>where the T x's are contextually correlated. We want to translate them into target language by considering the contextual information.</p><p>Algorithm In our implementation, to translate a sentence x to target domain, we leverage the contextual information by taking both x and its preceding sentence x prev as inputs. x is fed into Enc, which is the same as sentence-level translation. Setting We use IWSLT'14 En↔De dataset as introduced in Section 5.1. The data is a collection of TED talks, where each talk consists of several sequences. We can extract the adjacent sentences for training, validation and test sets. The training strategy, hyperparameter selection and evaluation metric are the same for sentence-level translation. Baselines We use two baselines here. <ref type="formula" target="#formula_0">(1)</ref> To demonstrate how BERT works in our model, we replace BERT by a Transformer with configuration transformer iwslt de en, which is randomly initialized and jointly trained. <ref type="formula" target="#formula_2">(2)</ref> Another baseline is proposed by <ref type="bibr" target="#b18">Miculicich et al. (2018)</ref>, where multiple preceding sentences in a document are leveraged using a hierarchical attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. We can see that introducing contextual information from an additional encoder can boost the sentence-level baselines, but the improvement is limited (0.33 for En→De and 0.31 for De→En). <ref type="bibr" target="#b18">For Miculicich et al. (2018)</ref>, the best results we obtain are 27.94 and 33.97 respectively, which are worse than the sentence-level baselines. Combining BERT-fused model and document-level information, we can eventually achieve 31.02 for En→De and 36.69 for De→En. We perform significant test 1 between sentence-level and document-level translation. Our document-level BERT-fused model significantly outperforms sentence-level baseline with p-value less than 0.01. This shows that our approach not only works for sentence-level translation, but can also be generalized to document-level translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">APPLICATION TO SEMI-SUPERVISED NMT</head><p>We work on WMT'16 Romanian→English (Ro→En) translation to verify whether our approach can still make improvement over back translation <ref type="bibr" target="#b27">(Sennrich et al., 2016b)</ref>, the standard and powerful semi-supervised way to leverage monolingual data in NMT.</p><p>The number of bilingual sentence pairs for Ro→En is 0.6M . <ref type="bibr" target="#b26">Sennrich et al. (2016a)</ref> provided 2M back translated data 2 . We use newsdev2016 as validation set and newstest2016 as test set. Sentences were encoded using BPE with a shared source-target vocabulary of about 32k tokens. We use transformer big configuration. Considering there is no Romanian BERT, we use the cased multilingual BERT (please refer to Appendix D) to encode inputs. The drop-net rate p net is set as 1.0. The translation quality is evaluated by multi-bleu.perl. The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. The Transformer baseline achieves 33.12 BLEU score. With back-translation, the performance is boosted to 37.73. We use the model obtained with back-translation to initialize BERT-fused model, and eventually reach 39.10 BLEU. Such a score surpasses the previous best result 38.5 achieved by XLM <ref type="bibr" target="#b14">(Lample &amp; Conneau, 2019)</ref> and sets a new record. This demonstrates that our proposed approach is effective and can still achieve improvement over strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ABLATION STUDY</head><p>We conduct two groups of ablation studies on IWSLT'14 En→De translation to better understand our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study for training strategy and network architecture</head><p>We conduct ablation study to investigate the performance of each component of our model and training strategy. Results are reported in <ref type="table" target="#tab_6">Table 6</ref>:</p><p>(1) We randomly initialize the NMT module (i.e., encoder and decoder) of BERT-fused model instead of using a warm-start one as introduced in the training strategy of Section 5.1. In this way, we can only achieve 27.03 BLEU score, which cannot catch up with the baseline. We also jointly train BERT model with the NMT module. Although it can also boost the baseline from 28.57 to 28.87, it is not as good as fixing the BERT part, whose BLEU is 30.45.</p><p>(2) We feed the output of BERT into all layers of the encoder without attention models. That is, the Eqn. <ref type="formula" target="#formula_0">(1)</ref> is revised toh</p><formula xml:id="formula_8">l i = 1 2 attn S (h l−1 i , H l−1 E , H l−1 E ) + W l B h l−1 i ) , where W l B</formula><p>is learnable. In this case, the encoder and BERT have to share the same vocabulary. The BLEU score is 29.61, which is better than the standard Transformer but slightly worse than leveraging the output of BERT as embedding. This shows that the output of BERT should not be fused into each layer directly, and using the attention model to bridge the relation is better than using simple transformation. More results on different languages are included in Appendix B.3. To illustrate the effectiveness of our method, we choose another two kinds of ways to encode the input sequence rather than using BERT: (1) Using a fixed and randomly initialized embedding; (2) Using the encoder from another NMT model. Their BLEU scores are 28.91 and 28.99 respectively, indicating that the BERT pre-trained on large amount of unlabeled data can provide more helpful features to NMT.</p><p>(3) To verify where the output of BERT should be connected to, we remove the BERT-encoder attention (i.e., attn B in <ref type="figure" target="#fig_0">Eqn.(1)</ref>) and the BERT-decoder attention (i.e,, attn B in Eqn. <ref type="formula" target="#formula_2">(2)</ref>) respectively. Correspondingly, the BLEU score drops from 30.45 to 29.87 and 29.90. This indicates that the output of BERT should be leveraged by both encoder and decoder to achieve better performances. At last, considering that there are two stacked encoders in our model, we also choose ensemble models and deeper NMT models as baselines. Our approach outperforms the above baselines. The results are left in Appendix B.2 due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study on drop-net</head><p>To investigate the effect of drop-net, we conduct experiments on IWSLT'14 En→De dataset with different drop-net probability, p net ∈ {0, 0.2, 0.4, 0.6, 0.8, 1.0}. The results are shown in <ref type="figure" target="#fig_3">Figure 2</ref>. As can been seen, although larger p net leads to larger training loss, it leads to smaller validation loss and so better BLUE scores. This shows that the drop-net trick can indeed improve the generalization ability of our model. We fix p net = 1.0 in other experiments unless specially specified.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">APPLICATION TO UNSUPERVISED NMT</head><p>We work on unsupervised En↔Fr and En↔Ro translation. The data processing, architecture selection and training strategy is the same as <ref type="bibr" target="#b14">Lample &amp; Conneau (2019)</ref>.</p><p>Settings For En↔Fr, we use 190M monolingual English sentences and 62M monolingual French sentences from WMT News Crawl datasets, which is the same as that used in <ref type="bibr" target="#b30">(Song et al., 2019)</ref>. <ref type="bibr">3</ref> For unsupervised En↔Ro translation, we use 50M English sentences from News Crawl (sampled from the data for En→Fr) and collect 2.9M sentences for Romanian by concatenating News Crawl data sets and WMT'16 Romanian monolingual data following <ref type="bibr" target="#b15">Lample et al. (2018)</ref>. The data is preprocessed in the same way as <ref type="bibr" target="#b14">Lample &amp; Conneau (2019)</ref>.</p><p>We use the same model configuration as <ref type="bibr" target="#b14">Lample &amp; Conneau (2019)</ref>, with details in Appendix A.3. The BERT is the pre-trained XLM model (see Appendix D). We first train an unsupervised NMT model following <ref type="bibr" target="#b14">Lample &amp; Conneau (2019)</ref> until convergence. Then we initialize our BERT-fused model with the obtained model and continue training. We train models on 8 M40 GPUs, and the batchsize is 2000 tokens per GPU. We use the same optimization hyper-parameters as that described in <ref type="bibr" target="#b14">Lample &amp; Conneau (2019)</ref>. Results The results of unsupervised NMT are shown in <ref type="table" target="#tab_7">Table 7</ref>. With our proposed BERT-fused model, we can achieve 38.27, 35.62, 36.02 and 33.20 BLEU scores on the four tasks, setting stateof-the-art results on these tasks. Therefore, our BERT-fused model also benefits unsupervised NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>In this work, we propose an effective approach, BERT-fused model, to combine BERT and NMT, where the BERT is leveraged by the encoder and decoder through attention models. Experiments on supervised NMT (including sentence-level and document-level translations), semi-supervised NMT and unsupervised NMT demonstrate the effectiveness of our method.</p><p>For future work, there are many interesting directions. First, we will study how to speed up the inference process. Second, we can apply such an algorithm to more applications, like questioning and answering. Third, how to compress BERT-fused model into a light version is another topic. There are some contemporary works leveraging knowledge distillation to combine pre-trained models with NMT <ref type="bibr" target="#b40">(Yang et al., 2019a;</ref>, which is a direction to explore.  , for En↔De, we lowercase all words, split 7k sentence pairs from the training dataset for validation and concatenate dev2010, dev2012, tst2010, tst2011, tst2012 as the test set. For other tasks, we do not lowercase the words and use the official validation/test sets of the corresponding years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENT SETUP</head><p>For rich-resource scenario, we work on WMT'14 En→De and En→Fr, whose corpus sizes are 4.5M and 36M respectively. We concatenate newstest2012 and newstest2013 as the validation set and use newstest2014 as the test set.</p><p>We apply BPE <ref type="bibr" target="#b28">(Sennrich et al., 2016c)</ref> to split words into sub-units. The numbers of BPE merge operation for IWSLT tasks, WMT'14 En→De and En→Fr are 10k, 32k and 40k respectively. We merge the source and target language sentences for all tasks to build the vocabulary except En→Zh.</p><p>Model Configuration For IWSLT tasks, we use the transformer iwslt de en setting with dropout ratio 0.3. In this setting, the embedding dimension, FFN layer dimension and number of layers are 512, 1024 and 6. For WMT'14 En→De and En→Fr, we use transformer big setting (short for transformer vaswani wmt en de big) with dropout 0.3 and 0.1 respectively. In this setting, the aforementioned three parameters are 1024, 4096 and 6 respectively.</p><p>Evaluation We use multi-bleu.perl 4 to evaluate IWSLT'14 En↔De and WMT translation tasks for fair comparison with previous work. For the remaining tasks, we use a more advance implementation of BLEU score, detokenized sacreBLEU for evaluation 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DETAILED EXPERIMENT SETTING IN SECTION 3</head><p>The IWSLT'14 English-to-German data and model configuration is introduced in Section A.1.</p><p>For the training stategy, we use Adam <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2014)</ref> to optimize the network with β 1 = 0.9, β 2 = 0.98 and weight-decay = 0.0001. The learning rate scheduler is inverse sqrt, where warmup-init-lr = 10 −7 , warmup-updates = 4000 and max-lr = 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 DETAILED MODEL CONFIGURATION IN UNSUPERVISED NMT</head><p>We leverage one Transformer model with GELU activation function to work on translations of two directions, where each language is associated with a language tag. The embedding dimension, FFN layer dimension and number of layer are 1024, 4096 and 6. The BERT is initialized by the pretrained XLM model provided by <ref type="bibr" target="#b14">(Lample &amp; Conneau, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MORE EXPERIMENT RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 MORE RESULTS ON PRELIMINARY EXPLORATION OF LEVERAGING BERT</head><p>We use XLM to initialize the model for WMT'14 English→German translation task, whose training corpus is relative large. We eventually obtain 28.09 after 90 epochs, which is still underperform the baseline, 29.12 as we got. Similar problem is also reported in https://github.com/ facebookresearch/XLM/issues/32. We leave the improvement of supervised NMT with XLM as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 MORE ABLATION STUDY</head><p>Part I: A different way to deal with multiple attention models <ref type="bibr" target="#b12">Junczys-Dowmunt &amp; Grundkiewicz (2018)</ref> proposed a new way to handle multiple attention models. Instead of using Eqn. <ref type="formula" target="#formula_2">(2)</ref>, the input is processed by self-attention, encoder-decoder attention and BERT-decoder attention sequentially. Formally,</p><formula xml:id="formula_9">s l t = attn S (s l−1 t , S l−1 &lt;t+1 , S l−1 &lt;t+1 ); s l t = attn E (ŝ l t , H L E , H L E ); s l t = attn B (s l t , H B , H B ); s l t = FFN(s l t ).<label>(5)</label></formula><p>The BLEU score is 29.35 for this setting, not as good as our proposed method.</p><p>Part II: More results on IWSLT'14 En→De translation</p><p>Since our BERT-fused model contains two stacked encoders, we carry out two groups of additional baselines:</p><p>(1) Considering that stacking the BERT and encoder can be seen as a deeper model, we also train another two NMT models with deeper encoders, one with 18 layers (since BERT base consists of 12 layers) and the other with 12 layers (which achieved best validation performance ranging from 6 to 18 layers).</p><p>(2) We also compare the results of our approach with ensemble methods. To get an M -model ensemble, we independently train M models with different random seeds (M ∈ Z + ). We ensemble both standard Transformers and our BERT-fused models, which are denoted as M -model ensemble (standard) and M -model ensemble (BERT-fused) respectively. Please note that when we aggregate multiple BERT-fused models, we only need to store one replica of the BERT model because the BERT part is not optimized. The results are shown in <ref type="table" target="#tab_9">Table 8</ref>. We have the following observations:</p><p>1. Adding more layers can indeed boost the baseline, but still not as good as BERT-fused model. According to our experiments, when increasing the number of layers to 12, we achieve the best BLEU score, 29.27.</p><p>2. We also compare our results to ensemble methods. Indeed, ensemble significantly boost the baseline by more than one point. However, even if using ensemble of four models, the BLEU score is still lower than our BERT-fused model <ref type="bibr">(30.18 v.s. 30.45)</ref>, which shows the effectiveness of our method.</p><p>We want to point out that our method is intrinsically different from ensemble. Ensemble approaches usually refer to "independently" train several different models for the same task, and then aggregate the output of each model to get the eventually task. In BERT-fused model, although we include a pre-trained BERT into our model, there is still only one model serving for the translation task.</p><p>In this sense, we can also combine our BERT-fused model with ensemble. Our approach benefits from ensemble too. When ensembling two models, we can achieve 31.09 BLEU score. When adding the number of models to four, we eventually achieve 31.85 BLEU score, which is 1.67 point improvement over the ensemble of standard Transformer.</p><p>Part III: More results on IWSLT'14 De→En translation</p><p>We report the ensemble results on IWSLT'14 De→En translation in <ref type="table" target="#tab_10">Table 9</ref>. We can get similar conclusion compared to that of IWSLT'14 En→De. The ablation study on more languages is shown in <ref type="table" target="#tab_0">Table 10</ref>. Our method achieves the best results compared to all baselines. We summarize the BLEU scores on IWSLT'14 De→En of existed works and our BERT-fused model approach in <ref type="table" target="#tab_0">Table 11</ref>.   35.52 Loss to teach <ref type="bibr" target="#b37">(Wu et al., 2018)</ref> 34.80 Role-interactive layer <ref type="bibr" target="#b35">(Weissenborn et al., 2019)</ref> 34.74 Variational attention <ref type="bibr" target="#b5">(Deng et al., 2018)</ref> 33.68</p><p>Our BERT-fused model 36.11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 COMPARISON WITH BACK TRANSLATION</head><p>When using unlabeled data to boost machine learning systems, one of the most notable approaches is back translation (briefly, BT) <ref type="bibr" target="#b27">(Sennrich et al., 2016b)</ref>: We first train a reversed translation model, use the obtained model to translate the unlabeled data in the target domain back to source domain, obtain a synthetic dataset where the source data is back-translated and finally train the forward model on the augmented dataset.</p><p>Our method has two main differences with BT method.</p><p>1. In BT, the monolingual data from the target side is leveraged. In our proposed approach, we use a BERT of the source language, which indirectly leverages the monolingual data from the source side. In this way, our approach and BT are complementary to each other. In Section 5.4, we have already verified that our method can further improve the results of standard BT on Romanian-to-English translation.</p><p>2. To use BT, we have to train a reversed translation model and then back translate the monolingual data, which is time-cost due to the decoding process. In BERT-fused model, we only need to download a pre-trained BERT model, incorporate it into our model and continue training. Besides, the BERT module is fixed during training.</p><p>On IWSLT'14, we also implement BT on wikipedia data, which is a subset of the corpus of training BERT. The model used for back translation are standard Transformer baselines introduced in Section 5, whose BLEU scores are 28.57 and 34.64 respectively. We back translate 1M, 2M, 5M, 15M and 25M randomly selected German sentences.</p><p>The results are reported in <ref type="table" target="#tab_0">Table 12</ref>. The rows started with BT(·) represent the results of BT, and the numbers in the brackets are the number of sentences for back translation. IWSLT dataset is a collection of spoken language, and the bilingual training corpus is small (160k). In Wikipedia, the sentences are relatively formal compared to the spoken language, which is outof-domain of spoken languages. We can see that when using 1M or 2M monolingual data for BT, the BLEU scores can indeed improve from 28.57 to 29.42/29.76. However, simply adding more wikipedia data for BT does not result in more improvement. There is even a slight drop when adding more than 15M monolingual sentences. However, our BERT-fused model can achieve better performances than BT with wikipedia data. We compare the inference time of our approach to the baselines. The results are shown in <ref type="table" target="#tab_0">Table 13</ref>, where from the second column to the last column, the numbers are the inference time of standard Transformer, BERT-fused model, and the increase of inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPARISON OF INFERENCE TIME</head><p>Indeed, introducing BERT to encode the input brings additional inference time, resulting in about 40% to 49% increase. But considering the significant improvement of BLEU score, it is acceptable of such extra cost. We will study how to reduce inference time in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DOWNLOAD LINK OF PRE-TRAINED BERT MODELS</head><p>We leverage the pre-trained models provided by PyTorch-Transformers 6 .</p><p>For IWSLT'14 tasks, we choose BERT base model with 12 layers and hidden dimension 768.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of BERT-fused model. The left and right figures represent the BERT, encoder and decoder respectively. Dash lines denote residual connections. H B (red part) and H L E (green part) denote the output of the last layer from BERT and encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For the input of BERT, it is the concatenation of two sequences: ([cls], x prev , [sep], x, [sep]), where both [cls] and [sep] are special tokens of BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Training/validation curves with different p net 's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Preliminary explorations on IWSLT'14 English→German translation.</figDesc><table><row><cell>Algorithm</cell><cell>BLEU score</cell></row><row><cell>Standard Transformer</cell><cell>28.57</cell></row><row><cell>Use BERT to initialize the encoder of NMT</cell><cell>27.14</cell></row><row><cell>Use XLM to initialize the encoder of NMT</cell><cell>28.22</cell></row><row><cell>Use XLM to initialize the decoder of NMT</cell><cell>26.13</cell></row><row><cell>Use XLM to initialize both the encoder and decoder of NMT</cell><cell>28.99</cell></row><row><cell>Leveraging the output of BERT as embeddings</cell><cell>29.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>There are 160k, 183k, 236k, 235k bilingual sentence pairs for En↔De, En→Es, En→Fr and En→Zh tasks. Following the common practice, for En↔De, we lowercase all words. All sentences are preprocessed by BPE<ref type="bibr" target="#b28">(Sennrich et al., 2016c)</ref>. The model configuration is transformer iwslt de en, representing a six-layer model with embedding size 512 and FFN layer dimension 1024. For the rich-resource scenario, we work on WMT'14 En→De and En→Fr, whose corpus sizes are 4.5M and 36M respectively. We concatenate newstest2012 and newstest2013 as the validation set and use newstest2014 as the test set. The model configuration is transformer big, another six-layer network with embedding size 1024 and FFN layer dimension 4096. More details about data and model are left in Appendix A.1.</figDesc><table><row><cell>5.1 SETTINGS</cell></row><row><cell>Dataset For the low-resource scenario, we choose IWSLT'14 English↔German (En↔De),</cell></row><row><cell>English→Spanish (En→Es), IWSLT'17 English→French (En→Fr) and English→Chinese</cell></row><row><cell>(En→Zh) translation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>BLEU of all IWSLT tasks.</figDesc><table><row><cell></cell><cell cols="2">Transformer BERT-fused</cell></row><row><cell>En→De</cell><cell>28.57</cell><cell>30.45</cell></row><row><cell>De→En</cell><cell>34.64</cell><cell>36.11</cell></row><row><cell>En→Es</cell><cell>39.0</cell><cell>41.4</cell></row><row><cell>En→Zh</cell><cell>26.3</cell><cell>28.2</cell></row><row><cell>En→Fr</cell><cell>35.9</cell><cell>38.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>BLEU scores of WMT'14 translation.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">En→De En→Fr</cell></row><row><cell>DynamicConv (Wu et al., 2019)</cell><cell>29.7</cell><cell>43.2</cell></row><row><cell>Evolved Transformer (So et al., 2019)</cell><cell>29.8</cell><cell>41.3</cell></row><row><cell>Transformer + Large Batch (Ott et al., 2018)</cell><cell>29.3</cell><cell>43.0</cell></row><row><cell>Our Reproduced Transformer</cell><cell>29.12</cell><cell>42.96</cell></row><row><cell>Our BERT-fused model</cell><cell>30.75</cell><cell>43.78</cell></row><row><cell cols="3">5.3 TRANSLATION WITH DOCUMENT-LEVEL CONTEXTUAL INFORMATION</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>BLEU of document-level translation.</figDesc><table><row><cell></cell><cell cols="2">En→De De→En</cell></row><row><cell>Sentence-level</cell><cell>28.57</cell><cell>34.64</cell></row><row><cell>Our Document-level</cell><cell>28.90</cell><cell>34.95</cell></row><row><cell>Miculicich et al. (2018)</cell><cell>27.94</cell><cell>33.97</cell></row><row><cell>Sentence-level + BERT</cell><cell>30.45</cell><cell>36.11</cell></row><row><cell>Document-level + BERT</cell><cell>31.02</cell><cell>36.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">: BLEU scores of WMT'16 Ro→En.</cell></row><row><cell>Methods</cell><cell>BLEU</cell></row><row><cell>Sennrich et al. (2016a)</cell><cell>33.9</cell></row><row><cell>XLM (Lample &amp; Conneau, 2019)</cell><cell>38.5</cell></row><row><cell>Standard Transformer</cell><cell>33.12</cell></row><row><cell>+ back translation</cell><cell>37.73</cell></row><row><cell>+ BERT-fused model</cell><cell>39.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on IWSLT'14 En→De.</figDesc><table><row><cell>Standard Transformer</cell><cell>28.57</cell></row><row><cell>BERT-fused model</cell><cell>30.45</cell></row><row><cell>Randomly initialize encoder/decoder of BERT-fused model</cell><cell>27.03</cell></row><row><cell cols="2">Jointly tune BERT and encoder/decoder of BERT-fused model 28.87</cell></row><row><cell>Feed BERT feature into all layers without attention</cell><cell>29.61</cell></row><row><cell>Replace BERT output with random vectors</cell><cell>28.91</cell></row><row><cell cols="2">Replace BERT with the encoder of another Transformer model 28.99</cell></row><row><cell>Remove BERT-encoder attention</cell><cell>29.87</cell></row><row><cell>Remove BERT-decoder attention</cell><cell>29.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>BLEU scores of unsupervised NMT.</figDesc><table><row><cell></cell><cell cols="4">En→Fr Fr→En En→Ro Ro→En</cell></row><row><cell>Lample et al. (2018)</cell><cell>27.6</cell><cell>27.7</cell><cell>25.1</cell><cell>23.9</cell></row><row><cell>XLM (Lample &amp; Conneau, 2019)</cell><cell>33.4</cell><cell>33.3</cell><cell>33.3</cell><cell>31.8</cell></row><row><cell>MASS (Song et al., 2019)</cell><cell>37.50</cell><cell>34.90</cell><cell>35.20</cell><cell>33.10</cell></row><row><cell>Our BERT-fused model</cell><cell>38.27</cell><cell>35.62</cell><cell>36.02</cell><cell>33.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>A.1 IWSLT'14 &amp; WMT'14 SETTINGS    We mainly follow the scripts below to preprocess the data: https://github.com/pytorch/ fairseq/tree/master/examples/translation .</figDesc><table><row><cell>Dataset For the low-resource scenario, we choose IWSLT'14 English↔German (En↔De),</cell></row><row><cell>English→Spanish (En→Es), IWSLT'17 English→French (En→Fr) and English→Chinese</cell></row><row><cell>(En→Zh) translation. There are 160k, 183k, 236k, 235k bilingual sentence pairs for En↔De,</cell></row><row><cell>En→Es, En→Fr and En→Zh tasks. Following the common practice</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>More ablation study on IWSLT'14 En→De.</figDesc><table><row><cell>Algorithm</cell><cell>BLEU</cell></row><row><cell>Standard Transformer</cell><cell>28.57</cell></row><row><cell>BERT-fused model</cell><cell>30.45</cell></row><row><cell>12-layer encoder</cell><cell>29.27</cell></row><row><cell>18-layer encoder</cell><cell>28.92</cell></row><row><cell>2-model ensemble (standard)</cell><cell>29.71</cell></row><row><cell>3-model ensemble (standard)</cell><cell>30.08</cell></row><row><cell>4-model ensemble (standard)</cell><cell>30.18</cell></row><row><cell cols="2">2-model ensemble (BERT-fused) 31.09</cell></row><row><cell cols="2">3-model ensemble (BERT-fused) 31.45</cell></row><row><cell cols="2">4-model ensemble (BERT-fused) 31.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>More ablation study on IWSLT'14 De→En. MORE RESULTS ON FEEDING BERT OUTPUT TO NMT MODULE</figDesc><table><row><cell>Algorithm</cell><cell>BLEU</cell></row><row><cell>Standard Transformer</cell><cell>34.67</cell></row><row><cell>BERT-fused model</cell><cell>36.11</cell></row><row><cell>2-model ensemble (standard)</cell><cell>35.92</cell></row><row><cell>3-model ensemble (standard)</cell><cell>36.40</cell></row><row><cell>4-model ensemble (standard)</cell><cell>36.54</cell></row><row><cell cols="2">2-model ensemble (BERT-fused) 37.42</cell></row><row><cell cols="2">3-model ensemble (BERT-fused) 37.70</cell></row><row><cell cols="2">4-model ensemble (BERT-fused) 37.71</cell></row><row><cell>B.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>BLEU scores of IWSLT translation tasks.</figDesc><table><row><cell>Algorithm</cell><cell cols="5">En→De De→En En→Es En→Zh En→Fr</cell></row><row><cell>Standard Transformer</cell><cell>28.57</cell><cell>34.64</cell><cell>39.0</cell><cell>26.3</cell><cell>35.9</cell></row><row><cell>Feed BERT feature into embedding</cell><cell>29.67</cell><cell>34.90</cell><cell>39.5</cell><cell>28.1</cell><cell>37.3</cell></row><row><cell>Feed BERT feature into all layers of encoder</cell><cell>29.61</cell><cell>34.84</cell><cell>39.9</cell><cell>28.1</cell><cell>37.4</cell></row><row><cell>Our BERT-fused model</cell><cell>30.45</cell><cell>36.11</cell><cell>41.4</cell><cell>28.2</cell><cell>38.7</cell></row><row><cell cols="4">B.4 MORE BASELINES OF IWSLT'14 GERMAN-TO-ENGLISH TRANSLATION</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Previous results of IWSLT'14 De→En.</figDesc><table><row><cell>Approach</cell><cell>BLEU</cell></row><row><cell>Multi-agent dual learning (Wang et al., 2019)</cell><cell>35.56</cell></row><row><cell>Tied-Transformer</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>BLEU scores IWSLT'14 En←De by BT.</figDesc><table><row><cell>Algorithm</cell><cell>En→De</cell></row><row><cell>Standard Transformer</cell><cell>28.57</cell></row><row><cell>BERT-fused model</cell><cell>30.45</cell></row><row><cell>BT (1M)</cell><cell>29.42</cell></row><row><cell>BT (2M)</cell><cell>29.76</cell></row><row><cell>BT (5M)</cell><cell>29.10</cell></row><row><cell>BT (15M)</cell><cell>28.26</cell></row><row><cell>BT (25M)</cell><cell>27.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Comparisons on inference time (seconds), '+' is the increased ratio of inference time.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Transformer Ours</cell><cell>(+)</cell></row><row><cell>IWSLT'14 En→De</cell><cell>70</cell><cell>97</cell><cell>38.6%</cell></row><row><cell>IWSLT'14 De→En</cell><cell>69</cell><cell cols="2">103 49.3%</cell></row><row><cell>WMT'14 En→De</cell><cell>67</cell><cell>99</cell><cell>47.8%</cell></row><row><cell>WMT'14 En→Fr</cell><cell>89</cell><cell cols="2">128 43.8%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ analysis/bootstrap-hypothesis-difference-significance.pl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Data at http://data.statmt.org/rsennrich/wmt16_backtranslations/ro-en/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Data source: https://modelrelease.blob.core.windows.net/mass/en-fr.tar.gz.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/mjpost/sacreBLEU.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/huggingface/pytorch-transformers</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DETAILS OF THE NOTATIONS</head><p>Let attn(q, K, V ) denote the attention layer, where q, K and V indicate query, key and value respectively. Here q is a d q -dimensional vector (d ∈ Z), K and V are two sets with |K| = |V |.</p><p>The attention model works as follows:</p><p>where W q , W k and W v are the parameters to be learned. In <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>, attn is implemented as a multi-head attention model and we omit the details here to increase readability. Following <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>, we define the non-linear transformation layer as</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1409.0473v7.pdf" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2019 conference on machine translation (wmt19)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondȓej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W19-5301" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="61" />
		</imprint>
	</monogr>
	<note>Task Papers, Day 1)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign, iwslt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation<address><addrLine>Hanoi, Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Distilling the knowledge of bert for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03829</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent alignment and variational attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9712" to="9724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1810.04805.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ms-uedin submission to the wmt2018 ape shared task: Dual-source transformer for automatic post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2018 THIRD CON-FERENCE ON MACHINE TRANSLATION (WMT18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Phrase-based &amp; neural unsupervised machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07755</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1605.07648.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Document-level neural machine translation with hierarchical attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjay</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01576</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2018 third conference on machine translation (WMT18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.statmt.org/wmt16/pdf/W16-2323.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="16" to="1009" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/so19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/song19d.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multiagent dual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Contextualized role interaction for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryx3_iAcY7" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkVhlh09tX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to teach with dynamic loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename><surname>Jian-Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6466" to="6477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tied transformers: Neural machine translation with shared encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5466" to="5473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards making the most of bert in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05672</idno>
		<ptr target="https://arxiv.org/pdf/1908.05672.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iwslt14 En→{de</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zh}</forename><surname>Fr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>we choose bert-base-uncased</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">IWSLT14 De→En, we choose bert-base-german-cased</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">De}, we choose bert-large-uncased, which is a BERT large model with 24 layers and hidden dimension 1024</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>For</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>En→{fr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">we choose bert-base-multilingual-cased, because there is no BERT specially trained for the Romanian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>For Wmt16 Ro→en</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">For unsupervised En↔Fr and unsupervised En↔Ro, we choose xlm-mlm-enfr1024 and xlm-mlm-enro1024 respectively</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<ptr target="https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enro-1024-pytorch_model.bin" />
		<title level="m">The download links are summarized as follows: • bert-base</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
