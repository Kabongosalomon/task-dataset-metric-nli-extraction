<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong</forename><forename type="middle">Joon</forename><surname>Oh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">LINE Plus Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CI-FAR and ImageNet classification tasks, as well as on the Im-ageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (CNNs) have shown promising performances on various computer vision problems such as image classification <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref>, object de-ResNet-50 Mixup <ref type="bibr" target="#b51">[48]</ref> Cutout <ref type="bibr" target="#b2">[3]</ref> CutMix  tection <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b24">24]</ref>, semantic segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">25]</ref>, and video analysis <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b33">32]</ref>. To further improve the training efficiency and performance, a number of training strategies have been proposed, including data augmentation <ref type="bibr" target="#b19">[20]</ref> and regularization techniques <ref type="bibr" target="#b36">[34,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">38]</ref>.</p><p>In particular, to prevent a CNN from focusing too much on a small set of intermediate activations or on a small region on input images, random feature removal regularizations have been proposed. Examples include dropout <ref type="bibr" target="#b36">[34]</ref> for randomly dropping hidden activations and regional dropout <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b54">51,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2]</ref> for erasing random regions on the input. Researchers have shown that the feature removal strategies improve generalization and localization by letting a model attend not only to the most discriminative parts of objects, but rather to the entire object region <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>While regional dropout strategies have shown improvements of classification and localization performances to a certain degree, deleted regions are usually zeroed-out <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">33]</ref> or filled with random noise <ref type="bibr" target="#b54">[51]</ref>, greatly reducing the proportion of informative pixels on training images. We recognize this as a severe conceptual limitation as CNNs are generally data hungry <ref type="bibr" target="#b27">[27]</ref>. How can we maximally utilize the deleted regions, while taking advantage of better generalization and localization using regional dropout?</p><p>We address the above question by introducing an augmentation strategy CutMix. Instead of simply removing pixels, we replace the removed regions with a patch from another image (See <ref type="table" target="#tab_0">Table 1</ref>). The ground truth labels are also mixed proportionally to the number of pixels of combined images. CutMix now enjoys the property that there is no uninformative pixel during training, making training efficient, while retaining the advantages of regional dropout to attend to non-discriminative parts of objects. The added patches further enhance localization ability by requiring the model to identify the object from a partial view. The training and inference budgets remain the same.</p><p>CutMix shares similarity with Mixup <ref type="bibr" target="#b51">[48]</ref> which mixes two samples by interpolating both the image and labels. While certainly improving classification performance, Mixup samples tend to be unnatural (See the mixed image in <ref type="table" target="#tab_0">Table 1</ref>). CutMix overcomes the problem by replacing the image region with a patch from another training image. <ref type="table" target="#tab_0">Table 1</ref> gives an overview of Mixup <ref type="bibr" target="#b51">[48]</ref>, Cutout <ref type="bibr" target="#b2">[3]</ref>, and CutMix on image classification, weakly supervised localization, and transfer learning to object detection methods. Although Mixup and Cutout enhance ImageNet classification, they decrease the ImageNet localization or object detection performances. On the other hand, CutMix consistently achieves significant enhancements across three tasks.</p><p>We present extensive evaluations of CutMix on various CNN architectures, datasets, and tasks. Summarizing the key results, CutMix has significantly improved the accuracy of a baseline classifier on CIFAR-100 and has obtained the state-of-the-art top-1 error 14.47%. On ImageNet <ref type="bibr" target="#b31">[31]</ref>, applying CutMix to ResNet-50 and ResNet-101 <ref type="bibr" target="#b11">[12]</ref> has improved the classification accuracy by +2.28% and +1.70%, respectively. On the localization front, CutMix improves the performance of the weakly-supervised object localization (WSOL) task on CUB200-2011 <ref type="bibr" target="#b46">[44]</ref> and ImageNet <ref type="bibr" target="#b31">[31]</ref> by +5.4% and +0.9%, respectively. The superior localization capability is further evidenced by fine-tuning a detector and an image caption generator on CutMix-ImageNetpretrained models; the CutMix pretraining has improved the overall detection performances on Pascal VOC <ref type="bibr" target="#b5">[6]</ref> by +1 mAP and image captioning performance on MS-COCO <ref type="bibr" target="#b22">[23]</ref> by +2 BLEU scores. CutMix also enhances the model robustness and alleviates the over-confidence issue <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> of deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Regional dropout: Methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b54">51]</ref> removing random regions in images have been proposed to enhance the generalization performance of CNNs. Object localization methods <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b1">2]</ref> also utilize the regional dropout techniques for improving the localization ability of CNNs. CutMix is similar to those methods, while the critical difference is that the removed regions are filled with patches from another training images. DropBlock <ref type="bibr" target="#b7">[8]</ref> has generalized the regional dropout to the feature space and have shown enhanced generalizability as well. CutMix can also be performed on the feature space, as we will see in the experiments. Synthesizing training data: Some works have explored synthesizing training data for further generalizability. Generating new training samples by Stylizing ImageNet <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b6">7]</ref> has guided the model to focus more on shape than texture, leading to better classification and object detection performances. CutMix also generates new samples by cutting and pasting patches within mini-batches, leading to performance boosts in many computer vision tasks; unlike stylization as in <ref type="bibr" target="#b6">[7]</ref>, CutMix incurs only negligible additional cost for training. For object detection, object insertion methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref> have been proposed as a way to synthesize objects in the background. These methods aim to train a good represent of a single object samples, while CutMix generates combined samples which may contain multiple objects. Mixup: CutMix shares similarity with Mixup <ref type="bibr" target="#b51">[48,</ref><ref type="bibr" target="#b43">41]</ref> in that both combines two samples, where the ground truth label of the new sample is given by the linear interpolation of one-hot labels. As we will see in the experiments, Mixup samples suffer from the fact that they are locally ambiguous and unnatural, and therefore confuses the model, especially for localization. Recently, Mixup variants <ref type="bibr" target="#b44">[42,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">40]</ref> have been proposed; they perform feature-level interpolation and other types of transformations. Above works, however, generally lack a deep analysis in particular on the localization ability and transfer-learning performances. We have verified the benefits of CutMix not only for an image classification task, but over a wide set of localization tasks and transfer learning experiments. Tricks for training deep networks: Efficient training of deep networks is one of the most important problems in computer vision community, as they require great amount of compute and data. Methods such as weight decay, dropout <ref type="bibr" target="#b36">[34]</ref>, and Batch Normalization <ref type="bibr" target="#b17">[18]</ref> are widely used to efficiently train deep networks. Recently, methods adding noises to the internal features of CNNs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b49">46]</ref> or adding extra path to the architecture <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> have been proposed to enhance image classification performance. CutMix is complementary to the above methods because it operates on the data level, without changing internal representations or architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CutMix</head><p>We describe the CutMix algorithm in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Algorithm</head><p>Let x ∈ R W ×H×C and y denote a training image and its label, respectively. The goal of CutMix is to generate a new training sample (x,ỹ) by combining two training samples (x A , y A ) and (x B , y B ). The generated training sample (x,ỹ) is used to train the model with its original loss function. We define the combining operation as</p><formula xml:id="formula_0">x = M x A + (1 − M) x B y = λy A + (1 − λ)y B ,<label>(1)</label></formula><p>where M ∈ {0, 1} W ×H denotes a binary mask indicating where to drop out and fill in from two images, 1 is a binary mask filled with ones, and is element-wise multiplication. Like Mixup <ref type="bibr" target="#b51">[48]</ref>, the combination ratio λ between two data points is sampled from the beta distribution Beta(α, α). In our all experiments, we set α to 1, that is λ is sampled from the uniform distribution (0, 1). Note that the major difference is that CutMix replaces an image region with a patch from another training image and generates more locally natural image than Mixup does. To sample the binary mask M, we first sample the bounding box coordinates B = (r x , r y , r w , r h ) indicating the cropping regions on x A and x B . The region B in x A is removed and filled in with the patch cropped from B of x B .</p><p>In our experiments, we sample rectangular masks M whose aspect ratio is proportional to the original image. The box coordinates are uniformly sampled according to:</p><formula xml:id="formula_1">r x ∼ Unif (0, W ) , r w = W √ 1 − λ, r y ∼ Unif (0, H) , r h = H √ 1 − λ<label>(2)</label></formula><p>making the cropped area ratio rwr h W H = 1 − λ. With the cropping region, the binary mask M ∈ {0, 1} W ×H is decided by filling with 0 within the bounding box B, otherwise 1.</p><p>In each training iteration, a CutMix-ed sample (x,ỹ) is generated by combining randomly selected two training samples in a mini-batch according to Equation <ref type="bibr" target="#b0">(1)</ref>. Codelevel details are presented in Appendix A. CutMix is simple and incurs a negligible computational overhead as existing data augmentation techniques used in <ref type="bibr" target="#b38">[36,</ref><ref type="bibr" target="#b15">16]</ref>; we can efficiently utilize it to train any network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discussion</head><p>What does model learn with CutMix? We have motivated CutMix such that full object extents are considered as cues for classification, the motivation shared by Cutout, while ensuring two objects are recognized from partial views in a single image to increase training efficiency. To verify that CutMix is indeed learning to recognize two objects from  their respective partial views, we visually compare the activation maps for CutMix against Cutout <ref type="bibr" target="#b2">[3]</ref> and Mixup <ref type="bibr" target="#b51">[48]</ref>. <ref type="figure" target="#fig_1">Figure 1</ref> shows example augmentation inputs as well as corresponding class activation maps (CAM) <ref type="bibr" target="#b55">[52]</ref> for two classes present, Saint Bernard and Miniature Poodle. We use vanilla ResNet-50 model 1 for obtaining the CAMs to clearly see the effect of augmentation method only.</p><p>We observe that Cutout successfully lets a model focus on less discriminative parts of the object, such as the belly of Saint Bernard, while being inefficient due to unused pixels. Mixup, on the other hand, makes full use of pixels, but introduces unnatural artifacts. The CAM for Mixup, as a result, shows that the model is confused when choosing cues for recognition. We hypothesize that such confusion leads to its suboptimal performance in classification and localization, as we will see in Section 4.</p><p>CutMix efficiently improves upon Cutout by being able to localize the two object classes accurately. We summarize the key differences among Mixup, Cutout, and CutMix in <ref type="table" target="#tab_1">Table 2</ref>. Analysis on validation error: We analyze the effect of CutMix on stabilizing the training of deep networks. We compare the top-1 validation error during the training with CutMix against the baseline. We train ResNet-50 <ref type="bibr" target="#b11">[12]</ref> for ImageNet Classification, and PyramidNet-200 <ref type="bibr" target="#b10">[11]</ref> for CIFAR-100 Classification. <ref type="figure" target="#fig_2">Figure 2</ref> shows the results.</p><p>We observe, first of all, that CutMix achieves lower validation errors than the baseline at the end of training. At epoch 150 when the learning rates are reduced, the baselines suffer from overfitting with increasing validation error. CutMix, on the other hand, shows a steady decrease in validation error; diverse training samples reduce overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate CutMix for its capability to improve localizability as well as generalizability of a trained model on multiple tasks. We first study the effect of Cut-Mix on image classification (Section 4.1) and weakly supervised object localization (Section 4.2). Next, we show the transferability of a CutMix pre-trained model when it is fine-tuned for object detection and image captioning tasks (Section 4.3). We also show that CutMix can improve the model robustness and alleviate the model over-confidence in Section 4.4.</p><p>All experiments were implemented and evaluated on NAVER Smart Machine Learning (NSML) <ref type="bibr" target="#b18">[19]</ref> platform with PyTorch <ref type="bibr" target="#b29">[29]</ref>. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">ImageNet Classification</head><p>We evaluate on ImageNet-1K benchmark <ref type="bibr" target="#b31">[31]</ref>, the dataset containing 1.2M training images and 50K validation images of 1K categories. For fair comparison, we use the standard augmentation setting for ImageNet dataset such as resizing, cropping, and flipping, as done in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">37]</ref>. We found that regularization methods including Stochastic   Depth <ref type="bibr" target="#b16">[17]</ref>, Cutout <ref type="bibr" target="#b2">[3]</ref>, Mixup <ref type="bibr" target="#b51">[48]</ref>, and CutMix require a greater number of training epochs till convergence. Therefore, we have trained all the models for 300 epochs with initial learning rate 0.1 decayed by factor 0.1 at epochs 75, 150, and 225. The batch size is set to 256. The hyperparameter α is set to 1. We report the best performances of CutMix and other baselines during training. We briefly describe the settings for baseline augmentation schemes. We set the dropping rate of residual blocks to 0.25 for the best performance of Stochastic Depth <ref type="bibr" target="#b16">[17]</ref>. The mask size for Cutout <ref type="bibr" target="#b2">[3]</ref> is set to 112 × 112 and the location for dropping out is uniformly sampled. The performance of DropBlock <ref type="bibr" target="#b7">[8]</ref> is from the original paper and the difference from our setting is the training epochs which is set to 270. Manifold Mixup <ref type="bibr" target="#b44">[42]</ref> applies Mixup operation on the randomly chosen internal feature map. We have tried α = 0.5 and 1.0 for Mixup and Manifold Mixup and have chosen 1.0 which has shown better performances. It is also possible to extend CutMix to feature-level augmentation (Feature Cut-Mix). Feature CutMix applies CutMix at a randomly chosen layer per minibatch as Manifold Mixup does. Comparison against baseline augmentations: Results are given in <ref type="table" target="#tab_3">Table 3</ref>. We observe that CutMix achieves   <ref type="bibr" target="#b14">[15]</ref> and GE <ref type="bibr" target="#b13">[14]</ref> boosts +1.56% and +1.80%, respectively. Note that unlike above architectural boosts improvements due to Cut-Mix come at little or memory or computational time. CutMix for Deeper Models: We have explored the performance of CutMix for the deeper networks, ResNet-101 <ref type="bibr" target="#b11">[12]</ref> and ResNeXt-101 (32×4d) <ref type="bibr" target="#b48">[45]</ref>, on ImageNet. As seen in <ref type="table" target="#tab_4">Table 4</ref>, we observe +1.60% and +1.71% respective improvements in top-1 errors due to CutMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">CIFAR Classification</head><p>We set mini-batch size to 64 and training epochs to 300. The learning rate was initially set to 0.25 and decayed by the factor of 0.1 at 150 and 225 epoch. To ensure the effectiveness of the proposed method, we used a strong baseline, PyramidNet-200 <ref type="bibr" target="#b10">[11]</ref> with widening factorα = 240. It has 26.8M parameters and achieves the state-of-the-art performance 16.45% top-1 error on CIFAR-100. <ref type="table" target="#tab_6">Table 5</ref> shows the performance comparison against other state-of-the-art data augmentation and regularization methods. All experiments were conducted three times and the averaged best performances during training are reported.   <ref type="table">Table 7</ref>: Impact of CutMix on CIFAR-10.</p><p>Hyper-parameter settings: We set the hole size of Cutout <ref type="bibr" target="#b2">[3]</ref> to 16 × 16. For DropBlock <ref type="bibr" target="#b7">[8]</ref>, keep prob and block size are set to 0.9 and 4, respectively. The drop rate for Stochastic Depth <ref type="bibr" target="#b16">[17]</ref> is set to 0.25. For Mixup <ref type="bibr" target="#b51">[48]</ref>, we tested the hyper-parameter α with 0.5 and 1.0. For Manifold Mixup <ref type="bibr" target="#b44">[42]</ref>, we applied Mixup operation at a randomly chosen layer per minibatch. Combination of regularization methods: We have evaluated the combination of regularization methods. Both Cutout <ref type="bibr" target="#b2">[3]</ref> and label smoothing <ref type="bibr" target="#b40">[38]</ref> does not improve the accuracy when adopted independently, but they are effective when used together. Dropblock <ref type="bibr" target="#b7">[8]</ref>, the feature-level generalization of Cutout, is also more effective when label smoothing is also used. Mixup <ref type="bibr" target="#b51">[48]</ref> and Manifold Mixup <ref type="bibr" target="#b44">[42]</ref> achieve higher accuracies when Cutout is applied on input images. The combination of Cutout and Mixup tends to generate locally separated and mixed samples since the cropped regions have less ambiguity than those of the vanilla Mixup. The superior performance of Cutout and Mixup combination shows that mixing via cutand-paste manner is better than interpolation, as much evidenced by CutMix performances. CutMix achieves 14.47% top-1 classification error on CIFAR-100, +1.98% higher than the baseline performance 16.45%. We have achieved a new state-of-the-art performance 13.81% by combining CutMix and ShakeDrop <ref type="bibr" target="#b49">[46]</ref>, a regularization that adds noise on intermediate features.</p><p>CutMix for various models: <ref type="table" target="#tab_8">Table 6</ref> shows CutMix also significantly improves the performance of the weaker baseline architectures, such as PyramidNet-110 <ref type="bibr" target="#b10">[11]</ref> and ResNet-110. CutMix for CIFAR-10: We have evaluated CutMix on CIFAR-10 dataset using the same baseline and training setting for CIFAR-100. The results are given in <ref type="table">Table 7</ref>. On   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Ablation Studies</head><p>We conducted ablation study in CIFAR-100 dataset using the same experimental settings in Section 4.1.2. We evaluated CutMix with α ∈ {0.1, 0.25, 0.5, 1.0, 2.0, 4.0}; the results are given in <ref type="figure" target="#fig_3">Figure 3</ref>, left plot. For all α values considered, CutMix improves upon the baseline (16.45%). The best performance is achieved when α = 1.0. The performance of feature-level CutMix is given in <ref type="figure" target="#fig_3">Figure 3</ref>, right plot. We changed the layer on which Cut-Mix is applied, from image layer itself to higher feature levels. We denote the index as (0=image level, 1=after first conv-bn, 2=after layer1, 3=after layer2, 4=after layer3). CutMix achieves the best performance when it is applied on the input images. Again, feature-level Cut-Mix except the layer3 case improves the accuracy over the baseline (16.45%).</p><p>We explore different design choices for CutMix. <ref type="table" target="#tab_10">Table 8</ref> shows the performance of CutMix variations. 'Center Gaussian CutMix' samples the box coordinates r x , r y of Equation (2) according to the Gaussian distribution with mean at the image center, instead of the original uniform distribution. 'Fixed-size CutMix' fixes the size of cropping region (r w , r h ) at 16 × 16 (i.e. λ = 0.75). 'Scheduled Cut-Mix' linearly increases the probability to apply CutMix as  training progresses, as done by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, from 0 to 1. 'Onehot CutMix' decides the mixed target label by committing to the label of greater patch portion (single one-hot label), rather than using the combination strategy in Equation <ref type="formula" target="#formula_0">(1)</ref>. 'Complete-label CutMix' assigns the mixed target label as y = 0.5y A + 0.5y B regardless of the combination ratio λ.</p><p>The results show that above variations lead to performance degradation compared to the original CutMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weakly Supervised Object Localization</head><p>Weakly supervised object localization (WSOL) task aims to train the classifier to localize target objects by using only the class labels. To localize the target well, it is important to make CNNs extract cues from full object regions and not focus on small discriminant parts of the target. Learning spatially distributed representation is thus the key for improving performance on WSOL task. CutMix guides a classifier to attend to broader sets of cues to make decisions; we expect CutMix to improve WSOL performances of classifiers. To measure this, we apply CutMix over baseline WSOL models. We followed the training and evaluation strategy of existing WSOL methods <ref type="bibr" target="#b52">[49,</ref><ref type="bibr" target="#b53">50,</ref><ref type="bibr" target="#b1">2]</ref> with VGG-GAP and ResNet-50 as the base architectures. The quantitative and qualitative results are given in <ref type="table" target="#tab_12">Table 9</ref> and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection</head><p>Image Captioning SSD <ref type="bibr" target="#b24">[24]</ref> (mAP) Faster-RCNN <ref type="bibr" target="#b30">[30]</ref> (mAP) NIC <ref type="bibr" target="#b45">[43]</ref> (BLEU-1) NIC   tion 3.2, more ambiguity in Mixup samples make a classifier focus on even more discriminative parts of objects, leading to decreased localization accuracies. Although Cutout <ref type="bibr" target="#b2">[3]</ref> improves the accuracy over the baseline, it is outperformed by CutMix: +2.03% and +0.56% on CUB200-2011 and ImageNet, respectively. CutMix also achieves comparable localization accuracies on CUB200-2011 and ImageNet, even when compared against the dedicated state-of-the-art WSOL methods <ref type="bibr" target="#b55">[52,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b52">49,</ref><ref type="bibr" target="#b53">50,</ref><ref type="bibr" target="#b1">2]</ref> that focus on learning spatially dispersed representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Learning of Pretrained Model</head><p>ImageNet pre-training is de-facto standard practice for many visual recognition tasks. We examine whether Cut-Mix pre-trained models leads to better performances in certain downstream tasks based on ImageNet pre-trained models. As CutMix has shown superiority in localizing less discriminative object parts, we would expect it to lead to boosts in certain recognition tasks with localization elements, such as object detection and image captioning. We evaluate the boost from CutMix on those tasks by replacing the backbone network initialization with other ImageNet pre-trained models using Mixup <ref type="bibr" target="#b51">[48]</ref>, Cutout <ref type="bibr" target="#b2">[3]</ref>, and CutMix. ResNet-50 is used as the baseline architecture in this section.</p><p>Transferring to Pascal VOC object detection: Two popular detection models, SSD <ref type="bibr" target="#b24">[24]</ref> and Faster RCNN <ref type="bibr" target="#b30">[30]</ref>, are considered. Originally the two methods have utilized VGG-16 as backbones, but we have changed it to ResNet-50. The ResNet-50 backbone is initialized with various ImageNetpretrained models and then fine-tuned on Pascal VOC 2007 and 2012 <ref type="bibr" target="#b5">[6]</ref> trainval data. Models are evaluated on VOC 2007 test data using the mAP metric. We follow the fine-tuning strategy of the original methods <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b30">30]</ref>; implementation details are in Appendix C. Results are shown in <ref type="table" target="#tab_0">Table 10</ref>. Pre-training with Cutout and Mixup has failed to improve the object detection performance over the vanilla pre-trained model. However, the pre-training with CutMix improves the performance of both SSD and Faster-RCNN. Stronger localizability of the CutMix pre-trained models leads to better detection performances. Transferring to MS-COCO image captioning: We used Neural Image Caption (NIC) <ref type="bibr" target="#b45">[43]</ref> as the base model for image captioning experiments. We have changed the backbone network of encoder from GoogLeNet <ref type="bibr" target="#b45">[43]</ref> to ResNet-50. The backbone network is initialized with various ImageNet pre-trained models, and then trained and evaluated on MS-COCO dataset <ref type="bibr" target="#b22">[23]</ref>. Implementation details and evaluation metrics (METEOR, CIDER, etc.) are in Appendix D. <ref type="table" target="#tab_0">Table 10</ref> shows the results. CutMix outperforms Mixup and Cutout in both BLEU1 and BLEU4 metrics. Simply replacing backbone network with our CutMix pre-trained model gives performance gains for object detection and image captioning tasks at no extra cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Robustness and Uncertainty</head><p>Many researches have shown that deep models are easily fooled by small and unrecognizable perturbations on the input images, a phenomenon referred to as adversarial attacks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">39]</ref>. One straightforward way to enhance robustness and uncertainty is an input augmentation by generating unseen samples <ref type="bibr" target="#b26">[26]</ref>. We evaluate robustness and uncertainty improvements due to input augmentation methods including Mixup, Cutout, and CutMix. Robustness: We evaluate the robustness of the trained models to adversarial samples, occluded samples, and in-between class samples. We use ImageNet pre-trained ResNet-50 models with same setting as in Section 4.1.1.</p><p>Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b8">[9]</ref> is used to gen-  erate adversarial perturbations and we assume that the adversary has full information of the models (white-box attack). We report top-1 accuracies after attack on ImageNet validation set in <ref type="table" target="#tab_0">Table 11</ref>. CutMix significantly improves the robustness to adversarial attacks compared to other augmentation methods.</p><p>For occlusion experiments, we generate occluded samples in two ways: center occlusion by filling zeros in a center hole and boundary occlusion by filling zeros outside of the hole. In <ref type="figure" target="#fig_6">Figure 5a</ref>, we measure the top-1 error by varying the hole size from 0 to 224. For both occlusion scenarios, Cutout and CutMix achieve significant improvements in robustness while Mixup only marginally improves it. Interestingly, CutMix almost achieves a comparable performance as Cutout even though CutMix has not observed any occluded sample during training unlike Cutout.</p><p>Finally, we evaluate the top-1 error of Mixup and CutMix in-between samples. The probability to predict neither two classes by varying the combination ratio λ is illustrated in <ref type="figure" target="#fig_6">Figure 5b</ref>. We randomly select 50, 000 in-between samples in ImageNet validation set. In both experiments, Mixup and CutMix improve the performance while improvements due to Cutout are almost negligible. Similarly to the previous occlusion experiments, CutMix even improves the robustness to the unseen Mixup in-between class samples. Uncertainty: We measure the performance of the out-ofdistribution (OOD) detectors proposed by <ref type="bibr" target="#b12">[13]</ref> which determines whether the sample is in-or out-of-distribution by score thresholding. We use PyramidNet-200 trained on CIFAR-100 datasets with same setting as in Section 4.1.2. In <ref type="table" target="#tab_0">Table 12</ref>, we report the averaged OOD detection performances against seven out-of-distribution samples from <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>, including TinyImageNet, LSUN <ref type="bibr" target="#b50">[47]</ref>, uniform noise, Method TNR at TPR 95% AUROC Detection Acc.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced CutMix for training CNNs with strong classification and localization ability. CutMix is easy to implement and has no computational overhead, while being surprisingly effective on various tasks. On ImageNet classification, applying CutMix to ResNet-50 and ResNet-101 brings +2.28% and +1.70% top-1 accuracy improvements. On CIFAR classification, CutMix significantly improves the performance of baseline by +1.98% leads to the state-of-the-art top-1 error 14.47%. On weakly supervised object localization (WSOL), CutMix substantially enhances the localization accuracy and has achieved comparable localization performances as the state-of-the-art WSOL methods. Furthermore, simply using CutMix-ImageNetpretrained model as the initialized backbone of the object detection and image captioning brings overall performance improvements. Finally, we have shown that CutMix results in improvements in robustness and uncertainty of image classifiers over the vanilla model as well as other regularized models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CutMix Algorithm</head><p>We present the code-level description of CutMix algorithm in Algorithm A1. N, C, and K denote the size of minibatch, channel size of input image, and the number of classes. First, CutMix shuffles the order of the minibatch input and target along the first axis of the tensors. And the lambda and the cropping region (x1,x2,y1,y2) are sampled. Then, we mix the input and input s by replacing the cropping region of input to the region of input s. The target label is also mixed by interpolating method.</p><p>Note that CutMix is easy to implement with few lines (from line 4 to line 15), so is very practical algorithm giving significant impact on a wide range of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weakly-supervised Object Localization</head><p>We describe the training and evaluation procedure of weakly-supervised object localization in detail. Network modification: Basically weakly-supervised object localization (WSOL) has the same training strategy as image classification does. Training WSOL is starting from ImageNet-pretrained model. From the base network structures, VGG-16 and ResNet-50 <ref type="bibr" target="#b11">[12]</ref>, WSOL takes larger spatial size of feature map 14 × 14 whereas the original models has 7 × 7. For VGG network, we utilize VGG-GAP, which is a modified VGG-16 introduced in <ref type="bibr" target="#b55">[52]</ref>. For ResNet-50, we modified the final residual block (layer4) to have no stride (= 1), which originally has stride 2.</p><p>Since the network is modified and the target dataset could be different from ImageNet <ref type="bibr" target="#b31">[31]</ref>, the last fullyconnected layer is randomly initialized with the final output dimension of 200 and 1000 for CUB200-2011 <ref type="bibr" target="#b46">[44]</ref> and ImageNet, respectively. Input image transformation: For fair comparison, we used the same data augmentation strategy except Mixup, Cutout, and CutMix as the state-of-the-art WSOL methods do <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b52">49]</ref>. In training, the input image is resized to 256 × 256 size and randomly cropped 224 × 224 size images are used to train network. In testing, the input image is resized to 256 × 256, cropped at center with 224 × 224 size and used to validate the network, which called single crop strategy. Estimating bounding box: We utilize class activation mapping (CAM) <ref type="bibr" target="#b55">[52]</ref> to estimate the bounding box of an object. First we compute CAM of an image, and next, we decide the foreground region of the image by binarizing the CAM with a specific threshold. The region with intensity over the threshold is set to 1, otherwise to 0. We use the threshold as a specific rate σ of the maximum intensity of the CAM. We set σ to 0.15 for all our experiments. From the binarized foreground map, the tightest box which can cover the largest connected region in the foreground map is selected to the bounding box for WSOL.</p><p>Evaluation metric: To measure the localization accuracy of models, we report top-1 localization accuracy (Loc), which is used for ImageNet localization challenge <ref type="bibr" target="#b31">[31]</ref>. For top-1 localization accuracy, intersection-over-union (IoU) between the estimated bounding box and ground truth position is larger than 0.5, and, at the same time, the estimated class label should be correct. Otherwise, top-1 localization accuracy treats the estimation was wrong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. CUB200-2011</head><p>CUB-200-2011 dataset <ref type="bibr" target="#b46">[44]</ref> contains over 11 K images with 200 categories of birds. We set the number of training epochs to 600. For ResNet-50, the learning rate for the last fully-connected layer and the other were set to 0.01 and 0.001, respectively. For VGG network, the learning rate for the last fully-connected layer and the other were set to 0.001 and 0.0001, respectively. The learning rate is decaying by the factor of 0.1 at every 150 epochs. We used SGD optimizer, and the minibatch size, momentum, weight decay were set to 32, 0.9, and 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. ImageNet dataset</head><p>ImageNet-1K <ref type="bibr" target="#b31">[31]</ref> is a large-scale dataset for general objects consisting of 13 M training samples and 50 K validation samples. We set the number of training epochs to 20. The learning rate for the last fully-connected layer and the other were set to 0.1 and 0.01, respectively. The learning rate is decaying by the factor of 0.1 at every 6 epochs. We used SGD optimizer, and the minibatch size, momentum, weight decay were set to 256, 0.9, and 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transfer Learning to Object Detection</head><p>We evaluate the models on the Pascal VOC 2007 detection benchmark <ref type="bibr" target="#b5">[6]</ref> with 5 K test images over 20 object categories. For training, we use both VOC2007 and VOC2012 trainval (VOC07+12). Finetuning on SSD 2 <ref type="bibr" target="#b24">[24]</ref>: The input image is resized to 300 × 300 (SSD300) and we used the basic training strategy of the original paper such as data augmentation, prior boxes, and extra layers. Since the backbone network is changed from VGG16 to ResNet-50, the pooling location conv4 3 of VGG16 is modified to the output of layer2 of ResNet-50. For training, we set the batch size, learning rate, and training iterations to 32, 0.001, and 120 K, respectively. The learning rate is decayed by the factor of 0.1 at 80 K and 100 K iterations. Finetuning on Faster-RCNN 3 <ref type="bibr" target="#b30">[30]</ref>: Faster-RCNN takes fully-convolutional structure, so we only modify the backbone from VGG16 to ResNet-50. The batch size, learning rate, training iterations are set to 8, 0.01, and 120 K. The Algorithm A1 Pseudo-code of CutMix 1: for each iteration do <ref type="bibr">2:</ref> input, target = get minibatch(dataset) input is N×C×W×H size tensor, target is N×K size tensor. <ref type="bibr">3:</ref> if mode == training then <ref type="bibr">4:</ref> input s, target s = shuffle minibatch(input, target) CutMix starts here. model update() 21: end for learning rate is decayed by the factor of 0.1 at 100 K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transfer Learning to Image Captioning</head><p>MS-COCO dataset <ref type="bibr" target="#b22">[23]</ref> contains 120 K trainval images and 40 K test images. From the base model NIC 4 <ref type="bibr" target="#b45">[43]</ref>, the backbone model is changed from GoogLeNet to ResNet-50. For training, we set batch size, learning rate, and training epochs to 20, 0.001, and 100, respectively. For evaluation, the beam size is set to 20 for all the experiments. Image captioning results with various metrics are shown in <ref type="table" target="#tab_0">Table A1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robustness and Uncertainty</head><p>In this section, we describe the details of the experimental setting and evaluation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Robustness</head><p>We evaluate the model robustness to adversarial perturbations, occlusion and in-between samples using Ima-geNet trained models. For the base models, we use ResNet-50 structure and follow the settings in Section 4.1.1. For comparison, we use ResNet-50 trained without any additional regularization or augmentation techniques, ResNet-50 trained by Mixup strategy, ResNet-50 trained by Cutout strategy and ResNet-50 trained by our proposed CutMix strategy. <ref type="bibr" target="#b3">4</ref> https://github.com/stevehuanghe/image captioning Fast Gradient Sign Method (FGSM): We employ Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b8">[9]</ref> to generate adversarial samples. For the given image x, the ground truth label y and the noise size , FGSM generates an adversarial sample as the followingx = x + sign (∇ x L(θ, x, y)) ,</p><p>where L(θ, x, y) denotes a loss function, for example, cross entropy function. In our experiments, we set the noise scale = 8/255. Occlusion: For the given hole size s, we make a hole with width and height equals to s in the center of the image. For center occluded samples, we zeroed-out inside of the hole and for boundary occluded samples, we zeroed-out outside of the hole. In our experiments, we test the top-1 ImageNet validation accuracy of the models with varying hole size from 0 to 224. In-between class samples: To generate in-between class samples, we first sample 50, 000 pairs of images from the ImageNet validation set. For generating Mixup samples, we generate a sample x from the selected pair x A and x B by x = λx A + (1 − λ)x B . We report the top-1 accuracy on the Mixup samples by varying λ from 0 to 1. To generate CutMix in-between samples, we employ the center mask instead of the random mask. We follow the hole generation process used in the occlusion experiments. We evaluate the top-1 accuracy on the CutMix samples by varing hole size s from 0 to 224.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Class activation mapping (CAM)<ref type="bibr" target="#b55">[52]</ref> visualizations on 'Saint Bernard' and 'Miniature Poodle' samples using various augmentation techniques. From top to bottom rows, we show the original images, input augmented image, CAM for class 'Saint Bernard', and CAM for class 'Miniature Poodle', respectively. Note that CutMix can take advantage of the mixed region on image, but Cutout cannot.Mixup Cutout CutMixUsage of full image region Regional dropout Mixed image &amp; label</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Top-1 test error plot for CIFAR100 (left) and Im-ageNet (right) classification. Cutmix achieves lower test errors than the baseline at the end of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Impact of α and CutMix layer depth on CIFAR-100 top-1 error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 ,</head><label>4</label><figDesc>respectively. Full implementation details are in Appendix B. Comparison against Mixup and Cutout: CutMix outperforms Mixup<ref type="bibr" target="#b51">[48]</ref> on localization accuracies by +5.51% and +1.41% on CUB200-2011 and ImageNet, respectively. Mixup degrades the localization accuracy of the baseline model; it tends to make a classifier focus on small regions as shown inFigure 4. As we have hypothesized in Sec-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison of the baseline (ResNet-50), Mixup, Cutout, and CutMix for weakly supervised object localization task on CUB-200-2011 dataset. Ground truth and predicted bounding boxes are denoted as red and green, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Analysis for in-between class samples Robustness experiments on the ImageNet validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>-18.6) 71.3 (-10.7) CutMix 69.0 (+42.7) 94.4 (+7.1) 89.1 (+7.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>, :, x1:x2, y1:y2] = input s[:, :, x1:x2, y1:y2] 15: lambda = 1 -(x2-x1)*(y2-y1)/(W*H) Adjust lambda to the exact area ratio. 16: target = lambda * target + (1 -lambda) * target s CutMix ends. loss(output, target) 20:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of the results of Mixup, Cutout, and our CutMix on ImageNet classification, ImageNet localization, and Pascal VOC 07 detection (transfer learning with SSD<ref type="bibr" target="#b24">[24]</ref> finetuning) tasks. Note that CutMix significantly improves the performance on various tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison among Mixup, Cutout, and CutMix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>ImageNet classification results based on ResNet-50 model. '*' denotes results reported in the original papers.</figDesc><table><row><cell>Model</cell><cell># Params</cell><cell>Top-1 Err (%)</cell><cell>Top-5 Err (%)</cell></row><row><cell>ResNet-101 (Baseline) [12]</cell><cell>44.6 M</cell><cell>21.87</cell><cell>6.29</cell></row><row><cell>ResNet-101 + Cutout [3]</cell><cell>44.6 M</cell><cell>20.72</cell><cell>5.51</cell></row><row><cell>ResNet-101 + Mixup [48]</cell><cell>44.6 M</cell><cell>20.52</cell><cell>5.28</cell></row><row><cell>ResNet-101 + CutMix</cell><cell>44.6 M</cell><cell>20.17</cell><cell>5.24</cell></row><row><cell>ResNeXt-101 (Baseline) [45]</cell><cell>44.1 M</cell><cell>21.18</cell><cell>5.57</cell></row><row><cell>ResNeXt-101 + CutMix</cell><cell>44.1 M</cell><cell>19.47</cell><cell>5.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Impact of CutMix on ImageNet classification for ResNet-101 and ResNext-101.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of state-of-the-art regularization methods on CIFAR-100. the best result, 21.40% top-1 error, among the considered augmentation strategies. CutMix outperforms Cutout and Mixup, the two closest approaches to ours, by +1.53% and +1.18%, respectively. On the feature level as well, we find CutMix preferable to Mixup, with top-1 errors 21.78% and 22.50%, respectively. Comparison against architectural improvements: We have also compared improvements due to CutMix versus architectural improvements (e.g. greater depth or additional modules). We observe that CutMix improves the performance by +2.28% while increased depth (ResNet-50 → ResNet-152) boosts +1.99% and SE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Impact of CutMix on lighter architectures on CIFAR-100.</figDesc><table><row><cell>PyramidNet-200 (α=240)</cell><cell>Top-1 Error (%)</cell></row><row><cell>Baseline</cell><cell>3.85</cell></row><row><cell>+ Cutout</cell><cell>3.10</cell></row><row><cell>+ Mixup (α=1.0)</cell><cell>3.09</cell></row><row><cell>+ Manifold Mixup (α=1.0)</cell><cell>3.15</cell></row><row><cell>+ CutMix</cell><cell>2.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Performance of CutMix variants on CIFAR-100.CIFAR-10, CutMix also enhances the classification perfor- mances by +0.97%, outperforming Mixup and Cutout per- formances.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Weakly supervised object localization results on CUB200-2011 and ImageNet. * denotes results reported in the original papers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Impact of CutMix on transfer learning of pretrained model to other tasks, object detection and image captioning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table /><note>Top-1 accuracy after FGSM white-box attack on ImageNet validation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>: Out-of-distribution (OOD) detection results with</cell></row><row><cell>CIFAR-100 trained models. Results are averaged on seven</cell></row><row><cell>datasets. All numbers are in percents; higher is better.</cell></row><row><cell>Gaussian noise, etc. More results are illustrated in Ap-</cell></row><row><cell>pendix E. Mixup and Cutout augmentations aggravate the</cell></row><row><cell>over-confidence of the base networks. Meanwhile, CutMix</cell></row><row><cell>significantly alleviates the over-confidence of the model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table A1 :</head><label>A1</label><figDesc>Image captioning results on MS-COCO dataset.</figDesc><table><row><cell>Method</cell><cell>TNR at TPR 95%</cell><cell>AUROC</cell><cell>Detection Acc. TNR at TPR 95%</cell><cell>AUROC</cell><cell>Detection Acc.</cell></row><row><cell></cell><cell></cell><cell>TinyImageNet</cell><cell cols="3">TinyImageNet (resize)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use ImageNet-pretrained ResNet-50 provided by PyTorch<ref type="bibr" target="#b29">[29]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/amdegroot/ssd.pytorch 3 https://github.com/jwyang/faster-rcnn.pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Clova AI Research team, especially Jung-Woo Ha and Ziad Al-Halah for their helpful feedback and discussion.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Uncertainty</head><p>Deep neural networks are often overconfident in their predictions. For example, deep neural networks produce high confidence number even for random noise <ref type="bibr" target="#b12">[13]</ref>. One standard benchmark to evaluate the overconfidence of the network is Out-of-distribution (OOD) detection proposed by <ref type="bibr" target="#b12">[13]</ref>. The authors proposed a threshold-baed detector which solves the binary classification task by classifying in-distribution and out-of-distribution using the prediction of the given network. Recently, a number of reserchs are proposed to enhance the performance of the baseline detector <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> but in this paper, we follow only the baseline detector algorithm without any input enhancement and temperature scaling <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup:</head><p>We compare the OOD detector performance using CIFAR-100 trained models described in Section 4. Evaluation Metrics and Out-of-distributions: In this work, we follow the experimental setting used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. To measure the performance of the OOD detector, we report the true negative rate (TNR) at 95% true positive rate (TPR), the area under the receiver operating characteristic curve (AUROC) and detection accuracy of each OOD detector. We use seven datasets for out-of-distribution: TinyIma-geNet (crop), TinyImageNet (resize), LSUN <ref type="bibr" target="#b50">[47]</ref> (crop), LSUN (resize), iSUN, Uniform noise and Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>We report OOD detector performance to seven OODs in <ref type="table">Table A2</ref>. Overall, CutMix outperforms baseline, Mixup and Cutout. Moreover, we find that even though Mixup and Cutout outperform the classification performance, Mixup and Cutout largely degenerate the baseline detector performance. Especially, for Uniform noise and Gaussian noise, Mixup and Cutout seriously impair the baseline performance while CutMix dramatically improves the performance. From the experiments, we observe that our proposed CutMix enhances the OOD detector performance while Mixup and Cutout produce more overconfident predictions to OOD samples than the baseline.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling visual context is key to augmenting object detection datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="364" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1301" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10750" to="10760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02499</idno>
		<title level="m">Mixup as locally linear out-of-manifold regularization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9423" to="9433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">NSML: meet the mlaas platform with a real-world case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nako</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<idno>abs/1810.09957</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<publisher>Andrej Karpathy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved mixedexample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael J Dinneen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1262" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ricap: Random image cropping and patching data augmentation for deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Between-class learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5486" to="5494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<title level="m">Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Kise</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02375</idno>
		<title level="m">Shakedrop regularization for deep residual learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-produced guidance for weaklysupervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Out-of-distribution (OOD) detection results on TinyImageNet, LSUN, iSUN, Gaussian noise and Uniform noise using CIFAR-100 trained models. All numbers are in percents</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>higher is better</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
