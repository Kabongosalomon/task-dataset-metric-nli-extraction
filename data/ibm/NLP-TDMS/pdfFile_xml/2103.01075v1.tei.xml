<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OmniNet: Omnidirectional Representations from Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jai</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
						</author>
						<title level="a" type="main">OmniNet: Omnidirectional Representations from Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes Omnidirectional Representations from Transformers (OMNINET). In OmniNet, instead of maintaining a strictly horizontal receptive field, each token is allowed to attend to all tokens in the entire network. This process can also be interpreted as a form of extreme or intensive attention mechanism that has the receptive field of the entire width and depth of the network. To this end, the omnidirectional attention is learned via a meta-learner, which is essentially another self-attention based model. In order to mitigate the computationally expensive costs of full receptive field attention, we leverage efficient self-attention models such as kernel-based (Choromanski et al., 2020), low-rank attention ) and/or Big Bird (Zaheer et al., 2020 as the meta-learner. Extensive experiments are conducted on autoregressive language modeling (LM1B, C4), Machine Translation, Long Range Arena (LRA), and Image Recognition. The experiments show that OmniNet achieves considerable improvements across these tasks, including achieving state-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena. Moreover, using omnidirectional representation in Vision Transformers leads to significant improvements on image recognition tasks on both few-shot learning and fine-tuning setups.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, characterized by stacked self-attention modules and feed-forward transformations, have become a staple in modern deep learning, natural language processing <ref type="bibr" target="#b12">(Devlin et al., 2018;</ref><ref type="bibr" target="#b30">Raffel et al., 2019)</ref> and even computer vision <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>. One key defining characteristics in the self-attention mechanism is the global receptive field in which each token is accessible to every other token in the sequence, serving as an enabler for learning global contextual representations. This paper proposes learning omnidirectional representations from transformers. The key idea is to move beyond horizontally global receptive fields and explore the possibility of omnidirectional receptive fields. In short, we allow each token to not only attend to all other tokens in the same layer, but also all token in all the layers of the network. This global access enables tokens to have a full view of the network and as a result access the knowledge and intermediate representations of every token at each layer. By modeling the relationships amongst tokens of different hierarchical levels, we are also able to capture patterns pertaining to the propagation of representations across time. Finally, this approach can be also be interpreted as a form of dense residual connection <ref type="bibr" target="#b18">(Huang et al., 2017)</ref>, which has been shown to be beneficial by aiding gradient flow.</p><p>Learning omnidirectional receptive fields is non-trivial for two key reasons. Firstly, given the quadratic complexity of the scaled dot product attention, the complexity of designing such a receptive field is increased from N 2 L to (N L) 2 , where L is the depth of the network and N is the sequence length. We postulate that this is one big challenge that has prohibited this type of architecture from being explored in the past. Secondly, simply enabling omnidirectional attention from the get-go would easily cause a degeneration of the transformer into a flat network, losing much of its representational power that is enabled by sequentially refining its representations across network layers.</p><p>To mitigate these issues, our omnidirectional attention is implemented as a form of meta-learner that acts upon a standard transformer model. The meta-learner is yet another self-attention model that accepts all hidden representations across all layers as an input and refines them based on all the available information. In order to mitigate the prohibitive memory and computational costs of omnidirectional attention, we explore and evaluate multiple efficient alternatives of parameterizing the meta-learner, e.g., including fast attention via generalizable kernel attention <ref type="bibr" target="#b9">(Choromanski et al., 2020)</ref>, low-rank self-attention , and/or block-based sparsity <ref type="bibr" target="#b39">(Zaheer et al., 2020)</ref>. Additionally, we further hypothesize that employing methods that try to learn the low-rank factorized structure of the entire network can lead to improved generalization capabilitiesas demonstrated in our few-shot learning experiments.</p><p>Aside from varying the parameterization of the meta-learner, we also introduce partitioned variants of OmniNet in which the meta-learner is applied to blocks of p consecutive layers. In short, this partitioning strategy groups the full network of L layers into L p partitions. After computing each partition, the meta-learner learns the omnidirectional attention of all nodes across all layers in the partition.</p><p>Via extensive experiments, we show that OmniNet achieves very promising results on a myriad of language, vision, and logic tasks. Specifically, we report strong experimental results on autoregressive language modeling <ref type="bibr" target="#b7">(Chelba et al., 2013;</ref><ref type="bibr" target="#b30">Raffel et al., 2019)</ref>, five collections of WMT machine translation, Long Range Arena <ref type="bibr" target="#b35">(Tay et al., 2020a)</ref>, and Image Recognition using Vision Transformers <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>. Moreover, we systematically evaluate OmniNets through the lens of the performance-compute trade-off and show that they are pareto-optimal in this regard.</p><p>On machine translation, OmniNet outperforms ADMIN <ref type="bibr" target="#b24">(Liu et al., 2020)</ref>, the current state-of-the-art 60 layers deep transformer model on two well-established machine translation collections (WMT'14 English-German and WMT'14 English-French). On the one billion language modeling benchmark, OmniNet outperforms existing state-of-the-art models such as Transformer-XL <ref type="bibr" target="#b10">(Dai et al., 2019)</ref>. On LRA, OmniNet improves aggregate performance over Performers <ref type="bibr" target="#b9">(Choromanski et al., 2020)</ref> by +8.9% and vanilla Transformers by +2.6%. On Image Recognition tasks, OmniNet demonstrates stellar few-shot learning and finetuning performance, outperforming ViT <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref> by up to ≈ +3% on both finetuning and few-shot learning experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Just across the past several years, attention mechanisms <ref type="bibr" target="#b2">(Bahdanau et al., 2014)</ref> have made a significant impact on machine learning research <ref type="bibr" target="#b37">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b12">Devlin et al., 2018;</ref><ref type="bibr" target="#b13">Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b30">Raffel et al., 2019;</ref><ref type="bibr" target="#b5">Brown et al., 2020;</ref><ref type="bibr" target="#b11">Dehghani et al., 2018)</ref>. Simply speaking, these parameterized pooling mechanisms learn to align representations and route information based on the notion of relative importance. While early work in this area was mainly concerned with learning an alignment function between two or more sequences <ref type="bibr" target="#b2">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b27">Parikh et al., 2016)</ref>, there have been more focus on self-alignment (e.g., self-attention) in the recent research climate <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>.</p><p>Attention mechanisms are generally applied layer-wise and operate across a one-dimensional sequence. Attention is generally bidirectional, or unidirectional in the case where a token is to be denied access to future tokens. There have been early attempts to mix information across layers in pursuit of improving gradient flow and model trainability. For example, <ref type="bibr" target="#b3">(Bapna et al., 2018)</ref> proposed transparent attention in which the decoder gains access to all encoder layers. <ref type="bibr" target="#b16">(He et al., 2018)</ref> proposed layer-wise coordination between encoder-decoder for machine translation. <ref type="bibr" target="#b34">(Tay et al., 2018)</ref> proposed to densely connect the attention across stacked RNN encoder layers for language understanding tasks. The recent Realformer (residual attention) <ref type="bibr" target="#b15">(He et al., 2020)</ref> proposed connecting the attention activations in a residual fashion. We believe there is sufficient evidence in the literature to suggest that mixing representations across layers is beneficial. This is further supported by fundamental work in this area such as ResNets <ref type="bibr" target="#b14">(He et al., 2016)</ref>, highway networks <ref type="bibr" target="#b32">(Srivastava et al., 2015)</ref> and DenseNets <ref type="bibr" target="#b18">(Huang et al., 2017)</ref>.</p><p>In this paper, we are mainly interested in methods for efficiently learning omnidirectional attention -an attention over the entire width and depth of the network. To this end, we leverage the recent advances in making transformers fast and efficient <ref type="bibr" target="#b39">(Zaheer et al., 2020;</ref><ref type="bibr" target="#b9">Choromanski et al., 2020;</ref><ref type="bibr" target="#b38">Wang et al., 2020)</ref>. Many of these approaches learn an approximation via low-rank projection, kernels or block-based sparsity. An overview and extensive empirical comparison can be found at <ref type="bibr" target="#b36">(Tay et al., 2020b;</ref><ref type="bibr">a)</ref>. To this end, the proposed approach leverages these recent advances to make what was previously not possible. By leveraging fast and efficient self-attention, we enable scalable and powerful omnidirectional attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>This section introduces OmniNet. We first begin by reviewing the standard Transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformer Architectures</head><p>This section provides a brief background for the Transformer architecture. The Transformer block accepts N × d input, where N denotes the number of tokens in the sequence and d denotes the size of the representation. Each Transformer block is characterized by a self-attention block and a two layered feed-forward network with ReLU activations in-between that is applied position-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">SELF-ATTENTION</head><p>The self-attention mechanism first projects each input X into Q,K,V representations using linear transformations, corresponding to queries, keys, and values. The self-attention mechanism is typically multi-headed where multiple similar linear projections are executed in parallel.  each self-attention head h at layer l is written as:</p><formula xml:id="formula_0">y h,l = softmax Q h,l K h,l √ d k V h,l ,<label>(1)</label></formula><p>where y h,l is the output of head h at layer l and d k is the size of each head. The output from the multiple heads is then concatenated and then passed through another linear transformation via W o,l which projects the concatenation of all heads down to d m . This is wrapped via a layer normalization followed by a residual connection and can be written as: LayerNorm(W o,l concat([y 1,l ···y H,l ))) + x l−1 as the final output of the self-attention module.</p><p>Feed Forward Layers The FFN block of the Transformer block performs a two layer transformation defined as:</p><formula xml:id="formula_1">z l = LayerNorm(W 1,l ReLU(W 2,l (Y )))+z l−1 ,<label>(2)</label></formula><p>where W 1 ,W 2 are trainable parameters (weight transforms) of the FFN layer. Bias parameters are omitted for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">OmniNet</head><p>The proposed OmniNet method operates on an arbitrary multi-layered architecture that accepts sequential inputs. In our description, this typically refers to a stacked X-former architecture in this section. Note that while this is typically a transformer model, it can also be an arbitrary variant <ref type="bibr" target="#b9">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b38">Wang et al., 2020)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates a brief overview of the proposed OmniNet architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">OMNIDIRECTIONAL REPRESENTATIONS</head><p>In a stacked network of L layers, each layer exposes a sequence of N vectors of d dimensions each. Specifically, OmniNet operates across all layers and connects the multi-layered network architecture in a grid like fashion. We describe the network as xformer which accepts X as an input and returns a tensor of L×N ×d dimensions.</p><formula xml:id="formula_2">xformer(X) = X 1 ,X 2 ···X L ,<label>(3)</label></formula><p>where X i ∈ R N ×d . Let X i j be the representation of X at layer i and position j of the sequence. The OmniNet mechanism can be written as:</p><formula xml:id="formula_3">O = Attend(IndexSort(X 1 ,X 2 ,···X L )),<label>(4)</label></formula><p>where Attend denotes an arbitrary self-attention block. The IndexSort operation takes X 1 , X 2 , X L and sorts, 1 tokens within each matrix by index such that the adjacent token of the i th token in layer l are the i th token from l − 1 and l + 1 respectively. Next, given that the input sequence length is LN , it is advantageous for Attend to be as efficient as possible. We describe three variants of OmniNet's core linear-time self-attention mechanism in subsequent sections.</p><p>Given O ∈ R (L×N )×d , the output of the omnidirectional attention, we perform P (.) a pooling operator. While there are many choices of pooling operators, parameterized or otherwise, we adopt a simple pooling function -a max pooling of stride L.</p><formula xml:id="formula_4">O = MaxPool1D(O),<label>(5)</label></formula><p>where O ∈ R N ×d . Given O , the final representation of an OmniNet augmented network is defined as:</p><formula xml:id="formula_5">OmniNet(X) = xformer(X) L +O .<label>(6)</label></formula><p>The OmniNet and main transformer model are trained together in an end-to-end fashion, i.e., gradients flow to both networks concurrently at each backward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">MAINTAINING CAUSALITY AND AUTOREGRESSIVE DECODING</head><p>A key point to note with IndexSort is that this order enables us to apply a causal mask to the Attend function, namely if tokens are sorted according to sequence index first as opposed to layer first, then it would be easy to apply a causal mask M , where M [i,j] = 0 when i ≤ j and −inf when i &gt; j. This enables OmniNet to be used in autoregressive settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">EFFICIENT TRANSFORMERS</head><p>We describe several choices of linear-time self-attention mechanisms that are used in OmniNet's omnidirectional attention. Generally, Attend refers to an attention block with an attention function and a two-layered positional FFN in a similar structure to the transformer backbone. For the sake of brevity, we only describe the core attention mechanism here. Our choice of the efficient transformer is informed by <ref type="bibr" target="#b35">(Tay et al., 2020a)</ref> selecting models that perform well on the compute-performance trade-off. For a list of potential variants to adopt, we refer readers to <ref type="bibr" target="#b36">(Tay et al., 2020b)</ref>.</p><p>Kernel-based This variant uses the generalizable kernel attention, the fast attention mechanism proposed in Performer <ref type="bibr" target="#b9">(Choromanski et al., 2020)</ref>. Specifically, this is written as:</p><formula xml:id="formula_6">o = W o concat(D h −1 (φ(Q h )(φ(K h )) V h )), whereD h = diagφ(Q h )((φ(K h )) 1 L ) and φ(.)</formula><p>is a random feature map that projects R d to R r . We refer readers to <ref type="bibr" target="#b9">(Choromanski et al., 2020)</ref> for more details.</p><p>Low-rank Inspired by Linformer's  self-attention mechanism, we set Attend to be:</p><formula xml:id="formula_7">o = W o (concat(softmax Q h (W K h ) √ d k (W V h )),</formula><p>where W ∈ R N ×k are low-rank projection transformations that are shared across heads and across keys and values. The complexity of this self-attention mechanism is N k instead of N 2 , where k ≪ N .</p><p>Block and Memory based Lastly, we also explore a block and memory-based variant of efficient Transformers based on Big Bird <ref type="bibr" target="#b39">(Zaheer et al., 2020)</ref>. In short, this is a combination of windowed attention, global attention, and sparse attention. The output for token i is defined as:</p><formula xml:id="formula_8">o i = x i + H h=1 softmax Q h,i K h,N (i) V h,i ,</formula><p>where N (i) is the neighborhood function which denotes the out-neighbors of node i, H is the total number of heads and h represents a head. The neighborhood function is mainly dependent on the width of the windowed attention. We refer the reader to <ref type="bibr" target="#b39">(Zaheer et al., 2020)</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">PARTITIONED OMNINET</head><p>This section describes the types of partitioning variants that we explore in OmniNet. When L is large, the eventual representation input to OmniNet can be extremely large. 2 Let P be an integer valued hyperparameter that determines the partition size. For a L layer transformer network, when mod P is 0, we insert a meta-learner block. X = Attend(X −P ,···X −1 )), if modP = 0 xformer(X −1 )</p><p>In short, whenever mod P = 0, we activate an omnidirectional attention layer, aggregating representations all the way from the previous partition − P layer up till − 1. In this case, we skip the original xformer layer, hence maintaining approximately the same parameter size of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on autoregressive language modeling, machine translation, long range sequence modeling and a series of image recognition tasks. Our implementation uses Flax <ref type="bibr" target="#b17">(Heek et al., 2020)</ref> and Jax <ref type="bibr" target="#b4">(Bradbury et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Autoregressive Language Modeling</head><p>We run experiments on large-scale autoregressive (unidirectional) language modeling. We use two large-scale datasets, language modeling one billion (LM1B) <ref type="bibr" target="#b7">(Chelba et al., 2013)</ref> and the Colossal Cleaned Common Crawl corpus (C4) <ref type="bibr" target="#b30">(Raffel et al., 2019)</ref>.  <ref type="bibr">(Ott et al., 2018)</ref> 28.6 41.4 60L Trans. <ref type="bibr" target="#b24">(Liu et al., 2020)</ref> 29  <ref type="bibr" target="#b9">(Choromanski et al., 2020)</ref>, and BigBird <ref type="bibr" target="#b39">(Zaheer et al., 2020)</ref>. We also add the recent Realformer (residual attention Transformer) <ref type="bibr" target="#b15">(He et al., 2020)</ref> as a strong baseline. For OmniNet, we tune the partition amongst {2,3,6}. All models have approximately 50M parameters. Next, we are interested in (1) how OmniNet scales to large sizes and (2) comparing with other published works <ref type="bibr" target="#b10">(Dai et al., 2019)</ref>. Hence, we implement a larger OmniNet with MLPs of size 8K and head size of 2K. <ref type="table" target="#tab_1">Table 1</ref> reports results on LM. We observe that OmniNet P,T outperforms all baselines by about +9.1% on LM1b and +4.2% on C4. We also outperform strong baselines such as Realformer, BigBird, and vanilla Transformers on both corpora. We also observe that OmniNet P performs reasonably close to OmniNet T , which ascertains that using an efficient Transformer may be sufficient for omnidirectional attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">RESULTS ON LANGUAGE MODELING</head><p>On the other hand, <ref type="table">Table 2</ref> reports a comparison with other published works on LM1B. Notably, OmniNet P,T (large) outperforms Transformer-XL and achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Neural Machine Translation</head><p>We conduct experiments on machine translation, a sequenceto-sequence task. for evaluating Transformer models.    <ref type="bibr" target="#b24">(Liu et al., 2020)</ref>, a 60-layer deep transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Long Range Arena</head><p>We conduct experiments on the recently proposed Long Range Arena benchmark <ref type="bibr" target="#b35">(Tay et al., 2020a)</ref>. The goal of this experiment is to show that OmniNet improves long-range sequence modeling. A dual goal is to show that it is possible to combine different inductive biases to obtain a better efficient Transformer model that is versatile on different types of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">EXPERIMENTAL SETUP</head><p>We run two key experiments using Transformer and Performer as the main backbone model and vary the metalearner in OmniNet, using Linformer and Performer variants of the OmniNet meta-learner. The goal is to demonstrate that OmniNet translates to backbone agnostic improvements. We run OmniNet experiments using the LRA codebase and run OmniNet models using the same hyperparameters as the results reported in <ref type="bibr" target="#b35">(Tay et al., 2020a)</ref>. Note that LRA is comprised of five benchmarks, however, we omit Image and Pathfinder experiments since the best hyperparameters on these tasks turn out to be shallow (1-2 layered) models. We report the average of the text, retrieval, and ListOps tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">RESULTS ON LRA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image Recognition</head><p>Transformer-based models started showing competitive performance on different vision tasks like classification, object detection, and segmentation <ref type="bibr" target="#b8">(Chen et al., 2020;</ref><ref type="bibr" target="#b13">Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b6">Carion et al., 2020;</ref><ref type="bibr" target="#b22">Kumar et al., 2021)</ref>.</p><p>To showcase the power of omnidirectional representations in yet another task, we incorporate the omnidirectional represen-tation in Vision Transformer (ViT) <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>, when pre-trained on a large amount of data in a supervised fashion and evaluated on downstream image recognition tasks, either through few-shot learning or fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">VISION TRANSFORMER</head><p>Vision Transformers (ViT) <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref> have recently shown impressive results on image classification compared to state-of-the-art convolutional networks, while they require significantly fewer computational resources to train. ViT is a standard Transformer that is directly applied to images. To do so, we first split the input images into nonoverlapping patches and embedded them using a linear projection. The patch embeddings are provided as a sequence of tokens to a Transformer. When pre-trained on large datasets (14M-300M images) at a sufficient scale, ViT shows excellent results that are transferable to tasks with fewer data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">EXPERIMENTAL SETUP</head><p>Similar to the ViT setup, we pre-train our OmniNet models on the JFT dataset <ref type="bibr" target="#b33">(Sun et al., 2017)</ref> with 18k classes and 303M images, for 7 epochs. We evaluate our models in the transfer setup (few-shot and fine-tuning) on several downstream tasks: ImageNet, CIFAR-10, CIFAR-100 <ref type="bibr" target="#b20">(Krizhevsky et al., 2009</ref>), Oxford-IIIT Pets <ref type="bibr" target="#b28">(Parkhi et al., 2012)</ref>, and Oxford Flowers-102 <ref type="bibr" target="#b25">(Nilsback &amp; Zisserman, 2008)</ref>. We follow the pre-processing from <ref type="bibr" target="#b19">(Kolesnikov et al., 2019)</ref>  In our OmniNet models, we replace the final layer of ViT with an omnidirectional layer. In other words, we set the portion size P = 12. In this task, we limit our experiments to using Performers <ref type="bibr" target="#b9">(Choromanski et al., 2020)</ref> in the omnidirectional attention, given their strong results among the efficient transformer variants.</p><p>During pre-training, we use a batch size of 4096 using Adam with β 1 = 0.9 and β 2 = 0.999, and use a weight decay of 0.05 for OmniNet. We use a learning rate of 8e−4 with a linear decay and a linear warmup of 10K steps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">RESULTS ON IMAGE RECOGNITION</head><p>We first present the results of OmniNet and corresponding ViT models as baselines in the fine-tuning setup. For fine-tuning, we use SGD with momentum and a batch size 512 in all downstream tasks. <ref type="table" target="#tab_9">Table 6</ref> presents the results of fine-tuning experiments. We also report the total pre-training compute, in terms of number of FLOPs for each model. As we can see, introducing a module that learns omnidirectional representations to Vision Transformers leads to improvements on different downstream tasks. Given these improvements and comparing the number of FLOPs for OmniNets and ViTs, we can see that the additional compute, thanks to efficient attention techniques, is fairly reasonable.</p><p>For evaluating OmniNet in the few-shot learning setup, similar to ViT, we train a linear classifier on top of the representations from the frozen pre-trained model, given only a subset of training examples. Plots in <ref type="figure" target="#fig_1">Figure 2</ref> illustrate the accuracy of OmniNet and ViT, using different numbers of shots. The results indicate that adding omnidirectional representations to ViT leads to better transfer across all downstream datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Effect of Partition</head><p>Size and Compute/Performance Trade-offs</p><p>OmniNet offers the flexibility to apply the Omnidirectional layers on different partition sizes. With smaller partition sizes, we attend to tokens from fewer layers, and with bigger partition, we widen the vertical receptive field of the omnidirectional attention, which might be effective for learning better representations by capturing information from more levels. In terms of computational costs, however, there is a trade-off when choosing the partition size. Small partition sizes means running attention on smaller sequences while repeating it more frequent, and bigger partition sizes means dealing with longer sequences, but having fewer omnidirectional layers in the network.</p><p>We train OmniNet B/32 and OmniNet B/16 with different partition sizes: P = {1,2,4,6,12}. Partition size P = 1 is simply having no vertical attention and it is just replacing normal attention in ViT, with Performer. We compare these  models in terms of their linear 5-shot accuracy on ImageNet dataset (similar to the ablation studies in <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>). <ref type="figure" target="#fig_2">Figure 3</ref> presents the performance of each model as well as their computational cost during pre-training.</p><p>A few patterns can be observed. For both OmniNet B/32 and OmniNet B/16 , the power of omnidirectional directional representations kicks in when we work with partition sizes of more than 2. The input resolution during pre-training is 224 × 224, so for /32 and /16 models the input sequence length to the model is 49 and 196. So when setting P = 1 or P = 2, with such sequence lengths, when using an efficient attention engine, like Performer, which provides an approximation of the dot-product-attention, we do not gain a lot on the speed and we lose a bit of performance. However, when using a larger partition size, the additional compute with respect to the performance gain becomes reasonable.</p><p>In both /32 and /16, the computation cost is almost similar for P = 4 and P = 6. With P = 4, we have three omnidirectional attention, each applied on 4 layers, while with P = 6 we have two omnidirectional attention, each applied on 6 layers. However, P = 6 gives slightly better results in terms of accuracy and is placed on a sweeter spot in this trade-off. With P = 12, the computational costs of OmniNet increase, but the gain in the performance helps the model to be on the frontier of the compute-performance trade-off, when it is compared to OmniNet B/32 and OmniNet B/16 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization</head><p>OmniNet combines information from different layers via two sequential mechanisms ( §3.2.1): (1) omnidirectional attention, where representations of all tokens in all layers get updated with respect to each other using an efficient attention mechanism; and (2) a pooling operation, where for each token, we collect the best values from all layers.</p><p>In order to understand how these two mechanisms combine information across different layers, we visualize attention maps <ref type="bibr" target="#b0">(Abnar &amp; Zuidema, 2020)</ref> and pooling statistics for a set of examples in the image recognition task. <ref type="figure" target="#fig_3">Figure 4</ref> depicts three example inputs, where we show how OmniNet attends to different layers, as well as each layer's contribution during the pooling operation.</p><p>We can see that in some layers, attention seems to detect the objects in the image via attending to the edges or specific parts of the object, while in other layers, the attention mechanism uses mostly background information. It is clear that omnidirectional attention does indeed use such information by actively attending to layers of varying depth.</p><p>Additionally, when performing the element-wise pool operation over all the layers for each token, only a fraction of values from each layer's representation make it to the final representation. The bottom rows in <ref type="figure" target="#fig_3">Figure 4</ref> illustrate this fraction for each token (image patch) across different layers. In most examples, we observe that a majority of the representation after the pooling operation comes from the first few layers. This is further evidence of how OmniNet can provide an explicit path for directing fine-grained information that is captured by the early layers to the final output, leading to much richer representations. For the sake of brevity, we refer readers to the Appendix for more detailed plots for these examples as well as other examples, which illustrate the same trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed OmniNet, which uses omnidirectional attention to connect all tokens across the entire network via self-attention. In order to manage the computational costs of the full receptive field, the meta-learner in OmniNet is parameterized by fast and efficient self-attention models. The proposed method achieves stellar performance on a myriad of language and vision tasks. Concretely, OmniNet achieves state-of-the-art performance on WMT EnDe and EnFr, outperforming deep 60-layer transformers. OmniNet also demonstrates substantial improvement over ViT on image recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed Experimental Setup</head><p>This section describes several details of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Dataset Specific Setups</head><p>For all experiments, we implement code using Python and JAX. Specifically, the main Transformer blocks and codebase for most experiments are derived from FLAX examples. For WMT'17, we build sentencepiece tokenizers of 32K from the dataset. WMT'17 collections are obtained from Tensorflow datasets (TFDS). For autoregressive language modeling, the C4 corpus is similarly found in TFDS. For both LM1B and C4 tasks, we use a sentencepiece vocab of 32K subwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Efficient Transformer Hyperparameters</head><p>For Xformers (efficient transformers), we use implementations derived from FLAX 5 .For Linformer, we use k = 32 for the low-rank down projection with shared parameters for both key and value. For Performer, we use the default setup from the official implementation. This corresponds to the generalized attention with ReLU activations. We do not use any random features. For BigBird, our codebase similarly links to the official implementation and use the default hyperparameters. The block size is 64 for BigBird and the number of random blocks is 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualisation of Contributions of Layers in Omnidirectional Representations</head><p>Figures 5 to 9 (in subsequent pages) show contributions of different layers in omnidirectional representations in terms of detailed attention maps (attention distribution over all layers, in all heads, when the CLS token in the omnidirectional layer is considered as the query) as well as contribution of different layers in the pooling operation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of OmniNet. In the diagram, the omnidirectional module, when partition size is P = L, combines the information across all positions (1 : N ), across all layers (1 : L−1), and for each position selects the best of all layers via a pooling operation to generate the final representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Performance of pre-trained OmniNet and equivalent ViT models in few-shot learning setup on downstream tasks, when transferred using only few images (1, 5, 10, and 25) per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Performance of ViT and OmniNet (with different partition sizes) in terms of top-1 accuracy on ImageNet 5-shot linear, versus their computational costs in terms of number of FLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Contribution of different layers in Omnidirectional representations for a given set of examples. On top, we plot the omnidirectional attention maps (using OmniNet B/16 -P12 ) of one of the heads, over all layers, when CLS token in the last layer is used as query. On the bottom, we show the contribution of each layer to the pooling operation of the Omnidirectional module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Experimental results (quality, i.e., perplexity scores at 30K and 100K respectively) on autoregressive language modeling. All models are approximately 50M parameters.</figDesc><table><row><cell>Model</cell><cell>LM1B</cell><cell>C4</cell></row><row><cell>Transformer</cell><cell>33.14</cell><cell>34.86</cell></row><row><cell>Realformer</cell><cell>32.95</cell><cell>35.63</cell></row><row><cell>Performer</cell><cell>34.33</cell><cell>35.68</cell></row><row><cell>BigBird</cell><cell>32.90</cell><cell>38.36</cell></row><row><cell>OmniNet B</cell><cell cols="3">33.69 (-1.7%) 34.73 (+0.4%)</cell></row><row><cell>OmniNet P</cell><cell cols="3">30.19 (+9.0%) 33.97 (+2.6%)</cell></row><row><cell>OmniNet T</cell><cell cols="3">30.12 (+9.1%) 33.39 (+4.2%)</cell></row><row><cell cols="4">Table 2. Comparison with existing state-of-the-art and published</cell></row><row><cell cols="4">works on One Billion Word Language modeling (Chelba et al.,</cell></row><row><cell>2013) benchmark.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="2">#Params PPL</cell></row><row><cell cols="2">Adaptive Input (Baevski &amp; Auli)</cell><cell>0.5B</cell><cell>24.1</cell></row><row><cell cols="2">Adaptive Input (Baevski &amp; Auli)</cell><cell>1.0B</cell><cell>23.7</cell></row><row><cell cols="2">Transformer-XL (Dai et al.)</cell><cell>0.5B</cell><cell>23.5</cell></row><row><cell cols="2">Transformer-XL (Dai et al.)</cell><cell>0.8B</cell><cell>21.8</cell></row><row><cell cols="2">OmniNet P (Large)</cell><cell>0.1B</cell><cell>21.6</cell></row><row><cell cols="2">OmniNet B (Large)</cell><cell>0.1B</cell><cell>22.0</cell></row><row><cell cols="2">OmniNet T (Large)</cell><cell>0.1B</cell><cell>21.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Results on five collections from the WMT'17 machine translation task. +0.7%) 20.8 (+1.5%) 22.8 (+2.7%) 43.3 (+0.7%) 36.2 (+1.1%) OmniNet B28.8 (+0.7%) 20.9 (+2.0%) 22.6 (+1.8%) 43.2 (+0.5%) 34.2 (-4.5%) OmniNet P 29.0 (+1.4%) 20.9 (+2.0%) 23.0 (+3.6%) 43.1 (+0.2%) 36.2 (+1.1%) Comparisons with the state-of-the-art on WMT'14 En-De and WMT'14 En-Fr. OmniNet outperforms ADMIN<ref type="bibr" target="#b24">(Liu et al., 2020)</ref>, the current state-of-the-art deep transformer model for MT.</figDesc><table><row><cell>Model</cell><cell cols="2">En-De</cell><cell>En-Fi</cell><cell>Cs-En</cell><cell>En-Fr</cell><cell>Ru-En</cell></row><row><cell>Transformer.</cell><cell>28.6</cell><cell></cell><cell>20.5</cell><cell>22.2</cell><cell>43.0</cell><cell>35.8</cell></row><row><cell cols="2">OmniNet L 28.8 (Model</cell><cell cols="2">En-De En-Fr</cell><cell></cell><cell></cell></row><row><cell cols="2">Evolved Trans. (So et al., 2019)</cell><cell>29.2</cell><cell>n/a</cell><cell></cell><cell></cell></row><row><cell>Large Trans.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Results on Long Range Arena<ref type="bibr" target="#b35">(Tay et al., 2020a)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="4">Text Retrieval ListOps Avg</cell></row><row><cell>Linformer</cell><cell>53.9</cell><cell>52.3</cell><cell>35.7</cell><cell>47.3</cell></row><row><cell>BigBird</cell><cell>64.0</cell><cell>54.7</cell><cell>36.1</cell><cell>51.6</cell></row><row><cell>Performer</cell><cell>65.4</cell><cell>53.8</cell><cell>18.0</cell><cell>45.7</cell></row><row><cell cols="2">+OmniNet P 65.6</cell><cell>60.9</cell><cell>18.2</cell><cell>48.2</cell></row><row><cell cols="2">+OmniNet L 63.1</cell><cell>63.7</cell><cell>37.1</cell><cell>54.6</cell></row><row><cell cols="2">Transformer 62.5</cell><cell>57.5</cell><cell>36.4</cell><cell>52.1</cell></row><row><cell cols="2">+OmniNet P 65.1</cell><cell>58.8</cell><cell>37.2</cell><cell>53.7</cell></row><row><cell cols="2">+OmniNet L 63.1</cell><cell>63.8</cell><cell>37.2</cell><cell>54.7</cell></row><row><cell cols="3">4.2.1. EXPERIMENTAL SETUP</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>We use</cell></row><row><cell cols="5">a SentencePiece (Kudo &amp; Richardson, 2018) vocabulary of</cell></row><row><cell cols="5">32K built for each language specifically. More details can</cell></row><row><cell cols="2">be found in the appendix.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">4.2.2. RESULTS ON WMT'17 MACHINE TRANSLATION</cell></row></table><note>We use five collections/datasets from WMT-17, 3 namely En- De (English → German), En-Cs (English → Czech), En-Fi (English → Finnish), En-Fr (English → French) and En-Ru (English → Russian). We train all models using 16 TPU-V3 chips with a batch size of 256. Our base Transformer model has 6 layers, a hidden size of 4096, embedding size of 1024, and a head size of 1024. The number of heads is 16.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>4.2.3. COMPARISONS AGAINST STATE-OF-THE-ART</cell></row><row><cell>We train a large OmniNet model and compare it with the</cell></row><row><cell>state-of-the-art approaches. We compare with ADMIN (Liu</cell></row><row><cell>et al., 2020), a very deep (60 layers) Transformer model</cell></row><row><cell>that achieves state-of-the-art performance on the WMT</cell></row><row><cell>En-De dataset. We use a 8 layer OmniNet model with 4096</cell></row><row><cell>MLP dimensions, 1024 hidden dimensions and embedding</cell></row><row><cell>dimensions. We compare models using sacrebleu (Post,</cell></row><row><cell>2018). For OmniNet, given the strong performance of the</cell></row><row><cell>Performer variant on WMT'17 collections, we only train a</cell></row></table><note>reports results on all 5 collections of WMT'17. Over- all, OmniNet P outperforms the vanilla Transformer model on all five collections, with up to ≈ +3.6% performance im- provement. Similar to LM, we find that the performer variant works the best and the BigBird variant works the worse.single P variant OmniNet for comparing with SOTA models. Further details of the setup can be found in the appendix.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>reports results on WMT'14 En-De and En-Fr. Our results show that OmniNet outperforms the existing state-of-the-art ADMIN model</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>reports the results on our LRA experiments. Firstly, we observe that OmniNet makes substantial improvements to the base model, regardless of whether it is a Transformer or Performer. Notably, with OmniNet L , the Linformer meta-learner, the Performer model is improved by almost 6 to 7 absolute percentage points. An interesting observation can be made on the ListOps task where Omninet P (Performer variant) does not result in much improvement over the base Performer. However, the performance doubles with OmniNet L . Since the base Linformer model does pretty well on this task, we postulate that this is due to OmniNet L providing a Linformer-like inductive bias to the Performer model. Finally, we observe that OmniNet improves the vanilla Transformer in both cases (P or L), improving the average score by about +2.6% absolute percentage points.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>In our experiments, we train and evaluate OmniNet B/32 and OmniNet B/16 , which are based on ViT B/32 and ViT B/16 . 4 Similar to ViT B/32 and ViT B/16 , OmniNet B/32 and OmniNet B/16 are both "base" models, adopted from BERT, and use patch sizes of 32×32 and 16×16 respectively.</figDesc><table><row><cell>on both</cell></row><row><cell>upstream and downstream datasets, which is used in the</cell></row><row><cell>original ViT experiments.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Transfer performance of pre-trained OmniNet and equivalent ViT models in fine-tuning setup on popular image classification benchmarks. All models are pre-trained on the JFT-300M dataset and fine-tuned on the target dataset. ViT B/32 OmniNet B/32 ViT B/16 OmniNet B/16</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">ImageNet</cell><cell cols="4">0.8073 → 0.8374</cell><cell></cell><cell cols="5">0.8415 → 0.8626</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">CIFAR-10</cell><cell cols="4">0.9861 → 0.9900</cell><cell></cell><cell cols="5">0.9900 → 0.9940</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">CIFAR-100</cell><cell cols="4">0.9049 → 0.9153</cell><cell></cell><cell cols="5">0.9186 → 0.9224</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Oxford-IIIT Pets</cell><cell cols="4">0.9340 → 0.9441</cell><cell></cell><cell cols="5">0.9580 → 0.9674</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Oxford Flowers-102</cell><cell cols="4">0.9927 → 0.9954</cell><cell></cell><cell cols="5">0.9956 → 0.9961</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">exaFLOPs</cell><cell></cell><cell cols="2">165</cell><cell>193</cell><cell></cell><cell></cell><cell cols="2">743</cell><cell>891</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ViTB/32</cell><cell></cell><cell cols="2">ViTB/16</cell><cell>OmniNetB/32</cell><cell cols="4">OmniNetB/16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.70</cell><cell>ImageNet</cell><cell></cell><cell></cell><cell>0.95</cell><cell>CIFAR10</cell><cell></cell><cell></cell><cell>0.80</cell><cell>CIFAR100</cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell>Oxford_IIIT_Pets</cell><cell></cell><cell></cell><cell>1.00</cell><cell cols="2">Oxford Flowers-102</cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.98</cell><cell></cell></row><row><cell>accuracy</cell><cell>0.35 0.40 0.45 0.50 0.55 0.60</cell><cell>1 5 10 num shots</cell><cell>25</cell><cell>accuracy</cell><cell>0.60 0.65 0.70 0.75 0.80 0.85</cell><cell>1 5 10 num shots</cell><cell>25</cell><cell>accuracy</cell><cell>0.45 0.50 0.55 0.60 0.65 0.70</cell><cell>1 5 10 num shots</cell><cell>25</cell><cell>accuracy</cell><cell>0.70 0.75 0.80 0.85</cell><cell cols="2">1 5 10 num shots</cell><cell>25</cell><cell>accuracy</cell><cell>0.90 0.92 0.94 0.96</cell><cell>1 5 10 num shots</cell><cell>25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Since attention is permutation invariant this sorting simply makes it easier to (1) compute casual masks and (2) aggregate representations index-wise.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A sequence length of 1K would result in a 11K input sequence length for a 12 layered Transformer model, when using an omnidirectional layer as the final layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.statmt.org/wmt17/ translation-task.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that SOTA results on the downstream tasks we use here are from ViT H/14<ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>, which has more than seven times as many parameters than the base models we use as baselines. Here, we aim at merely showcasing the gain of using omnidirectional representations in the image recognition task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/google/flax.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10853</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training deeper neural machine translation models with transparent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07561</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<title level="m">Rethinking attention with performers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Ł. Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainslie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Realformer</surname></persName>
		</author>
		<title level="m">Transformer likes residual attention. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Layer-wise coordination between encoder and decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32Nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7955" to="7965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Flax: A neural network library and ecosystem for JAX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rondepierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Zee</surname></persName>
		</author>
		<ptr target="http://github.com/google/flax" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04432</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Colorization transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crafting papers on machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning</title>
		<editor>Langley, P.</editor>
		<meeting>the 17th International Conference on Machine Learning<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep transformers for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07772</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6301</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-6301" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium, Oc</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>tober 2018. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A call for clarity in reporting bleu scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08771</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11117</idno>
		<title level="m">The evolved transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Densely connected attention propagation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04210</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<title level="m">Long range arena: A benchmark for efficient transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<title level="m">Transformers for longer sequences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
