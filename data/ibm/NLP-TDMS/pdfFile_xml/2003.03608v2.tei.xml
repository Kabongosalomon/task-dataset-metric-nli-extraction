<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">DASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/JSTARS.2020.3037893</idno>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Change detection</term>
					<term>high-resolution images</term>
					<term>dual attention</term>
					<term>Siamese network</term>
					<term>weighted double-margin contrastive loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Change detection is a basic task of remote sensing image processing. The research objective is to identify the change information of interest and filter out the irrelevant change information as interference factors. Recently, the rise in deep learning has provided new tools for change detection, which have yielded impressive results. However, the available methods focus mainly on the difference information between multitemporal remote sensing images and lack robustness to pseudo-change information. To overcome the lack of resistance in current methods to pseudo-changes, in this paper, we propose a new method, namely, dual attentive fully convolutional Siamese networks (DASNet), for change detection in high-resolution images. Through the dual attention mechanism, long-range dependencies are captured to obtain more discriminant feature representations to enhance the recognition performance of the model. Moreover, the imbalanced sample is a serious problem in change detection, i.e., unchanged samples are much more abundant than changed samples, which is one of the main reasons for pseudo-changes. We propose the weighted double-margin contrastive loss to address this problem by punishing attention to unchanged feature pairs and increasing attention to changed feature pairs. The experimental results of our method on the change detection dataset (CDD) and the building change detection dataset (BCDD) demonstrate that compared with other baseline methods, the proposed method realizes maximum improvements of 2.9% and 4.2%, respectively, in the F1 score. Our PyTorch implementation is available at https://github.com/lehaifeng/DASNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>urban expansion research, resource management, and damage assessment <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b8">[9]</ref>. With the development of satellite imaging technology, many high-resolution remote sensing images can be obtained more easily. In high-resolution remote sensing images, ground objects have more space and shape features; hence, high-resolution remote sensing images become an important data source for change detection <ref type="bibr" target="#b9">[10]</ref>. The effective extraction and learning of the rich feature information in highscoring remote sensing images, reduction in the interference of pseudo-changes, which means changes that are not truly occurring and changes that do not interest us, and further improvement in the accuracy of change detection are important issues in the field of remote sensing change detection.</p><p>Traditional change detection methods can be divided into two categories according to the research objects: pixel-based change detection methods and object-based change detection methods <ref type="bibr" target="#b10">[11]</ref>. Pixel-based change detection methods typically generate a difference graph by directly comparing the spectral information or texture information of the pixels and obtain the final result graph via threshold segmentation or clustering <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Although this method is simple to implement, it ignores the spatial context information and generates a substantial amount of salt-and-pepper noise during processing. Another type of method divides the remote sensing image into disjoint objects and uses the rich spectral, textural, structural, and geometric information in the image to analyze the differences of temporal images <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Although this type of method uses the spatial context information of high-resolution remote sensing images, traditional manual feature extraction is complex and exhibits poor robustness.</p><p>In recent years, deep learning has yielded impressive results in image analysis, natural language processing, and other fields <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. Neural networks with FCN <ref type="bibr" target="#b26">[27]</ref> structures are widely used in remote sensing change detection tasks <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b32">[33]</ref>. These methods can be divided into two categories: methods in the first category fuse unchanged images and changed images and input them into a network with FCN structure to detect the changes by maximizing the boundaries instead of directly measuring the changes. The methods in the other category detect changes by measuring the distances between feature pairs that are extracted from images.</p><p>However, the available methods have low robustness to pseudo-changes. There are two main reasons for this:</p><p>First, due to the lack of satisfactory features that can effectively distinguish between changed areas and unchanged areas, the available features are often sensitive to factors such as noise, angle, shadow, and context. Second, the distributions of the changed and unchanged data are often severely unbalanced, namely, the number of unchanged samples is much larger than the number of changed samples.</p><p>To address the first problem, we propose the DASNet framework. The basic strategy is to use a dual attention mechanism to locate the changed areas and to obtain more discriminant feature representations, which renders the learned features more robust to changes. Many previous studies <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> have demonstrated that more distinguishable feature representations can be obtained by capturing long-range dependencies <ref type="bibr" target="#b35">[36]</ref>. Attention mechanisms can model long-range dependencies and have been widely used in many tasks <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b41">[42]</ref>. Among them, the self-attention mechanism <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> explores the performances of nonlocal operations of images and videos in the space-time dimension. The self-attention mechanism is of substantial significance for the long-range dependencies of images. Additionally, many researchers <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref> began to combine spatial attention and channel attention so that the network could not only focus on the region of interest but also improve the discrimination of features. Therefore, we introduce an extended attention mechanism <ref type="bibr" target="#b46">[47]</ref>, which is composed of a channel attention module and a spatial attention module, for obtaining more discriminative image features to enhance the model's performance in recognizing changes and to improve its robustness to pseudo-changes.</p><p>Aiming at overcoming the second problem, due to the imbalance of data in change detection and the unbalanced contributions to the network of the changed feature pairs and the unchanged feature pairs in the contrastive loss <ref type="bibr" target="#b47">[48]</ref> of traditional Siamese networks, we established with weighted double-margin contrastive (WDMC) loss. The WDMC loss can mitigate the effects of having more unchanged regions than changed regions in the original data by setting weight coefficients. It can also alleviate the imbalance in the punishment between the unchanged feature pairs and the changed feature pairs during network training by setting double margins.</p><p>The main contributions of this paper are as follows: 1. We propose a new remote sensing change detection method that is based on deep metric learning, which uses dual attention modules to improve feature discrimination to more robustly distinguish changes.</p><p>2. We propose the weighted double-margin contrastive loss (WDMC loss), which increases the distance between changed feature pairs and reduces the distance between unchanged feature pairs while balancing the impacts of changed regions and unchanged regions on the network. Thereby enhancing the network's performance in recognizing change information and its robustness to pseudo-changes..</p><p>3. Compared with the selected baselines, the proposed method yielded SOTA results on the CDD dataset <ref type="bibr" target="#b48">[49]</ref> and the BCDD dataset <ref type="bibr" target="#b49">[50]</ref>, and the F1 scores reached 91.9% and 89.8%, respectively. Compared with other baseline methods, the maximum increases were 2.9% and 4.2%, respectively.</p><p>The remainder of the paper is organized as follows. Section II reviews the related works. Section III describes the proposed method in detail. To evaluate our method, experiments are designed in Section IV, and the proposed method is discussed. Finally, Section V summarizes our work in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Change detection is a basic task in the field of remote sensing, and researchers have developed many change detection technologies for this task. Remote sensing change detection methods typically include feature extraction and changing area identification. The objective of the former is to extract meaningful features, such as color distribution, texture characteristics, and context information. The objective of the latter is to use technical algorithms to analyze previously extracted features to identify the changing regions in multitemporal remote sensing images. Based on the techniques that are utilized in these change detection methods, we divide the remote sensing change detection methods into traditional change detection methods and deep learning change detection methods.</p><p>Traditional change detection methods mainly accept the feature differences and ratios of pairs of pixels or objects as input and detect changes by determining thresholds. Change vector analysis (CVA) <ref type="bibr" target="#b11">[12]</ref> conducts different calculations on the data of each band of images in various periods to obtain the change amount in each pixel in each band to form a change vector, and it identifies the changed and unchanged regions using thresholds. Principal component analysis (PCA) <ref type="bibr" target="#b13">[14]</ref>, which is a classical mathematical transformation method, obtains the difference image by extracting the effective information in the image bands, performs the difference calculation with the first principal component and obtains the change map via threshold segmentation. Multivariate alteration detection (MAD) <ref type="bibr" target="#b50">[51]</ref> extracts change information by maximizing the variance in the image difference. Slow feature analysis (SFA) <ref type="bibr" target="#b51">[52]</ref> can extract invariant features from multitemporal remote sensing image data, reduce the radiation differences of unchanged pixels, and improve the separability between changed and unchanged pixels.</p><p>The remote sensing change detection methods that are based on deep learning use the features that are extracted from multitemporal images by deep neural networks to determine the change information in ground objects. In the field of natural image change detection, deep learning methods perform well <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. In the field of remote sensing, the strategy of using deep learning for change detection has also been utilized. SCCN <ref type="bibr" target="#b27">[28]</ref> uses a deep symmetrical network to study changes in remote sensing images, and DSCN <ref type="bibr" target="#b28">[29]</ref> uses two branch networks that share weights for feature extraction and uses the features that are obtained by the last layer of the two branches for threshold segmentation to obtain a binary change map. CDNet <ref type="bibr" target="#b29">[30]</ref>, FC-EF, FC-Siam-Conc, FC-Siam-Diff in <ref type="bibr" target="#b30">[31]</ref>, and BiDateNet <ref type="bibr" target="#b31">[32]</ref> implement end-to-end training on the change detection dataset and learn the decision boundary to obtain the change map.</p><p>In contrast to the above methods, the proposed method directly measures changes and obtains a more discriminative feature representation through the dual attention module, which can capture long-range dependencies. Our proposed method uses the WDMC loss to improve the degree of intraclass compactness and to balance the influences of the changed regions and the unchanged regions on the network to enhance the recognition performance of the network for the changed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we introduce the proposed network in detail. First, the overall structure of the network that is proposed in this paper is introduced <ref type="figure" target="#fig_0">(Fig. 1</ref>). The spatial attention and channel attention modules are explained subsequently. Finally, the proposed WDMC loss function is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Compared with general optical remote sensing images, high-resolution remote sensing images have more abundant information and higher requirements for feature extraction. We first use the Siam-Conv module to generate local features (F t0 , F t1 ) by inputting high-resolution image pairs that are obtained at different times in the same region. Then, the dual attention mechanism is used to establish the connections between local features, which are used to obtain global context information to better distinguish the changing area from the unchanged area.</p><p>Consider spatial attention as an example, which generates new features that contain spatial long-range contextual information via the following three steps. The first step is to use a spatial attention matrix to model the spatial relationship between any two pixels of the features obtained from Siam-Conv. The next step is to conduct matrix multiplication between the spatial attention matrix and the original features. Third, the final representations are obtained by applying the element-byelement summation operation on the matrix that was obtained in the second step, and the original features. The channel attention module captures the long-range context information in the channel dimension.</p><p>The process of capturing channel relationships is similar to that of spatial attention modules. The first step is to compute the channel attention matrix in the channel dimension. The second step is to conduct matrix multiplication between the channel attentional matrix and the original features. Finally, the matrix that is obtained in the second step and the original features are summed element-by-element. After that, the outputs of the two attention modules are aggregated to obtain a better representation of the features.</p><p>Then, the features A F t0 and A F t1 that are obtained through the dual attention mechanism are mapped to the feature space. We use the WDMC loss function to decrease the pixel pair distance of the unchanged area and to increase the pixel pair distance of the changing area. Our method uses metric learning and uses the distance between deep features as the basis for discrimination, thereby substantially increasing the accuracy of change detection. As discussed previously, discriminant feature representations are important for determining the types of features and, thus, for identifying changed and unchanged areas. However, many studies <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> have shown that using only local features that are generated by traditional FCNs may lead to misclassification. To model the rich context of local features, we introduce a spatial attention module. The spatial attention module encodes the context information of a long range into local features, thereby enhancing the feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial Attention Mechanism</head><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, the feature F ∈ R C×H×W , where C represents the number of channels of feature F, while H and W represent the height and width of feature F, respectively, that is obtained by Siam-Conv is input into 3 convolutional layers that have the same structure to obtain 3 new features, namely, Fa, Fb and Fc, where {Fa, Fb, Fc} ∈ R C×H×W . Then, we reshape Fa and Fb to R C×N , where N = H × W . After that, we conduct matrix multiplication between the transpose of Fb and Fa and obtain the spatial attention map Fs ∈ R N ×N through a softmax layer.</p><formula xml:id="formula_0">F s ji = exp(F a i · F b j ) N i=1 exp(F a i · F b j )<label>(1)</label></formula><p>F s ji can be used to measure the effectiveness of the feature at position i on the feature at position j. The stronger the connection between the two features, the larger the value of F s ji .</p><p>We reshape Fc to R C×N and conduct matrix multiplication with Fs to obtain the result and reshape it to R C×H×W . Finally, we multiply the result from the previous step by a scale parameter η and perform an elementwise summation operation with F to obtain the final output:</p><formula xml:id="formula_1">F sa j = η N i=1 (F s ji F c j ) + F j (2)</formula><p>where η is initialized as 0 and gradually learns to assign more weight. From formula (2), it can be concluded that the resulting feature Fsa at each position is the result of a weighted sum of the features at all positions and the original features. Therefore, it has a global context view and selectively aggregates contexts based on spatial attention maps. Similar semantic features promote each other, which improves the compactness and semantic consistency within the class and enables the network to better distinguish between changes and pseudo-changes. Each high-level feature channel map can be regarded as a response to a ground object, and the semantic responses are related to each other. By utilizing the correlation between channel maps, interdependent feature maps can be enhanced, and feature representations with specified semantics can be improved to better distinguish changes. Therefore, we construct a channel attention module for establishing the relationships between channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Channel Attention Mechanism</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, in contrast to the spatial attention module, the convolution operation is not used to obtain the new features in the channel attention module. Feature F ∈ R C×H×W that was obtained by Siam-Conv is reshaped to R C×N , where N = H × W . After that, matrix multiplication is performed between the transpose of F and F to obtain the channel attention map Fx ∈ R N ×N through a softmax layer:</p><formula xml:id="formula_2">F x ji = exp(F i · F j ) C i=1 exp(F i · F j )<label>(3)</label></formula><p>F x ji can be used to measure the impact of the ith channel on the jth channel. Similarly, the stronger the connection between the two channels, the larger the value of F x ji .</p><p>We reshape F to R C×N and conduct matrix multiplication with Fx to obtain the result. Finally, we multiply the result from the previous step by a scale parameter γ and conduct an elementwise summation operation with F to obtain the final output:</p><formula xml:id="formula_3">F ca j = γ C i=1 (F c ji F i ) + F j<label>(4)</label></formula><p>where γ is initialized as 0 and gradually learns to assign more weight. From formula (4), it is concluded that the final feature of each channel is the result of a weighted sum of the features of all channels and the original feature, which models the long-term semantic dependency between the feature graphs. It enhances the identifiability of the feature and highlights the feature representation of the region of change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. WDMC Loss Function</head><p>In the traditional contrastive loss, a problem of imbalance is encountered in the punishment for the changed feature pairs and the unchanged feature pairs during training. The traditional contrastive loss can be formulated as:</p><formula xml:id="formula_4">contrastive loss = i,j 1 2 [(1 − y i,j ) d 2 i,j + y i,j max (d i,j − m, 0) 2 ]<label>(5)</label></formula><p>We denote the feature map of the unchanged image through our model as f 0 and the feature map of the changed image through our model as f 1 , d i,j is the distance between the feature vectors of f 0 and f 1 at (i,j), m is the margin that is enforced for changed feature pairs, and y ∈ {0, 1}, where y = 0 if the corresponding pixel pair is deemed unchanged, and y = 1 if it is deemed changed.</p><p>According to formula (5), the traditional contrastive loss does not contribute to the loss value only if the distance between the unchanged feature pairs is 0. However, in the remote sensing change detection task, the unchanged area is affected by various imaging conditions, and the imaging difference is sometimes very large; hence, many noise changes will hinder the optimization of the distance between the unchanged feature pairs to 0. However, the changed feature pairs can only contribute to the loss if the distance exceeds the margin, which results in the imbalance of the punishment degree in the training process and affects the network's judgment of the change. In addition, the unchanged area in remote sensing change detection tasks is much larger than the changed area; hence, there is a severe imbalance between the unchanged samples and the changed samples. In response to these problems, we propose the WDMC loss function.</p><p>The proposed WDMC loss can be formulated as follows:</p><formula xml:id="formula_5">W DM C loss = i,j 1 2 [w 1 (1 − y i,j ) max(d i,j − m 1 , 0) 2 + w 2 y i,j max(d i,j − m 2 , 0) 2 ]<label>(6)</label></formula><p>where m 1 and m 2 represent the margins of the unchanged sample pairs and the changed sample pairs, respectively, and w 1 and w 2 represent the weights of the unchanged feature pairs and the changed feature pairs, respectively.</p><formula xml:id="formula_6">w 1 = 1 P U (7) w 2 = 1 P C<label>(8)</label></formula><p>where P U and P C are the frequencies of the changed and unchanged pixel pairs, respectively. According to formula (6), by setting the margins for the unchanged sample pairs, we can alleviate the imbalance in the punishment between the unchanged feature pairs and the changed feature pairs during network training. We set the weight coefficients to mitigate the effects of having more unchanged regions than changed regions in the original data. Finally, these parameters balance the network's interest in the changed area and the unchanged area.</p><p>Inspired by the strategy of deep supervision <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, we calculate the WDMC loss for the feature pairs that are obtained through the spatial attention module, the feature pairs that are obtained by the channel attention module and the final output feature pairs. λ i represents the weight for each loss, L sa represents the WDMC loss that is calculated using the feature pairs that are extracted from the spatial attention module, L ca represents the WDMC loss that is calculated using the feature pairs that are extracted from the channel attention module, and L e represents the WDMC loss that is calculated using the final output feature pairs. Thus, our loss function is ultimately expressed as:</p><formula xml:id="formula_7">Loss = λ 1 L sa + λ 2 L ca + λ 3 L e<label>(9)</label></formula><p>In summary, we balance the contributions of the unchanged feature pairs and the changed feature pairs to the loss value during the training process and use the strategy of deep supervision to enhance the feature representation performance of the hidden layers. The discriminative feature representations improve the network's performance in recognizing changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>Now, we introduce in detail the relevant parameter design in the model design process.</p><p>First, we describe the design of the network structure. In terms of Siam-Conv network structure selection, we chose two basic networks, VGG16 <ref type="bibr" target="#b58">[59]</ref> and ResNet50 <ref type="bibr" target="#b59">[60]</ref>. For VGG16, we keep only the first five convolution modules, and we remove the max-pooling layer of the last module. In the first 5 convolution modules, the size of the convolution kernel is.3 × 3. For ResNet50, we remove the downsampling operations and employ dilated convolutions in the last two ResNet blocks. Then, the dual attention module composed of the spatial attention module and the channel attention module is connected to Siam-Conv to form the complete DSANet.</p><p>In terms of loss design, we set four parameters to balance the contributions of the unchanged region feature pairs and the changed region feature pairs to the network and to enhance the network's performance in the identification of changed regions. The values of parameters w 1 and w 2 are the pixel ratios of the changed area and the unchanged area in the dataset. The parameters m 1 and m 2 must be manually adjusted to optimize the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND DISCUSSION</head><p>To evaluate the performance of the proposed method, we compared other change detection methods on the CDD and BCDD datasets, and we designed ablation experiments for evaluating the proposed structure and improved loss function. Finally, the experimental results are analyzed comprehensively.</p><p>A. Databases 1) CDD Dataset: CDD <ref type="figure" target="#fig_3">(Fig. 4)</ref> is an open remote sensing change detection dataset. The dataset is composed of multisource remote sensing images with 11 original image pairs, which include 7 pairs of seasonal change images with a size of 4, 725 × 2, 200 pixels and 4 pairs of images with a size of 1, 900 × 1, 000 pixels. The resolution varies from 3 cm to 100 cm per pixel, with the seasons varying widely among the bitemporal images. In <ref type="bibr" target="#b49">[50]</ref>, the author processed the original data to generate datasets with training sets of size 10,000 and test and validation sets of size 3,000 via clipping and rotation.</p><p>2) BCDD Dataset: The BCDD dataset <ref type="figure" target="#fig_3">(Fig. 4</ref>) covers the area of the 6.3 magnitude earthquake that struck Christchurch, New Zealand, in February 2011. The dataset contains two image scenes that were captured at the same location in 2012 and 2016, along with semantic labels and change detection labels for buildings. Since the size of each image is 32, 507 × 15, 354 pixels, we divided the two images into nonoverlapping 256 × 256 -pixel image pairs. Then, we used the cropped images to form the training set, the verification set, and the test set according to the ratio of 8:1:1.</p><p>We counted the number of changed pixels and the number of unchanged pixels in the CDD dataset and the BCDD dataset, and the result can be seen in <ref type="table" target="#tab_0">Table I</ref>. In the CDD dataset, the ratio of changed pixels to unchanged pixels was 0.147. In the BCDD dataset, the ratio was 0.045. This means that in these two datasets, the area that changed is much smaller than the area that has not changed.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metrics and Implementation details</head><p>To evaluate the performance of the proposed method, we used four evaluation indicators: precision (P), recall (R), F1 score (F1), and overall accuracy (OA). In the change detection task, the higher the precision value, the fewer false detections of the predicted result occur, and the larger the recall value, the fewer predicted results were missed. F1 score and OA are the overall evaluation metrics of the prediction results. The larger their values, the better the prediction results. They are expressed as follows:</p><formula xml:id="formula_8">P = T P T P + F P (10) R = T P T P + F N<label>(11)</label></formula><formula xml:id="formula_9">f 1 = 2P R P + R<label>(12)</label></formula><formula xml:id="formula_10">OA = T P + T N T P + T N + F P + F N<label>(13)</label></formula><p>where TP is the number of true positives, FP is the number of false positives, TN is the number of true negatives, and FN is the number of false negatives. The proposed method was implemented with PyTorch as the backend, which is powered by 3 × GTX TITAN XP. The Adam optimizer was adopted with a learning rate of 1e-4 as an optimization algorithm, and the batch size of the training data was set to 8. According to the experimental performance, the value of m 1 was set to 0.3, the value of m 2 was set to 2.2, and the values of λ 1 , λ 3 and λ 2 were set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of the WDMC Loss Function</head><p>The measurement values differ between feature pairs that are obtained by different distance metrics. In the task of change detection, a suitable distance metric must increase the distance between the changed feature pairs and decrease the distance between the unchanged feature pairs.</p><p>To obtain a suitable distance metric, we evaluated the impacts of the l2 distance and the cosine similarity on the results of the change detection task. We visualize the results from various distance metrics. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, we visualized the output of two trained DASNet networks, which were all set up in the same way except for different distance metrics, and we obtained the distance maps based on various distance metrics. For the same feature layer, the background of the receiving map that was obtained using the cosine similarity was noisier, and its performance in describing the changing boundary was weaker. Thus, the performance of this metric in excluding noisy changes was weak. The resulting graph that was obtained by using the second-order Euclidean distance had a cleaner background and a higher contrast in the front background, which endowed the model with stronger change recognition performance. In the change detection task, there were far more unchanging sample pairs than changing sample pairs. The traditional contrastive loss has an unbalanced penalty for unchanged feature pairs and changed feature pairs. As a result, the network with the traditional contrastive loss performed weakly in discerning change information. We propose the WDMC loss to strengthen the network's performance in the identification of change information.  Since the selection of the values of m 1 and m 2 will have a great influence on the results of the network, many experiments were designed to find the values of m 1 and m 2 with the best performance. <ref type="figure" target="#fig_6">Fig. 7</ref> shows the performance of a DSANet network using ResNet50 as the backbone on the CDD dataset with different m1 and m2 values. We also tested the effect of different values of λ 1 , λ 2 and λ 3 on the CDD data set to the final result. As seen in <ref type="table" target="#tab_0">Table II</ref>, the appropriate choice of λ 1 , λ 2 and λ 3 improved the accuracy of the model to a certain extent. <ref type="table" target="#tab_0">Table III</ref> shows the comparison between the improved loss function and the traditional contrastive loss, and precision, recall, F1 score, and OA were used as judgment indicators to evaluate the performance.</p><p>According to the results in <ref type="table" target="#tab_0">Table III</ref>, when we used VGG16 as the backbone, compared with the traditional contrastive loss, the recall and F1 score of our proposed WDMC loss function with l2 distance improved by 7.4% and 3.3%, respectively, and OA improved by 0.6%, while other experimental conditions remained unchanged. When we used ResNet50 as the backbone, for Siam-Conv, compared with the traditional contrastive loss, the recall and F1 score of our proposed WDMC loss function with l2 distance improved by 6.1% and 2.4%, respectively, and OA improved by 0.4%, while other experimental conditions remained unchanged. We also carried out experiments on the improved loss and traditional contrastive loss based on the proposed method, and they also proved the effectiveness of the proposed WDMC loss. Therefore, the use of the proposed WDMC loss can improve the performance of the network.</p><p>To verify the robustness of the proposed loss function to pseudo-changes, we visualized the output results of Siam-Conv using traditional contrastive loss. Similarly, we also visualized the output results of Siam-Conv using WDMC loss. The use of Siam-Conv is to avoid the influence of the dual attention mechanism on the results. It can be clearly seen in <ref type="figure" target="#fig_7">Fig. 8</ref> that the network trained with WDMC loss was more resistant to pseudo-changes. The network trained with WDMC loss was better able to distinguish the pseudo-changes due to seasonal changes, sensor changes, etc. However, the traditional contrastive loss could not do this well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study for Attention Modules</head><p>To better focus on the change regions and to improve the recognition accuracy of the network, on the basis of the WDMC loss, a dual attention module was incorporated for the identification of long-range dependencies to obtain better feature representations. We designed ablation experiments to evaluate the performance of the dual attention mechanism. As <ref type="figure" target="#fig_8">Fig. 9</ref> shows, we also give the change process of the two parameters η and γ in 30 epochs.  <ref type="table" target="#tab_0">Table IV</ref>, the dual attention module improved the model's performance comprehensively. Compared with the baseline Siamese network (a Siamese network that is based on VGG16 or ResNet50), for using VGG16 as the backbone, the recall, precision, F1 score and OA of the network with the spatial attention module were 0.919, 0.901, 0.910 and 0.978, respectively, which correspond to increases of 2.3%, 1.3%, 1.8%, and 0.5%, respectively. For using ResNet50 as the backbone, the recall, precision, F1 score, and OA of the network with the spatial attention module were 0.927, 0.903, 0.916, and 0.979, respectively, which correspond to increases of 2.5%, 0.5%, 1.6%, and 0.5%, respectively. The recall, precision, F1 score, and OA of the network that used VGG16 as the backbone with the channel attention module were 0.922, 0.892, 0.906, and 0.976, respectively, which correspond to increases of 2.6%, 0.4%, 1.4%, and 0.3%, respectively. The recall, precision, F1 score, and OA of the network that used ResNet50 as the backbone with the channel attention module were 0.930, 0.894, 0.912, and 0.978, respectively, which correspond to increases of 2.8%,.0.4%, 1.2%, and 0.4%, respectively. After both the spatial attention module and the channel attention module were added into the network, the network that used VGG16 performance was further improved. The recall, precision, F1 score and OA were 0.925, 0.914, 0.919 and 0.980, respectively, which correspond to increases of 2.9%, 2.6%, 3.7% and 0.7%, respectively. The network that used ResNet50 performance was further improved. The recall, precision, F1 score and OA were 0.932, 0.922, 0.927 and 0.982, respectively, which correspond to increases of 3.0%, 2.4%, 2.7% and 0.6%, respectively.</p><p>The addition of a dual attention module to the Siamese network overcomes the problem that the WDMC loss function cannot improve the precision while improving the recall. Both the spatial attention mechanism and the channel attention mechanism improve the accuracy of the network. When these two attention mechanisms are combined, the spatial information and channel information of the features are fully utilized, and the comprehensive performance of the network is improved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization of the Dual Attention Mechanism Effect</head><p>To more intuitively reflect the role of the dual attention module, we used the t-SNE <ref type="bibr" target="#b60">[61]</ref> algorithm to visualize the last feature layer of the Siamese net without adding the dual attention mechanism (Siam-Conv) and the last feature layer of the proposed model ( <ref type="figure" target="#fig_0">Fig. 10</ref>). In the t-SNE diagram, the purple parts represent the unchanged feature vectors in 2 dimensions, and the green parts represent the changed feature vectors in 2 dimensions. Compared with Siam-Conv, the main advantage of the DASNet model was that changed and unchanged features were both well clustered, with strong discrimination between them. This enlarged the margin between the types of features and effectively increased the accuracy of feature classification.</p><p>To further illustrate the effectiveness of the attention mechanism, we applied the Grad-CAM <ref type="bibr" target="#b61">[62]</ref> to different networks using images from the CDD dataset. Grad-CAM is a visualization method that uses gradients to calculate the importance of spatial positions in the convolutional layer. We compared the visualization results of Siam-Conv and DASNet. <ref type="figure" target="#fig_0">Fig. 11</ref> illustrates the visualization results. In <ref type="figure" target="#fig_0">Fig. 11</ref>, we can clearly see that the Grad-CAM mask of DASNet covers the target area better than Siam-Conv. In other words, DASNet can make good use of the information in the target object area and aggregate characteristics from it. <ref type="figure" target="#fig_0">Fig. 11</ref> also demonstrates the robustness of the dual attention mechanism to pseudo-changes. Siam-Conv focuses not only on change information but also on pseudo-change due to seasonal changes in trees, agricultural fields, etc. This is a manifestation of the network's lack of resistance to the interference of pseudo-change information. DASNet, however, is more concerned with real changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Performance Experiment</head><p>To evaluate the performance of the proposed method, we compared it with several image-based deep learning change detection methods: CDnet <ref type="bibr" target="#b29">[30]</ref> is used in the study of street scene change detection. It is composed of contraction blocks and expansion blocks, and the change map is obtained through a softmax layer. FC-EF <ref type="bibr" target="#b30">[31]</ref> stacks image pairs as the input images. It uses the skip connection structure to fuse the shallow and deep features and finally obtains the change map through the softmax layer. FC-Siam-Conc <ref type="bibr" target="#b30">[31]</ref> is an extension of FC-EF. Its encoding layers are separated into two streams with equal structure and shared weights. Then, the skip connections are concatenated in the decoder part.   <ref type="bibr" target="#b30">[31]</ref> is another extension of FC-EF. It differs from FC-Siam-Conc in that it does not fuse the feature results that are obtained from the encoder streams in the decoder part but calculates the absolute value of the difference between the feature results. BidateNET <ref type="bibr" target="#b31">[32]</ref> integrates the convolution module in LSTM <ref type="bibr" target="#b62">[63]</ref>into UNet <ref type="bibr" target="#b63">[64]</ref> so that the model can better learn the temporal change pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC-Siam-Diff</head><p>We conducted comparative experiments with the above methods on the CDD and BCDD datasets.</p><p>From the data in <ref type="table" target="#tab_0">Table V and Table VI</ref>, the proposed method performed significantly better than the other change detection methods. For a more intuitive evaluation, we visualized the experimental results ( <ref type="figure" target="#fig_0">Fig. 12 and Fig. 13</ref>). According to <ref type="table">Table  4</ref>, the proposed method realized satisfactory performance in remote sensing change detection. Compared with the other optimal change detection methods, the F1 score improved by 2.9% on the CDD dataset and by 4.2% on the BCDD dataset.</p><p>To evaluate the performance of our method more intuitively, we visualized the experimental results. According to <ref type="figure" target="#fig_0">Fig. 12</ref> and <ref type="figure" target="#fig_0">Fig. 13</ref>, the proposed method realized satisfactory results on both the CDD and BCDD datasets.</p><p>In <ref type="figure" target="#fig_0">Fig. 12</ref>, the first row demonstrates our model's performance in recognizing small changes in complex scenarios. The colors of images before and after the change in the second row vary substantially, but this has a highly limited impact on our network; there are many pseudo-change interferences in the images before and after the change in the third row, while the change in the bottom-left corner of the fourth row is not readily observable. Our method performed well under both conditions. According to <ref type="figure" target="#fig_0">Fig. 13</ref>, compared with other change detection methods, the proposed method focuses more on the changing area of interest. Since the BCDD is a dataset for building change detection, it is only necessary to identify building changes. Other changes, such as road changes, can be regarded as pseudo-changes. The first row and the third row clearly show that the method proposed in this paper not only performs well in identifying changes but also exhibits satisfactory resistance to the interference of pseudo-changes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we proposed a DASNet for high-score remote sensing image change detection, which directly measures changes by learning implicit metrics. To minimize the distance between the unchanged areas and maximize the distance between the changed areas, we used spatial attention and channel attention to obtain better feature representations and used the WDMC loss to balance the influences of the changed regions and the unchanged regions on the network. Compared with other baseline methods, our proposed network performed well on both the CDD and BCDD datasets. The Siamese network structure can learn the change representations of remote sensing images well, and the attention mechanism can describe the local features of the changes and recognize the pseudo-changes.</p><p>In the future, we will conduct further research on small samples and on open-world and noisy environments to improve the mobility and robustness of change detection. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of the dual attentive fully convolutional Siamese network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Spatial attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Spatial attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Multitemporal images that were selected as the training set from the CDD dataset. First row: unchanged images, second row: changed images, and third row: labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Multitemporal images that were selected as the training set from the BCDD dataset. First row: unchanged images, second row: changed images, and third row: labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Distance map visualizations for various layers: (a) unchanged image, (b) changed image, (c) label, (d) distance map of DASNet (VGG16) with the cosine similarity, (e) distance map of DASNet (VGG16) with the l2 distance. The changed parts are shown in white, while the unchanged parts are shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>The influence of the values of m 1 and m 2 on the performance of DSANet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Visualization results of Siam-Conv (ResNet50) using different losses: (a) unchanged image, (b) changed image, (c) label, (d) result of Siam-Conv (ResNet50) with contrastive loss, (e) result of Siam-Conv (ResNet50) with WDMC loss. The changed parts are shown in white, while the unchanged parts are shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Change process of the two parameters η and γ in 30 epochs According to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>t-SNE visual comparison diagram: (a) unchanged image, (b) changed image, (c) label, (d) result of Siam-Conv, (e) result of DASNet, (f) the last feature layer of Siam-Conv, and (g) the last feature layer of DASNet. The changed parts are shown in white, while the unchanged parts are shown in black. Grad-CAM visualization results: (a) unchanged image, (b) result of the fusion of the changed image with Grad-CAM mask of the Siam-Conv, (c) result of the fusion of the changed image with Grad-CAM mask of the DASNet, and (d) label. The changed parts are shown in white, while the unchanged parts are shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :Fig. 13 :</head><label>1213</label><figDesc>Visualized comparison of the results of various change detection methods on the CDD dataset: (a) the unchanged image, (b) the changed image, (c) the label, (d) CDnet, (e) FC-EF, (f) FC-Siam-Diff, (g) FC-Siam-Con, (h) BiDateNet, (i) DASNet(VGG16), and (j) DASNet(ResNet50). The changed parts are shown in white, while the unchanged parts are shown in black. Visualized comparison of the results of various change detection methods on the BCDD dataset: (a) the unchanged image, (b) the changed image, (c) the label, (d) CDnet, (e) FC-EF, (f) FC-Siam-Diff, (g) FC-Siam-Con, (h) BiDateNet, (i) DASNet(VGG16), and (j) DASNet(ResNet50). The changed parts are shown in white, while the unchanged parts are shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Statistics on the number of changed pixels and the number of unchanged pixels in the CDD dataset and BCDD dataset.</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="3">changed pixels unchanged pixels c/uc</cell></row><row><cell></cell><cell>train</cell><cell>83,981,014</cell><cell>571,378,986</cell><cell>0.147</cell></row><row><cell>CDD</cell><cell>val test</cell><cell>24,676,497 25,411,239</cell><cell>171,800,431 171,196,761</cell><cell>0.144 0.148</cell></row><row><cell></cell><cell>total</cell><cell>134,068,750</cell><cell>914,376,178</cell><cell>0.147</cell></row><row><cell></cell><cell>train</cell><cell>17,071,534</cell><cell>382,435,922</cell><cell>0.044</cell></row><row><cell>BCDD</cell><cell>val test</cell><cell>1,854,764 2,426,517</cell><cell>48,083,668 47,511,915</cell><cell>0.039 0.051</cell></row><row><cell></cell><cell>total</cell><cell>21,352,815</cell><cell>478,031,505</cell><cell>0.045</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Influence of different values of λ 1 , λ 2 and λ 3 .</figDesc><table><row><cell>λ 1</cell><cell>λ 2</cell><cell>λ 3</cell><cell>Rec</cell><cell>Pre</cell><cell>F1</cell><cell>OA</cell></row><row><cell>0</cell><cell>0</cell><cell>1</cell><cell cols="4">0.924 0.921 0.922 0.981</cell></row><row><cell cols="7">0.25 0.25 0.5 0.927 0.925 0.926 0.982</cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="4">0.932 0.922 0.927 0.982</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Loss comparison studies on the CDD dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">BaseNet Rec Pre</cell><cell>F1</cell><cell>OA</cell></row><row><cell>Siam-Conv with contrastive loss</cell><cell>VGG16</cell><cell cols="3">0.822 0.913 0.859 0.967</cell></row><row><cell cols="2">Siam-Conv with WDMC loss(cos) VGG16</cell><cell cols="3">0.889 0.827 0.856 0.963</cell></row><row><cell cols="2">Siam-Conv with WDMC loss(l2) VGG16</cell><cell cols="3">0.896 0.888 0.892 0.973</cell></row><row><cell>DASNet with contrastive loss</cell><cell>VGG16</cell><cell cols="3">0.844 0.915 0.878 0.971</cell></row><row><cell>DASNet with WDMC loss(cos)</cell><cell>VGG16</cell><cell cols="3">0.896 0.841 0.871 0.970</cell></row><row><cell>DASNet with WDMC loss(l2)</cell><cell>VGG16</cell><cell cols="3">0.925 0.914 0.919 0.980</cell></row><row><cell>Siam-Conv with contrastive loss</cell><cell cols="4">ResNet50 0.841 0.915 0.876 0.971</cell></row><row><cell cols="5">Siam-Conv with WDMC loss(cos) ResNet50 0.893 0.841 0.866 0.969</cell></row><row><cell cols="5">Siam-Conv with WDMC loss(l2) ResNet50 0.902 0.898 0.900 0.975</cell></row><row><cell>DSANet with contrastive loss</cell><cell cols="4">ResNet50 0.878 0.905 0.891 0.973</cell></row><row><cell>DASNet with WDMC loss(cos)</cell><cell cols="4">ResNet50 0.901 0.879 0.890 0.973</cell></row><row><cell>DASNet with WDMC loss(l2)</cell><cell cols="4">ResNet50 0.932 0.922 0.927 0.982</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Attention mechanism ablation research on the CDD dataset, where CAM denotes channel attention and SAM denotes spatial attention.</figDesc><table><row><cell>Method</cell><cell cols="2">BaseNet SAM CAM Rec</cell><cell>Pre</cell><cell>F1</cell><cell>OA</cell></row><row><cell cols="2">Siam-Conv VGG16</cell><cell cols="4">0.896 0.888 0.892 0.973</cell></row><row><cell>DASNet</cell><cell>VGG16</cell><cell cols="4">0.919 0.901 0.910 0.978</cell></row><row><cell>DASNet</cell><cell>VGG16</cell><cell cols="4">0.922 0.892 0.906 0.976</cell></row><row><cell>DASNet</cell><cell>VGG16</cell><cell cols="4">0.925 0.914 0.919 0.980</cell></row><row><cell cols="2">Siam-Conv ResNet50</cell><cell cols="4">0.902 0.898 0.900 0.974</cell></row><row><cell>DASNet</cell><cell>ResNet50</cell><cell cols="4">0.927 0.903 0.916 0.979</cell></row><row><cell>DASNet</cell><cell>ResNet50</cell><cell cols="4">0.930 0.894 0.912 0.978</cell></row><row><cell>DASNet</cell><cell>ResNet50</cell><cell cols="4">0.932 0.922 0.927 0.982</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Results on the CDD dataset.</figDesc><table><row><cell>Method</cell><cell>Rec</cell><cell>Pre</cell><cell>F1</cell><cell>OA</cell></row><row><cell>CDnet</cell><cell cols="4">0.817 0.827 0.822 0.964</cell></row><row><cell>FC-EF</cell><cell cols="4">0.528 0.684 0.596 0.911</cell></row><row><cell>FC-Siam-Diff</cell><cell cols="4">0.703 0.677 0.691 0.931</cell></row><row><cell>FC-Siam-Conc</cell><cell cols="4">0.666 0.710 0.687 0.925</cell></row><row><cell>BiDateNet</cell><cell cols="4">0.894 0.901 0.898 0.975</cell></row><row><cell>DSANet(VGG16)</cell><cell cols="4">0.925 0.914 0.919 0.980</cell></row><row><cell cols="5">DSANet(ResNet50) 0.932 0.922 0.927 0.982</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>Results on the BCDD dataset.</figDesc><table><row><cell>Method</cell><cell>Rec</cell><cell>Pre</cell><cell>F1</cell><cell>OA</cell></row><row><cell>CDnet</cell><cell cols="4">0.821 0.908 0.862 0.989</cell></row><row><cell>FC-EF</cell><cell cols="4">0.746 0.841 0.791 0.981</cell></row><row><cell>FC-Siam-Diff</cell><cell cols="4">0.710 0.700 0.704 0.971</cell></row><row><cell>FC-Siam-Conc</cell><cell cols="4">0.736 0.631 0.679 0.966</cell></row><row><cell>BiDateNet</cell><cell cols="4">0.819 0.889 0.852 0.986</cell></row><row><cell>DASNet(VGG16)</cell><cell cols="4">0.905 0.892 0.898 0.990</cell></row><row><cell cols="5">DASNet(ResNet50) 0.905 0.900 0.910 0.991</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the people who share data and basic models of their research to the community. Their selflessness truly drives the research process for the entire community.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Review article digital change detection techniques using remotely-sensed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of remote sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="1003" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Updating the 2001 national land cover database land cover classification to 2006 by using landsat imagery change detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Homer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1133" to="1147" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Landslide inventory mapping from bitemporal high-resolution remote sensing images using change detection and multiscale segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1520" to="1532" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building change detection using high resolution remotely sensed data and gis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sofina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehlers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3430" to="3438" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Review articledigital change detection methods in ecosystem monitoring: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Coppin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jonckheere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nackaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Muys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lambin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of remote sensing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1565" to="1596" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Urban change detection based on dempster-shafer theory for multitemporal very high-resolution imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">980</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Change detection techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mausel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brondizio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of remote sensing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2365" to="2401" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Earthquake damage assessment of buildings using vhr optical and sar imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2403" to="2420" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Use of landsat 5 for change detection at 1998 indian and pakistani nuclear test sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Zelinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3453" to="3460" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel framework for the design of change-detection systems for very-high-resolution remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="609" to="630" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Change detection from remotely sensed images: From pixel-based to object-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic analysis of the difference image for unsupervised change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Prieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote sensing</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised change detection in satellite images using principal component analysis and k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Celik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="772" to="776" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pca-based land-use change detection and analysis using multitemporal and multisensor satellite data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4823" to="4838" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A post-classification change detection method based on iterative slow feature analysis and bayesian soft fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page" from="241" to="255" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Use of a dark object concept and support vector machines to automate forest cover change analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Masek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Goward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote sensing of environment</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="970" to="985" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic change detection in high-resolution remote-sensing images by means of level set evolution and support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of remote sensing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="6255" to="6270" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised change detection in vhr images using contextual information and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kanevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Earth Observation and Geoinformation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object-based change detection for vhr images based on multiscale uncertainty analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-resolution remote sensing image change detection by statistical-object-based method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2440" to="2447" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Description and validation of a new set of objectbased temporal geostatistical features for land-use/land-cover change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gil-Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Á</forename><surname>Balaguer-Beser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hermosilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="77" to="91" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object-based land cover change detection for cross-sensor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="6723" to="6737" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object-based change detection in urban areas: The effects of segmentation strategy, scale, and feature space on unsupervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">761</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Internet of things and big data analytics for smart and connected communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Jara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="766" to="773" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d panoramic virtual reality video quality assessment based on 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="38" to="669" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deep convolutional coupling network for change detection based on heterogeneous optical and radar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="545" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Change detection based on deep siamese convolutional network for optical aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1845" to="1849" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Streetview change detection with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gherardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1301" to="1322" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4063" to="4067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting urban changes with recurrent neural networks from multitemporal sentinel-2 data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Papadomanolaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="214" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-resolution remote sensing image change detection combined with pixel-level and object-level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="78" to="909" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3547" to="3555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scene segmentation with dag-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1480" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene segmentation with dag-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1480" to="1493" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image annotation by k nn-sparse graph-based label propagation over noisily tagged web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition via incorporating latent data structure and prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1899" to="1908" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scacnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6298" to="6306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Change detection in remote sensing images using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">V</forename><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Rubis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>International Archives of the Photogrammetry</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multivariate alteration detection (mad) and maf postprocessing in multispectral, bitemporal image data: New approaches to change detection studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Conradsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Slow feature analysis for change detection in multispectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2858" to="2874" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning to measure change: Fully convolutional siamese metric networks for scene change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09111</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Changenet: a deep learning architecture for visual change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gubbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balamuralidhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Large kernel mattersimprove semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
