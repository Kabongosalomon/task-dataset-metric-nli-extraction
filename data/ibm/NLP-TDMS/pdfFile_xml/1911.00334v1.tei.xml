<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING A REPRESENTATION FOR COVER SONG IDENTIFICATION USING CONVOLUTIONAL NEURAL NETWORK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhesong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING A REPRESENTATION FOR COVER SONG IDENTIFICATION USING CONVOLUTIONAL NEURAL NETWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Music Information Retrieval, Cover Song Identification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cover song identification represents a challenging task in the field of Music Information Retrieval (MIR) due to complex musical variations between query tracks and cover versions. Previous works typically utilize hand-crafted features and alignment algorithms for the task. More recently, further breakthroughs are achieved employing neural network approaches. In this paper, we propose a novel Convolutional Neural Network (CNN) architecture based on the characteristics of the cover song task. We first train the network through classification strategies; the network is then used to extract music representation for cover song identification. A scheme is designed to train robust models against tempo changes. Experimental results show that our approach outperforms state-of-the-art methods on all public datasets, improving the performance especially on the large dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Cover song identification has long been a popular task in the music information retrieval community, with potential applications in areas such as music license management, music retrieval, and music recommendation. Cover song identification can also be seen as measuring the similarity between music melodies. Given those cover songs may differ from the original song in key transposition, speed change and structural variations, identifying cover songs is a rather challenging task. Over the past ten years, researchers initially attempt to address the problem employing Dynamic Programming (DP) approaches. Typically, chroma sequences representing the intensity of twelve pitch classes are used to describe recordings, and then a DP method is utilized for finding an optimal alignment between two given recordings, resolving the discrepancy caused by tempo changes and structural variations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Such approaches work well when facing structural variations and tempo changes in the music; however, the involvement of element-to-element distance computation with quadratic time complexity makes it unsuitable for large-scale datasets.</p><p>Alternatively, some researchers attempted to identify  <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> represent the music with fixed-dimensional vectors, which enables a direct measure of the music similarity. These approaches highly improved the efficiency compared to alignment methods, while the loss of the temporal information of music in these approaches may yield a lower precision. Moreover, deep learning approaches are introduced to cover song identification. For instance, CNNs are utilized to measure the similarity matrix <ref type="bibr" target="#b5">[6]</ref> or learn features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. While these methods have achieved promising results, there is still room for improvement. In this paper, a specially designed CNN architecture is proposed to overcome challenges of key transposition, speed change and structural variations existing in cover song identification. Notably, the use of the specialized kernel size is fist ever utilized in the field of music information retrieval. The dilation convolution and the method of data augmentation are also introduced. Our approach outperforms state-of-the-art methods on all public datasets with better accuracy but lower time complexity. Furthermore, to our best knowledge, our method is currently the best method to identify cover songs in huge real-life corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem Formulation</head><p>As shown in <ref type="figure">Figure 1</ref>, we have a training dataset D = {(x n , t n )}, where x n is a recording and t n is a one-hot vector denoting to which song (or class) the recording belongs. Different versions of the same song are viewed as the samples from the same class, and different songs are regarded as the different classes. We aim to train a classification net-arXiv:1911.00334v1 [cs.MM] 1 Nov 2019 work model parameterized as {θ, λ} from D. As shown in <ref type="figure">Figure 2</ref>, θ is the parameter of all convolutions and FC0; λ is the parameter of FC1; f θ is the output of the FC0 layer. Then, this model could be used for cover song retrieval. More specifically, after the training, given a query Q and references R n in the dataset, we extract latent features f θ (Q), f θ (R n ) which we call as music representations obtained by the network, and then we use a metric s to measure the similarity between them. In the following sections, we will discuss the low-level representation used, the design of network structure and a robust trained model against key transposition and tempo change. We use lowercase for audio and uppercase for the CQT to discriminate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Low-level Representation</head><p>The CQT, mapping frequency energy into musical notes, is extracted by Librosa <ref type="bibr" target="#b10">[11]</ref> for our experiment. The audio is resampled to 22050 Hz, the number of bins per octave is set as 12 and Hann window is used for extraction with a hop size of 512. Finally, a 20-point mean down-sampling in the time direction is applied to the CQT, and the resulting feature is a sequence with a feature rate of about 2 Hz. It could also be viewed as an 84 × T matrix where T depends on the duration of input audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Network Structure</head><p>Inspired by successful applications of network architectures like <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we design a novel Network architecture for the cover song task. We stack small filters following with max pooling operations, except that in initial layers, we design the height of filter to be 12 or 13 (see <ref type="figure">Figure 2</ref>) as the number of bins per octave is set as 12 in the CQT. This setting results in that the units of the third layer have a receptive field with a height of 36; it spans across three octaves or thirty-six semitones.</p><p>We also introduce dilated convolution into the model to enlarge the receptive field because cover song identification focuses on the long-term melody of the music. The design is consistent with the ideas of existing works in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, which extracted features or measured the similarity from a long range.</p><p>More importantly, our model does not involve any downsample pooling operation in the frequency dimension; in other words, the vertical stride is always set to 1, different from prevalent network structures like VGG and ResNet <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. The motivation behind this design focuses on the fact that key transposition may be one or two semitones, corresponding to moving the CQT matrix vertically for merely one or two elements. Without downsampling feature map, the network remains a higher resolution and deals with key transposition better. We experimentally validate that this design helps improve the precision (see Section 4.1).</p><p>Furthermore, after several convolutional and pooling layers, we apply an adaptive pooling layer to the feature map, whose length varies depending on the input audio. Obviously, this global pooling has the advantage of converting variable-length feature maps into fixed-dimensional vectors, connected with two fully-connected layers to exploit more information. Without the global pooling, the fully-connected layers only allow fixed-dimensional inputs, which are not the cases of music as different compositions may have different durations. Comparing with <ref type="bibr" target="#b8">[9]</ref> who utilized Temporal Pyramid Pooling, we utilize global pooling because it performs the same as TPP in our model. One explanation is that our convolutional network is much deeper.</p><p>Given the inputs of network X, the output of FC0 is f θ (X) ∈ R 300 , and the prediction of network is y = softmax(λf θ (X)). Cross-entropy loss L is used for training. In our cases, different versions of a song are considered as the same category, and different songs are viewed as different classes.  <ref type="figure">Fig. 2</ref>. Network structure. K: kernel size, C: channel number, D: dilation and S: stride. The stride is set to 1 × 1 for the convolutional layers, and pooling layers has a dilation of 1×1. The output dimension is 4611, the number of classes in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training Scheme</head><p>For each batch, we sample some recordings from the training set and extract CQTs from them. For each CQT, we randomly crop three subsequences with a length of 200, 300 and 400 for training, corresponding to roughly 100s, 150s and 200s, respectively. x ← simulate tempo changes on x with a changing factor r 8:</p><p>X ← extract the CQT from x 9:</p><p>X ← crop a subsequence from X with a length l Feed-forward with T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Backpropagation to update θ and W <ref type="bibr">14:</ref> end for <ref type="bibr">15:</ref> until Network converges Despite the training set contains covers performed at different speeds, each song merely owns several covers on average for training. Moreover, as our model does not explicitly handle tempo changes in cover songs, it may be difficult for the model to learn a representation robust against tempo changes automatically. Therefore, we perform data augmentation during model training. As shown in Algorithm 1, we sample a changing factor from (0.7, 1.3) for each recording in the batch following uniform distribution and simulate tempo changes using Librosa <ref type="bibr" target="#b10">[11]</ref> on the recording before cropping subsequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Retrieval</head><p>After the training, the network is used to extract music representations. As shown in <ref type="figure">Figure 1</ref>, given a query q and a reference r, we first extract their CQT descriptors Q and R respectively, which are fed into the network to obtain music representations f θ (Q) and f θ (R)), and then the similarity s, defined as their cosine similarity, are measured. After computing the pair-wise similarity between the query and references in the dataset, a ranking list is returned for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETTINGS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>Second Hand Songs 100K (SHS100K), which is collected from Second Hand Songs website by <ref type="bibr" target="#b7">[8]</ref>, consisting of 8858 songs with various covers and 108523 recordings. This dataset is split into three subsets -SHS100K-TRAIN, SHS100K-VAL and SHS100K-TEST with a ratio of 8 : 1 : 1.</p><p>Youtube is collected from the YouTube website, containing 50 compositions of multiple genres <ref type="bibr" target="#b13">[14]</ref>. Each song in Youtube has 7 versions, with 2 original versions and 5 different versions and thus results in 350 recordings in total. In our experiment, we use the 100 original versions as references and the others as queries following the same as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Covers80 is a widely used benchmark dataset in the literature. It has 80 songs, with 2 covers for each song, and has 160 recordings in total. To compare with existing methods, we compute the similarity of any pair of recordings.</p><p>Mazurkas is a classical music collection consisting of 2914 recordings of 49 Chopin's Mazurkas, originated from the Mazurka Project 1 . The number of covers for each piece varies between 41 and 95. For this dataset, we follow the experimental setting of <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation</head><p>For evaluation, we calculate the common evaluation metrics: mean average precision (MAP), precision at 10 (P@10) and the mean rank of the first correctly identified cover (MR1). These metrics are the ones used in Mirex Audio Cover Song Identification contest 2 . Additionally, query time is recorded for efficiency evaluation. All the experiments are run in a Linux server with two TITAN X (Pascal) GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULT AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Exploration of Network Structure</head><p>Firstly we explore the kernel size of CNNs and replace the kernel size of the initial three layers with 3 × 3, 7 × 3, 15 × 3, 7 × 7, 12 × 12. The result of experiment shows that the height of filter to be 12 or 13 performs the best.</p><p>Additional, we change the vertical strides of max-pooling layers and conduct several experiments to explore its influence on accuracy. The original model is denoted as CQT-Net, and the modified network is denoted as CQT-Net{4}, CQT-Net{3, 4} and CQT-Net{2, 3, 4} respectively, where the numbers indicate the shape of corresponding pooling layers are changed to <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2)</ref>. For instance, CQT-Net{2} means that Pool2 is replaced with a pooling operation with a stride and size of (2, 2). That is, the total vertical strides for CQT-Net{4}, CQT-Net{3, 4} and CQT-Net{2, 3, 4} are 2, 4 and 8, respectively. When the vertical stride increases, MAP degrades on the four datasets consistently, as well as MR1 and P@10. We suppose this is because the key transposition may shift one or two semitones; the network having a higher resolution of feature dimension (that is, setting vertical stride to be 1) could capture these changes and help improve the precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP P@10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MR1 Time</head><p>Results on Youtube DPLA <ref type="bibr" target="#b1">[2]</ref> 0.525 0.132 9.43 2420s SiMPle <ref type="bibr" target="#b14">[15]</ref> 0.591 0.140 7.91 18.7s Fingerprinting <ref type="bibr" target="#b15">[16]</ref> 0.648 0.145 8.27 -SuCo-DTW <ref type="bibr" target="#b16">[17]</ref> 0.800 0.180 3.42 4.59s Ki-CNN <ref type="bibr" target="#b7">[8]</ref> 0.656 0.155 6.26 0.35ms TPPNet <ref type="bibr" target="#b8">[9]</ref> 0.859 0.188 2.85 0.04ms CQT-Net 0.917 0.192 2.50 0.04ms Results on Covers80 NCP-WIDI <ref type="bibr" target="#b17">[18]</ref> 0.645 ---CRP <ref type="bibr" target="#b2">[3]</ref> 0.544 0.061 --Fusing <ref type="bibr" target="#b18">[19]</ref> 0.625 0.071 --Ki-CNN <ref type="bibr" target="#b7">[8]</ref> 0.506 0.068 16.4 0.55ms TPPNet <ref type="bibr" target="#b8">[9]</ref> 0.744 0.086 6.88 0.06ms CQT-Net 0.840 0.091 3.85 0.06ms Results on Mazurkas DTW <ref type="bibr" target="#b14">[15]</ref> 0.882 0.949 4.05 -NCD <ref type="bibr" target="#b19">[20]</ref> 0.767 ---Compression <ref type="bibr" target="#b20">[21]</ref> 0.795 ---Fingerprinting <ref type="bibr" target="#b21">[22]</ref> 0.819 ---SiMPle <ref type="bibr" target="#b14">[15]</ref> 0.880 0.952 2.33 -SuCo-repeat <ref type="bibr" target="#b16">[17]</ref> 0.850 0.940 2.77 -2DFM <ref type="bibr" target="#b3">[4]</ref> 0  <ref type="table">Table 1</ref>. Performance on different datasets (-indicates the results are not shown in original works).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison</head><p>We compare with other state-of-the-art methods on different datasets. <ref type="table">Table 4</ref>.1 shows our approach outperforms stateof-the-art methods on all datasets. The advantages of our approach lie in without relying on complicated hand-crafted features and elaborately-designed alignment algorithms, our approach exploits massive data and feature learning and obtains high precision. By collecting a larger dataset, our approach may obtain higher precision. It is worth noting that our training set SHS100K-TRAIN mainly consists of pop music while Mazurkas contains classical music. Our approach outperforms state-of-the-art methods on this dataset, which indicates a good generalization ability. We do not show the result of <ref type="bibr" target="#b21">[22]</ref> in Mazurka Project because our test sets are different. As for the large dataset SHS100K-TEST, our method performs much better than state-of-the-art methods. Moreover, the query time shown in the table does not include the time of feature extracting. Therefore, our method has the same time consumed as <ref type="bibr" target="#b8">[9]</ref>. It extracts a fixeddimensional feature whatever the duration of input audio is. Theoretically, it has linear time complexity, faster than sequence alignment methods with quadratic time complexity. One could find that the query time of our approach is shorter than approaches such as DPLA, SiMPle by several magnitudes. For Ki-CNN and TPPNet, they model music with a fixed-dimensional vector and have similar time complexity. In our implementation, our approach learns a 300-dimensional representation, which is the same as TPP-Net, explaining why the time consumption of our approach is the same as TPPNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Result Demonstration and Error Analysis</head><p>We listen to the Top10 retrieval results and attempt to make some analysis on SHS100K-TEST. Our approach could identify versions when performed by different genders, accompanied by different instruments, sung in different languages, etc. Especially, as our training goal is to classify the song and different versions of the same song often have similar styles, melodic and chord structures, we find that even though some candidates in the Top10 may not be the cover of the query, but they have similar properties such as accompaniment and genre with the query. For instance, Everybody Knows This Is Nowhere by the Bluebeaters has a similar accompaniment with that of Waiting in Vain by Bob Marley &amp; The Wailers. In this sense, our approach may also be used to retrieve similar music of the query and extended to content-based music recommendation.</p><p>Furthermore, we find that Top1 precision of our model is 0.81, suggesting that it could find a cover as the Top1 candidate for 81% queries. However, it works worse in some cases. This may explain why our approach obtains a MAP of 0.655 while only a MR1 of 54.9 on this dataset. Most importantly, the high Top1 precision and the fast retrieval speed make our method possible to handle the real-life cover song task instead of just staying in the lab stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>Different from conventional techniques, we propose CNNs for feature learning towards cover song identification. Utilizing specific kernels and dilated convolutions to extend the receptive field, we show that it could be used to capture melodic structures underlying the music and learn key-invariant representations. By casting the problem into a classification task, we train a model that is used for music version identification. Additionally, we design a training strategy to enhance the model's robustness against tempo changes and to deal with inputs with different lengths. Combined with these techniques, our approach outperforms state-of-the-art methods on all public datasets with low time complexity. Furthermore, we show that this model could retrieve various music versions and discover similar music. Eventually, we believe our method is competent to solve real-life cover song problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Conv0 (K: 12x3, C: 32, D: 1x1), BN, ReLU Conv1 (K: 13x3, C: 64, D: 1x2), BN, ReLU Maxpool0 (K: 1x2, S: 1x2) Conv2 (K: 13x3, C: 64, D: 1x1), BN, ReLU Conv3 (K: 3x3, C: 64, D: 1x2), BN, ReLU Maxpool1 (K: 1x2, S: 1x2) Conv4 (K: 3x3, C: 128, D: 1x1), BN, ReLU Conv5 (K: 3x3, C: 128, D: 1x2), BN, ReLU Maxpool2 (K: 1x2, S: 1x2) Conv6 (K: 3x3, C: 256, D: 1x1), BN, ReLU Conv7 (K: 3x3, C: 256, D: 1x2), BN, ReLU Maxpool3 (K: 1x2, S: 1x2) Conv8 (K: 3x3, C: 512, D: 1x1), BN, ReLU Conv9 (K: 3x3, C: 512, D: 1x2), BN, ReLU AdaptiveMaxpool (Output Size: 1x1x512)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 4 :r</head><label>14</label><figDesc>Data augmentation and training strategy Input: Training set D, batch size n, changing range (a, b) Output: Optimized parameter θ, W 1: repeat 2: for L ∈ {200, 300, 400} do 3: sample a batch of recordings B from the training set D ← sample from U (a, b) 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 1. Training procedure and retrieval procedure.</figDesc><table><row><cell>Training Audio</cell><cell>CQTs</cell><cell>Network</cell><cell>Predictions</cell><cell></cell></row><row><cell>{xn}</cell><cell>{Xn}</cell><cell></cell><cell>{yn}</cell><cell></cell></row><row><cell>Query Audio</cell><cell>CQT</cell><cell>Network</cell><cell>Representation</cell><cell></cell></row><row><cell>q</cell><cell>Q</cell><cell></cell><cell>fθ(Q)</cell><cell>Distance Measurement</cell><cell>Ranking list</cell></row><row><cell>Reference Audio</cell><cell>CQTs</cell><cell>Network</cell><cell>Representations</cell><cell></cell></row><row><cell>{rn}</cell><cell>{Rn}</cell><cell></cell><cell>{fθ(Rn)}</cell><cell></cell></row></table><note>cover songs by modeling the music. For instance, Serrà et al. studied time series modeling for cover song identification</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.mazurka.org.uk 2 https://www.music-ir.org/mirex/wiki/2019: Audio_Cover_Song_Identification</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identifying cover songs with chroma features and dynamic programming beat tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">E</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poliner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chroma binary similarity and local alignment applied to cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrã</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilia</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perfecto</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serrã</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1138" to="1151" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross recurrence quantification for cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrã</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serrã</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">G</forename><surname>Andrzejak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">93017</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Largescale cover song recognition using the 2d fourier transform magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Thierry Bertin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhancing cover song identification with hierarchical rank aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Osmalsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Embrechts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio cover song identification using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungkyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juheon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyogu</forename><surname>Sang Keun Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Machine Learning for Audio Signal Processing at NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Audio feature learning with triplet-based embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4979" to="4980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Keyinvariant convolutional neural network toward efficient cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal pyramid pooling convolutional neural network for cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhesong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4846" to="4852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cover detection using dominant melody embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Doras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffroy</forename><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Music shapelets for fast cover song recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego Furtado</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma De</forename><surname>Vinícius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Eapa</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="441" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple: assessing music similarity using subsequences joins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Diego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Chin M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Enrique</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Almeida Prado Alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cover song identification with 2d fourier transform sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafar</forename><surname>Rafii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="616" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Summarizing and comparing music data and its application on cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego Furtado</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Vieira</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazareno</forename><surname>Andrade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective music feature ncp: Enhancing cover song recognition with music transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="925" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fusing similarity functions for cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2629" to="2652" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring structural similarity in music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2013" to="2025" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A video compressionbased approach to measure music structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hélene</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eapa</forename><surname>Gustavo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Pw</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structure-based audio fingerprinting for music retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Grosche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrã</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meinard</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep Lluis</forename><surname>Arcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
