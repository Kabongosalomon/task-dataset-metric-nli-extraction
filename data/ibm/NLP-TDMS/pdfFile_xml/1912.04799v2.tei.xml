<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Depth-Guided Convolutions for Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Research 5 Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<email>shijianping@sensetime.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Research 5 Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
							<email>luzhiwu@ruc.edu.cnhongweiyi@pku.edu.cnwangzhe</email>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hkbohony</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Depth-Guided Convolutions for Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 st on KITTI monocular 3D object detection benchmark at the time of submission (car, December 2019) . The code is available at https://github.com/dingmyu/D4LCN.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection from a single image without LiDAR is a challenging task due to the lack of accurate depth information. Conventional 2D convolutions are unsuitable for this task because they fail to capture local object and its scale information, which are vital for 3D object detection. To better represent 3D structure, prior arts typically transform depth maps estimated from 2D images into a pseudo-LiDAR representation, and then apply existing 3D point-cloud based object detectors. However, their results depend heavily on the accuracy of the estimated depth maps, resulting in suboptimal performance. In this work, instead of using pseudo-LiDAR representation, we improve the fundamental 2D fully convolutions by proposing a new local convolutional network (LCN), termed Depth-guided Dynamic-Depthwise-Dilated LCN (D 4 LCN), where the filters and their receptive fields can be automatically learned from image-based depth maps, making different pixels of different images have different filters. D 4 LCN overcomes the limitation of conventional 2D convolutions and narrows the gap between image representation and 3D point cloud representation. Extensive experiments show that D 4 LCN outperforms existing works by large margins. For example, the relative improvement of D 4 LCN against the state-of-theart on KITTI is 9.1% in the moderate setting. D 4 LCN ranks 1 st on KITTI monocular 3D object detection benchmark at the time of submission (car, December 2019) . The code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection is a fundamental problem and has many applications such as autonomous driving and robotics. Previous methods show promising results by utilizing Li-DAR device, which produces precise depth information in terms of 3D point clouds. However, due to the high-cost show pseudo-LiDAR points generated by the supervised depth estimator, DORN <ref type="bibr" target="#b10">[11]</ref> and the unsupervised Monodepth <ref type="bibr" target="#b12">[13]</ref> respectively. The green box represents groundtruth (GT) 3D box. Pseudo-LiDAR points generated by inaccurate depth as shown in (b) have large offsets comapred to the GT box. (c) and (d) show the detection results of our method and Pseudo-Lidar <ref type="bibr" target="#b47">[48]</ref> by using a coarse depth map. The performance of <ref type="bibr" target="#b47">[48]</ref> depends heavily on the accuracy of the estimated depth maps, while our method achieves accurate detection results when accurate depth maps are missing. and sparse output of LiDAR, it is desirable to seek cheaper alternatives like monocular cameras. This problem remains largely unsolved, though it has drawn much attention. Recent methods towards the above goal can be generally categorized into two streams as image-based approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4]</ref> and pseudo-LiDAR point-based approaches <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50]</ref>. The image-based approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref> typically leverage geometry constraints including object shape, ground plane, and key points. These constraints are formulated as different terms in loss function to improve detection results. The pseudo-LiDAR point-based approaches transform depth maps estimated from 2D images to point cloud representations to mimic the LiDAR signal. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, both of these methods have drawbacks, resulting in suboptimal performance.</p><p>Specifically, the image-based methods typically fail to capture meaningful local object scale and structure informa-tion, because of the following two factors. (1) Due to perspective projection, the monocular view at far and near distance would cause significant changes in object scale. It is difficult for traditional 2D convolutional kernels to process objects of different scales simultaneously (see <ref type="figure" target="#fig_2">Figure 2</ref>). <ref type="bibr" target="#b1">(2)</ref> The local neighborhood of 2D convolution is defined in the camera plane where the depth dimension is lost. In this nonmetric space (i.e. the distance between pixels does not have a clear physical meaning like depth), a filter cannot distinguish objects from the background. In that case, a car area and the background area would be treated equally.</p><p>Although pseudo-LiDAR point-based approaches have achieved progressive results, they still possess two key issues. (1) The performance of these approaches heavily relies on the precision of estimated depth maps (see <ref type="figure" target="#fig_0">Figure 1</ref>). The depth maps extracted from monocular images are often coarse (point clouds estimated using them have wrong coordinates), leading to inaccurate 3D predictions. In other words, the accuracy of the depth map limits the performance of 3D object detection. (2) Pseudo-LiDAR methods cannot effectively employ high-level semantic information extracted from RGB images, leading to many false alarms. This is because point clouds provide spatial information but lose semantic information. As a result, regions like roadblocks, electrical boxes and even dust on the road may cause false detection, but they can be easily discriminated by using RGB images.</p><p>To address the above problems, we propose a novel convolutional network, termed Depth-guided Dynamic-Depthwise-Dilated local convolutional network (D 4 LCN), where the convolutional kernels are generated from the depth map and locally applied to each pixel and channel of individual image sample, rather than learning global kernels to apply to all images. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, D 4 LCN treats the depth map as guidance to learn local dynamicdepthwise-dilated kernels from RGB images, so as to fill the gap between 2D and 3D representation. More specifically, the learned kernel in D 4 LCN is sample-wise (i.e. exemplar kernel <ref type="bibr" target="#b14">[15]</ref>), position-wise (i.e. local convolution <ref type="bibr" target="#b19">[20]</ref>), and depthwise (i.e. depthwise convolution <ref type="bibr" target="#b17">[18]</ref>), where each kernel has its own dilation rate (i.e. different exemplar kernels have different receptive fields). D 4 LCN is carefully designed with four considerations. (1) The exemplar kernel is to learn specific scene geometry for each image. (2) The local convolution is to distinguish object and background regions for each pixel. (3) The depth-wise convolution is to learn different channel filters in a convolutional layer with different purposes and to reduce computational complexity. (4) The exemplar dilation rate is to learn different receptive fields for different filters to account for objects with diverse scales. The above delicate designs can be easily and efficiently implemented by combing linear operators of shift and element-wise product. As a  Comparisons among different convolutional approaches. (a) is the traditional 2D convolution that uses a single convolutional kernel applied on each pixel to convolve the entire image. (b) applies multiple fixed convolutional kernels on different regions (slices) of an image. (c) uses the depth map to generate dynamic kernels with the same receptive fields for each pixel. (d) denotes our approach, where the filter is dynamic, depth-wise, and has adaptive receptive fields for each pixel and channel of the feature map. It can be implemented more efficiently with fewer parameters than (c). Best viewed in color. result, the efficient D 4 LCN can not only address the problem of the scale-sensitive and meaningless local structure of 2D convolutions, but also benefit from the high-level semantic information from RGB images compared with the pseudo-LiDAR representation.</p><p>Our main contributions are three-fold. (1) A novel component for 3D object detection, D 4 LCN, is proposed, where the depth map guides the learning of dynamic-depthwisedilated local convolutions from a single monocular image.</p><p>(2) We carefully design a single-stage 3D object detection framework based on D 4 LCN to learn better 3D representation for reducing the gap between 2D convolutions and 3D point cloud-based operations. (3) Extensive experiments show that D 4 LCN outperforms state-of-the-art monocular 3D detection methods and takes the first place on the KITTI benchmark <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image-based Monocular 3D Detection. Previous monocular 3D detection methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b53">54]</ref> usually make assumptions about the scene geometry and use this as a constraint to train the 2D-to-3D mapping. Deep3DBox <ref type="bibr" target="#b35">[36]</ref> uses the camera matrix to project a pre-dicted 3D box onto the 2D image plane, constraining each side of the 2d detection box, such that it corresponds to any of the eight corners of the 3D box. OFTNet <ref type="bibr" target="#b42">[43]</ref> introduces the orthographic feature transform, which maps imagebased features into an orthographic 3D space. It is helpful when scale of objects varies drastically. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref> investigated different ways of learning the confidence to model heteroscedastic uncertainty by using a 3D intersection-overunion (IoU) loss. To introduce more prior information, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b52">53]</ref> used 3D shapes as templates to get better object geometry. <ref type="bibr" target="#b22">[23]</ref> predicts a point cloud in an objectcentered coordinate system and devises a projection alignment loss to learn local scale and shape information. <ref type="bibr" target="#b33">[34]</ref> proposes a 3D synthetic data augmentation algorithm via in-painting recovered meshes directly onto the 2D scenes.</p><p>However, as it is not easy for 2D image features to represent 3D structures, the above geometric constraints fail to restore accurate 3D information of objects from just a single monocular image. Therefore, our motivation is to utilize depth information, which essentially bridges gap between 2D and 3D representation, to guide learning the 2D-to-3D feature representation.</p><p>Point Cloud-based Monocular 3D Detection. Previous monocular methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50]</ref> convert image-based depth maps to pseudo-LiDAR representations for mimicking the LiDAR signal. With this representation, existing LiDAR-based detection algorithms can be directly applied to monocular 3D object detection. For example, <ref type="bibr" target="#b49">[50]</ref> detects 2D object proposals in the input image and extracts a point cloud frustum from the pseudo-LiDAR for each proposal. <ref type="bibr" target="#b32">[33]</ref> proposes a multi-modal features fusion module to embed the complementary RGB cue into the generated point clouds representation. However, this depth-to-LiDAR transformation relies heavily on the accuracy of depth map and cannot make use of RGB information. In contrast, our method treats depth map as guidance to learn better 3D representation from RGB images.</p><p>LiDAR-based 3D Detection. With the development of deep learning on point sets, 3D feature learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b58">59]</ref> is able to learn deep point-based and voxel-based features. Benefit from this, LiDAR-based methods have achieved promising results in 3D detection. For example, <ref type="bibr" target="#b58">[59]</ref> divides point clouds into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation. <ref type="bibr" target="#b46">[47]</ref> applies the FPN technique to voxel-based detectors. <ref type="bibr" target="#b54">[55]</ref> investigates a sparse convolution for voxel-based networks. <ref type="bibr" target="#b24">[25]</ref> utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). <ref type="bibr" target="#b37">[38]</ref> leverages mature 2D object detectors to learn directly from 3D point clouds. <ref type="bibr" target="#b48">[49]</ref> aggregates point-wise features as frustum-level feature vectors. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b7">8]</ref> directly generated a small number of high-quality 3D proposals from point clouds via segmenting the point clouds of the whole scene into foreground and background. There are also some works focus on multi-sensor fusion (LIDAR as well as cameras) for 3D object detection. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref> proposed a continuous fusion layer that encodes both discrete-state image features as well as continuous geometric information. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref> used LIDAR point clouds and RGB images to generate features and encoded the sparse 3D point cloud with a compact multi-view representation.</p><p>Dynamic Networks. A number of existing techniques can be deployed to exploit the depth information for monocular 3D detection. M3D-RPN <ref type="bibr" target="#b0">[1]</ref> proposes depth-aware convolution which uses non-shared kernels in the row-space to learn spatially-aware features. However, this rough and fixed spatial division has bias and fail to capture object scale and local structure. Dynamic filtering network <ref type="bibr" target="#b19">[20]</ref> uses the sample-specific and position-specific filters but has heavy computational cost, and it also fails to solve the scalesensitive problem of 2D convolutions. Trident network <ref type="bibr" target="#b26">[27]</ref> utilizes manually defined multi-head detectors for 2D detection. However, it needs to manually group data for different heads. Other techniques like deformable convolution <ref type="bibr" target="#b8">[9]</ref> and variants of <ref type="bibr" target="#b19">[20]</ref> such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref>, fail to capture object scale and local structure as well. In this work, our depth-guided dynamic dilated local convolutional network is proposed to solve the two problems associated with 2D convolutions and narrow the gap between 2D convolution and point cloud-based 3D processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As a single-stage 3D detector, our framework consists of three key components: a network backbone, a depth-guided filtering module, and a 2D-3D detection head (see <ref type="figure" target="#fig_3">Figure 3</ref>). Details of each component are given below. First, we give an overview of our architecture as well as backbone networks. We then detail our depth-guided filtering module which is the key component for bridging 2D convolutions and the point cloud-based 3D processing. Finally, we outline the details of our 2D-3D detection head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone</head><p>To utilize depth maps as guidance of 2D convolutions, we formulate our backbone as a two-branch network: the first branch is the feature extraction network using RGB images, and the other is the filter generation network to generate convolutional kernels for feature extraction network using the estimated depth as input. These two networks process the two inputs separately and their outputs of each block are merged by the depth-guided filtering module.</p><p>The backbone of the feature extraction network is ResNet-50 <ref type="bibr" target="#b15">[16]</ref> without its final FC and pooling layers, and is pre-trained on the ImageNet classification dataset <ref type="bibr" target="#b9">[10]</ref>. To obtain a larger field-of-view and keep the network stride at 16, we find the last convolutional layer (conv5 1, block4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Map</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimate</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth-Guided Filtering Module</head><p>Shift with different dilation rates Adaptive weights that decreases resolution and set its stride to 1 to avoid signal decimation, and replace all subsequent convolutional layers with dilated convolutional layers (the dilation rate is 2). For the filter generation network, we only use the first three blocks of ResNet-50 to reduce computational costs. Note the two branches have the same number of channels of each block for the depth guided filtering module.</p><formula xml:id="formula_0">! , " , # ∈ ℝ 2D bbox [x', y', w', h'] 2D 3D shape [w', h', l'] 3D 3D center [x', y'] P , z' 3D 3D rotation ' 3D 3D corners ' (m) , y' (m) , z' (m) Element-wise product 3D detection result NMS &amp; Transform I 4 w 4 h 4 c 4 I 1 I 2 I 3 h 1 h 2 h 3 w 1 w 2 w 3 c 3 c 2 c 1 D 1 D 2 D 3 h 1 h 2 h 3 w 1 w 2 w 3 c 3 c 2 c 1 = 3 = 2 = 1 × ( , )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Depth-Guided Filtering Module</head><p>Traditional 2D convolution kernels fail to efficiently model the depth-dependent scale variance of the objects and effectively reason about the spatial relationship between foreground and background pixels. On the other hand, pseudo-lidar representations rely too much on the accuracy of depth and lose the RGB information. To address these problems simultaneously, we propose our depth-guided filtering module. Notably, by using our module, the convolutional kernels and their receptive fields (dilation) are different for different pixels and channels of different images.</p><p>Since the kernel of our feature extraction network is trained and generated by the depth map, it is sample-specific and position-specific, as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref>, and thus can capture meaningful local structures as the point-based operator in point clouds. We first introduce the idea of depthwise convolution <ref type="bibr" target="#b17">[18]</ref> to the network, termed depth-wise local convolution (DLCN). Generally, depth-wise convolution (DCN) involves a set of global filters, where each filter only operates at its corresponding channel, while DLCN requires a feature volume of local filters the same size as the input feature maps. As the generated filters are actually a feature volume, a naive way to perform DLCN requires to convert the feature volume into h n × w n location-specific filters and then apply depth-wise and local convolutions to the feature maps, where h n and w n are the height and width of the feature maps at layer n. This implementation would be time-consuming as it ignores the redundant computations in neighboring pixels. To reduce the time cost, we employ the shift and element-wise product operators, in which shift <ref type="bibr" target="#b50">[51]</ref> is a zero-flop zero-parameter operation, and element-wise product requires little calculation. Concretely, let I n ∈ R hn×wn×cn and D n ∈ R hn×wn×cn be the output of the feature extraction network and filter generation network, respectively, where n is the index of the block (note that block n corresponds to the layer conv n+1 in ResNet). Let k denote the kernel size of the feature extraction network. By defining a shifting grid {(g i , g j )}, g ∈ (int)[1−k/2, k/2−1] that contains k·k elements, for every vector (g i , g j ), we shift the whole feature map D towards the direction and step size indicated by (g i , g j ) and get the result D (gi,gj ) . For example, g ∈ {−1, 0, 1} when k = 3, and the feature map is moved towards nine directions with a horizontal or vertical step size of 0 or 1. We then use the sum and element-wise product operations to compute our filtering result:</p><formula xml:id="formula_1">I = I 1 k · k gi,gj D (gi,gj ) .<label>(1)</label></formula><p>To encourage information flow between channels of the depth-wise convolution, we further introduce a novel shiftpooling operator in the module. Considering n f as the number of channels with information flow, we shift the feature maps along the channel axis for n f times by 1, 2, .., n f − 1 to obtain new n f − 1 shifted feature maps I (ni) s , n i ∈ {1, 2, ..., n f − 1}. Then we perform element-wise mean to the shifted feature maps and the original I to obtain the new feature map as the input of the module. The process of this shift-pooling operation is shown in <ref type="figure" target="#fig_5">Figure 4</ref> (n f = 3).</p><p>Compared to the idea 'group' of depth-wise convolution in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref> which aims to group many channels into a group to perform information fusion between them, the proposed shift-pooling operator is more efficient and adds no additional parameters to the convolution. The size of our convolutional weights of each local kernel is always k × k × c n when applying shift-pooling, while it changes significantly in <ref type="bibr" target="#b17">[18]</ref> for different number of groups from k × k × c n to k×k×c n ×c n in group convolution (assume that the convolution keeps the number of channels unchanged). Note that it is difficult for the filter generation network to generate so many kernels for the traditional convolutions F between all channels, and the characteristic of being position-specific dramatically increases their computational cost.</p><p>With our depth-wise formulation, different kernels can have different functions. This enables us to assign different dilation rates <ref type="bibr" target="#b55">[56]</ref> for each filter to address the scalesensitive problem. Since there are huge intra-class and inter-class scale differences in an RGB image, we use I to learn an adaptive dilation rate for each filter to obtain different sizes of receptive fields by an adaptive function A. Specifically, let d denote our maximum dilation rate, the adaptive function A consists of three layers: (1) an Adap-tiveMaxPool2d layer with the output size of d × d and channel number c; (2) a convolutional layer with a kernel size of d × d and channel number d × c; (3) a reshape and softmax layer to generate d weights A w (I), w ∈ (int) <ref type="bibr">[1, d]</ref> with a sum of 1 for each filter. Formally, our guided filtering with adaptive dilated function (D 4 LCN) is formulated as follows:</p><formula xml:id="formula_2">I = 1 d · k · k · I w A w (I) gi,gj D (gi * w,gj * w) ,<label>(2)</label></formula><p>For different images, our depth-guided filtering module assigns different kernels on different pixels and adaptive receptive fields (dilation) on different channels. This solves the problem of scale-sensitive and meaningless local structure of 2D convolutions, and also makes full use of RGB information compared to pseudo-LiDAR representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">2D-3D Detection Head</head><p>In this work, we adopt a single-stage detector with priorbased 2D-3D anchor boxes <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b31">32]</ref> as our base detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Formulation</head><p>Inputs: The output feature map I 4 ∈ R h4×w4 of our backbone network with a network stride factor of 16. Following common practice, we use a calibrated setting which assumes that per-image camera intrinsics K ∈ R 3×4 are available both at the training and test time. The 3D-to-2D  projection can be written as:</p><formula xml:id="formula_3">  x y 1   P · z 3D = K ·     x y z 1     3D<label>(3)</label></formula><p>where  <ref type="bibr" target="#b33">[34]</ref>. Note that we use the minimum enclosing rectangle of the projected 3D box as our ground truth 2D bounding box.</p><p>Outputs: Let n a denote the number of anchors and n c denote the number of classes. For each position (i, j) of the input, the output for an anchor contains 35 + n c parameters:</p><formula xml:id="formula_4">{[t x , t y , t w , t h ] 2D , [t x , t y ] P , [t z , t w , t h , t l , t α ] 3D , t (m) C , s}, where [t x , t y , t w , t h ] 2D</formula><p>is the predicted 2D box; [t x , t y ] P is the position of the projected 3D corner in the 2D plane, [t z , t w , t h , t l , t α ] 3D denotes the depth, predicted 3D shape and rotation, respectively; t</p><formula xml:id="formula_5">(m) C = {[t (m) x , t (m) y ] P , [t (m) z ] 3D }, m ∈ {1, 2, .</formula><p>.., 8} denotes 8 projected 3D corners; s denotes the classification score of each class. The size of the output is h 4 × w 4 × n a × (35 + n c ), where (h 4 , w 4 ) is the size of the input image with a down sampling factor of 16. The output is actually an anchorbased transformation of the 2D-3D box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">2D-3D Anchor</head><p>Inspired by <ref type="bibr" target="#b0">[1]</ref>, we utilize 2D-3D anchors with priors as our default anchor boxes. More specifically, a 2D-3D anchor is first defined on the 2D space as in <ref type="bibr" target="#b31">[32]</ref> and then use the corresponding priors in the training dataset to calculate the part of it in the 3D space. One template anchor is defined using parameters of both spaces:</p><formula xml:id="formula_6">{[A x , A y , A w , A h ] 2D , [A z , A w , A h , A l , A α ] 3D }, where [A z , A w , A h , A l , A α ] 3D</formula><p>denotes the 3D anchor (depth, shape, rotation).</p><p>For 2D anchors [A x , A y , A w , A h ] 2D , we use 12 different scales ranging from 30 to 400 pixels in height following the power function of 30 * 1.265 exp , exp ∈ (int)[0, 11] and aspect ratios of [0.5, 1.0, 1.5] to define a total of 36 anchors. We then project all ground truth 3D boxes to the 2D space. For each projected box, we calculate its intersection over union (IoU) with each 2D anchor and assign the corresponding 3D box to the anchors that have an IoU ≥ 0.5. For each 2D anchor, we thus use the statistics across all matching ground truth 3D boxes as its corresponding 3D anchor</p><formula xml:id="formula_7">[A z , A w , A h , A l , A α ] 3D</formula><p>. Note that we use the same anchor parameters [A x , A y ] 2D for the regression of [t x , t y ] 2D and [t x , t y ] P . The anchors enable our network to learn a relative value (residual) of the ground truth, which significantly reduces the difficulty of learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Data Transformation</head><p>We combine the output of our network which is an anchorbased transformation of the 2D-3D box and the pre-defined anchors to obtain our estimated 3D boxes: </p><p>where [x , y ] P , [z , z (m) , α ] 3D denote respectively the estimated 3D center projection in 2D plane, the depth of 3D center and eight corners, the 3D rotation by combining output of the network and the anchor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Losses</head><p>Our overall loss contains a classification loss, a 2D regression loss, a 3D regression loss and a 2D-3D corner loss. We use the idea of focal loss <ref type="bibr" target="#b29">[30]</ref> to balance the samples. Let s t and γ denote the classification score of target class and the focusing parameter, respectively. We have:</p><formula xml:id="formula_9">L = (1 − s t ) γ (L class + L 2d + L 3d + L corner ),<label>(5)</label></formula><p>where γ = 0.5 in all experiments, and L class , L 2d , L 3d , L corner are the classification loss, 2D regression loss, 3D regression loss and D-3D corner loss, respectively.</p><p>In this work, we employ the standard cross-entropy (CE) loss for classification:</p><formula xml:id="formula_10">L class = − log(s t ).<label>(6)</label></formula><p>Moreover, for both 2D and 3D regression, we simply use the SmoothL1 regression losses:</p><formula xml:id="formula_11">L2D = SmoothL1([x , y , w , h ]2D, [x, y, w, h]2D), L3D = SmoothL1([w , h , l , z , α ]3D, [w, h, l, z, α]3D), + SmoothL1([x , y ]P , [x, y]P ), Lcorner = 1 8 SmoothL1([x (m) , y (m) ]P , [x (m) , y (m) ]P ) + SmoothL1([z (m) ]3D, [z]3D),<label>(7)</label></formula><p>where [x (m) , y (m) ] P denotes the projected corners in image coordinates of the GT 3D box and [z] 3D is its GT depth.  <ref type="bibr" target="#b39">40</ref> and split1/split2 in AP|R 11 . We use red to indicate the highest result with relative improvement in parentheses and blue for the second-highest result of the class car. Our method achieves 7 firsts and 2 seconds in 9 items.</p><p>second layer, where n c is set to 4 for three object classes and the background class, and n a is set to 36. Non Maximum Suppression (NMS) with an IoU threshold of 0.4 is used on the network output in 2D space. Since the regression of the 3D rotation α is more difficult than other parameters, a hillclimbing post-processing step is used for optimizing α as in <ref type="bibr" target="#b0">[1]</ref>. The input images are scaled to 512 × 1760 and horizontal flipping is the only data augmentation. n f is set to 2 and the maximum dilation rate d is set to 3 in all experiments. The network is optimized by stochastic gradient descent (SGD), with a momentum of 0.9 and a weight decay of 0.0005. We take a mini-batch size of 8 on 4 Nvidia Tesla v100 GPUs (16G). We use the 'poly' learning rate policy and set the base learning rate to 0.01 and power to 0.9. The iteration number for the training process is set to 40,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparative Results</head><p>We conduct experiments on the official test set and two splits of validation set of the KITTI dataset. <ref type="table" target="#tab_0">Table 1</ref> includes the top 14 monocular methods in the leaderboard, among which our method ranks top-1. We can observe that: (1) Our method outperforms the second-best competitor for monocular 3D car detection by a large margin (relatively 9.1% for 10.74 vs. 11.72) under the moderate setting (which is the most important setting of KITTI). (2) Most competitors, such as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b0">1]</ref>, utilize the detector (e.g. Faster-RCNN) pre-trained on COCO/KITTI or resort to multi-stage training to obtain better 2D detection and stable 3D results, while our model is trained end-to-end using the standard ImageNet pre-trained model. However, we still achieve the state-of-the-art 3D detection results, validating the effectiveness of our D 4 LCN to learn 3D structure. <ref type="bibr" target="#b2">(3)</ref> Recently KITTI uses AP| R40 instead of AP| R11 , however, all existing methods report the results under the old metric. We thus also give results under AP| R11 on the validation set for fair comparison. It can be seen that our method outper- forms all others on the two splits for 3D car detection. Our results under AP| R40 on validation set are shown in ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detailed Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation Study</head><p>To conduct ablation study on our model, we make comparison among five versions of our model: <ref type="formula" target="#formula_1">(1)</ref>    AP| R11 and AP| R40 metrics, respectively. This suggests that it is indeed effective to capture the meaningful local structure for 3D object detection. <ref type="formula" target="#formula_3">(3)</ref> The main improvement comes from our adaptive dilated convolution (2.69 and 1.76 for AP| R11 and AP| R40 , respectively), which allows each channel of the feature map to have different receptive fields and thus solves the scale-sensitive problem. Note that we have tried different values of n f ∈ {1, 2, 3, 4, 5, 6}, and found that n f = 3 is the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Evaluation of Depth Maps</head><p>To study the impact of accuracy of depth maps on the performance of our method, we extract depth maps using four different methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b2">3]</ref> and then apply them to 3D detection. As reported in previous works on depth estimation, the three supervised methods (i.e. PSMNet, Disp-Net, and DORN) significantly outperform the unsupervised method <ref type="bibr" target="#b12">[13]</ref>. Among the supervised methods, Stereo-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref> are better than monocular-based DORN. With these conclusions, we have the following observations from <ref type="table" target="#tab_3">Table 3</ref>: <ref type="formula" target="#formula_1">(1)</ref> The accuracy of 3D detection is higher with better depth map. This is because that better depth map can provide better scene geometry and local structure.</p><p>(2) As the quality of depth map increases, the growth of detection accuracy becomes slower. (3) Even with the depth maps obtained by unsupervised learning <ref type="bibr" target="#b12">[13]</ref>, our method achieves state-of-the-art results. Compared to the pseudolidar based method <ref type="bibr" target="#b32">[33]</ref>, our method relies less on the quality of depth maps (19.63 vs. 15.45 using MonoDepth).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Evaluation of Convolutional Appoaches</head><p>To show the effectiveness of our guided filtering module for 3D object detection, we compare it with several alternatives: Dynamic Convolution <ref type="bibr" target="#b19">[20]</ref>, Dynamic Local Filtering <ref type="bibr" target="#b19">[20]</ref>, and Deformable Convolution <ref type="bibr" target="#b8">[9]</ref>. Our method belongs  <ref type="table">Table 5</ref>. Multi-class 3D detection results of our method on the three data splits. Note that all pseudo-LiDAR based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b47">48]</ref> fail to detect pedestrians and cyclists. to dynamic networks but yields less computation cost and stronger representation. For the first two methods, we conduct experiments using the same depth map as ours. For the third method, we apply deformable convolution on both RGB and depth branches and merge them by element-wise product. From <ref type="table" target="#tab_4">Table 4</ref>, we can observe that our method performs the best. This indicates that our method can better capture 3D information from RGB images due to the special design of our D 4 LCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Multi-Class 3D Detection</head><p>Since a person is a non-rigid body, its shape varies and its depth information is hard to accurately estimate. For this reason, 3D detection of pedestrians and cyclists becomes particularly difficult. Note that all pseudo-LiDAR based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b47">48]</ref> fail to detect these two categories. However, as shown in <ref type="table">Table 5</ref>, our method still achieves satisfactory performance on 3D detection of pedestrians and cyclists. Moreover, we also show the active maps corresponding to different filters of our D 4 LCN in <ref type="figure">Figure 5</ref>. Different filters on the same layer of our model use different sizes of receptive fields to handle objects of different scales, including pedestrians (small) and cars (big), as well as distant cars (big) and nearby cars (small).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a Depth-guided Dynamic-Depthwise-Dilated Local ConvNet (D 4 LCN) for monocular 3D objection detection, where the convolutional kernels and their receptive fields (dilation rates) are different for different pixels and channels of different images. These kernels are generated dynamically conditioned on the depth map to compensate the limitations of 2D convolution and narrow the gap between 2D convolutions and the point cloudbased 3D operators. As a result, our D 4 LCN can not only address the problem of the scale-sensitive and meaningless local structure of 2D convolutions, but also benefit from the high-level semantic information from RGB images. Extensive experiments show that our D 4 LCN better captures 3D information and ranks 1 st for monocular 3D object detection on the KITTI dataset at the time of submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We would like to thank Dr. Guorun Yang for his careful proofreading. Ping Luo is partially supported by the HKU Seed Funding for Basic Research and SenseTime's Donation for Basic Research. Zhiwu Lu is partially supported by National Natural Science Foundation of China (61976220, 61832017, and 61573363), and Beijing Outstanding Young Scientist Program (BJJWZYJH012019100020098).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition of 3D Corners</head><p>We define the eight corners of each ground truth box as follows:</p><formula xml:id="formula_12">C (m) =   x (m) y (m) 1   P · z (m) 3D =     ry ·     ±w/2 ±h/2 ±l/2 0     3D +     x y z 1     3D     (8)</formula><p>where m ∈ (int) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> in a defined order, and r y is the egocentric rotation matrix. Note that we use allocentric pose for regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons between Two Rotation Definitions</head><p>As shown in <ref type="figure">Figure 6</ref>, while egocentric poses undergo viewpoint changes towards the camera when translated, allocentric poses always exhibit the same view, independent of the objects location. The allocentric pose α and the egocentric pose r y can be converted to each other according to the viewing angle θ.  <ref type="figure">Figure 6</ref>. Comparisons between egocentric (ry) and allocentric (α) poses. The car1 and car2 have the same egocentric pose, but they are observed on different sides (views). We use allocentric pose to keep the same view (car1 and car3).</p><formula xml:id="formula_13">α = r y − θ<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablative Results for Convolutional Methods</head><p>The Depth-guided filtering module in our D 4 LCN model can be decomposed into basic convolutional components:</p><p>• Traditional Convolutional Network The ablative results for these convolutional methods are shown in <ref type="table" target="#tab_7">Table 7</ref>. We can observe that: (1) Using the depth map to guide the convolution of each pixel brings a considerable improvement. (2) Depth-wise convolution with shift-pooling operator not only has fewer parameters (Section 3.2 of our main paper) but also gets better performance than the standard convolution. (3) The main improvement comes from our adaptive dilated convolution, which allows each channel of the feature map to have different receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparisons of Labeling Information and Training Strategies</head><p>We compare the labeling information and training strategies used in different monocular detection methods, as shown in <ref type="table">Table 6</ref>.</p><p>It can be seen that: (1) our model outperforms all existing methods by only using the depth map extracted from Method Depth CAD Points Freespace Segmentation Pretrain/MST End-to-end Deep3DBox <ref type="bibr" target="#b35">[36]</ref>, GS3D <ref type="bibr" target="#b25">[26]</ref>, MonoGRNet <ref type="bibr" target="#b40">[41]</ref>, OFTNet <ref type="bibr" target="#b42">[43]</ref> FQNet <ref type="bibr" target="#b30">[31]</ref>, SS3D <ref type="bibr" target="#b20">[21]</ref> ROI-10D <ref type="bibr" target="#b33">[34]</ref> Multi-Level Fusion <ref type="bibr" target="#b53">[54]</ref>, D 4 LCN (Ours) M3D-RPN <ref type="bibr" target="#b0">[1]</ref>, MONODIS <ref type="bibr" target="#b44">[45]</ref>, Shift R-CNN [37] AM3D <ref type="bibr" target="#b32">[33]</ref> Pseudo-LiDAR <ref type="bibr" target="#b47">[48]</ref>, Mono3D-PLiDAR <ref type="bibr" target="#b49">[50]</ref>, MonoPSR <ref type="bibr" target="#b22">[23]</ref> Deep-MANTA <ref type="bibr" target="#b1">[2]</ref> 3DVP <ref type="bibr" target="#b52">[53]</ref> Mono3D <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> Mono3D++ <ref type="bibr" target="#b16">[17]</ref>  <ref type="table">Table 6</ref>. Comparisons of the labeling information and training strategies used in different monocular detection methods. Notations: Pretrain -pre-trained on COCO/KITTI datasets; MST -multi-stage training; End-to-end -end-to-end training; Depth -the depth map extracted from monocular image; CAD -the CAD model; Points -the characteristic points labeling information; Freespace -the free space labeling information; Segmentation -the segmentation labeling information;  the monocular image.</p><p>(2) our model can be trained in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Distributions of Different Dilation</head><p>We show the average ratio of different channels with different dilation rates in three blocks of our model over the validation set of split1 ( <ref type="figure">Figure 7</ref>). It can be seen that: (1) For the first block with insufficient receptive field, the model tends to increase the receptive field by large dilation rate, and then it uses small receptive field for the second block.</p><p>(2) In the third block, the model uses three different dilation rates evenly to deal with the object detection of different scales. We also show the active maps corresponding to different filters of the third block of our D 4 LCN in our main paper ( <ref type="figure">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block1</head><p>Block2 <ref type="formula" target="#formula_3">Block3</ref>   <ref type="figure">Figure 7</ref>. The average ratio of different channels with different dilation rates in three blocks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Pseudo-LiDAR points from DORN (b) Pseudo-LiDAR from MonoDepth (c) Our result using MonoDepth (d) Result of Pesudo-LiDAR (a) and (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparisons among different convolutional approaches. (a) is the traditional 2D convolution that uses a single convolutional kernel applied on each pixel to convolve the entire image. (b) applies multiple fixed convolutional kernels on different regions (slices) of an image. (c) uses the depth map to generate dynamic kernels with the same receptive fields for each pixel. (d) denotes our approach, where the filter is dynamic, depth-wise, and has adaptive receptive fields for each pixel and channel of the feature map. It can be implemented more efficiently with fewer parameters than (c). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Overview of our framework for monocular 3D object detection. The depth map is first estimated from the RGB image and used as the input of out two-branch network together with the RGB image. Then the depth-guided filtering module is used to fuse there two information of each residual block. Finally, a one-stage detection head with Non-Maximum Suppression (NMS) is employed for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>An example of our shift-pooling operator of depth-wise convolution in depth-guided filtering module when n f is 3. It is efficiently implemented by shift and element-wise mean operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>[x, y, z] 3D denotes the horizontal position, height and depth of the 3D point in camera coordinates, and [x, y] P is the projection of the 3D point in 2D image coordinates. Ground Truth: We define a ground truth (GT) box using the following parameters: the 2D bounding box [x, y, w, h] 2D , where (x, y) is the center of 2D box and w, h are the width and height of 2D box; the 3D center [x, y, z] 3D represents the location of 3D center in camera coordinates; the 3D shapes [w, h, l] 3D (3D object dimensions: height, width, length (in meters)), and the allocentric pose α 3D in 3D space (observation angle of object, ranging [−π, π])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>[x , y ]2D = [Ax, Ay]2D + [tx, ty]2D * [Aw, A h ]2D [x , y ]P = [Ax, Ay]2D + [tx, ty]P * [Aw, A h ]2D [x (m) , y (m) ]P = [Ax, Ay]2D + [t (m) x , t (m) y ]P * [Aw, A h ]2D [w , h ]2D = [Aw, A h ]2D · exp([tw, t h ]2D) [w , h , l ]3D = [Aw, A h , A l ]3D · exp([tw, t h , t l ]3D) [z , z (m) , α ]3D = [Az, Az, Aα] + [tz, tz, t alpha ]3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Filter No. 41 :Figure 5 .</head><label>415</label><figDesc>adaptive dilated weights 0.14, 0.60 0.26 Filter No.89: adaptive dilated weights 0.05, 0.03, 0.92 Filter No.70: adaptive dilated weights 0.96, 0.02, 0.02 Input Image Visualization of active maps corresponding to different filters of block 3 of our D 4 LCN. Each filter learns three weights representing dilation rate of 1, 2, 3, respectively. Different filters have different functions in our model to handle the scale problem adaptively. For example, filter 89 has large receptive fields for large-scale cars, while filter 70 deals with the small-scale cars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>•</head><label></label><figDesc>Depth-guided ConvNet (CN) • Depth-guided Local CN (LCN) • Depth-guided Depth-wise LCN (DLCN) • Depth-guided DLCN with Shift-pooling (SP-DLCN) • D 4 LCN (Our full model)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>R11 proposed in the Pascal VOC benchmark is separately computed on each difficulty class and each object class. After that, the 40 recall positions-based metric AP| R40 is used instead of AP| R11 , following<ref type="bibr" target="#b44">[45]</ref>. All methods are ranked by AP| R11 of the 3D car detection in the moderate setting. Comparative results on the KITTI 3D object detection dataset. For the test set, only AP|R 40 is provided by the official leaderboard. We thus show the results on the test set in AP|R</figDesc><table><row><cell>4. Experiments</cell></row><row><cell>4.1. Dataset and Setting</cell></row><row><cell>KITTI Dataset. The KITTI 3D object detection dataset</cell></row><row><cell>[12] is widely used for monocular and LiDAR-based 3D</cell></row><row><cell>detection. It consists of 7,481 training images and 7,518</cell></row><row><cell>test images as well as the corresponding point clouds and</cell></row><row><cell>the calibration parameters, comprising a total of 80,256 2D-</cell></row><row><cell>3D labeled objects with three object classes: Car, Pedes-</cell></row><row><cell>trian, and Cyclist. Each 3D ground truth box is assigned</cell></row><row><cell>to one out of three difficulty classes (easy, moderate, hard)</cell></row><row><cell>according to the occlusion and truncation levels of objects.</cell></row><row><cell>There are two train-val splits of KITTI: the split1 [5] con-</cell></row><row><cell>tains 3,712 training and 3,769 validation images, while the</cell></row><row><cell>split2 [53] uses 3,682 images for training and 3,799 images</cell></row><row><cell>for validation. The dataset includes three tasks: 2D detec-</cell></row><row><cell>tion, 3D detection, and Bird's eye view, among which 3D</cell></row><row><cell>detection is the focus of 3D detection methods.</cell></row><row><cell>Evaluation Metrics. Precision-recall curves are used for</cell></row><row><cell>evaluation (with the IoU threshold of 0.7). Prior to Aug.</cell></row><row><cell>2019, 11-point Interpolated Average Precision (AP) met-</cell></row><row><cell>ric AP| Implementation Details. We use our depth-guided filter-</cell></row><row><cell>ing module three times on the first three blocks of ResNet,</cell></row><row><cell>which have different network strides of 4,8,16, respectively.</cell></row><row><cell>[11] is used for depth estimation. A drop-channel layer with</cell></row><row><cell>a drop rate of 0.2 is used after each module and a dropout</cell></row><row><cell>layer with a</cell></row></table><note>drop rate of 0.5 is used after the output of the network backbone. For our single-stage detector, we use two convolutional layers as our detection head. The number of channels in the first layer is 512, and n a * (35+n c ) for the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the class car on the KITTI split1.</figDesc><table><row><cell cols="2">Method Task</cell><cell cols="5">AP| R 11 Easy Moderate Hard Easy Moderate Hard AP| R 40</cell></row><row><cell></cell><cell>2D detection</cell><cell>93.42</cell><cell>85.16</cell><cell>68.14 94.13</cell><cell>84.45</cell><cell>65.73</cell></row><row><cell>3DNet</cell><cell>3D detection</cell><cell>17.94</cell><cell>14.61</cell><cell>12.74 16.72</cell><cell>12.13</cell><cell>09.46</cell></row><row><cell></cell><cell cols="2">Bird's-eye view 24.87</cell><cell>19.89</cell><cell>16.14 23.19</cell><cell>16.67</cell><cell>13.39</cell></row><row><cell></cell><cell>2D detection</cell><cell>94.04</cell><cell>85.56</cell><cell>68.50 94.98</cell><cell>84.93</cell><cell>66.11</cell></row><row><cell>+CL</cell><cell>3D detection</cell><cell>20.66</cell><cell>15.57</cell><cell>13.41 17.10</cell><cell>12.09</cell><cell>09.47</cell></row><row><cell></cell><cell cols="2">Bird's-eye view 2903</cell><cell>23.82</cell><cell>19.41 24.12</cell><cell>17.75</cell><cell>13.66</cell></row><row><cell></cell><cell>2D detection</cell><cell>92.98</cell><cell>85.35</cell><cell>68.63 93.81</cell><cell>86.71</cell><cell>70.19</cell></row><row><cell>+DLCN</cell><cell>3D detection</cell><cell>23.25</cell><cell>17.92</cell><cell>15.58 18.32</cell><cell>13.50</cell><cell>10.61</cell></row><row><cell></cell><cell cols="2">Bird's-eye view 27.76</cell><cell>22.89</cell><cell>18.73 26.78</cell><cell>18.68</cell><cell>15.14</cell></row><row><cell></cell><cell>2D detection</cell><cell>92.57</cell><cell>85.14</cell><cell>68.40 93.35</cell><cell>86.52</cell><cell>67.93</cell></row><row><cell>+SP</cell><cell>3D detection</cell><cell>25.30</cell><cell>19.02</cell><cell>17.26 19.69</cell><cell>14.44</cell><cell>11.52</cell></row><row><cell></cell><cell cols="2">Bird's-eye view 31.39</cell><cell>24.40</cell><cell>19.85 26.91</cell><cell>20.07</cell><cell>15.77</cell></row><row><cell></cell><cell>2D detection</cell><cell>93.59</cell><cell>85.51</cell><cell>68.81 94.25</cell><cell>86.93</cell><cell>70.34</cell></row><row><cell>D 4 LCN</cell><cell>3D detection</cell><cell>26.97</cell><cell>21.71</cell><cell>18.22 22.32</cell><cell>16.20</cell><cell>12.30</cell></row><row><cell></cell><cell cols="2">Bird's-eye view 34.82</cell><cell>25.83</cell><cell>23.53 31.53</cell><cell>22.58</cell><cell>17.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>{15.57, 12.09} to {21.71, 16.20} w.r.t. the Depth AP| R 11 AP| R 40 Easy Moderate Hard Easy Moderate Hard</figDesc><table><row><cell>3DNet: the base-line model using L 2D and L 3D without our depth-guided filtering module; (2) + CL: the Corner Loss is added to 3DNet; (3) + DLCN: depth-guided depth-wise local filter-ing is added; (4) + SP: shift-pooling operator is added (with n f = 3); (5) D 4 LCN (our full model): adaptive dilation rates are added, as in Eq. 2. From Table 2, we can observe that: (1) The performance continuously increases when more components are used for 3D object detection, showing the contribution of each component. (2) Our depth-guided filtering module increases the 3D detection AP scores (mod-erate) from MonoDepth [13] 22.43 19.63 16.38 16.82 13.18 10.87 DORN [11] 26.97 21.71 18.22 22.32 16.20 12.30 DispNet [35] 30.95 24.06 20.29 25.73 18.56 15.10 PSMNet [3] 30.03 25.41 21.63 25.24 19.80 16.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparisons of depth maps of different quality for 3D detection on the class car on the KITTI split1.</figDesc><table><row><cell>Conv module</cell><cell>AP| R 11 Easy Moderate Hard Easy Moderate Hard AP| R 40</cell></row><row><cell>Dynamic [20]</cell><cell>23.01 17.67 15.85 17.47 12.18 09.53</cell></row><row><cell cols="2">Dynamic Local [20] 25.15 18.42 16.27 21.09 13.93 11.31</cell></row><row><cell>Deformable [9]</cell><cell>23.98 18.24 16.11 19.05 13.42 10.07</cell></row><row><cell>D 4 LCN (ours)</cell><cell>26.97 21.71 18.22 22.32 16.20 12.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of different convolutional modules for car 3D detection on the KITTI split1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Car 26.97/24.29/16.65 21.71/19.54/11.72 18.22/16.38/9.51 Pedestrian 12.95/12.52/4.55 11.23/10.37/3.42 11.05/10.23/2.83 Cyclist 5.85/7.05/2.45 4.41/6.54/1.67 4.14/6.54/1.36</figDesc><table><row><cell>Class</cell><cell>Easy [split1/split2/test] [split1/split2/test] [split1/split2/test] Moderate Hard</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Comparisons of different convolutional methods for car 3D detection on the KITTI split1.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Céline</forename><surname>Teulière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03446</idno>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint monocular 3d vehicle detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou-Ning</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5390" to="5399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection and box fitting trained endto-end using intersection-over-union loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eskil</forename><surname>Jörgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08070</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rui Hu, and Raquel Urtasun. Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Wanli Ouyang, and Xin Fan</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Shift r-cnn: Deep monocular 3d object detection with closed-form geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andretti</forename><surname>Naiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Paunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongmo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09970</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Byeong-Moon Jeon, and Marius Leordeanu</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08188</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12365</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning guided convolutional network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fei-Peng Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01238</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Voxel-fpn: multiscale voxel feature aggregation in 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05286</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01864</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection with pseudo-lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09847</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Shift: A zero flop, zero parameter alternative to spatial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9127" to="9135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic filtering with large sampling field for convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandrajit</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Are cars just 3d boxes?-jointly estimating the 3d shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Muhammad Zeeshan Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3678" to="3685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
