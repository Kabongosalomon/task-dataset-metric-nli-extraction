<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab ± The</orgName>
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong Tencent AI</orgName>
								<orgName type="institution" key="instit3">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab ± The</orgName>
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong Tencent AI</orgName>
								<orgName type="institution" key="instit3">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab ± The</orgName>
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong Tencent AI</orgName>
								<orgName type="institution" key="instit3">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<email>yu.zhang.ust@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab ± The</orgName>
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong Tencent AI</orgName>
								<orgName type="institution" key="instit3">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
							<email>xzhangax@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab ± The</orgName>
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong Tencent AI</orgName>
								<orgName type="institution" key="instit3">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
							<email>lixin@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab ± The</orgName>
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong Tencent AI</orgName>
								<orgName type="institution" key="instit3">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">±</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab ± The</orgName>
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong Tencent AI</orgName>
								<orgName type="institution" key="instit3">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<email>qyang@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab ± The</orgName>
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong Tencent AI</orgName>
								<orgName type="institution" key="instit3">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab ± The</orgName>
								<orgName type="institution" key="instit1">University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong Tencent AI</orgName>
								<orgName type="institution" key="instit3">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-level sentiment classification (ASC) aims at identifying sentiment polarities towards aspects in a sentence, where the aspect can behave as a general Aspect Category (AC) or a specific Aspect Term (AT). However, due to the especially expensive and labor-intensive labeling, existing public corpora in AT-level are all relatively small. Meanwhile, most of the previous methods rely on complicated structures with given scarce data, which largely limits the efficacy of the neural models. In this paper, we exploit a new direction named coarse-to-fine task transfer, which aims to leverage knowledge learned from a rich-resource source domain of the coarse-grained AC task, which is more easily accessible, to improve the learning in a low-resource target domain of the fine-grained AT task. To resolve both the aspect granularity inconsistency and feature mismatch between domains, we propose a Multi-Granularity Alignment Network (MGAN). In MGAN, a novel Coarse2Fine attention guided by an auxiliary task can help the AC task modeling at the same finegrained level with the AT task. To alleviate the feature false alignment, a contrastive feature alignment method is adopted to align aspect-specific feature representations semantically. In addition, a large-scale multi-domain dataset for the AC task is provided. Empirically, extensive experiments demonstrate the effectiveness of the MGAN. Source domain (Restaurant-R1) Target domain (Laptop-L) resolution Aspect term: Polarity: Positive fonts Aspect term: Polarity: Negative food food cheese Aspect category: Polarity: Positive food seafood sea Aspect category: Polarity: Positive restaurant cuisine Aspect category: Polarity: Negative MGAN w/o C2F MGAN PaS attention C2F attention C2A attention Prediction: Negative</p><p>The shells were crisp and authentic, the filling was real ricotta cheese, not the fake junk a lot of Italian places use these days.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Aspect-level sentiment classification (ASC) aims to infer sentiment polarities over aspect categories (AC) or aspect terms (AT) distributed in sentences <ref type="bibr">(Pang, Lee, and others 2008;</ref><ref type="bibr" target="#b11">Liu 2012</ref>). An aspect category implicitly appears in the sentence, which describes a general category of the entities. For example, in the sentence "The salmon is tasty while the waiter is very rude", the user speaks positively and negatively towards two aspect categories "food" and "service", respectively. An aspect term characterizes a specific entity that explicitly occurs in the sentence. Considering the same sentence "The salmon is tasty while the waiter is very rude", the aspect terms are "salmon" and "waiter", and the user expresses positive and negative sentiments over them, respectively. In terms of the aspect granularity, the AC task is coarse-grained while the AT task is fine-grained.</p><p>To model aspect-oriented sentiment analysis, equipping Recurrent Neural Networks (RNNs) with the attention Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. mechanism has become a mainstream approach <ref type="bibr" target="#b16">(Tang et al. 2015;</ref><ref type="bibr" target="#b17">Wang et al. 2016;</ref><ref type="bibr" target="#b13">Ma et al. 2017;</ref>, where RNNs aim to capture sequential patterns and the attention mechanism is to emphasize appropriate context features for encoding aspect-specific representations. Typically, attention-based RNN models can achieve good performance only when large corpora are available. However, AT-level datasets require the aspect terms to be comprehensively manually labeled or extracted by sequence labeling algorithms from the sentences, which is especially costly to obtain. Thus, existing public AT-level datasets are all relatively small, which limits the potential of neural models.</p><p>Nonetheless, we observe that plentiful AC-level corpora are more easily accessible. This is because that aspect categories are usually in a small set of general aspects that can be pre-defined. For example, commercial services such as review sites or social media can define a set of valuable aspect categories towards products or events in a particular domain (e.g., "food", "service", "speed", and "price" in the Restaurant domain). As a result, the mass collections of user preferences towards different aspect categories become practicable. Motivated by this observation, we propose a new problem named coarse-to-fine task transfer across both domain and granularity, with the aim of borrowing knowledge from an abundant source domain of the coarse-grained AC task to a small-scale target domain of the fine-grained AT task.</p><p>The challenges in fulfillment of this setting are two-fold: (1) task discrepancy: the two tasks concern with the aspects with different granularity. Source aspects are coarse-grained aspect categories, which lack a priori position information in the context. However, target aspects are fine-grained aspect terms, which have accurate position information. Thus, inconsistent granularity in aspects causes the discrepancy between tasks; (2) feature distribution discrepancy: generally the domains in the two tasks are different, which causes the distribution shift for both the aspects and its context between domains. For example, in the source Restaurant domain, tasty and delicious are used to express positive sentiment towards the aspect category "food", while lightweight and responsive often indicate positive sentiment towards the aspect term "mouse" in the target Laptop domain.</p><p>To resolve the challenges, we propose a novel framework named Multi-Granularity Alignment Network (MGAN) to simultaneously align aspect granularity and aspect-specific feature representations across domains. Specifically, the MGAN consists of two networks for learning aspect-specific representations for the two domains, respectively. First, to reduce the task discrepancy between domains, i.e., modeling the two tasks at the same fine-grained level, we propose a novel Coarse2Fine (C2F) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category (e.g., "salmon" to the "food"). Without any additional labeling, the C2F attention module can learn the coarse-to-fine process by an auxiliary task. Actually, more specific aspect terms and their position information are most directly pertinent to the expression of sentiment. The C2F module makes up these missing information for the source task, which effectively reduces the aspect granularity gap between tasks and facilitates the subsequent feature alignment.</p><p>Second, considering that a sentence may contain multiple aspects with different sentiments, thus capturing incorrect sentiment features towards the aspect can mislead feature alignment. To prevent false alignment, we adopt the Contrastive Feature Alignment (CFA) (Motiian et al. 2017) to semantically align aspect-specific representations. The CFA considers both semantic alignment by maximally ensuring the equivalent distributions from different domains but the same class, and semantic separation by guaranteeing distributions from both different classes and domains to be as dissimilar as possible. Moreover, we build a large-scale multidomain dataset named YelpAspect with 100K samples for each domain to serve as highly beneficial source domains. Empirically, extensive experiments demonstrate that the proposed MGAN model can achieve superior performances on two AT-level datasets from SemEval'14 ABSA challenge and an ungrammatical AT-level twitter dataset.</p><p>Our contributions of this paper are four-fold: (1) to the best of our knowledge, a novel transfer setting cross both domain and granularity is first proposed for aspect-level sentiment analysis; (2) a new large-scale, multi-domain AC-level dataset is constructed; (3) the novel Coarse2Fine attention is proposed to effectively reduce the aspect granularity gap between tasks; (4) empirical studies verify the effectiveness of the proposed model on three AT-level benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Traditional supervised learning algorithms highly depend on extensive handcrafted features to solve aspect-level sentiment classification <ref type="bibr" target="#b8">(Jiang et al. 2011;</ref><ref type="bibr" target="#b9">Kiritchenko et al. 2014</ref>). These models fail to capture semantic relatedness between the aspect and its context. To overcome this issue, the attention mechanism, which has been successfully applied in many NLP tasks <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014;</ref><ref type="bibr" target="#b16">Sukhbaatar et al. 2015;</ref><ref type="bibr" target="#b18">Yang et al. 2016;</ref><ref type="bibr" target="#b16">Shen et al. 2017)</ref>, can help the model explicitly capture intrinsic aspect-context association <ref type="bibr" target="#b16">(Tang et al. 2015;</ref><ref type="bibr" target="#b17">Tang, Qin, and Liu 2016;</ref><ref type="bibr" target="#b17">Wang et al. 2016;</ref><ref type="bibr" target="#b13">Ma et al. 2017;</ref><ref type="bibr" target="#b13">Ma, Peng, and Cambria 2018;</ref><ref type="bibr" target="#b9">Li et al. 2018a</ref>). However, most of these methods highly rely on data-driven RNNs or tailor-made structures to deal with complicated cases, which requires substantial AT-level data to train effective neural models. Different from them, the proposed model can highly benefit from useful knowledge learned from a related abundant domain of the AC-level task.</p><p>Existing domain adaptation tasks for sentiment analysis focus on traditional sentiment classification without considering the aspect <ref type="bibr" target="#b1">(Blitzer, Dredze, and Pereira 2007;</ref><ref type="bibr" target="#b13">Pan et al. 2010;</ref><ref type="bibr" target="#b7">Glorot, Bordes, and Bengio 2011;</ref><ref type="bibr" target="#b3">Chen et al. 2012;</ref><ref type="bibr" target="#b1">Bollegala, Weir, and Carroll 2013;</ref><ref type="bibr" target="#b19">Yu and Jiang 2016;</ref><ref type="bibr" target="#b10">Li et al. 2018b)</ref>. In terms of data scarcity and the value of task, transfer learning is more urgent for aspect-level sentiment analysis that characterizes users' different preferences. To the best of our knowledge, only a few studies have explored to transfer from a single aspect category to another in a same domain based on adversarial training <ref type="bibr" target="#b20">(Zhang, Barzilay, and Jaakkola 2017)</ref>. Different from that, we explore a motivated and challenging setting which aims to transfer cross both aspect granularity and domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Granularity Alignment Network</head><p>In this section, we introduce the proposed MGAN model. We first present the problem definition and notations, followed by an overview of the model. Then we detail the model with each components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition and Notations</head><p>Coarse-to-fine task transfer Suppose that we have sufficient AC-level labeled data</p><formula xml:id="formula_0">X s = {(x s k , a s k ), y s k } N s k=1 in a source domain D s , where y s k is the sentiment label for the k-th sentence-aspect pair (x s k , a s k ). Besides, only a small amount of AT-level labeled data X t = {(x t k , a t k ), y t k } N t k =1</formula><p>is available in a target domain D t . Note that each source aspect a s k belongs to a set of pre-defined aspect categories C while each target aspect a t k is a sub-sequence of x t k , i.e., aspect term. The goal of this task is to learn an accurate classifier to predict the sentiment polarity of target testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Overview of the MGAN model</head><p>The goal of the MGAN aims to transfer from a rich-resource source domain of an AC task to facilitate a low-resource target domain of an AT task. The architecture of the proposed MGAN is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, the MGAN consists of two networks for tackling the two aspect-level tasks respectively. To reduce the task discrepancy, the two networks contain different numbers of attention hops such that they can keep a consistent granularity and the symmetric information towards the aspect. In MGAN, two basic hop units are used similarly as common attention-based RNN models, where the Context2Aspect (C2A) attention aims to measure the importance of each aspect word and generate the aspect representation with the aid of each context word, and the Position-aware Sentiment (PaS) attention utilizes the obtained aspect representation and the position information of the aspect to capture relevant sentiment features in the context for encoding the aspect-specific representation.</p><p>Moreover, we build a Coarse2Fine (C2F) attention upon the C2A module to specifically model the source aspect before feeding to the PaS module. The C2F module uses the source aspect representation to attend corresponding aspect terms in the context and then the attended context features  is conversely predicted the category of the source aspect (pseudo-label). After obtaining aspect-specific representations, the knowledge transfer between the two tasks is via the contrastive feature alignment. In summary, the source network acts as a "teacher", which consists of three-level attention hops (C2A+C2F+PaS) for the AC task, while the target network is like a "student" that only uses two basic attention hops (C2A+PaS) for the AT task. In the following sections, we introduce each component of MGAN in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-directional LSTM layer</head><p>Given a sentence-aspect pair (x, a) from the source or target domain, we assume that the sentence consists of n words, i.e., x = {w 1 , w 2 , ..., w n }, and the aspect contains m words, i.e., a = {w a 1 , w a 2 , ..., w a m }. Then we map them into its embedding vectors e = {e i } n i=1 ∈ R n×dw and e a = {e a j } m j=1 ∈ R m×dw respectively. To capture phrase-level sentiment features in the context (e.g., "not satisfactory"), we employ a Bi-directional LSTM (Bi-LSTM) to preserve the contextual information for each word of the input sentence. The Bi-LSTM transforms the input e into the con-</p><formula xml:id="formula_1">textualized word representations h = {h i } n i=1 ∈ R n×2d h (i.e.</formula><p>hidden states of Bi-LSTM). For simplicity, we denote the operation of an LSTM unit on e i as LSTM(e i ). Thus, the contextualized word representation h i ∈ R 2d h is obtained as</p><formula xml:id="formula_2">h i = [ − −−− → LSTM(e i ); ← −−− − LSTM(e i )], i ∈ [1, n],<label>(1)</label></formula><p>where ";" denotes the vector concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context2Aspect (C2A) Attention</head><p>Not all aspect words contribute equally to the semantic of the aspect. For example, in the aspect term "techs at HP", the sentiment is usually expressed over the headword "techs" but seldom over modifiers like the brand name "HP". Thus, "techs" is more important than "at" and "HP". This also applies to the aspect category (e.g., "food seafood fish"). Thus, we propose the Context2Aspect (C2A) attention to measure the importance of the aspect words with regards to each context word. Formally, we calculate a pair-wise alignment matrix M ∈ R n×m between the context and the aspect, where the alignment score M (i, j) between the i-th context word and the j-th aspect word is obtained as</p><formula xml:id="formula_3">M (i, j) = tanh(W a [h i ; e a j ] + b a ),<label>(2)</label></formula><p>where W a and b a are learnable parameters. Then, we apply a row-wise softmax function to get probability distributions in each row. By defining δ(i) ∈ R m as the individual aspectlevel attention given the i-th context word, we average all the δ(i)'s to get the C2A attention as α = 1 n n i=1 δ(i). The C2A attention further contributes the context-aware aspect representation by h a * = m j=1 α j e a j , where * ∈ {s, t} denotes the source or target domain. We tackle the aspect representation h a * for the two tasks differently, where h a s is fed to the C2F module while h a t is directly fed to the PaS module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse2Fine (C2F) Attention</head><p>Aspect terms, which act as the true "opinion entity", are the most directly pertinent to the expression of sentiment. However, source task concerns with coarse-grained aspect categories that lack of detailed position information in the context. We wish to achieve task alignment such that the target task can leverage more useful knowledge learned from the source task at the same fine-grained level. It is observed that the number of source aspects is much smaller and many instances contain same aspect category, but the underlying entities can behave diversely in different contexts. For example, the aspect category "food seafood fish" can be instantiated as "salmon", "tuna", "taste" and etc. Based on this observation, we can capture more specific semantics of the source aspect and its position information conditioned on its context. Motivated by autoencoders , we introduce an auxiliary pseudo-label prediction task for the source task. In this task, a source aspect a s is not only regarded as a sequence of aspect words, but also as a pseudo-label (category of the aspect) y c , where c ∈ C and C is a set of aspect categories. We utilize the obtained aspect representation h a s for a s to attend the context and then the induced attention scores aggregate the context information to conversely predict the pseudo category label of a s itself. If the context contains the aspect term correlated closely to the source aspect, then the attention mechanism can emphasize it for better prediction. We denote this mechanism as Coarse2Fine attention, which is calculated as:</p><formula xml:id="formula_4">z f i = (u f ) T tanh(W f [h i ; h a s ] + b f ),<label>(3)</label></formula><formula xml:id="formula_5">β f i = exp(z f i ) n i =1 exp(z f i ) , (4) v a = n i=1 β f i h i ,<label>(5)</label></formula><p>where W f ∈ R du×(2d h +de) , b f ∈ R du and u f ∈ R du are the weights of the layer. We feed the attended representation v a to a softmax layer for the auxiliary task prediction, which is trained by minimizing the cross-entropy loss between the predicted pseudo-labelŷ c k and its ground-truth y c k as:</p><formula xml:id="formula_6">Laux = − 1 Ns Ns k=1 c∈C y c k logŷ c k .<label>(6)</label></formula><p>However, there may not exist corresponding aspect term when the context implicitly expresses a sentiment toward the aspect category. To overcome this issue, similar to the gate mechanism in RNN variants (Jozefowicz, Zaremba, and Sutskever 2015), we adopt a fusion gate F to adaptively controls the passed proportions of h a s and v a towards a more specific source aspect representation r a s :</p><formula xml:id="formula_7">F = sigmoid(W[v a ; h a s ]+b), (7) r a s = F h a s + (1 − F) W v a ,<label>(8)</label></formula><p>where W ∈ R de×(de+2d h ) and b ∈ R de are the weights of the gate, W ∈ R de×2d h performs dimension reduction, and denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position-aware Sentiment (PaS) Attention</head><p>Following an important observation found in <ref type="bibr" target="#b17">(Tang, Qin, and Liu 2016;</ref>) that a closer sentiment word is more likely to be the actual modifier of the aspect term (e.g., in "great food but the service is dreadful", "great" is more closer to "food" than "service"), we take the position information of the aspect term into consideration for designing the PaS attention. For the target domain, we adopt a proximity strategy to calculate the target position relevance between the i-th context word and aspect term as follows:</p><formula xml:id="formula_8">p t i =    1 − m0−i n i &lt; m 0 0 m 0 ≤ i ≤ m 0 + m 1 − i−(m0+m) n i &gt; m 0 + m ,<label>(9)</label></formula><p>where m 0 is the index of the first aspect word, n and m are the length of the sentence and aspect, respectively. Unfortunately, in the source domain where aspect category is given, the exact position of the corresponding aspect term is not directly accessible. Instead, the C2F attention vector β f ∈ R n , indicating the probability of each context word being an aspect term, can help establish the position relevance. We first define a location matrix L ∈ R n×n to represent the proximity of each word in the sentence:</p><formula xml:id="formula_9">L ii = 1 − |i − i | n , i, i ∈ [1, n].<label>(10)</label></formula><p>Then we calculate the source position relevance for i-th context word with the aid of C2F attention weights by</p><formula xml:id="formula_10">p s i = L i β f .<label>(11)</label></formula><p>Obviously, the i-th context word closer to a possible aspect term with a large value in β f will have a larger position relevance p s i . Finally, the PaS attention is calculated by a general form for both domains:</p><formula xml:id="formula_11">z o i = (u o ) T tanh(W o [h i ; r a * ] + b o ),<label>(12)</label></formula><formula xml:id="formula_12">γ o i = exp(p * i z o i ) n i =1 exp(p * i z o i ) ,<label>(13)</label></formula><formula xml:id="formula_13">v o = n i=1 γ o i h i ,<label>(14)</label></formula><p>where p * i is the position relevance and r a * is the input aspect representation, with * ∈ {s, t} denoting the source or target domain (note that r a t = h a t ). Then we pass the aspectspecific representation v o to a fully-connected layer and a softmax layer for sentiment classification. The sentiment classification tasks for both domain are trained by minimizing two cross-entropy losses L s sen and L t sen , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Feature Alignment</head><p>After obtaining aspect-specific representations of two domains at the same granularity, we would further bridge the distribution gap across domains. The prevalent unsupervised domain adaptation methods <ref type="bibr" target="#b7">(Gretton et al. 2007;</ref><ref type="bibr" target="#b6">Ganin et al. 2016</ref>) require enormous unlabeled target data to achieve satisfactory performances, which is impractical in our problem where collecting unlabeled data needs laborintensive annotations of all aspect terms in the sentences. Therefore, inspired by (Motiian et al. 2017), we perform Contrastive Feature Alignment (CFA) by fully utilizing the limited target labeled data to semantically align representations across domains. Mathematically, we parameterize the two networks by g s and g t , and denote the probability distribution by P. Specifically, the CFA consisits of semantic alignment (SA) and semantic separation (SS). The SA aims to ensure identical distributions of feature representations P(g s (X s )) and P(g t (X t )) conditioned on different domains but the same class, while the SS further alleviates false alignment by guaranteeing P(g s (X s )) and P(g t (X t )) to be as dissimilar as possible conditioned on both different domains and classes. Considering that only a small amount of target labeled data is available, we revert the CFA characterizing distributions with enough data to pair-wise surrogates as:</p><formula xml:id="formula_14">L cf a = k,k ω(gs(x s k , a s k ), gt(x t k , a t k )),<label>(15)</label></formula><p>where ω(·, ·) is a contrastive function that performs semantic alignment or separation in terms of supervised information from both domains. Formally, ω(·, ·) is defined as:</p><formula xml:id="formula_15">ω(u, v) = u−v 2 if y s k = y t k , max(0, D− u−v 2 ) if y s k = y t k ,<label>(16)</label></formula><p>where D is a parameter dictating the degree of separation and is set to 1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternating Training</head><p>Combining the losses we introduced before together with a 2 regularization, we constitute the overall losses for the source and target networks as:</p><formula xml:id="formula_16">L src = L s sen + L aux + λL cf a + ρL s reg ,<label>(17)</label></formula><p>L tar = L t sen + λL cf a + ρL t reg ,</p><p>where λ, ρ balance the effect of the CFA loss and the 2 regularization loss, respectively. The source network has one more auxiliary loss L aux compared with the target one to achieve task alignment. The whole training procedure consists of two stages: (1) To prevent early overfitting of the target domain, the source network S is individually trained on the source domain by optimizing L s sen + L aux + ρL s reg . Then, S and the BiLSTM, C2A, and PaS modules of S are used to initialize the source and target networks of the MGAN, respectively. (2) We alternately optimize L src for the source network and L tar for the target network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>Source: AC-level We build a large-scale, multi-domain dataset named YelpAspect as source domains, which is obtained similarly as the Yelp recommendation dataset (Bauman, <ref type="bibr" target="#b0">Liu, and Tuzhilin 2017)</ref>. Specifically, YelpAspect contains three domains: Restaurant (R1), Beautyspa (B), and Hotel (H). The statistics of the YelpAspect dataset are summarized in <ref type="table" target="#tab_2">Table 1</ref>. Yelp reviews are collected in US cities over six years. Aspect categories and sentiment labels are identified by the "industrial-strength" Opinion Parser (OP) system <ref type="bibr" target="#b15">(Qiu et al. 2011;</ref>. To be consistent with the target domain datasets, YelpAspect is preprocessed in the sentence level by OP, while the dataset in (Bauman, <ref type="bibr" target="#b0">Liu, and Tuzhilin 2017)</ref> is in the document level. Moreover, we manually double-check to correct wrong annotations produced by OP system and purposely select more negation, contrastive and question instances to make it more challenging. The dataset is available at https://github.com/ hsqmlzno1/MGAN.   . The Twitter dataset is collected by <ref type="bibr" target="#b5">(Dong et al. 2014)</ref>, containing ungrammatical twitter posts.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>To evaluate our proposed method, we construct eight coarseto-fine transfer tasks: R1→L, H→L, B→L, H→R2, B→R2, R1→T, H→T, B→T, where we do not use the pair (R1, R2) as they come from the same domain. For each transfer pair D s →D t , the training data from domain D s and randomly sampled 90% training data from domain D t are used for training, the rest 10% training data from D t is used for validation, and the testing data from D t is used for testing. Evaluation metrics are Accuracy and Macro-Average F1, where the latter is more suitable for imbalanced datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>The word embeddings are initialized with 200-dimension GloVE vectors <ref type="bibr" target="#b14">(Pennington, Socher, and Manning 2014)</ref> and fine-tuned during the training. d e , d h , d u are set to be 200, 150 and 100, respectively. The fc layer size is 300. The Adam (Kingma and Ba 2014) is used as the optimizer with the initial learning rate 10 −4 . Gradients with the 2 norm larger than 40 are normalized to be 40. All weights in networks are randomly initialized from a uniform distribution U (−0.01, 0.01). The batch sizes are 64 and 32 for source and target domains, respectively. The control-off factors λ, ρ are set to be 0.1 and 10 −6 . To alleviate overfitting, we apply dropout on the word embeddings of the context with dropout rate 0.5. We also perform early stopping on the validation set during the training process. The hyperparameters are tuned on 10% randomly held-out training data of the target domain in R1→L task and are fixed to be used in all transfer pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>The baseline methods are divided into two groups: Non-Transfer To demonstrate the benefits from coarse-tofine task transfer, we compare with the following state-ofthe-art AT-level methods without transfer:   • Target Network (TN): It is our proposed base model (BiLSTM+C2A+Pas) trained on D t for the target task.</p><p>For IAN, we report the results in the original paper and use the source codes of other methods for experiments.</p><p>Transfer To investigate the effectiveness of the CFA , we also compare the following transfer methods:</p><p>• Source-only (SO): It uses a source network trained on D s to initialize a target network and then tests it on D t .</p><p>• Fine-tuning (FT): It advances SO with further finetuning the target network on D t .</p><p>• M-DAN: It is a multi-adversarial version of Domain Adversarial Network (DAN) <ref type="bibr" target="#b6">(Ganin et al. 2016</ref>) based on multiple domain discriminators. All discriminators are built upon the PaS layers of the two networks, each of which aligns one class distribution between domains.</p><p>• M-MMD: Similar with M-DAN, M-MMD aligns different class distributions between domains based on multiple Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b7">(Gretton et al. 2007)</ref>. For each MMD, following the <ref type="bibr" target="#b2">(Bousmalis et al. 2016)</ref>, we use a linear combination of 19 RBF kernels with the width parameters ranging from 10 −6 to 10 6 .</p><p>The original DAN and MMD are unsupervised domain adaptation methods. Thus, for fair comparison, we use the source code of DAN and MMD, and extend them to M-DAN and M-MMD that utilize target supervised information and have higher performances, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result Analysis</head><p>Comparison with Non-Transfer Note that we are the first to explore transfer techniques and achieve the best performances in this task. Thus, it is necessary to show our improvements over current superior non-transfer methods. The classification results are shown in  TN that does not utilize the knowledge from the source task, can only compete against with the baselines. It could be more convincing that the MGAN can achieve superior performances even with a simple model for the target task. This also indicates that the efficacy of MGAN benefits from leveraging useful knowledge learned from the source task.</p><p>(2) MGAN consistently outperforms the MGAN w/o C2F, where C2F module of the source network is removed and the source position information is missed (we set all p s i to 1), by 1.41%, 1.03%, 1.09% for accuracy and 1.79%, 3.62% and 1.16% for Macro-F1 on average. This is because that the C2F can effectively reduce the aspect granularity gap between tasks such that more useful knowledge can be distilled to facilitate the target task. (3) Position information is crucial for aspect-level sentiment analysis. The MGAN w/o PI, which does not utilize the position information, performs very poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Transfer</head><p>To avoid the effect of aspect granularity gap, all these models keep the C2F module. The compared results are shown in <ref type="table" target="#tab_8">Table 4</ref>. SO performs poorly due to no adaptation applied. The popular technique FT cannot achieve satisfactory results since fine-tuning may cause the oblivion of useful knowledge from the source task. The full model MGAN outperforms M-DAN and M-MMD by 1.80% and 1.33% for accuracy and 1.90% and 1.66% for Marco-F1 on average, respectively. We derive two possible    reasons: First, enormous target data is unavailable since it is hard to obtain, thus, it may be insufficient to represent target distributions by limited target labeled data for the distribution alignment based methods; Second, M-DAN and M-MMD focus on the semantic alignment but ignore semantic separation. Remarkably, MGAN considers both of them in a point-wise surrogate, which altogether improves the performance of our method. Besides, MGAN outperforms its ablation MGAN w/o SS removing the semantic separation loss of the CFA by 0.81% for accuracy and 1.00% for Macro-F1 on average, which implies that the semantic separation plays an important role in alleviating false alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of C2F Attention Module</head><p>We now give some illustrated examples to show the effect of C2F for solving aspect granularity inconsistency, by comparing MGAN and MGAN w/o C2F. Some hard cases containing multiple sentiment-aspect paris in the R1→L task are shown in <ref type="figure" target="#fig_3">Figure 2</ref>. In the source domain R1, both models first utilize the C2A to attend the informative part of the aspect category, e.g., "cheese", "seafood sea" and "cuisine", which are representatives for each aspect. Then, compared with MGAN w/o C2F, MGAN further uses C2F to capture more specific aspect terms from the context towards the aspect category, such as "shells" to food seafood sea, which helps the source task capture more fine-grained semantics of aspect category and detailed position information like the target task, such that the sentiment attention can be positionaware and identify more relevant sentiment features towards the aspect. For example, in the (a) and (c), the user expresses a positive sentiment over food food cheese but a negative attitude towards restaurant cuisine (cuisine means a style of cooking especially as a characteristic of a particular country or region). MGAN captures the regional words for the cooking style, i.e., "italian place" towards restaurant cuisine and the related n-gram sentiment feature "fake junk" instead of the "not the fake junk" for the "ricotta cheese", and finally makes a correct prediction, which helps distill more useful knowledge for subsequent feature alignment. While MGAN w/o C2F locates wrong sentiment contexts and fails in (c). As such, benefited from distilled knowledge from the source task, MGAN can better model the complicated relatedness between the context and aspect term for the target domain L, but MGAN w/o C2F performs poorly though it make true predictions in (d) and (e). Moreover, as shown in <ref type="figure" target="#fig_4">Figure 3</ref>, we list some samples of captured associated aspect terms towards different aspect categories based on the highest C2F attention weight. These underlying aspect terms make the source task more correlated to the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we explore a motivated direction for aspectlevel sentiment classification named coarse-to-fine task transfer and build a large-scale YelpAspect dataset as highly beneficial source benchmarks. A novel MGAN model is proposed to solve both aspect granularity inconsistency and domain feature mismatch problems, and achieves superior performances. Moreover, there are many other potential directions, like transferring between different aspect categories across domains, transferring to a AT-level task where the aspect terms are also not given and need to be firstly identified. We believe all these can help improve the ASC task and there will be more effective solutions coming in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of the Multi-Granularity Alignment Network (MGAN) model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>TD-LSTM<ref type="bibr" target="#b16">(Tang et al. 2015)</ref>: It employs two LSTMs to model the left and right contexts of the aspect and then concatenates the context representations for prediction.• AE-LSTM, and ATAE-LSTM (Wang et al. 2016): AE-LSTM is a simple LSTM model incorporating the aspect embedding as input, while ATAE-LSTM extends AE-LSTM with the attention mechanism. • MemNet (Tang, Qin, and Liu 2016): it applies a memory network with multi-hops attentions and predicts sentiment based on the top-most context representations. • IAN (Ma et al. 2017): It adopts two LSTMs to learn the representations of the context and the aspect interactively; • RAM (Chen et al. 2017): It employs multiple attentions with a GRU cell to non-linearly combine the aggregation of word features in each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>resolution but the fonts are small .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>✔Figure 2 :</head><label>2</label><figDesc>Prediction: NegativeAir has higher resolution but the fonts are small . Visualization of attention: MGAN versus MGAN w/o C2F in the R1→L task. Deeper color denotes higher weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Associated aspect terms towards different aspect categories captured by C2F attention in the R1→L task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The YelpAspect dataset. #Asp denotes the number of aspect categories.</figDesc><table><row><cell>Target: AT-level For target domains, we use three public</cell></row><row><cell>benchmark datasets: Laptop (L), Restaurant (R2) and Twit-</cell></row><row><cell>ter (T). The Laptop and Restaurant are from SemEval'14</cell></row><row><cell>ABSA challenge (Kiritchenko et al. 2014) by removing a</cell></row><row><cell>few examples which have "conflict labels" as done in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>summarizes the statistics of the target domain datasets.</figDesc><table><row><cell>Target Domain</cell><cell></cell><cell cols="3">#Pos #Neu #Neg</cell></row><row><cell>Laptop (L)</cell><cell>Train Test</cell><cell>980 340</cell><cell>454 171</cell><cell>858 128</cell></row><row><cell>Restaurant (R2)</cell><cell cols="3">Train 2,159 632 Test 730 196</cell><cell>800 195</cell></row><row><cell>Tweets (T)</cell><cell cols="4">Train 1,567 3,127 1,563 Test 174 346 174</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the target domain datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Experimental results (%). The marker † refers to p-value &lt; 0.05 when comparing with MGAN w/o C2F , while the marker ‡ refers to p-value &lt; 0.05 when comparing with RAM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>. The results of our</cell></row><row><cell>full model and its ablations are calculated by averaging over</cell></row><row><cell>each target domain among eight transfer pairs (e.g., R2 is ob-</cell></row><row><cell>tained by averaging over H→R2 and B→R2). Based on the</cell></row><row><cell>table, we have the following observations: (1) Our full model</cell></row><row><cell>MGAN consistently and significantly achieves the best re-</cell></row><row><cell>sults in all target domains, outperforming the strongest base-</cell></row><row><cell>line RAM by 4.13%, 3.58%, 5.26% for accuracy and 2.99%,</cell></row><row><cell>2.94% and 6.23% for Macro-F1 on average. Our base model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Experimental results (%). The marker † refers to p-value &lt; 0.05 when comparing with MGAN w/o SS.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by Hong Kong CERG grants (16209715 and 16244616), and NSFC (61473087 and 61673202).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction: Positive</head><p>The shells were crisp and authentic, the filling was real ricotta cheese, not the fake junk a lot of Italian places use these days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MGAN w/o C2F MGAN Prediction: Positive</head><p>Air has higher resolution but the fonts are small .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction: Negative</head><p>Air has higher resolution but the fonts are small .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction: Positive</head><p>The shells were crisp and authentic, the filling was real ricotta cheese, not the fake junk a lot of Italian places use these days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction: Positive</head><p>The shells were crisp and authentic, the filling was real ricotta cheese, not the fake junk a lot of Italian places use these days. The shells were crisp and authentic, the filling was real ricotta cheese, not the fake junk a lot of Italian places use these days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction: Positive</head><p>The shells were crisp and authentic, the filling was real ricotta cheese, not the fake junk a lot of Italian places use these days.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aspect based recommendations: Recommending items with the most valuable aspects based on user reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tuzhilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dredze</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Bollegala, Weir, and Carroll</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1719" to="1731" />
		</imprint>
	</monogr>
	<note>Cross-domain sentiment classification using a sentiment sensitive thesaurus</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bousmalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="767" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for targetdependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bordes</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bengio ; Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end adversarial memory network for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiritchenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Adam: A method for stochastic optimization</title>
		<editor>IJCAI, 2237. [Li et al.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical attention transfer network for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00893</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Sentiment analysis: Mining opinions, sentiments, and emotions</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
	<note>Opinion mining and sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manning ; Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<idno>arXiv:1512.01100</idno>
	</analytic>
	<monogr>
		<title level="m">Effective lstms for target-dependent sentiment classification</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Liu ; Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08900</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Attention-based lstm for aspect-level sentiment classification</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barzilay</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jaakkola ; Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00188</idno>
		<title level="m">Aspect-augmented adversarial networks for domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
