<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Bifurcated Backbone Strategy for RGB-D Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Zhai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Ling</forename><surname>Shao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Junwei</forename><forename type="middle">Han</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Bifurcated Backbone Strategy for RGB-D Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales. When multi-level features meet multi-modal cues, the optimal feature aggregation and multi-modal learning strategy become a hot potato. In this paper, we leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to devise a novel cascaded refinement network. In particular, first, we propose to regroup the multi-level features into teacher and student features using a bifurcated backbone strategy (BBS). Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. Then, RGB and depth modalities are fused in a complementary way. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is simple, efficient, and backbone-independent. Extensive experiments show that BBS-Net significantly outperforms eighteen SOTA models on eight challenging datasets under five evaluation measures, demonstrating the superiority of our approach (âˆ¼4% improvement in S-measure vs. the top-ranked model: DMRA-iccv2019). In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research.</p><p>Index Terms-RGB-D salient object detection, bifurcated backbone strategy, multi-level features, cascaded refinement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE goal of salient object detection (SOD) is to find and segment the most visually prominent object(s) in an image <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Over the last decade, SOD has attracted significant attention due to its widespread applications in object recognition <ref type="bibr" target="#b3">[4]</ref>, content-based image retrieval <ref type="bibr" target="#b4">[5]</ref>, image segmentation <ref type="bibr" target="#b5">[6]</ref>, image editing <ref type="bibr" target="#b6">[7]</ref>, video analysis <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and visual tracking <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Traditional SOD algorithms <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> are typically based on handcrafted features and fall short in capturing high-level semantic information (see also <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>). Recently, convolutional neural networks (CNNs) have been used for RGB SOD <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, achieving better performance compared to the traditional methods.</p><p>However, the performance of RGB SOD models tends to drastically decrease when faced with certain complex scenarios (e.g., cluttered backgrounds, multiple objects, varying illuminations, transparent objects, etc) <ref type="bibr" target="#b17">[18]</ref>. One of the most important reasons behind these failure cases may be the lack of depth information, which is critical for saliency prediction. For example, an object with less texture but closer to the camera will be more salient than an object with more texture but farther away. Depth maps contain abundant spatial structure and layout information <ref type="bibr" target="#b18">[19]</ref>, providing geometrical cues for improving the performance of SOD. Besides, depth information can easily be obtained using popular devices, e.g., stereo cameras, Kinect and smartphones, which are becoming increasingly more ubiquitous. Therefore, various algorithms (e.g., <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>) have been proposed to solve the SOD problem by combining RGB and depth information (i.e., RGB-D SOD).</p><p>To efficiently integrate RGB and depth cues for SOD, researchers have explored different but complementary multi-modal and multi-level strategies <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref> and have achieved encouraging results. However, existing RGB-D SOD methods still have to solve the following challenges to make further progress:</p><p>(1) Effectively aggregating multi-level features. As discussed in <ref type="bibr" target="#b15">[16]</ref>, teacher features 1 contain rich semantic macro information and can serve as strong guidance for locating salient objects, while student features provide affluent micro details that are beneficial for refining object edges. Therefore, current RGB-D SOD methods use either a dedicated aggregation strategy <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> or a progressive merging process <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> to leverage multi-level features. However, because they directly fuse multi-level features without considering level-specific characteristics, these operations suffer from the inherent problem of noisy low-level features <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b26">[27]</ref>. As a result, several methods are easily confused by the background (e.g., first and second rows in <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>(2) Excavating informative cues from the depth modality. Previous algorithms usually regard the depth map as a fourth-channel input <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> of the original three-channel <ref type="bibr" target="#b0">1</ref>   <ref type="bibr" target="#b18">[19]</ref>, CPFP <ref type="bibr" target="#b20">[21]</ref>, TANet <ref type="bibr" target="#b17">[18]</ref>, PCF <ref type="bibr" target="#b21">[22]</ref> and Ours) and methods based on handcrafted features (i.e., SE <ref type="bibr" target="#b27">[28]</ref> and LBE <ref type="bibr" target="#b28">[29]</ref>). Our method generates higher-quality saliency maps and suppresses background distractors in challenging scenarios (top: complex background; bottom: depth with noise).</p><p>RGB image, or fuse RGB and depth features by simple summation <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and multiplication <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. However, these methods treat depth and RGB information from the same perspective and ignore the fact that RGB images capture color and texture, whereas depth maps capture the spatial relations among objects. Due to this modality difference, the above-mentioned simple combination methods are not very efficient. Further, depth maps are often of low quality, which introduces randomly distributed errors and redundancy into the network <ref type="bibr" target="#b36">[37]</ref>. For example, the depth map in the last row of <ref type="figure" target="#fig_0">Fig. 1</ref> is blurry and noisy. As a result, many methods (e.g., the top-ranked model DMRA <ref type="bibr" target="#b18">[19]</ref>) fail to detect the full extent of the salient object.</p><p>To address the above issues, we propose a novel Bifurcated Backbone Strategy Network (BBS-Net) for RGB-D SOD. The proposed method exploits multi-level features in a cascaded refinement way to suppress distractors in the lower layers. This strategy is based on the observation that teacher features provide discriminative semantic information without redundant details <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b26">[27]</ref>, which may contribute significantly to eliminating the lower-layer distractors. As shown in <ref type="figure">Fig. 2 (b)</ref>, BBS-Net contains two cascaded decoder stages: (1) Cross-modal teacher features are integrated by the first cascaded decoder CD1 to predict an initial saliency map S 1 . (2) Student features are refined by an element-wise multiplication with the initial saliency map S 1 and are then aggregated by another cascaded decoder CD2 to produce the final saliency map S 2 . To fully capture the informative cues in the depth map and improve the compatibility of RGB and depth features, we further introduce a depth-enhanced module (DEM). This module exploits the inter-channel and spatial relations of the depth features and discovers informative depth cues. Our main contributions are summarized as follows:</p><p>â€¢ We propose a powerful Bifurcated Backbone Strategy Network (BBS-Net) to deal with multiple complicated real-world scenarios in RGB-D SOD. To address the long-overlooked problem of noise in low-level features decreasing the performance of saliency models, we carefully explore the characteristics of multi-level features in a bifurcated backbone strategy (BBS), i.e., features are split into two groups, as shown in <ref type="figure">Fig. 2</ref> </p><formula xml:id="formula_0">(b).</formula><p>In this way, noise in student features can be eliminated effectively by the saliency map generated from teacher features. â€¢ We further introduce a depth-enhanced module (DEM) in BBS-Net to enhance the depth features before merging them with the RGB features. The DEM module concentrates on the most informative parts of depth maps by two sequential attention operations. We leverage the attention mechanism to excavate important cues from the depth features of multiple side-out layers. This module is simple but has proven effective for fusing RGB and depth modalities in a complementary way. â€¢ We conduct a comprehensive comparison with 18 SOTA methods using various metrics (e.g., max Fmeasure, MAE, S-measure, max E-measure, and PR curves). Experimental results show that BBS-Net outperforms all of these methods on eight public datasets, by a large margin. In terms of the quality of predicted saliency maps, BBS-Net generates maps with sharper edges and fewer background distractors compared to existing models. â€¢ We conduct a number of cross-dataset experiments to evaluate the quality of current popular RGB-D datasets and introduce a training set with high generalization ability for fair comparison and future research. Current RGB-D methods train their networks using the fixed training-test splits of different datasets, without exploring the difficulties of those datasets. To the best of our knowledge, we are the first to investigate this important but overlooked problem in the area of RGB-D SOD. This work is based on our previous conference paper <ref type="bibr" target="#b0">[1]</ref> and extends it significantly in four ways: 1) We provide more details and experiments regarding our BBS-Net model, including motivation, feature visualizations, experimental settings, etc. 2) We investigate several previously unexplored issues, including cross-dataset generalization ability, postprocessing methods, etc. 3) To further demonstrate our model performance, we conduct several comprehensive experiments over the recently released dataset, DUT <ref type="bibr" target="#b18">[19]</ref>. 4) We provide several failure cases of BBS-Net, perform indepth analyses and draw several novel conclusions which are critical in developing more powerful models in the future. We are hopeful that our study will provide deep insights into the underlying design mechanisms of RGB-D SOD, and will spark novel ideas. The complete algorithm implementations, benchmark results, and postprocessing toolbox have been made publicly available at https://github.com/zyjwuyan/BBS-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Salient Object Detection</head><p>Over the past several decades, SOD <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref> has garnered significant research attention due to its diverse applications <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref>. In early years, SOD methods were mainly based on intrinsic prior knowledge such as center-surround (a) Existing multi-level feature aggregation methods for RGB-D SOD <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>. (b) In this paper, we adopt a bifurcated backbone strategy (BBS) to split the multi-level features into student and teacher features. The initial saliency map S1 is utilized to refine the student features to effectively suppress distractors. Then, the refined features are passed to another cascaded decoder to generate the final saliency map S2.</p><p>color contrast <ref type="bibr" target="#b43">[44]</ref>, global region contrast <ref type="bibr" target="#b11">[12]</ref>, background prior <ref type="bibr" target="#b44">[45]</ref> and appearance similarity <ref type="bibr" target="#b45">[46]</ref>. However, these methods heavily rely on heuristic saliency cues and lowlevel handcrafted features, thus lacking the guidance of high-level semantic information.</p><p>Recently, to solve this problem, deep learning based methods <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b50">[51]</ref> have been explored, exceeding handcrafted features based methods in complex scenarios. These deep methods <ref type="bibr" target="#b51">[52]</ref> usually leverage CNNs to extract multilevel multi-scale features from RGB images and then aggregate them to predict the final saliency map. Such multilevel multi-scale features <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> can help the model better understand the contextual and semantic information to generate high-quality saliency maps. Besides, since imagebased SOD may be limited in some real-world applications such as video captioning <ref type="bibr" target="#b54">[55]</ref>, autonomous driving <ref type="bibr" target="#b55">[56]</ref> and robotic interaction <ref type="bibr" target="#b56">[57]</ref>, SOD algorithms <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> have also been explored for video analysis.</p><p>To further break through the limits of deep models, researchers have also proposed to excavate edge information <ref type="bibr" target="#b57">[58]</ref> to guide prediction. These methods use an auxiliary boundary loss to improve the training and representative ability of segmentation tasks <ref type="bibr" target="#b58">[59]</ref>- <ref type="bibr" target="#b60">[61]</ref>. With the auxiliary guidance from the edge information, deep models can predict maps with finer and sharper edges. In addition to edge guidance, another useful type of auxiliary information are depth maps, which capture the spatial distance information. These are the main focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RGB-D Salient Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ Traditional Models.</head><p>Previous algorithms for RGB-D SOD mainly rely on extracting handcrafted features <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> from RGB and depth images. Contrast-based cues, including edge, color, texture and region, are largely utilized by these methods to compute the saliency of a local region. For example, Desingh et al. <ref type="bibr" target="#b61">[62]</ref> adopted the regionbased contrast to calculate contrast strengths for the segmented regions. Ciptadi et al. <ref type="bibr" target="#b62">[63]</ref> used surface normals and color contrast to compute saliency. However, the local contrast methods are easily disturbed by high-frequency content <ref type="bibr" target="#b63">[64]</ref>, since they mainly rely on the boundaries of salient objects. Therefore, some algorithms, such as spatial prior <ref type="bibr" target="#b34">[35]</ref>, global contrast <ref type="bibr" target="#b64">[65]</ref>, and background prior <ref type="bibr" target="#b65">[66]</ref>, proposed to compute saliency by combining both local and global information.</p><p>To combine saliency cues from RGB and depth modalities more effectively, researchers have explored multiple fusion strategies. Some methods <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> process RGB and depth images together by regarding depth maps as fourthchannel inputs (early fusion). This operation is simple but does not achieve reliable results, since it disregards the differences between the RGB and depth modalities. Therefore, some algorithms <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref> extract the saliency information from the two modalities separately by first leveraging two backbones to predict saliency maps and then fusing the saliency results (late fusion). Besides, to enable the RGB and depth modalities share benefits, other methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b66">[67]</ref> fuse RGB and depth features in a middle stage and then produce the corresponding saliency maps (middle fusion). Deep models also use the above three fusion strategies, and our method falls under the middle fusion category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ Deep Models.</head><p>Early deep methods <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref> compute saliency confidence scores by first extracting handcrafted features, and then feeding them to CNNs. However, these algorithms need the low-level handcrafted features to be manually designed as input, and thus cannot be trained in an end-to-end manner. More recently, researchers have begun to extract deep RGB and depth features using CNNs in a bottom-up fashion <ref type="bibr" target="#b68">[69]</ref>. Unlike handcrafted features, deep features contain a lot of contextual and semantic information, and can thus better capture representations of the RGB and depth modalities. These methods have achieved encouraging results, which can be attributed to two important aspects of feature fusion. One is their extraction and fusion of multi-level and multi-scale features from different layers, while the other is the mechanism by which the two different modalities (RGB and depth) are combined.</p><p>Various architectures have been designed to effectively integrate the multi-scale features. For example, Liu et al. <ref type="bibr" target="#b24">[25]</ref> obtained saliency map outputs from each side-out features by feeding a four-channel RGB-D image into a single backbone (single stream). Chen et al. <ref type="bibr" target="#b21">[22]</ref> leveraged two independent networks to extract RGB and depth features respectively, and then combined them in a progressive merging way (double stream). Furthermore, to learn supplementary features, <ref type="bibr" target="#b17">[18]</ref> designed a three-stream network consisting of two modality-specific streams and a parallel cross-modal </p><formula xml:id="formula_1">F CD2 BConv3 C BConv3 BConv3 BConv3 BConv3 C UPÃ—4 (a) F CD1 Cascaded Decoder UPÃ—2 UPÃ—2 UPÃ—2 UPÃ—2 BConv3 Conv3Ã—3 11Ã—11Ã—2048 22Ã—22Ã—1024 44Ã—44Ã—512 UPÃ—8 88Ã—88Ã—64 88Ã—88Ã—256 UPÃ—4 UPÃ—2 T 1 (b) T 2 PTM Conv1Ã—1 TransB Conv1Ã—1 TransB Conv1Ã—1 88Ã—88Ã—32 S 1 S 2 G 352Ã—352Ã—1 352Ã—352Ã—3</formula><p>Depth RGB  âˆ¼ f cm 5 ) are first aggregated by the cascaded decoder (a) to produce the initial saliency map S1. Stage 2: Then, student features (f cm 1 âˆ¼ f cm 3 ) are refined by the initial saliency map S1 and are integrated by another cascaded decoder to predict the final saliency map S2. See Â§ 3 for details. distillation stream to exploit complementary cross-modal information in the bottom-up feature extraction process (three streams). Depth maps are sometimes low-quality and may thus contain significant noise or misleading information, which greatly decreases the performance of SOD models. To address this issue, Zhao et al. <ref type="bibr" target="#b20">[21]</ref> proposed a contrastenhanced network to improve the quality of depth maps using the contrast prior. Fan et al. <ref type="bibr" target="#b36">[37]</ref> designed a depth depurator unit to evaluate the quality of depth maps and filter out the low-quality ones automatically. Three recent works have explored uncertainty <ref type="bibr" target="#b69">[70]</ref>, depth prediction <ref type="bibr" target="#b70">[71]</ref> and a joint learning strategy <ref type="bibr" target="#b71">[72]</ref> for saliency detection and achieved reasonable performance. There were also some concurrent works published in recent top conferences (e.g., ECCV <ref type="bibr" target="#b72">[73]</ref>- <ref type="bibr" target="#b74">[75]</ref>). Discussing these works in detail is beyond the scope of this article. Please refer to the online benchmark (http://dpfan.net/d3netbenchmark/) and the latest survey <ref type="bibr" target="#b75">[76]</ref> for more details.</p><p>Different from the above works, which overlook the intrinsic noise in low-level features, we devise a bifurcated backbone strategy with a cascaded refinement mechanism, thereby effectively suppressing the noise in student features with the guidance of the teacher features (the last column in <ref type="figure">Fig. 8</ref>). Details of our approach are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Current popular RGB-D SOD models directly integrate multi-level features using a single decoder ( <ref type="figure">Fig. 2 (a)</ref>). In contrast, the network flow of the proposed BBS-Net ( <ref type="figure">Fig. 3</ref>) explores a bifurcated backbone strategy. In Â§ 3.2, we first detail the proposed bifurcated backbone strategy with the cascaded refinement mechanism. Then, to fully excavate informative cues from the depth map, we introduce a new depth-enhanced module in Â§ 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bifurcated Backbone Strategy (BBS)</head><p>Our cascaded refinement mechanism leverages the rich semantic information in high-level cross-modal features to suppress background distractors. To support such a feat, we devise a bifurcated backbone strategy (BBS). It divides the multi-level cross-modal features into two groups, i.e., G 1 = {Conv1, Conv2, Conv3} and G 2 ={Conv3, Conv4, Conv5}, where Conv3 is the split point. The original multi-scale information is well preserved by each group.</p><p>â€¢ Cascaded Refinement Mechanism. To effectively leverage the characteristics of the two groups' features, we train the network using a cascaded refinement mechanism. This mechanism first generates an initial saliency map with three cross-modal teacher features (i.e., G 2 ) and then enhances the details of the initial saliency map S 1 with three crossmodal student features (i.e., G 1 ), which are refined by the initial saliency map. This is based on the observation that high-level features contain rich semantic information that helps locate salient objects, while low-level features provide micro-level details that are beneficial for refining the boundaries. In other words, by exploring the characteristics of the multi-level features, this strategy can efficiently suppress noise in low-level cross-modal features, and can produce the final saliency map through a progressive refinement.</p><p>Specifically, we first merge RGB and depth features processed by the DEM <ref type="figure" target="#fig_3">(Fig. 4</ref>) to obtain the cross-modal features {f cm i ; i = 1, 2, ..., 5}. In stage one, the three crossmodality teacher features (i.e., f cm 3 , f cm 4 , f cm 5 ) are aggregated by the first cascaded decoder, which is denoted as:</p><formula xml:id="formula_2">S 1 = T 1 F CD1 (f cm 3 , f cm 4 , f cm 5 ) ,<label>(1)</label></formula><p>where F CD1 is the first cascaded decoder, S 1 is the initial saliency map, and T 1 represents two simple convolutional layers that transform the channel number from 32 to 1. In stage two, we leverage the initial saliency map S 1 to refine the three cross-modal student features, which is defined as:</p><formula xml:id="formula_3">f cm i = f cm i S 1 ,<label>(2)</label></formula><p>where f cm i (i âˆˆ {1, 2, 3}) represents the refined features and denotes the element-wise multiplication. After that, the three refined student features are aggregated by another decoder followed by a progressively transposed module (PTM), which is formulated as:</p><formula xml:id="formula_4">S 2 = T 2 F CD2 (f cm 1 , f cm 2 , f cm 3 ) ,<label>(3)</label></formula><p>where F CD2 is the second cascaded decoder, S 2 denotes the final saliency map, and T 2 represents the PTM module.</p><p>â€¢ Cascaded Decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>After computing the two groups of multi-level cross-modal features ({f</head><formula xml:id="formula_5">cm i , f cm i+1 , f cm i+2 }, i âˆˆ {1, 3}),</formula><p>which are a fusion of the RGB and depth features from multiple layers, we need to efficiently leverage the multi-scale multi-level information in each group to carry out the cascaded refinement. Therefore, we introduce a light-weight cascaded decoder <ref type="bibr" target="#b26">[27]</ref> to integrate the two groups of multi-level cross-modal features. As shown in <ref type="figure">Fig. 3 (a)</ref>, the cascaded decoder consists of three global context modules (GCM) and a simple feature aggregation strategy. The GCM is refined from the RFB module <ref type="bibr" target="#b76">[77]</ref>. Specifically, it contains an additional branch to enlarge the receptive field and a residual connection <ref type="bibr" target="#b67">[68]</ref> to preserve the information. The GCM module thus includes four parallel branches. For all of these branches, a 1 Ã— 1 convolution is first applied to reduce the channel size to 32. Then, for the k th (k âˆˆ {2, 3, 4}) branch, a convolution operation with a kernel size of 2k âˆ’ 1 and dilation rate of 1 is applied. This is followed by another 3 Ã— 3 convolution operation with the dilation rate of 2k âˆ’ 1. We aim to excavate the global contextual information from the cross-modal features. Next, the outputs of the four branches are concatenated together and a 1Ã—1 convolution operation is then applied to reduce the channel number to 32. Finally, the concatenated features form a residual connection with the input features. The GCM module operation in the two cascaded decoders is denoted by:</p><formula xml:id="formula_6">f gcm i = F GCM (f i ).<label>(4)</label></formula><p>To further improve the representations of cross-modal features, we leverage a pyramid multiplication and concatenation feature aggregation strategy to aggregate the crossmodal features ({f</p><formula xml:id="formula_7">gcm i , f gcm i+1 , f gcm i+2 }, i âˆˆ {1, 3})</formula><p>. As illustrated in <ref type="figure">Fig. 3 (a)</ref>, first, each refined feature f gcm i is updated by multiplying it with all higher-level features: are not of the same scale.</p><formula xml:id="formula_8">f gcm i = f gcm i Î  kmax k=i+1 Conv F U P (f gcm k ) ,<label>(5)</label></formula><p>represents the element-wise multiplication, and Conv(Â·) represents the standard 3Ã—3 convolution operation. Then, the updated features are integrated by a progressive concatenation strategy to produce the output:</p><formula xml:id="formula_9">S = T f gcm k ; Conv F U P f gcm k+1 ; Conv F U P (f gcm k+2 ) ,<label>(6)</label></formula><p>where S is the predicted saliency map, [x; y] denotes the concatenation operation of x and y, and k âˆˆ {1, 3}. In the first stage, T denotes two sequential convolutional layers (i.e., T 1 ), while, for the second stage, it represents the PTM module (i.e., T 2 ). The scale of the output of the second decoder is 88Ã—88, which is 1/4 of the groundtruth (352Ã—352), so directly upsampling the output to the size of the ground-truth will lose some details. To address this issue, we propose a simple yet effective progressively transposed module (PTM, <ref type="figure">Fig. 3 (b)</ref>) to generate the final predicted map (S 2 ) in a progressive upsampling way. It consists of two residual-based transposed blocks <ref type="bibr" target="#b77">[78]</ref> and three sequential 1 Ã— 1 convolutions. Each residual-based transposed block contains a 3Ã—3 convolution and a residualbased transposed convolution.</p><p>Note that the proposed cascaded refinement mechanism is different from the recent refinement strategies CRN <ref type="bibr" target="#b78">[79]</ref>, SRM <ref type="bibr" target="#b79">[80]</ref>, R3Net <ref type="bibr" target="#b80">[81]</ref>, and RFCN <ref type="bibr" target="#b16">[17]</ref> in its usage of the initial map and multi-level features. The obvious difference and advantage of the proposed design is that our model only requires one round of saliency refinement to produce a good saliency map, while CRN, SRM, R3Net, and RFCN all need more iterations, which increases both the training time and computational resources. Besides, the proposed cascaded mechanism is also different from CPD <ref type="bibr" target="#b26">[27]</ref> in that it exploits both the details in student features and the semantic information in teacher features, while suppressing the noise in the student features at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Depth-Enhanced Module (DEM)</head><p>To effectively fuse the RGB and depth features, two main problems need to be solved: a) the compatibility of RGB and depth features needs to be improved due to the intrinsic modality difference, and b) the redundancy and noise in low-quality depth maps must be reduced. Inspired by <ref type="bibr" target="#b81">[82]</ref>, we design a depth-enhanced module (DEM) to address the issues by improving the compatibility of multi-modal features and excavating informative cues from the depth features.</p><p>Specifically, let f rgb i , f d i represent the feature maps of the i th (i âˆˆ 1, 2, ..., 5) side-out layer from the RGB and depth branches, respectively. As shown in <ref type="figure">Fig. 3</ref>, each DEM is added before each side-out feature map from the depth branch to enhance the compatibility of the depth features. This side-out process improves the saliency representation of depth features and, at the same time, preserves the multilevel multi-scale information. The fusion process of the two modalities is depicted as:</p><formula xml:id="formula_10">f cm i = f rgb i + F DEM (f d i ),<label>(7)</label></formula><p>where f cm i denotes the cross-modal features of the i th layer. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the DEM module contains a sequential channel attention operation and a spatial attention operation, which are formulated as:</p><formula xml:id="formula_11">F DEM (f d i ) = S att C att (f d i ) ,<label>(8)</label></formula><p>in which C att (Â·) and S att (Â·) represent the spatial and channel attention operations, respectively. More specifically, the channel attention is implemented as:</p><formula xml:id="formula_12">C att (f ) = M P max (f ) âŠ— f,<label>(9)</label></formula><p>where P max (Â·) denotes the global max pooling operation for each feature map, M(Â·) represents a multi-layer (twolayer) perceptron, f denotes the input feature map, and âŠ— is the multiplication by the dimension broadcast. The spatial attention is denoted as:</p><formula xml:id="formula_13">S att (f ) = Conv R max (f ) f,<label>(10)</label></formula><p>where R max (Â·) is the global max pooling operation for each point in the feature map along the channel axis. The proposed depth enhanced module is different from previous RGB-D algorithms, which fuse the multi-level cross-modal features by direct concatenation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, enhance the multi-level depth features by a simple convolutional layer <ref type="bibr" target="#b18">[19]</ref> or improve the depth map by contrast prior <ref type="bibr" target="#b20">[21]</ref>.</p><p>To the best of our knowledge, we are the first to introduce the attention mechanism to excavate informative cues from depth features in multiple side-out layers. Our experiments (see Tab. 7 and <ref type="figure">Fig. 9</ref>) demonstrate the effectiveness of our approach in improving the compatibility of multi-modal features.</p><p>Besides, the spatial and channel attention mechanisms are different from the operation proposed in <ref type="bibr" target="#b81">[82]</ref>. Based on the fact that SOD aims at finding the most prominent objects in an image, we only leverage a single global max pooling <ref type="bibr" target="#b82">[83]</ref> to excavate the most critical cues in depth features, which reduces the complexity of the module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>â€¢ Training Loss.</p><p>Let H and W denote the height and width of the input images. Given the input RGB image X âˆˆ R HÃ—WÃ—3 and its corresponding depth map D âˆˆ R HÃ—WÃ—1 , our model predicts an initial saliency map S 1 âˆˆ [0, 1] HÃ—WÃ—1 and a final saliency map S 2 âˆˆ [0, 1] HÃ—WÃ—1 . Let G âˆˆ {0, 1} HÃ—WÃ—1 denote the binary ground-truth saliency map. We jointly optimize the two cascaded stages by defining the total loss:</p><formula xml:id="formula_14">L = Î± ce (S 1 , G) + (1 âˆ’ Î±) ce (S 2 , G),<label>(11)</label></formula><p>in which ce represents the binary cross entropy loss <ref type="bibr" target="#b20">[21]</ref> and Î± âˆˆ [0, 1] controls the trade-off between the two parts of the losses. The ce is computed as:</p><formula xml:id="formula_15">ce (S, G) = G log S + (1 âˆ’ G) log(1 âˆ’ S),<label>(12)</label></formula><p>where S is the predicted saliency map.</p><p>â€¢ Training Protocol. We use the PyTorch <ref type="bibr" target="#b86">[87]</ref> framework to implement our model on a single 1080Ti GPU. Parameters of the backbone network (ResNet-50 <ref type="bibr" target="#b67">[68]</ref>) are initialized from the model pre-trained on ImageNet <ref type="bibr" target="#b87">[88]</ref>. We discard the last pooling and fully connected layers of ResNet-50 and leverage each middle output of the five convolutional blocks as the side-out feature maps. The two branches do not share weights and the only difference between them is that the depth branch has the input channel number set to one.</p><p>Other parameters are initialized using the default PyTorch settings. The Adam algorithm <ref type="bibr" target="#b88">[89]</ref> is used to optimize our model. We set the initial learning rate to 1e-4 and divide it by 10 every 60 epochs. The input RGB and depth images are resized to 352Ã—352 for both the training and test phases. We augment all the training images using multiple strategies (i.e., random flipping, rotating and border clipping). It takes about ten hours to train the model with a mini-batch size of 10 for 150 epochs. Our experiments show that the model is robust to the hyper-parameter Î±. Thus, we set Î± to 0.5 (i.e., same importance for the two losses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>In Â§ 4.1, we first introduce the eight RGB-D datasets used in the experiments along with their training/test splits, and the five evaluation metrics. Then, we provide quantitative results and visual comparisons with SOTA methods in Â§ 4.2.</p><p>Finally, in Â§ 4.3, we perform a detailed ablation analysis to investigate the aggregation strategies and different modules in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>â€¢ Datasets. We conduct our experiments on eight challenging RGB-D SOD benchmark datasets: NJU2K <ref type="bibr" target="#b66">[67]</ref>, NLPR <ref type="bibr" target="#b31">[32]</ref>, STERE <ref type="bibr" target="#b83">[84]</ref>, DES <ref type="bibr" target="#b34">[35]</ref>, LFSD <ref type="bibr" target="#b84">[85]</ref>, SSD <ref type="bibr" target="#b85">[86]</ref>, SIP <ref type="bibr" target="#b36">[37]</ref> and DUT <ref type="bibr" target="#b18">[19]</ref>. Tab. 1 provides an overview of these RGB-D datasets. NJU2K <ref type="bibr" target="#b66">[67]</ref> is the largest RGB-D dataset containing 1, 985 image pairs. The stereo images were collected from the Internet and 3D movies, while photographs were taken by a Fuji W3 camera. NLPR <ref type="bibr" target="#b31">[32]</ref> consists of 1, 000 image pairs captured by a standard Microsoft Kinect with a resolution of 640 Ã— 480.</p><p>The images include indoor and outdoor scenes (e.g., offices, campuses, streets and supermarkets). STERE <ref type="bibr" target="#b83">[84]</ref> is the first stereoscopic photo collection, containing 1, 000 stereoscopic images downloaded from the Internet. Note that this dataset has two versions (979 vs. 1, 000); here, we utilize the 1, 000 images version in all experiments. DES <ref type="bibr" target="#b34">[35]</ref> is a small-scale RGB-D dataset that includes 135 indoor image pairs collected using a Microsoft Kinect with a resolution of 640 Ã— 480.  DUT <ref type="bibr" target="#b18">[19]</ref> includes multiple challenging scenes (e.g., transparent objects, multiple objects, complex backgrounds and low-intensity environments). It contains 800 indoor and 400 outdoor scenes. Image pairs were captured by a Lytro2 camera with a resolution of 600 Ã— 400.</p><p>â€¢ Training/Testing. We follow the same settings as <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref> for fair comparison. In particular, the training set contains 1, 485 samples from the NJU2K dataset and 700 samples from the NLPR dataset. The test set consists of the remaining images from NJU2K (500) and NLPR (300), and the whole of STERE (1, 000), DES, LFSD, SSD and SIP. As for the recent proposde DUT <ref type="bibr" target="#b18">[19]</ref> dataset, following <ref type="bibr" target="#b18">[19]</ref>, we adopt the same training data of DUT, NJU2K, and NLPR to train the compared deep models (i.e., DMRA <ref type="bibr" target="#b18">[19]</ref>, A2dele <ref type="bibr" target="#b89">[90]</ref>, SSF <ref type="bibr" target="#b90">[91]</ref>, and our BBS-Net) and test the performance on the test set of DUT. Please refer to Tab. 2 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ Evaluation Metrics.</head><p>We employ five widely used metrics, including S-measure (S Î± ) <ref type="bibr" target="#b91">[92]</ref>, E-measure (E Î¾ ) <ref type="bibr" target="#b92">[93]</ref>, F-measure (F Î² ) <ref type="bibr" target="#b93">[94]</ref>, mean absolute error (MAE), and precision-recall (PR) curves to evaluate various methods.</p><p>F-measure <ref type="bibr" target="#b93">[94]</ref> is a common evaluation metric based on the region similarity. It is defined as:</p><formula xml:id="formula_16">F i Î² = (1 + Î² 2 )P i Ã— R i Î² 2 Ã— P i + R i ,<label>(13)</label></formula><p>where P i and R i are the corresponding precision and recall for the threshold i (i âˆˆ {1, 2, Â· Â· Â· , 255}), respectively. Î² controls the trade-off between P i and R i . We set Î² 2 to 0.3, as suggested in <ref type="bibr" target="#b93">[94]</ref>. In the experiments, we utilize the maximum F-measure (for all thresholds), the adaptive Fmeasure (i.e., the threshold is defined as the double mean Performance of different models on the DUT <ref type="bibr" target="#b18">[19]</ref> dataset. Models are trained and tested on the DUT using the proposed training and test sets split from <ref type="bibr" target="#b18">[19]</ref>. A: handcrafted features-based methods. B: CNN-based methods. <ref type="bibr" target="#b94">[95]</ref> .607 .577 .691 LHM <ref type="bibr" target="#b31">[32]</ref> .568 .659 .767 DESM <ref type="bibr" target="#b34">[35]</ref> .659 .668 .733 DCMC <ref type="bibr" target="#b95">[96]</ref> .499 .406 .712 CDCP <ref type="bibr" target="#b35">[36]</ref> .687 .633 .794 B DMRA <ref type="bibr" target="#b18">[19]</ref> .888 .883 .927 A2dele <ref type="bibr" target="#b89">[90]</ref> .886 .892 .929 SSF <ref type="bibr" target="#b90">[91]</ref> .916 .924 .951 BBS-Net (ours)</p><formula xml:id="formula_17"># Method Dataset DUT [19] SÎ± â†‘ max F Î² â†‘ max E Î¾ â†‘ A MB</formula><p>.920 .927 .955</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3</head><p>Speed test of BBS-Net. We test the speed of only predicting the initial saliency map S1, and the final map S2, respectively. 'BS' denotes the batch size, of which the maximum value for a single GTX 1080Ti GPU is ten. 'io' is the time consumed by reading and writing. value of the saliency map) and mean F-measure (i.e., the average F-measure for all thresholds) to evaluate different methods. S-measure <ref type="bibr" target="#b91">[92]</ref> is a structure measure which combines the region-aware structural similarity (S r ) and object-aware structural similarity (S o ). It is defined as:</p><formula xml:id="formula_18">S Î± = Î± Ã— S o + (1 âˆ’ Î±) Ã— S r ,<label>(14)</label></formula><p>where Î± âˆˆ [0, 1] is a hyper-parameter to balance S r and S o . We set it to 0.5 as the default setting. E-measure, recently proposed by <ref type="bibr" target="#b92">[93]</ref>, is based on cognitive vision studies and utilizes both image-level and local pixel-level statistics for evaluating the binary saliency map. It is defined as:</p><formula xml:id="formula_19">E Î¾ = 1 w Ã— h w x=1 h y=1 Î¾(x, y),<label>(15)</label></formula><p>where w and h are the width and height of the saliency map. Î¾ is the enhanced alignment matrix. Similar to the F-measure, we also provide the results of max E-measure, adaptive E-measure and mean E-measure. MAE represents the average absolute error between the predicted saliency map and the ground truth. It is denoted as:</p><formula xml:id="formula_20">M = 1 N |S âˆ’ G|,<label>(16)</label></formula><p>where S and G are the predicted saliency map and groundtruth binary map, respectively. N represents the total number of pixels. PR curves are generated by a series of precision-recall (PR) pairs, which are calculated by the binarized saliency map with a threshold varying from 0 to 255. Specifically, the precision (P ) and recall (R) are calculated as:</p><formula xml:id="formula_21">P = |S âˆ© G| |S | , R = |S âˆ© G| |G| ,<label>(17)</label></formula><p>where S is the binarized mask for the predicted map S according to the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with SOTAs</head><p>â€¢ Contenders. We compare the proposed BBS-Net with ten algorithms based on handcrafted features <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b95">[96]</ref>- <ref type="bibr" target="#b98">[99]</ref> and eight methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b68">[69]</ref> that use deep learning. We train and test these methods using their default settings. For the methods without released source codes, we compare with their reported results.</p><p>â€¢ Quantitative Results. As shown in Tab. 2, Tab. 4, and <ref type="figure" target="#fig_5">Fig. 6</ref>, our method outperforms all algorithms based on handcrafted features as well as SOTA CNN-based methods by a large margin, in terms of all four evaluation metrics (i.e., S-measure (S Î± ), F-measure (F Î² ), E-measure (E Î¾ ) and MAE (M )). Performance gains over the best compared algorithms (ICCV'19 DMRA <ref type="bibr" target="#b18">[19]</ref> and CVPR'19 CPFP <ref type="bibr" target="#b20">[21]</ref>) are (2.5% âˆ¼ 3.5%, 0.7% âˆ¼ 3.9%, 0.8% âˆ¼ 2.3%, 0.009 âˆ¼ 0.016) for the metrics (S Î± , maxF Î² , maxE Î¾ , M ) on the seven challenging datasets. The PR curves of different methods on various datasets are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. It can be easily deduced from the PR curves that our method (i.e., solid red lines) outperforms all the SOTA algorithms. In terms of speed, BBS-Net achieves 14 fps on a single GTX 1080Ti GPU (batch size of one), as shown in Tab. 3. When fully exploiting the single GTX 1080Ti GPU (batch size of ten), Quantitative comparison of models using S-measure (SÎ±), adaptive F-measure (adpF Î² ), mean F-measure (meanF Î² ), max F-measure (maxF Î² ), adaptive E-measure (adpE Î¾ ), mean E-measure (meanE Î¾ ), max E-measure (maxE Î¾ ) and MAE (M ) scores on seven public datasets. â†‘ (â†“) denotes that the higher (lower) the score, the better. The best score in each row is highlighted in boldface. From left to right: ten models based on handcrafted features and eight CNNs-based models. 'S1' and 'S2' denote the initial and final saliency output results of the proposed method, respectively. BBS-Net can run at a speed of 48 fps, which is suitable for real-time applications.</p><p>There are three popular backbone models used in deep RGB-D models (i.e., VGG-16 <ref type="bibr" target="#b99">[100]</ref>, VGG-19 <ref type="bibr" target="#b99">[100]</ref> and ResNet-50 <ref type="bibr" target="#b67">[68]</ref>). To further validate the effectiveness of the proposed method, we provide performance comparisons using different backbones in Tab. 5. We find that ResNet-50 performs best among the three backbones, and VGG-19 and VGG-16 have similar performances. Besides, the proposed method exceeds the SOTA methods (e.g., TANet <ref type="bibr" target="#b17">[18]</ref>, CPFP <ref type="bibr" target="#b20">[21]</ref>, and DMRA <ref type="bibr" target="#b18">[19]</ref>) with any of the backbones.   Performance comparison using different backbone models. We experiment with multiple popular backbone models used in RGB-D SOD, including VGG-16 <ref type="bibr" target="#b99">[100]</ref>, VGG-19 <ref type="bibr" target="#b99">[100]</ref> and ResNet-50 <ref type="bibr" target="#b67">[68]</ref>.</p><p>Models NJU2K <ref type="bibr" target="#b66">[67]</ref> NLPR <ref type="bibr" target="#b31">[32]</ref> STERE <ref type="bibr" target="#b83">[84]</ref> DES <ref type="bibr" target="#b34">[35]</ref> LFSD <ref type="bibr" target="#b84">[85]</ref> SSD <ref type="bibr" target="#b85">[86]</ref> SIP <ref type="bibr" target="#b36">[37]</ref>  <ref type="bibr" target="#b17">[18]</ref> . â€¢ Visual Comparison. <ref type="figure" target="#fig_6">Fig. 7</ref> provides examples of maps predicted by our method and several SOTA algorithms. Visualizations cover simple scenes (a) and various challenging scenarios, including small objects (b), multiple objects (c), complex backgrounds (d), and low contrast scenes (e). First, the first row of (a) shows an easy example. The flower in the foreground is evident in the original RGB image, but the depth map is of low quality and contains some misleading information. The SOTA algorithms, such as DMRA and CPFP, fail to predict the whole extent of the salient object due to the interference from the depth map. Our method can eliminate the side-effects of the depth map by utilizing the complementary depth information more effectively. Second, two examples of small objects are shown in (b). Despite the handle of the teapot in the first row being tiny, our method can accurately detect it. Third, we show two examples with multiple objects in an image in (c). Our method locates all salient objects in the image. It segments the objects more accurately and generates sharper edges compared to other algorithms. Even though the depth map in the first row of (c) lacks clear information, our algorithm predicts the salient objects correctly. Fourth, (d) shows two examples with complex backgrounds. Here, our method produces reliable results, while other algorithms confuse the background as a salient object. Finally, (e) presents two examples in which the contrast between the object and the background is low. Many algorithms fail to detect and segment the entire extent of the salient object. Our method produces satisfactory results by suppressing background distractors and exploring the informative cues from the depth map. Comparison of different feature aggregation strategies on seven datasets. 1: Only aggregating the low-level features (Conv1âˆ¼3), 2: Only aggregating the high-level features (Conv3âˆ¼5), 3: Directly integrating all five-level features (Conv1âˆ¼5) by a single decoder, 4: Our model without the refinement flow, 5: High-level features (Conv3âˆ¼5) are first refined by the initial map aggregated by low-level features (Conv1âˆ¼3) and are then integrated to generate the final saliency map, and 6: The proposed cascaded refinement mechanism. # Settings NJU2K <ref type="bibr" target="#b66">[67]</ref> NLPR <ref type="bibr" target="#b31">[32]</ref> STERE <ref type="bibr" target="#b83">[84]</ref> DES <ref type="bibr" target="#b34">[35]</ref> LFSD <ref type="bibr" target="#b84">[85]</ref> SSD <ref type="bibr" target="#b85">[86]</ref> SIP <ref type="bibr" target="#b36">[37]</ref>  Ablation analysis of our BBS-Net on different datasets. 'BM' denotes the base model. 'CA' and 'SA' represent the channel attention and spatial attention mechanisms of the depth-enhanced module, respectively. 'PTM' is the progressively transposed module.</p><formula xml:id="formula_22">SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ TANet (VGG-16)</formula><formula xml:id="formula_23">SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ 1</formula><p># Settings NJU2K <ref type="bibr" target="#b66">[67]</ref> NLPR <ref type="bibr" target="#b31">[32]</ref> STERE <ref type="bibr" target="#b83">[84]</ref> DES <ref type="bibr" target="#b34">[35]</ref> LFSD <ref type="bibr" target="#b84">[85]</ref> SSD <ref type="bibr" target="#b85">[86]</ref> SIP <ref type="bibr" target="#b36">[37]</ref>  </p><formula xml:id="formula_24">BM CA SA PTM SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M â†“ SÎ± â†‘ M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>â€¢ Analysis of Different Aggregation Strategies. To validate the effectiveness of our cascaded refinement mechanism, we conduct several experiments to explore different aggregation strategies. Results are shown in Tab. 6 and <ref type="figure">Fig.  8</ref>. 'Low3' means that we only integrate the low-level features (Conv1âˆ¼3) using the decoder without the refinement from the initial map. Low-level features contain abundant details that are beneficial for refining the object edges, but at the same time introduce a lot of background distraction. Integrating only low-level features produces inadequate results and generates many distractors (e.g., the first and second rows in <ref type="figure">Fig. 8</ref>) or fails to locate the salient objects (e.g., the third row in <ref type="figure">Fig. 8</ref>). 'High3' only integrates the high-level features (Conv3âˆ¼5) to predict the saliency map. Compared with low-level features, high-level features contain more semantic information. As a result, they help locate the salient objects and preserve edge information. Thus, integrating high-level features leads to better results. 'All5' aggregates features from all five levels (Conv1âˆ¼5) directly, using a single decoder for training and testing. It achieves comparable results with the 'High3' but may include background noise introduced by the low-level features (see column 'All5' in <ref type="figure">Fig. 8</ref>). 'BBS-NoRF' indicates that we directly remove the refinement flow of our model. This leads to poor performance. 'BBS-RH' is a reverse refinement strategy to our cascaded refinement mechanism, where teacher features (Conv3âˆ¼5) are first refined by the initial map aggregated by low-level features (Conv1âˆ¼3) and are then integrated to generate the final saliency map. It performs worse than the proposed mechanism (BBS-RL), because noise in lowlevel features cannot be effectively suppressed in this reverse refinement strategy. Besides, compared to 'All5', our method fully utilizes the features at different levels, and thus achieves significant performance improvement (i.e., the last row in Tab. 6) with fewer background distractors and sharper edges (i.e., the last column in <ref type="figure">Fig. 8</ref>).</p><p>â€¢ Impact of Different Modules.  <ref type="figure">Fig. 9</ref>. Analysis of gradually adding various modules. The first three columns are the RGB, depth, and ground-truth images, respectively. '#' denotes the corresponding row of Tab. 7.</p><p>9. The base model (BM) is our BBS-Net without additional modules (i.e., CA, SA, and PTM). Note that the BM alone performs better than the SOTA methods over almost all datasets, as shown in Tab. 4 and Tab. 7. Adding the channel attention (CA) and spatial attention (SA) modules enhances the performance on most of the datasets (see the results shown in the second and third rows of Tab. 7). When we combine the two modules (the fourth row in Tab. 7), the performance is greatly improved on all datasets, compared to the BM. We can easily conclude from the '#3' and '#4' columns in <ref type="figure">Fig. 9</ref> that the spatial attention and channel attention mechanisms in DEM allow the model to focus on the informative parts of the depth features, which results in better suppression of background clutter. Finally, we add a progressively transposed block before the second decoder to gradually upsample the feature map to the same resolution as the ground truth. The results in the fifth row of Tab. 7 and the '#5' column of <ref type="figure">Fig. 9</ref> show that the 'PTM' achieves impressive performance gains on all datasets and generates sharper edges with finer details.</p><p>To further analyze the effectiveness of the cascaded decoder, we experiment with changing it to an element-wise summation mechanism. That is to say, we first change the features from different layers to the same dimension using 1 Ã— 1 convolution and upsampling operation and then fuse them by element-wise summation. Experimental results in Tab. 8 show that the cascaded decoder achieves comparable results on SIP, and outperforms the element-wise sum on the other six datasets, which demonstrates its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>In this section, we discuss four aspects pertaining to our model. In Â§ 5.1, we provide an analysis of the failure cases produced by our model. Then, we discuss the benefits of the depth information for SOD in Â§ 5.2. Further, we investigate the effects of different post-processing methods in Â§ 5.3. Finally, in Â§ 5.4, to provide a qualitative evaluation of different RGB-D datasets, we discuss the cross-dataset generalization ability of the widely used RGB-D datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Faliure Case Analysis</head><p>We illustrate six representative failure cases in <ref type="figure" target="#fig_0">Fig. 10</ref>. The failure examples are divided into four categories. In the first category, the model either misses the salient object or detects it imperfectly. For example, in column (a), our model fails to detect the salient object even when the depth map has clear boundaries. This is because the salient object has the same texture and content layout as the background in the RGB image. Thus, the model cannot find the salient object based only on the borders. In column (b), our method cannot fully segment the transparent salient objects, since the background has low contrast, and depth map lacks useful information. The second situation is that the model identifies the background as the salient part. For example, the lanterns in column (c) have a similar color to the background wallpaper, which confuses the model into thinking that the wallpaper is the salient object. Besides, the background of the RGB image in column (d) is complex and thus our model does not detect the complete salient objects. The third type of failure case is when an image contains several separate salient objects. In this case, our model may not detect them all. As shown in column (e), with five salient objects in the RGB images, the model fails to detect the two objects that are far from the camera. This may be because the model tends to consider the objects that are closer to the camera more salient. The final case is when salient objects are occluded by non-salient ones. Note that in column (f), the car is occluded by two ropes in front of the camera. Here our model predicts the ropes as salient objects. Most of these failure cases can be attributed to interference information from the background (e.g., color, contrast, and content). We propose some ideas that may be useful for solving these failure cases. The first is to introduce some human-designed prior knowledge, such as providing a boundary that can approximately distinguish the foreground from the background. Leveraging such prior knowledge, the model may better capture the characteristics of the background and salient objects. This strategy may contribute significantly to solving the failure cases especially for columns (a) and (b). Besides, the depth map can also be seen as a type of prior knowledge for this task. Thus, some failure cases (i.e., (b), (c) and (e)) may be solved when a high-quality depth map is available. Second, we find that in the current RGB-D datasets, the image pairs for challenging scenarios (e.g., complex backgrounds, low-contract backgrounds, transparent objects, multiple objects, shielded objects, and small objects) constitute a small fraction of the whole dataset. Therefore, adding more difficult examples to the training data could help mitigate the failure cases.  Finally, depth maps may sometimes introduce misleading information, such as in column (d). Considering how to exploit salient cues from the RGB image to suppress the noise in the depth map could be a promising solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Utility of Depth Information</head><p>To explore whether depth information can really contribute to the performance of SOD, we conduct two experiments, results of which are shown in Tab. 9. On the one hand, we compare the proposed method with five SOTA RGB SOD methods (i.e., PiCANet <ref type="bibr" target="#b100">[101]</ref>, PAGRN <ref type="bibr" target="#b47">[48]</ref>, R3Net <ref type="bibr" target="#b80">[81]</ref>, CPD <ref type="bibr" target="#b26">[27]</ref>, and PoolNet <ref type="bibr" target="#b15">[16]</ref>) by neglecting the depth information. We train and test CPD and PoolNet using the same training and test sets as our model. For other methods, we use the published results from <ref type="bibr" target="#b18">[19]</ref>. It is clear that the proposed methods (i.e., BBS-Net (w/ depth)) can significantly exceed SOTA RGB SOD methods thanks to depth information. On the other hand, we train and test the proposed method without using the depth information by setting the inputs of the depth branch to zero (i.e., BBSNet (w/o depth)). Comparing the results of the last two rows in the table, we find that depth information effectively improves the performance of the proposed model (especially over the small datasets, i.e., DES, LFSD, and SSD).</p><p>The two experiments together demonstrate the benefits of the depth information for SOD. Depth map serves as prior knowledge and provides spatial distance information and contour guidance to detect salient objects. For example, in <ref type="figure" target="#fig_0">Fig. 11, depth feature (b)</ref> has high activation on the object border. Thus, cross-modal feature (c) has clearer borders compared with the original RGB feature (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>GT BBS-Net BBS-Net+ADP BBS-Net+Ostu BBS-Net+CRF <ref type="figure" target="#fig_0">Fig. 12</ref>. Visual effects of different post-processing methods. We explore three methods, including the adaptive threshold cut ('ADP' in the paper), Ostu's method and the popular algorithm of conditional random fields (CRF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of Post-processing Methods</head><p>According to <ref type="bibr" target="#b101">[102]</ref>- <ref type="bibr" target="#b103">[104]</ref>, the predicted saliency maps can be further refined by post-processing methods. This may be useful to sharpen the salient edges and suppress the background response. We conduct several experiments to study the effects of various post-processing methods, including the adaptive threshold cut (i.e., the threshold is defined as the double of the mean value of the saliency map), Ostu's method <ref type="bibr" target="#b104">[105]</ref>, and conditional random field (CRF) <ref type="bibr" target="#b105">[106]</ref>. The performance comparisons of the post-processing methods in terms of MAE are shown in Tab. 12, while a visual comparison is provided in <ref type="figure" target="#fig_0">Fig. 12</ref>.</p><p>From the results, we draw the following conclusions. First, the three post-processing methods all make the salient edges sharper, as shown in the fourth to sixth columns in <ref type="figure" target="#fig_0">Fig. 12</ref>. Second, both Ostu and CRF help reduce the MAE effectively, as shown in Tab. 12. This is possibly because they can suppress the background noise. As shown in the third and fourth rows of <ref type="figure" target="#fig_0">Fig. 12</ref>, Ostu and CRF can significantly reduce the background noise, while the adaptive threshold operation further expands the background blur from the original results of BBS-Net. Further, the three postprocessing methods perform similarly when the original saliency map is of high quality (i.e., the first row in <ref type="figure" target="#fig_0">Fig. 12</ref>). They behave differently, however, when the predicted map is of low quality (i.e., the second to fourth rows in <ref type="figure" target="#fig_0">Fig. 12</ref>). In terms of overall results, CRF performs the best, while the adaptive threshold algorithm is the worst. Ostu performs worse than CRF, because it cannot always fully eliminate the background noise (e.g., the fifth and sixth columns in <ref type="figure" target="#fig_0">Fig. 12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Cross-Dataset Generalization Analysis</head><p>For a deep model to obtain reasonable performance in realworld scenarios, it not only requires an efficient design but must also be trained on a high-quality dataset with a great generalization power. A good dataset usually contains sufficient images, with all types of variations that occur in reality, so that deep models trained on it can generalize well to the real world. In the area of RGB-D SOD, there are several large-scale datasets (i.e., NJU2K, NLPR, STERE, SIP, and DUT), with around 1, 000 training images. Performance comparison when training with different datasets (i.e., NJU2K <ref type="bibr" target="#b66">[67]</ref>, NLPR <ref type="bibr" target="#b31">[32]</ref>, STERE <ref type="bibr" target="#b83">[84]</ref>, SIP <ref type="bibr" target="#b36">[37]</ref> and DUT <ref type="bibr" target="#b18">[19]</ref>    Performance comparison (MAE) of different post-processing strategies on seven datasets. The post-processing methods include the adaptive threshold cut algorithm (ADP), Ostu algorithm and conditional random fields (CRF). The row in gray shading represents the best method. â€¢ Single Dataset Generalization Analysis.</p><formula xml:id="formula_25">Drop â†“ SÎ± â†‘ F Î² â†‘ SÎ± â†‘ F Î² â†‘ SÎ± â†‘ F Î² â†‘ SÎ± â†‘ F Î² â†‘ SÎ± â†‘ F Î² â†‘ SÎ± â†‘ F Î² â†‘ SÎ± â†‘ F Î² â†‘ SÎ± F Î² NJU2K (1,400</formula><formula xml:id="formula_26">SÎ± â†‘ F Î² â†‘ M â†“ SÎ± â†‘ F Î² â†‘ M â†“ SÎ± â†‘ F Î² â†‘ M â†“ SÎ± â†‘ F Î² â†‘ M â†“ SÎ± â†‘ F Î² â†‘ M â†“ SÎ± â†‘ F Î² â†‘ M â†“ SÎ± â†‘ F Î² â†‘ M â†“ SÎ± â†‘ F Î² â†‘ M â†“ NJ+NL (2,100</formula><p>Here, we conduct cross-dataset generalization experiments on five datasets to measure their generalization ability. Following common split ratio strategies (e.g., CPFP <ref type="bibr" target="#b20">[21]</ref>, DMRA <ref type="bibr" target="#b18">[19]</ref>, and UC-Net <ref type="bibr" target="#b69">[70]</ref>), we create new training and test splits for NJU2K, NLPR, STERE, and SIP to carry out the experiments. For NJU2K, we randomly select 1, 400 image pairs for training and the remaining 585 images are used for testing. For NLPR, STERE, and SIP, we randomly choose 700 image pairs for training and the rest are used for testing. As for DUT, we maintain the original training-test split (i.e., 800 images for training and 400 images for testing) proposed by <ref type="bibr" target="#b18">[19]</ref>. We then retrain the proposed model on a single training set, and test it on all four test sets.</p><p>The results are summarized in Tab. 10. 'Self' represents the results of training and testing on the same dataset. 'Mean Others' indicates the average performance on all test sets except self. 'Drop' means the (percent) drop from 'Self' to 'Mean Others'. First, it can be seen from the table that DUT is the hardest dataset, since its 'Mean Others' of column 'DUT' is the lowest among the five datasets. This is because DUT includes multiple challenging scenes (e.g., transparent objects, multiple objects, complex backgrounds, etc). Second, STERE has the best generalization ability, because the drop is lowest among all five datasets. Besides, SIP generalizes worst (i.e., the drop is the largest among all five datasets), since it mainly focuses on a single person or multiple persons in the wild. We also notice that the score of the SIP column ('Mean Others') is the highest. This is likely because the quality of the depth maps captured by the Huawei Mate10 is higher than that produced by the traditional devices. Finally, none of the models trained with a single dataset perform best over all test sets. Thus, we further explore training on different combinations of datasets with the aim of building a dataset with a strong generalization ability for future research.</p><p>â€¢ Dataset Combination for Generalization Improvement.</p><p>According to the results in Tab. 10, the model trained on the SIP dataset does not generalize well to other datasets, so we discard it. We thus select four relatively large-scale datasets, i.e., NJU2K, NLPR, STERE, and DUT, to conduct our multi-dataset training experiments. As shown in Tab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11</head><p>, we consider all possible training combinations of these four datasets and test the models on all available test sets. From the results in the table, we draw the following conclusions. First, more training examples do not necessarily lead to better performance on some test sets. For example, although 'NJ+NL+ST', 'NJ+NL+DU' and 'NJ+NL+ST+DU' contain external training sets, unlike 'NJ+NL', they perform similarly with 'NJ+NL' on the test set of 'NL'. Second, including the NJU2K dataset is important for the model to generalize well to small datasets (i.e., LFSD, SSD). The model trained using the combinations without NJU2K (i.e., 'NL+ST' 'NL+DU', 'ST+DU' and 'NL+ST+DU') all obtain low F-measure values (less than 0.8) on the LFSD and SSD test sets. In contrast, including 'NL' in the training sets increases the F-measures on the LFSD and SSD datasets by over 0.05. Finally, including more examples in the training sets can improve the stability of the model, as it allows diverse scenarios to be taken into consideration. Thus, the model trained on 'NJ+NL+ST+DU', which has the most examples, obtains the best, or are very close to the best, performance. Due to the limited size of current RGB-D datasets, it is hard for a model trained using a single dataset to perform well under various scenarios. Thus, we recommend training a model using a combination of datasets with diverse examples to avoid model over-fitting issues. To promote the development of RGB-D SOD, we hope more challenging RGB-D datasets with diverse examples and high-quality depth maps can be proposed in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present a Bifurcated Backbone Strategy Network (BBS-Net) for the RGB-D SOD. To effectively suppress the intrinsic distractors in low-level cross-modal features, we propose to leverage the characteristics of multilevel cross-modal features in a cascaded refinement way: low-level features are refined by the initial saliency map that is produced by the high-level cross-modal features. Besides, we introduce a depth-enhanced module to excavate the informative cues from the depth features in the channel and spatial views, in order to improve the cross-modal compatibility when merging RGB and depth features. Experiments on eight challenging datasets demonstrate that BBS-Net outperforms 18 SOTA models, by a large margin, under multiple evaluation metrics. Finally, we conduct a comprehensive analysis of the existing RGB-D datasets and propose a powerful training set with a strong generalization ability for future research.</p><p>Yingjie Zhai is currently a PhD student with the Computer Vision Lab at College of Computer Science, Nankai University, Tianjin, China. His current research interests include deep learning and computer vision, especially salient object detection and image restoration. He is a Student Member of IEEE.</p><p>Deng-Ping Fan received his PhD degree from the Nankai University and then he joined Inception Institute of Artificial Intelligence (IIAI) in 2019. He has published about 20 top journal and conference papers such as CVPR, ICCV, etc. His research interests include computer vision, deep learning, and saliency detection, especially on co-salient object detection, RGB salient object detection, RGB-D salient object detection, and video salient object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Saliency maps of state-of-the-art (SOTA) CNN-based methods (i.e., DMRA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 . 1 âˆ¼ f rgb 5 )</head><label>315</label><figDesc>Overall architecture of the proposed BBS-Net. Feature Extraction: 'Conv1'âˆ¼'Conv5' denote different layers from ResNet-50 [68]. Multi-level features (f d 1 âˆ¼ f d 5 ) from the depth branch are enhanced by the DEM and are then fused with features (i.e., f rgb from the RGB branch. Stage 1: cross-modal teacher features (f cm 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>in which i âˆˆ {1, 2, 3}, k max = 3 or i âˆˆ {3, 4, 5}, k max = 5. F U P represents the upsampling operation if the features F DEM DEM (Depth-enhanced Module) Architecture of the depth-enhanced module (DEM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>PR Curves of the proposed model and 18 SOTA algorithms over six datasets. Dots on the curves represent the value of precision and recall at the maximum F-measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Ranking of 19 models from Tab. 4 in terms of max Fmeasure (maxF Î² ). Element (i,j) represents the number of times that model i ranks j th . Models are ranked by the mean rank (shown in brackets) over the seven datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative visual comparison of our model versus eight SOTA models. Unlike other models, our method not only locates the salient object accurately, but it also produces sharper edges with fewer background distractors in various scenarios, including simple scenes (a), small objects (b), multiple objects (c), complex backgrounds (d), and low contrast scenes (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Some representative failure cases of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Feature visualization. Here, (a), (b), and (c) are the average RGB feature, depth feature and cross-modal feature of the Conv3 layer. To visualize them, we average the feature maps along their channel axis to obtain the visualization map. 'Ours' refers to the BBS-Net (w/ depth).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Yingjie Zhai and Deng-Ping Fan contribute equally to this work. Yingjie Zhai, Deng-Ping Fan and Jufeng Yang are with College of Computer Science, Nankai University. (Email: zhaiyingjie@163.com, dengp-fan@gmail.com, yangjufeng@nankai.edu.cn) â€¢ Ali Borji is with HCL America, NYC, USA. (Email: aliborji@gmail.com) â€¢ Ling Shao is with the Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, and also with the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE. (Email: ling.shao@ieee.org)</figDesc><table /><note>â€¢â€¢â€¢ Junwei Han is with School of Automation, Northwestern Polytechnical University, China. (Email: junweihan2010@gmail.com)â€¢ Liang Wang is with the National Laboratory of Pattern Recognition, CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. (Email: wangliang@nlpr.ia.ac.cn)â€¢ A preliminary version of this work has appeared in ECCV 2020 [1].â€¢ Corresponding author: Jufeng Yang.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Note that we use the terms high-level features &amp; low-level features and teacher features &amp; student features interchangeably. arXiv:2007.02713v2 [cs.CV] 27 Aug 2020</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1</head><label>1</label><figDesc>Overview of the currently available RGB-D datasets.</figDesc><table><row><cell>#</cell><cell>Dataset</cell><cell>Size</cell><cell>Year</cell><cell>Resolution</cell><cell>Device</cell><cell>Type</cell></row><row><cell>1</cell><cell>STERE [84]</cell><cell>1,000</cell><cell>2012</cell><cell>[251 âˆ¼ 1, 200] Ã— [222 âˆ¼ 900]</cell><cell>Internet</cell><cell>Indoor/outdoor</cell></row><row><cell>2</cell><cell>NJU2K [67]</cell><cell>1,985</cell><cell>2014</cell><cell>[231 âˆ¼ 1, 213] Ã— [274 âˆ¼ 828]</cell><cell>Fuji W3 stereo camera</cell><cell>Internet/movies/photos</cell></row><row><cell>3</cell><cell>NLPR [32]</cell><cell>1,000</cell><cell>2014</cell><cell>640 Ã— 480, 480 Ã— 640</cell><cell>Remoulded Kinect</cell><cell>Indoor/outdoor</cell></row><row><cell>4</cell><cell>DES [35]</cell><cell>135</cell><cell>2014</cell><cell>640 Ã— 480</cell><cell>Micrsoft Kinect</cell><cell>Indoor scene</cell></row><row><cell>5</cell><cell>LFSD [85]</cell><cell>80</cell><cell>2014</cell><cell>360 Ã— 360</cell><cell>Lytro light field camera</cell><cell>Indoor/outdoor</cell></row><row><cell>6</cell><cell>SSD [86]</cell><cell>80</cell><cell>2017</cell><cell>960 Ã— 1, 080</cell><cell>Stere movies</cell><cell>Stere movies</cell></row><row><cell>7</cell><cell>DUT [19]</cell><cell>1,200</cell><cell>2019</cell><cell>600 Ã— 400</cell><cell>Commercial Lytro2 camera</cell><cell>Indoor (800)/outdoor (400)</cell></row><row><cell>8</cell><cell>SIP [37]</cell><cell>929</cell><cell>2020</cell><cell>744 Ã— 992</cell><cell>Huawei Mate10</cell><cell>Person in the wild</cell></row></table><note>LFSD [85] contains 60 image pairs from indoor scenes and 40 image pairs from outdoor scenes. The images were captured by a Lytro light field camera with a resolution of 360 Ã— 360. SSD [86] includes 80 images picked from three stereo movies with both indoor and outdoor scenes. The collected images have a high resolution of 960 Ã— 1, 080. SIP [37] consists of 1, 000 image pairs captured by a smart phone with a resolution of 992 Ã— 744, using a dual camera. This dataset focuses on salient persons in real-world scenes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Net LHM CDB DESM GP CDCP ACSD LBE DCMC MDSF SE DF AFNet CTMF MMCI PCF TANet CPFP DMRA Ours Ours SÎ± â†‘ .514 .624 .665 .527 .669 .699 .695 .686 .748 .664 .763 .772 .849 .858 .877 .878 .879 .886 .914 .921 adpF Î² â†‘ .638 .648 .632 .655 .624 .696 .740 .717 .757 .734 .804 .768 .788 .812 .844 .844 .837 .872 .889 .902 meanF Î² â†‘ .328 .482 .550 .357 .595 .512 .606 .556 .628 .583 .784 .764 .779 .793 .840 .841 .850 .873 .889 .902 maxF Î² â†‘ .632 .648 .717 .647 .621 .711 .748 .715 .775 .748 .650 .775 .845 .852 .872 .874 .877 .886 .911 .920 adpE Î¾ â†‘ .708 .745 .682 .716 .747 .786 .791 .791 .812 .772 .864 .846 .864 .878 .896 .893 .895 .908 .917 .924 meanE Î¾ â†‘ .447 .565 .590 .466 .706 .593 .655 .619 .677 .624 .835 .826 .846 .851 .895 .895 .910 .920 .930 .938 maxE Î¾ â†‘ .724 .742 .791 .703 .741 .803 .803 .799 .838 .813 .696 .853 .913 .915 .924 .925 .926 .927 .948 .949 M â†“ .205 .203 .283 .211 .180 .202 .153 .172 .157 .169 .141 .100 .085 .079 .059 .060 .053 .051 .040 .035 SÎ± â†‘ .630 .629 .572 .654 .727 .673 .762 .724 .805 .756 .802 .799 .860 .856 .874 .886 .888 .899 .923 .930 adpF Î² â†‘ .664 .613 .563 .659 .608 .535 .736 .614 .665 .692 .744 .747 .724 .730 .795 .796 .823 .854 .867 .882 meanF Î² â†‘ .427 .422 .430 .451 .609 .429 .626 .543 .649 .624 .664 .755 .740 .737 .802 .819 .840 .864 .881 .896 maxF Î² â†‘ .622 .618 .640 .611 .645 .607 .745 .648 .793 .713 .778 .771 .825 .815 .841 .863 .867 .879 .892 .918 adpE Î¾ â†‘ .813 .809 .698 .804 .800 .742 .855 .786 .812 .839 .868 .884 .869 .872 .916 .916 .924 .941 .947 .952 meanE Î¾ â†‘ .560 .565 .541 .571 .781 .578 .719 .684 .745 .742 .755 .851 .840 .841 .887 .902 .918 .940 .942 .950 maxE Î¾ â†‘ .766 .791 .805 .723 .820 .780 .855 .793 .885 .847 .880 .879 .929 .913 .925 .941 .932 .947 .959 .961 M â†“ .108 .114 .312 .146 .112 .179 .081 .117 .095 .091 .085 .058 .056 .059 .044 .041 .036 .031 .028 .023 SÎ± â†‘ .562 .615 .642 .588 .713 .692 .660 .731 .728 .708 .757 .825 .848 .873 .875 .871 .879 .835 .899 .908 adpF Î² â†‘ .703 .713 .594 .711 .666 .661 .595 .742 .744 .748 .742 .807 .771 .829 .826 .835 .830 .844 .867 .885 meanF Î² â†‘ .378 .489 .519 .405 .638 .478 .501 .590 .527 .610 .617 .806 .758 .813 .818 .828 .841 .837 .863 .883 maxF Î² â†‘ .683 .717 .700 .671 .664 .669 .633 .740 .719 .755 .757 .823 .831 .863 .860 .861 .874 .847 .892 .903 adpE Î¾ â†‘ .770 .808 .675 .784 .796 .793 .749 .831 .830 .825 .838 .886 .864 .901 .897 .906 .903 .900 .918 .925 meanE Î¾ â†‘ .484 .561 .579 .509 .751 .592 .601 .655 .614 .665 .691 .872 .841 .873 .887 .893 .912 .879 .917 .928 maxE Î¾ â†‘ .771 .823 .811 .743 .786 .806 .787 .819 .809 .846 .847 .887 .912 .927 .925 .923 .925 .911 .938 .942 M â†“ .172 .166 .295 .182 .149 .200 .250 .148 .176 .143 .141 .075 .086 .068 .064 .060 .051 .066 .048 .041 SÎ± â†‘ .578 .645 .622 .636 .709 .728 .703 .707 .741 .741 .752 .770 .863 .848 .842 .858 .872 .900 .929 .933 adpF Î² â†‘ .631 .729 .698 .686 .625 .717 .796 .702 .744 .726 .753 .730 .778 .762 .782 .795 .829 .866 .895 .906 meanF Î² â†‘ .345 .502 .483 .412 .585 .513 .576 .542 .523 .617 .604 .713 .756 .735 .765 .790 .824 .873 .896 .910 maxF Î² â†‘ .511 .723 .765 .597 .631 .756 .788 .666 .746 .741 .766 .728 .844 .822 .804 .827 .846 .888 .919 .927 adpE Î¾ â†‘ .761 .868 .759 .785 .816 .855 .911 .849 .869 .852 .877 .874 .911 .904 .912 .919 .927 .944 .966 .967 meanE Î¾ â†‘ .477 .572 .565 .503 .748 .612 .649 .632 .621 .707 .684 .810 .826 .825 .838 .863 .889 .933 .940 .949 maxE Î¾ â†‘ .653 .830 .868 .670 .811 .850 .890 .773 .851 .856 .870 .881 .932 .928 .893 .910 .923 .943 .965 .966 M â†“ .114 .100 .299 .168 .115 .169 .208 .111 .122 .090 .093 .068 .055 .065 .049 .046 .038 .030 .024 .021 SÎ± â†‘ .553 .515 .716 .635 .712 .727 .729 .753 .694 .692 .783 .738 .788 .787 .786 .801 .828 .839 .859 .864 adpF Î² â†‘ .714 .678 .676 .752 .695 .751 .705 .812 .795 .774 .802 .738 .778 .775 .788 .790 .809 .845 .850 .858 meanF Î² â†‘ .395 .374 .611 .516 .679 .562 .610 .652 .518 .636 .676 .732 .752 .718 .757 .767 .807 .841 .828 .843 maxF Î² â†‘ .708 .677 .762 .783 .702 .763 .722 .817 .779 .786 .813 .744 .787 .771 .775 .796 .826 .852 .854 .858 adpE Î¾ â†‘ .730 .696 .701 .776 .773 .794 .763 .835 .810 .777 .836 .802 .844 .832 .835 .838 .859 .892 .886 .889 meanE Î¾ â†‘ .488 .461 .632 .580 .748 .620 .664 .677 .583 .648 .719 .788 .802 .767 .810 .813 .856 .885 .871 .883 maxE Î¾ â†‘ .763 .871 .811 .824 .780 .829 .797 .856 .819 .832 .857 .815 .857 .839 .827 .847 .863 .893 .896 .901 M â†“ .218 .225 .253 .190 .172 .195 .214 .155 .197 .174 .145 .133 .127 .132 .119 .111 .088 .083 .079 .072 SÎ± â†‘ .566 .562 .602 .615 .603 .675 .621 .704 .673 .675 .747 .714 .776 .813 .841 .839 .807 .857 .878 .882 adpF Î² â†‘ .580 .628 .614 .749 .522 .656 .613 .679 .674 .693 .724 .694 .710 .748 .791 .767 .726 .821 .829 .849 meanF Î² â†‘ .367 .347 .502 .453 .515 .469 .489 .572 .470 .564 .624 .672 .689 .721 .777 .773 .747 .828 .829 .843 maxF Î² â†‘ .568 .592 .680 .740 .535 .682 .619 .711 .703 .710 .735 .687 .729 .781 .807 .810 .766 .844 .853 .859 adpE Î¾ â†‘ .730 .737 .683 .795 .705 .765 .729 .786 .772 .778 .812 .803 .838 .860 .886 .879 .832 .892 .903 .912 meanE Î¾ â†‘ .498 .477 .560 .529 .676 .566 .574 .646 .576 .631 .690 .762 .796 .796 .856 .861 .839 .897 .894 .904 maxE Î¾ â†‘ .717 .698 .769 .782 .700 .785 .736 .786 .779 .800 .828 .807 .865 .882 .894 .897 .852 .906 .922 .919 M â†“ .195 .196 .308 .180 .214 .203 .278 .169 .192 .165 .142 .118 .099 .082 .062 .063 .082 .058 .050 .044 SÎ± â†‘ .511 .557 .616 .588 .595 .732 .727 .683 .717 .628 .653 .720 .716 .833 .842 .835 .850 .806 .875 .879 adpF Î² â†‘ .592 .624 .644 .699 .495 .727 .733 .645 .694 .662 .673 .705 .684 .795 .825 .809 .819 .819 .862 .872 meanF Î² â†‘ .287 .341 .496 .411 .482 .542 .571 .499 .568 .515 .464 .702 .608 .771 .814 .803 .821 .811 .855 .868 maxF Î² â†‘ .574 .620 .669 .687 .505 .763 .751 .618 .698 .661 .657 .712 .694 .818 .838 .830 .851 .821 .877 .883 adpE Î¾ â†‘ .719 .771 .742 .774 .722 .827 .841 .786 .805 .756 .794 .815 .824 .886 .899 .893 .899 .863 .913 .916 meanE Î¾ â†‘ .437 .455 .564 .511 .683 .614 .651 .598 .645 .592 .565 .793 .705 .845 .878 .870 .893 .844 .898 .906 maxE Î¾ â†‘ .716 .737 .770 .768 .721 .838 .853 .743 .798 .771 .759 .819 .829 .897 .901 .895 .903 .875 .921 .922 M â†“ .184 .192 .298 .173 .224 .172 .200 .186 .167 .164 .185 .118 .139 .086 .071 .075 .064 .085 .060 .055</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>Hand-crafted-features-Based Models BBS-[32] [97] [35] [98] [36] [67] [29] [96] CNNs-Based Models [99] [28] [64] [30] [69] [23] [22] [18] [21] [19] (S 1 ) (S 2 )</cell></row><row><cell>NJU2K [67]</cell><cell></cell><cell></cell></row><row><cell>NLPR [32]</cell><cell></cell><cell></cell></row><row><cell>STERE [84]</cell><cell></cell><cell></cell></row><row><cell>DES [35]</cell><cell></cell><cell></cell></row><row><cell>LFSD [85]</cell><cell></cell><cell></cell></row><row><cell>SSD [86]</cell><cell></cell><cell></cell></row><row><cell>SIP [37]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 8</head><label>8</label><figDesc>Effectiveness analysis of the cascaded decoder in terms of the S-measure (SÎ±) on seven datasets.</figDesc><table><row><cell>Methods</cell><cell cols="3">NJU2K NLPR STERE DES SSD LFSD SIP [67] [32] [84] [35] [86] [85] [37]</cell></row><row><cell cols="2">Element-wise sum .915</cell><cell>.925</cell><cell>.897 .925 .868 .856 .880</cell></row><row><cell cols="2">Cascaded decoder .921</cell><cell>.930</cell><cell>.908 .933 .882 .864 .879</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 9 S</head><label>9</label><figDesc></figDesc><table><row><cell cols="4">-measure (SÎ±) comparison with SOTA RGB SOD methods on</cell></row><row><cell cols="4">seven datasets. 'w/o depth' and 'w/ depth' represent training</cell></row><row><cell cols="4">and testing the proposed method without/with the depth</cell></row><row><cell cols="4">information (i.e., the inputs of the depth branch are or are not</cell></row><row><cell></cell><cell cols="3">set to zeros).</cell></row><row><cell>Methods</cell><cell cols="3">NJU2K NLPR STERE DES LFSD SSD SIP [67] [32] [84] [35] [85] [86] [37]</cell></row><row><cell cols="2">PiCANet [101] .847</cell><cell>.834</cell><cell>.868 .854 .761 .832 -</cell></row><row><cell cols="2">PAGRN [48] .829</cell><cell>.844</cell><cell>.851 .858 .779 .793 -</cell></row><row><cell cols="2">R3Net [81] .837</cell><cell>.798</cell><cell>.855 .847 .797 .815 -</cell></row><row><cell cols="2">CPD [27] .894</cell><cell>.915</cell><cell>.902 .897 .815 .839 .859</cell></row><row><cell cols="2">PoolNet [16] .887</cell><cell>.900</cell><cell>.880 .873 .787 .773 .861</cell></row><row><cell cols="2">BBS-Net (w/o depth) .914</cell><cell>.925</cell><cell>.915 .912 .836 .855 .875</cell></row><row><cell cols="2">BBS-Net (w/ depth) .921</cell><cell>.930</cell><cell>.908 .933 .864 .882 .879</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 10</head><label>10</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>). The number in parentheses denotes the number of the corresponding training and test images. 'Self' indicates training and testing on the same dataset. 'Mean Others' represents the average performance on all test sets except 'self'. 'Drop' means the (percent) drop from 'Self' to 'Mean Others'.</figDesc><table><row><cell>Test</cell><cell>NJU2K (585)</cell><cell>NLPR (300)</cell><cell>STERE (300)</cell><cell>SIP (229)</cell><cell>DUT (400)</cell><cell>Self</cell><cell>Mean Others</cell></row><row><cell>Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 11</head><label>11</label><figDesc>Performance comparison when training with different combinations of multiple datasets (i.e., NJU2K<ref type="bibr" target="#b66">[67]</ref>, NLPR<ref type="bibr" target="#b31">[32]</ref>, STERE<ref type="bibr" target="#b83">[84]</ref> and SIP<ref type="bibr" target="#b36">[37]</ref>). 'NJ', 'NL', 'ST', 'SI' and 'DU' represent NJU2K, NLPR, STERE, SIP and DUT, respectively. The number in parentheses denotes the number of corresponding training and test images. The number of training images for each dataset is: NJ (1,400), NL (700), ST (700), SI (700), and DU (800). The row in gray shading represents the proposed training set. The corresponding training and test sets will be available at: https://drive.google.com/open?id=13bDYxibmryiwFtcm0xgmtIOpq1GVnXUM.</figDesc><table><row><cell>Test</cell><cell>NJ (585)</cell><cell>NL (300)</cell><cell>ST (300)</cell><cell>DES (135)</cell><cell>LFSD (80)</cell><cell>SSD (80)</cell><cell>SI (229)</cell><cell>DU (400)</cell></row><row><cell>Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 12</head><label>12</label><figDesc>) .920 .919 .037 .930 .920 .025 .904 .900 .041 .932 .923 .021 .851 .846 .081 .862 .845 .052 .884 .891 .053 .820 .788 .095 NJ+ST (2,100) .922 .923 .034 .873 .843 .046 .922 .918 .032 .920 .901 .026 .851 .847 .083 .870 .841 .049 .889 .887 .049 .787 .765 .099 NJ+DU (2,200) .921 .923 .035 .860 .823 .049 .889 .878 .047 .891 .854 .033 .862 .862 .076 .863 .841 .051 .838 .837 .073 .921 .929 .036 NL+ST (1,400) .773 .739 .109 .924 .913 .028 .913 .911 .038 .945 .941 .019 .723 .699 .146 .797 .757 .085 .896 .899 .049 .863 .851 .067 NL+DU (1,500) .799 .794 .092 .923 .913 .028 .890 .890 .048 .944 .941 .018 .769 .762 .116 .792 .765 .089 .889 .892 .053 .919 .927 .034 ST+DU (1,500) .821 .803 .081 .898 .882 .035 .915 .910 .035 .935 .927 .021 .788 .773 .109 .793 .758 .089 .905 .913 .043 .922 .929 .035 NJ+NL+ST (2,800) .921 .921 .035 .929 .918 .027 .924 .925 .032 .942 .938 .019 .854 .854 .081 .862 .833 .054 .897 .900 .046 .819 .803 .088 NJ+NL+DU (2,900) .921 .924 .035 .926 .915 .026 .898 .891 .046 .924 .912 .023 .863 .859 .073 .868 .838 .052 .888 .893 .048 .920 .927 .037 NJ+ST+DU (2,900) .923 .925 .035 .895 .874 .035 .919 .914 .034 .925 .912 .024 .855 .847 .079 .865 .838 .051 .891 .892 .049 .926 .934 .033 NL+ST+DU (2,200) .829 .820 .078 .929 .918 .025 .920 .919 .034 .938 .935 .019 .778 .770 .112 .820 .790 .073 .905 .910 .043 .922 .929 .035 NJ+NL+ST+DU (3,600) .923 .926 .034 .928 .916 .028 .923 .921 .032 .943 .943 .018 .863 .863 .073 .865 .840 .049 .903 .910 .043 .924 .930 .035</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://web.cecs.pdx.edu/fliu/ 2 http://mcg.nju.edu.cn/publication/2014/icip14-jur/index.html 3 https://sites.google.com/site/rgbdsaliency/home 4 https://github.com/HzFu/DES code 5 https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/ 6 https://github.com/ChunbiaoZhu/TPPF 7 https://github.com/jiwei0921/DMRA RGBD-SOD 8 http://dpfan.net/d3netbenchmark/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BBS-Net: RGB-D Salient Object Detection with a Bifurcated Backbone Strategy Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BING: binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Intelligent visual media processing: When graphics meets vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="121" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Repfinder: Finding approximately repeated scene elements for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="83" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semisupervised video salient object detection using pseudo-labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7284" to="7293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive object tracking by learning background context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a selfpaced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="207" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency prediction in the deep learning era: Successes and limitations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Salient object detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1734" to="1746" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth-induced multiscale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7254" to="7263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1605" to="1616" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for RGBD salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-Modal Weighting Network for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Salient object detection for RGB-D image by single stream recurrent convolution neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pdnet: Prior-model guided depth-enhanced network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Salient object detection for RGB-D image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Local background enclosure for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive fusion for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="55" to="277" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HSCS: Hierarchical sparsity based co-saliency detection for RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1660" to="1671" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">RGBD salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Salient region detection for stereoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="454" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narwaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2625" to="2636" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Internet Multimedia Computing and Service</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1509" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Frequencytuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Suppress and balance: A simple gated network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1529" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reverse attention-based residual network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3763" to="3776" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Selectivity or invariance: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3799" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A multistage refinement network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3534" to="3545" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Motion guided attention for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7274" to="7283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Progressive feature polishing network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">F3net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Video captioning with transferred semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="984" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Boundary-guided feature aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1800" to="1804" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">EGNet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7264" to="7273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Depth really matters: Improving visual salient region detection with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desingh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An in depth view of saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">RGBD salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Going from RGB to RGBD saliency: A depth-guided transformation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning RGB-D salient object detection using background enclosure, depth contrast, and top-down features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shigematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2749" to="2757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1115" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">CNNs-Based RGB-D saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8582" to="8591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Synergistic saliency and depth prediction for RGB-D saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01711</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3052" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cascade Graph Neural Networks for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">RGB-D Salient Object Detection: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00230</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="404" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">ACNet: Attention Based Network to Exploit Complementary Features for RGBD Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4039" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">R3Net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2806" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A three-pathway psychobiological framework of salient object detection using stereoscopic technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3008" to="3014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A2dele: Adaptive and Attentive Depth Distiller for Efficient RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9060" to="9069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Select, Supplement and Focus for RGB-D Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3472" to="3481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Structuremeasure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Frequencytuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A multilayer backpropagation saliency detection algorithm based on depth mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Stereoscopic saliency model using contrast and depth-guidedbackground prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="2227" to="2238" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Exploiting global priors for RGB-D saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Depthaware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">PiCANet: Learning Pixel-Wise Contextual Attention for Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Top-down visual saliency via joint crf and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="576" to="588" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7223" to="7233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
