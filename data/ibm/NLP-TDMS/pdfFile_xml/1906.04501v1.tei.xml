<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect-level Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinlong</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Applied Mathematics</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Hou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Combinatorics</orgName>
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Applied Mathematics</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect-level Sentiment Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sentiment classification</term>
					<term>Aspect-level</term>
					<term>Sentiment dependencies</term>
					<term>Graph convolutional networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-level sentiment classification aims to distinguish the sentiment polarities over one or more aspect terms in a sentence. Existing approaches mostly model different aspects in one sentence independently, which ignore the sentiment dependencies between different aspects. However, we find such dependency information between different aspects can bring additional valuable information. In this paper, we propose a novel aspect-level sentiment classification model based on graph convolutional networks (GCN) which can effectively capture the sentiment dependencies between multi-aspects in one sentence. Our model firstly introduces bidirectional attention mechanism with position encoding to model aspect-specific representations between each aspect and its context words, then employs GCN over the attention mechanism to capture the sentiment dependencies between different aspects in one sentence. We evaluate the proposed approach on the SemEval 2014 datasets. Experiments show that our model outperforms the state-of-the-art methods. We also conduct experiments to evaluate the effectiveness of GCN module, which indicates that the dependencies between different aspects is highly helpful in aspect-level sentiment classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aspect-level sentiment classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> is a fundamental natural language processing task that gets lots of attention in recent years. It is a fine-grained task in sentiment analysis, which aims to infer the sentiment polarities of aspects in their context. For example, in the sentence "The price is reasonable although the service is poor", the sentiment polarities for the two aspect terms, "price" and "service", are positive and negative respectively. An aspect term (or simply aspect) is usually an entity or an entity aspect.</p><p>Aspect-level sentiment classification is much more complicated than sentence-level sentiment classification, because identifying the parts of sentence describing the corresponding aspects is difficult. Traditional approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> mainly focus on statistical methods to design a set of handcrafted features to train a classifier (e.g., Support Vector Machine). However, such kind of feature-based work is labor-intensive. In recent years, neural network models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> are of growing interest for their capacity to automatically generate useful low dimensional representations from aspects and their contexts, and achieve great accuracy on the aspect-level sentiment classification without careful engineering of features. Especially, by the ability to effectively identify which words in the sentence are more important on a given aspect, attention mechanisms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> implemented by neural networks are widely used in aspect-level sentiment classification <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. Chen et al. <ref type="bibr" target="#b9">[10]</ref> model a multiple attention Email addresses: pinlongzhao@tju.edu.cn (Pinlong Zhao), llhou@mail.nankai.edu.cn (Linlin Hou), wuou@tju.edu.cn (Ou Wu) The setting is romantic, but the food is horrible, the service is pathetic.</p><p>setting food service aspect-1 aspect-2 aspect-3 opposite similar <ref type="figure">Fig. 1</ref>. An example to illustrate the usefulness of the sentiment dependencies between multiple aspects. The dependencies can be inferred by some knowledge in the sentence, e.g., conjunction. The evidence of the usefulness of the sentiment dependencies is that we can easily guess the true sentiment of "food" even if we mask the word "horrible".</p><p>mechanism with a gated recurrent unit network to capture the relevance between each context word and the aspect. Ma et al. <ref type="bibr" target="#b10">[11]</ref> design a model which learns the representations of the aspect and context interactively with two attention mechanisms. Song et al. <ref type="bibr" target="#b12">[13]</ref> propose an attentional encoder network, which employ multi-head attention for the modeling between context and aspect. These attention-based models have proven to be successful and effective in learning aspect-specific representations. Despite these advances, the studies above still remain problems. They all build models with each aspect individually ignoring the sentiment dependencies information between mul-tiple aspects, which will lose some additional valuable information. For example, as we can see from the example given in <ref type="figure">Fig. 1</ref>, the sentiment polarity of the first aspect "setting" is positive. From the conjunction "but", we are easy to know that the second aspect "food" has opposite sentiment polarity with "setting". By this sentiment dependency relation, we can guess the polarity of aspect "food" is negative. Similarly, from the second comma, we conjecture that the sentiment polarity of the last aspect "service" is likely the same as "food". Therefore, the sentiment dependencies are helpful to infer the sentiment polarities of aspects in one sentence.</p><p>In this paper, we propose a novel method to model Sentiment Dependencies with Graph Convolutional Networks (SDGCN) for aspect-level sentiment classification. GCN is a simple and effective convolutional neural network operating on graphs, which can catch inter-dependent information from rich relational data <ref type="bibr" target="#b14">[15]</ref>. For every node in graph, GCN encodes relevant information about its neighborhoods as a new feature representation vector. In our case, an aspect is treated as a node, and an edge represents the sentiment dependency relation of two nodes. Our model learns the sentiment dependencies of aspects via this graph structure. As far as we know, our work is the first to consider the sentiment dependencies between aspects in one sentence for aspect-level sentiment classification task. Furthermore, in order to capture the aspect-specific representations, our model applies bidirectional attention mechanism with position encoding before GCN. We evaluate the proposed approach on the SemEval 2014 datasets. Experiments show that our model outperforms the state-of-the-art methods. <ref type="bibr" target="#b0">1</ref> The main contributions of this paper are presented as follows:</p><p>• To the best of our knowledge, this is the first study to consider the sentiment dependencies between aspects in one sentence for aspect-level sentiment classification.</p><p>• We design bidirectional attention mechanism with position encoding to capture the aspect-specific representations.</p><p>• We propose a novel multi-aspects sentiment classification framework, which employs GCN to effectively capture the sentiment dependencies between different aspects in one sentence.</p><p>• We evaluate our method on the SemEval 2014 datasets. And experiments show that our model achieves superior performance over the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we will review related works on aspectlevel sentiment classification and graph convolutional network briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Aspect-level sentiment classification</head><p>Sentiment analysis, also known as opinion mining <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, is an important research topic in Natural Language Processing (NLP). Aspect-level sentiment classification is a fine-grained task in sentiment analysis. In aspect-level sentiment classification , early works mainly focus on extracting a set of features like bag-of-words features and sentiment lexicons features to train a sentiment classifier <ref type="bibr" target="#b17">[18]</ref>. These methods including rulebased methods <ref type="bibr" target="#b18">[19]</ref> and statistic-based methods <ref type="bibr" target="#b19">[20]</ref> rely on feature-engineering which are labor intensive. In recent years, deep neural network methods are getting more and more attention as they can generate the dense vectors of sentences without handcrafted features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. And the vectors are lowdimensional word representations with rich semantic information remained. Moreover, using the attention mechanism can enhance the sentence representation for concentrating on the key part of a sentence given an aspect <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Wang et al. <ref type="bibr" target="#b8">[9]</ref> propose ATAE-LSTM that combines LSTM and attention mechanism. The model makes embeddings of aspects to participate in computing attention weights. RAM is proposed by Chen et al. <ref type="bibr" target="#b9">[10]</ref> which adopts multiple-attention mechanism on the memory built with bidirectional LSTM. Ma et al. <ref type="bibr" target="#b10">[11]</ref> design a model with the bidirectional attention mechanism, which interactively learns the attention weights on context and aspect words respectively. Song et al. <ref type="bibr" target="#b12">[13]</ref> propose an attentional encoder network, which eschews recurrence and apply multi-head attention for the modeling between context and aspect. However, these attention works model each aspect separately in one sentence, which may loss some sentiment dependency information on multiple aspects case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph convolutional network</head><p>Graph convolutional network <ref type="bibr" target="#b25">[26]</ref> is effective at dealing with graph data which contains rich relation information. Many works dedicate to extending GCN for image tasks <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Chen et al. <ref type="bibr" target="#b30">[31]</ref> build the model via GCN for multi-label image recognition, which propagates information between multiple labels and consequently learns inter-dependent classifiers for each of image labels. GCN has also received growing attention in NLP recently such as semantic role labeling <ref type="bibr" target="#b31">[32]</ref>, machine translation <ref type="bibr" target="#b32">[33]</ref> and relation classification <ref type="bibr" target="#b33">[34]</ref>. Some works explore graph neural networks for text classification <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. They view a document, a sentence or a word as a graph node and rely on the relation of nodes to construct the graph. The studies above show that GCN can effectively capture relation between nodes. Inspired by these, we adopt GCN to get the sentiment dependencies between multi-aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Aspect-level sentiment classification can be formulated as follows. Given an input context consists of N words  <ref type="figure">Fig. 2</ref>. An example to illustrate the usefulness of the sentiment dependencies between multiple aspects. The dependencies can be inferred by some knowledge in the sentence, e.g., conjunction. The evidence of the usefulness of the sentiment dependencies is that we can easily guess the true sentiment of food even if we mask the word horrible.</p><formula xml:id="formula_0">W c = {w c 1 , w c 2 , . . . , w c N }, and K aspect teams W a = {W a 1 , W a 2 , . . . , W a K }. Each aspect W a i = {w a i 1 , w a i 2 , . . . , w a i M i } is a subsequence of sentence W c , which contains M i ∈ [1, N) P4 P3 Bi-LSTM Bi-LSTM Bi-LSTM Bi-LSTM Bi-LSTM Bi-LSTM Bi-LSTM</formula><p>words. It is required to construct a sentiment classifier that predicts the sentiment polarities of the multiple aspect teams.</p><p>We present the overall architecture of the proposed SDGCN in <ref type="figure">Fig. 2</ref>. It consists of the input embedding layer, the Bi-LSTM, the position encoding, the bidirectional attention mechanism, the GCN and the output layer. Next, we introduce all components sequentially from input to output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Input embedding layer</head><p>Input embedding layer maps each word to a high dimensional vector space. We employ the pretrained embedding matrix GloVe <ref type="bibr" target="#b36">[37]</ref> and pretrained model BERT <ref type="bibr" target="#b37">[38]</ref> to obtain the fixed word embedding of each word. Then each word will be represented by an embedding vector e t ∈ R d emb ×1 , where d emb is the dimension of word vectors. After embedding layer, the context embedding is denoted as a matrix E c ∈ R d emb ×N , and the i-th aspect embedding is denoted as a matrix E a i ∈ R d emb ×M i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bidirectional Long Short-Term Memory (Bi-LSTM)</head><p>We employ Bi-LSTM on top of the embedding layer to capture the contextual information for each word. After feeding word embedding to Bi-LSTM, the forward hidden state − → h t ∈ R d hid ×1 and the backward hidden state ← − h t ∈ R d hid ×1 are obtained, where d hid is the number of hidden units. We concatenate both the forward and the backward hidden state to form the final representation:</p><formula xml:id="formula_1">h t = [ − → h t , ← − h t ] ∈ R 2d hid ×1<label>(1)</label></formula><p>In our model, we employ two Bi-LSTM separately to get the sentence contextual hidden output</p><formula xml:id="formula_2">H c = [h c 1 , h c 2 , . . . , h c N ] ∈ R 2d hid ×N and each aspect contextual hidden output H a i = [h a i 1 , h a i 2 , . . . , h a i M i ] ∈ R 2d hid ×M i .</formula><p>Note that, the Bi-LSTM for each different aspect shares the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Position encoding</head><p>Based on the intuition that the polarity of a given aspect is easier to be influenced by the context words with closer distance to the aspect, we introduce position encoding to simulate this normal rules in natural language. Formally, given an aspect W a i that is one of the K aspects, where i ∈ [1, K] is the index of aspects, the relative distance d a i t between the t-th word and the i-th aspect is defined as follows:</p><formula xml:id="formula_3">d a i t =              1, dis = 0 1 − dis N , 1 ≤ dis ≤ s 0, dis &gt; s<label>(2)</label></formula><p>where dis is the distance between a context word and the aspect (here we treat an aspect as a single unit, and d = 0 means that the context word is also the aspect word), s is a pre-specified constant, and N is the length of the context. Finally, we can obtain the position-aware representation with position information:</p><formula xml:id="formula_4">p a i t = d a i t h c t P a i = P i = [p a i 1 , p a i 2 , . . . , p a i N ]<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Bidirectional attention mechanism</head><p>In order to capture the interactive information between the context and the aspect, we employ a bidirectional attention mechanism in our model. This mechanism consists of two modules: context to aspect attention module and aspect to context attention module. Firstly, the former module is used to get new representations of aspects based on the context. Secondly, based on the new representations, the later module is employed to obtain the aspect-specific context representations which will be fed into the downstream GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Context to aspect attention</head><p>Context to aspect attention learns to assign attention weights to the aspect words according to a query vector, where the query vector is h c ∈ R 2d hid ×1 which is obtained by average pooling operation over the context hidden output H c . For each hidden word vector h a i t ∈ R 2d hid ×1 in one aspect, the attention weight β a i t is computed as follows:</p><formula xml:id="formula_5">f ca (h c , h a i t ) = h c T · W ca · h a i t (4) β a i t = exp( f ca (h c , h a i t )) M i t=1 exp( f ca (h c , h a i t ))<label>(5)</label></formula><p>where W ca ∈ R 2d hid ×2d hid is the attention weight matrix. After computing the word attention weights, we can get the weighted combination of the aspect hidden representation as a new aspect representation:</p><formula xml:id="formula_6">m a i = M i t=1 β a i t · h a i t<label>(6)</label></formula><p>3.4.2. Aspect to context attention Aspect to context attention learns to capture the aspectspecific context representation, which is similar to context to aspect attention. Specifically, the attention scores is calculated by the new aspect representation m a i and the position-aware representation p a i t . The process can be formulated as follows:</p><formula xml:id="formula_7">f ac (m a i , p a i t ) = m a i T · W ac · p a i t (7) γ a i t = exp( f ac (m a i , p a i t )) N t=1 exp( f ac (m a i , p a i t ))<label>(8)</label></formula><formula xml:id="formula_8">x a i = x i = N t=1 γ a i t · h c t<label>(9)</label></formula><p>where W ac ∈ R 2d hid ×2d hid is the attention weight matrix. By now, we get the aspect-specific representations X = [x 1 , x 2 , . . . , x K ] between each aspect and its context words, where K is the number of aspects in the context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Graph convolutional network</head><p>GCN is widely used to deal with data which contains rich relationships and interdependency between objects, because GCN can effectively capture the dependence of graphs via message passing between the nodes of graphs. We also employ a graph to capture the sentiment dependencies between aspects. The final output of each GCN node is designed to be the classifier of the corresponding aspect in our task. Moreover, there are no explicit edges in our task. Thus, we need to define the edges from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Sentiment graph</head><p>We construct a graph, named sentiment graph, to capture the sentiment dependencies between multi-aspects in one sentence, where each node is regarded as an aspect and each edge is treated as the sentiment dependency relation. As shown in <ref type="figure">Fig. 3</ref>, we define two kinds of undirected sentiment graphs:</p><p>• adjacent-relation graph: An aspect is only connected to its nearby aspects.</p><p>• global-relation graph: An aspect is connected to all other aspects.</p><p>If two nodes are connected by an edge, it means that the two nodes are neighboring to each other. Formally, given a node v, we use N(v) to denote all neighbors of v. u ∈ N(v) means that u and v are connected with an edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Sentiment graph based GCN</head><p>GCN encodes relevant information about its neighborhood as a new representation vector, where each node in the graph indicates a representation of aspect. In addition, as Kipf et al. <ref type="bibr" target="#b14">[15]</ref> do, we assume all nodes contain self-loops. Then, the new node representation is computed as follows:</p><formula xml:id="formula_9">x 1 v = relu( u∈N(v) W cross x u + b cross ) + ReLU(W sel f x v + b sel f )<label>(10)</label></formula><p>where W cross , W sel f ∈ R d m ×d n , b cross , b sel f ∈ R d m ×1 , x u is the uth aspect-specific representation (see Eq. <ref type="formula" target="#formula_8">(9)</ref>), and ReLU is the   rectifier linear unit activation function. In this work, we use d m = d n = 2h hid . By stacking multiple GCN layers, the final hidden representation of each node can receive messages from a further neighborhood. Each GCN layer takes the node representations from previous layer as inputs and outputs new node representations:</p><formula xml:id="formula_10">x l+1 v = relu( u∈N(v) W l cross x l u + b l cross ) + relu(W l sel f x l v + b l sel f )<label>(11)</label></formula><p>where l denotes the layer number and 1 ≤ l ≤ L − 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Output layer</head><p>The final output of each GCN node x L i is treated as a classifier of the i-th aspect. At last, we use a fully-connected layer to map x L i into the aspect space of C classes:</p><formula xml:id="formula_11">z i = W z x L i + b z<label>(12)</label></formula><p>where W z ∈ R C×2d hid is the weight matrix, and b z ∈ R 2d hid ×C is the bias. The predicted probability of the i-th aspect with sentiment polarity j ∈ [1, C] is computed by:</p><formula xml:id="formula_12">y i j = exp(z i j ) C k=1 exp(z ik )<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Model training</head><p>Our model is trained by minimizing the cross entropy with L2-regularization term. For a given sentence, the loss function is defined as:</p><formula xml:id="formula_13">loss = K i=1 C j=1 y i j log(y i j ) + λ θ 2<label>(14)</label></formula><p>where y i j is a one-hot labels of the i-th aspect for the j-th class, λ is the coefficient for L2-regularization, θ is the parameters that need to be regularized. Furthermore, we adopt the dropout strategy during training step to avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data sets and experimental settings</head><p>To demonstrate the effectiveness of our proposed method, as most previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref>, we conduct experiments on two datasets from SemEval 2014 Task4 2 <ref type="bibr" target="#b38">[39]</ref>, which contains the reviews in laptop and restaurant. The details of the SemEval 2014 datasets are shown in <ref type="table" target="#tab_2">Table 1</ref>. Each dataset consists of train and test set. Each review (one sentence) contains one or more aspects and their corresponding sentiment polarities, i.e., positive, neutral and negative. To be specific, the number in table means the number of aspects in each sentiment category. To demonstrate the necessity of considering the sentiment dependencies between the aspects, we further calculate the number of aspects in each sentence, which is presented in <ref type="figure" target="#fig_2">Fig. 4</ref>. From the histogram in <ref type="figure" target="#fig_2">Fig. 4</ref>, we can see that each sentence contains one to thirteen aspects. The number of aspects in most reviews is 1 to 4. The pie chart shows the proportion of only one aspect and more than one aspect in one sentence. It can be seen that more than half of the aspects do not appear alone in a review. According to these statistics, we can conclude that it is common to have multi-aspects within one sentence. Our model mainly aims to model the sentiment dependencies between different aspects in one sentence. In our implementation, we respectively use the GloVe 3 <ref type="bibr" target="#b36">[37]</ref> word vector and the pre-trained language model word representation BERT 4 <ref type="bibr" target="#b37">[38]</ref> to initialize the word embeddings. The dimension of each word vector is 300 for GloVe and 768 for BERT. The number of LSTM hidden units is set to 300, and the output dimension of GCN layer is set to 600. The weight matrix of last fully connect layer is randomly initialized by a normal distribution N(0, 1). Besides the last fully connect layer, all the weight matrices are randomly initialized by a uniform distribution U(−0.01, 0.01). In addition, we add L2-regularization to the last fully connect layer with a weight of 0.01. During training, we set dropout to 0.5, the batch size is set to 32 and the optimizer is Adam Optimizer with a learning rate of 0.001. We implement our proposed model using Tensorflow 5 . To evaluate performance of the model, we employ Accuracy and Macro-F1 metrics. The Macro-F1 metric is more appropriate when the data set is not balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparative methods</head><p>To comprehensively evaluate the performance of proposed SDGAN, we compare our model with the following models.</p><p>• TD-LSTM <ref type="bibr" target="#b5">[6]</ref> constructs aspect-specific representation by the left context with aspect and the right context with aspect, then employs two LSTMs to model them respectively. The last hidden states of the two LSTMs are finally concatenated for predicting the sentiment polarity of the aspect.</p><p>• ATAE-LSTM <ref type="bibr" target="#b8">[9]</ref> first attaches the aspect embedding to each word embedding to capture aspect-dependent information, and then employs attention mechanism to get the sentence representation for final classification.</p><p>• MemNet <ref type="bibr" target="#b39">[40]</ref> uses a deep memory network on the context word embeddings for sentence representation to capture the relevance between each context word and the aspect. Finally, the output of the last attention layer is used to infer the polarity of the aspect.</p><p>• IAN <ref type="bibr" target="#b10">[11]</ref> generates the representations for aspect terms and contexts with two attention-based LSTM network separately. Then the context representation and the aspect representation are concatenated for predicting the sentiment polarity of the aspect.</p><p>• RAM [10] employs a gated recurrent unit network to model a multiple attention mechanism, and captures the relevance between each context word and the aspect. Then the output of the gated recurrent unit network is obtained for final classification.</p><p>• PBAN <ref type="bibr" target="#b40">[41]</ref> appends the position embedding into each word embedding. It then introduces a position-aware bidirectional attention network (PBAN) based on Bi-GRU to enhance the mutual relation between the aspect term and its corresponding sentence.</p><p>• TSN [25] is a two-stage framework for aspect-level sentiment analysis. The first stage, it uses a position attention to capture the aspect-dependent representation. The second stage, it introduces penalization term to enhance the difference of the attention weights towards different aspects in one sentence.</p><p>• AEN <ref type="bibr" target="#b12">[13]</ref> mainly consists of an embedding layer, an attentional encoder layer, an aspect-specific attention layer, and an output layer. In order to eschew the recurrence, it employs attention-based encoders for the modeling between the aspect and its corresponding context.</p><p>• AEN-BERT <ref type="bibr" target="#b12">[13]</ref> is AEN with BERT embedding. <ref type="table" target="#tab_3">Table 2</ref> shows the experimental results of competing models. In order to remove the influence with different word representations and directly compare the performance of different models, we compare GloVe-based models and BERT-based models separately. Our proposed model achieves the best performance on both GloVe-based models and BERT-based models, which demonstrates the effectiveness of our proposed model. In particularly, SDGCN-BERT obtains new state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Overall results</head><p>Among all the GloVe-based methods, the TD-LSTM approach performs worst because it takes the aspect information into consideration in a very coarse way. ATAE-LSTM, Men-Net and IAN are basic attention-based models. After taking the importance of the aspect into account with attention mechanism, they achieve a stable improvement comparing to the TD-LSTM. RAM achieves a better performance than other basic attention-based models, because it combines multiple attentions with a recurrent neural network to capture aspect-specific representations. PBAN achieves a similar performance as RAM by employing a position embedding. To be specific, PBAN is better than RAM on Restaurant dataset, but worse than RAN on Laptop dataset. Compared with RAM and PBAN, the overall performance of TSN is not perform well on both Restaurant dataset and Laptop dataset, which might because the framework of TSN is too simple to model the representations of context and aspect effectively. AEN is slightly better than TSN, but still worse than RAM and PBAN. It indicates that the discard of the recurrent neural networks can reduce the size of model while lead to the loss of performance.</p><p>Comparing the results of SDGCN-A w/o position and SDGCN-G w/o position, SDGCN-A and SDGCN-G, respectively, we observe that the GCN built with global-relation is slightly higher than built with adjacent-relation in both accuracy and Macro-F1 measure. This may indicate that the adjacent relationship is not sufficient to capture the interactive information among multiple aspects due to the neglect of the long-distance relation of aspects. Moreover, the two models (SDGCN-A and SDGCN-G) with position information gain a significant improvement compared to the two models without position information. It shows that the position encoding module is crucial for good performance.</p><p>Benefits from the power of pre-trained BERT, BERT-based models have shown huge superiority over GloVe-based models. Furthermore, compared with AEN-BERT, on the Restaurant dataset, SDGCN-BERT achieves absolute increases of 1.09% and 1.86% in accuracy and Macro-F1 measure respectively, and gains absolute increases of 1.42% and 2.03% in accuracy and Macro-F1 measure respectively on the Laptop dataset. The increments prove the effectiveness of our proposed SDGCN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The effect of GCN module</head><p>In this section, we design a series of models to further verify the effectiveness of GCN module. These models are:</p><p>• BiAtt+GCN is just another name of our proposed SDGCN model.</p><p>• BiAtt is based on BiAtt -GCN, where we remove the GCN module. Therefore, it predicts the sentiments of different aspects in one sentence independently.</p><p>• Att+GCN is a simplified version of BiAtt+GCN. The only difference between Att+GCN and BiAtt+GCN is that Att+GCN does not have context to aspect attention.  • Att is the model of Att+GCN removing the GCN module. <ref type="table" target="#tab_4">Table 3</ref> shows the performances of all these models. It is clear to see that, comparing with GCN-reduced models, the two models with GCN achieve higher performance, respectively. The results verify that the modeling of the sentiment dependencies between different aspects with GCN plays a great role in predicting the sentiment polarities of aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Impact of GCN layer number</head><p>The number of GCN layers is one very important setting parameter that affects the performance of our model. In order to investigate the impact of the GCN layer number, we conduct experiment with the different number of GCN layers from 1 to 8. The performance results are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. As can be seen from the results, in general, when the number of GCN layers is 2, the model works best. When the number of GCN layers is bigger than 2, the performance drops with the increase of the number of GCN layers on both the datasets. The possible reason for the phenomenon of the performance drop may be that with the increase of the model parameters, the model becomes more difficult to train and over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Case study</head><p>In order to have an intuitive understanding of the difference between with-GCN model (our proposed model) and without-GCN model, we use two examples with multiple aspects from laptop dataset as a case study. We draw heat maps to visualize the attention weights on the words computed by the two models, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. The deeper the color, the more attention the model pays to it.</p><p>As we can see from the first example, i.e., "i love the keyboard and the screen.", with two aspects "keyboard" and "screen", without-GCN model mainly focuses on the word "love" to predict the sentiment polarities of the two aspects. While for with-GCN model, besides the word "love", it also pays attention to the conjunction "and". This phenomenon indicates that with-GCN model captures the sentiment dependencies of the two aspects through the word "and", and then predicts the sentiments of "keyboard" and "screen" simultaneously.</p><p>The second example is "air has higher resolution but the fonts are small." with two aspects "resolution" and "fonts". It is obvious that the sentiments of the two aspects "resolution" and "fonts" are opposite connected by the conjunction "but". Without-GCN model predicts the polarity of aspect "resolution" by the word "higher and the polarity of aspect "fonts by the word "small" in isolation, which ignores the relation between the two aspects. In the contrary, with-GCN model enforces the model to pay attention on the word "but" when predicting the sentiment polarity for aspect "fonts".</p><p>From these examples, we can observe that our proposed model (with-GCN model) not only focuses the corresponding words which are useful for predicting the sentiment of each aspect, but also considers the textual information which is helpful for judging the relation between different aspects. By using attention mechanism to focus on the textual words describing the interdependence between different aspects, the downstream GCN module can effectively further represent the sentiment dependencies between different aspects in one sentence. With more useful information, our proposed model can predict aspect-level sentiment category more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we design a novel GCN based model (SDGCN) for aspect-level sentiment classification. The key idea of our model is to employ GCN to model the sentiment dependencies between different aspects in one sentence. Specifically, SDGCN first adopts bidirectional attention mechanism with position encoding to obtain aspect-specific representations, then captures the sentiment dependencies via message passing between aspects. Thus, SDGCN benefits from such dependencies which are always ignored in previous studies. Experiments on SemEval 2014 verify the effectiveness of the proposed mode, and SDGCN-BERT obtains new state-of-the-art results. The case study shows that SDGCN can not only pay attention to those words which are important for predicting the sentiment polarities of aspects, but also pay attention to the words which are helpful for judging the sentiment dependencies between different aspects.</p><p>In our future work, we will explore how to build a more precise sentiment graph structure between aspects. The two kinds of undirected sentiment graphs in this work are coarse. We conjecture that making use of textual information to define a graph may create a better graph structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>… a 1 … a 2 … a 3 … a 4 … a 5 Fig. 3 .</head><label>123453</label><figDesc>Illustration of our proposed sentiment graphs. a 1 , a 2 , a 3 , a 4 and a 5 denote five aspects in one context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Statistics of the number of aspects in one sentence on SemEval 2014 data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Comparisons with different depths of GCN in our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of attention weights obtained by model with GCN and without GCN respectively. (a) and (b) are two examples from the Laptop dataset. (a) Aspects: keyboard, screen; (b) Aspects: resolution, fonts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The details of the experimental data sets.</figDesc><table><row><cell>Data</cell><cell></cell><cell cols="3">Positive Negative Neutral</cell></row><row><cell>Restaurant</cell><cell>Train Test</cell><cell>2164 728</cell><cell>807 196</cell><cell>637 196</cell></row><row><cell>Laptop</cell><cell>Train Test</cell><cell>994 341</cell><cell>870 128</cell><cell>464 169</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with baseline models on the Restaurant dataset and Laptop dataset. The results of baseline models are retrieved from published papers. The best results in GloVe-based models and BERT-based models are all in bold separately. -A means that the model is based on adjacent-relation graph, and -G means the model is based on global-relation graph.</figDesc><table><row><cell cols="2">Word Embedding Models</cell><cell cols="2">Restaurant</cell><cell></cell><cell>Laptop</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell>Macro-F1</cell><cell>Acc</cell><cell>Macro-F1</cell></row><row><cell></cell><cell>TD-LSTM</cell><cell>75.63</cell><cell>-</cell><cell>68.13</cell><cell>-</cell></row><row><cell></cell><cell>ATAE-LSTM</cell><cell>77.20</cell><cell>-</cell><cell>68.70</cell><cell>-</cell></row><row><cell></cell><cell>MenNet</cell><cell>78.16</cell><cell>65.83</cell><cell>70.33</cell><cell>64.09</cell></row><row><cell></cell><cell>IAN</cell><cell>78.60</cell><cell>-</cell><cell>72.10</cell><cell>-</cell></row><row><cell></cell><cell>RAN</cell><cell>80.23</cell><cell>70.80</cell><cell>74.49</cell><cell>71.35</cell></row><row><cell>GloVe</cell><cell>PBAN TSN</cell><cell>81.16 80.1</cell><cell>--</cell><cell>74.12 73.1</cell><cell>--</cell></row><row><cell></cell><cell>AEN</cell><cell>80.98</cell><cell>72.14</cell><cell>73.51</cell><cell>69.04</cell></row><row><cell></cell><cell cols="2">SDGCN-A w/o p 81.61</cell><cell>72.22</cell><cell>73.20</cell><cell>68.54</cell></row><row><cell></cell><cell cols="2">SDGCN-G w/o p 81.61</cell><cell>72.93</cell><cell>73.67</cell><cell>68.70</cell></row><row><cell></cell><cell>SDGCN-A</cell><cell>82.14</cell><cell>73.47</cell><cell>75.39</cell><cell>70.04</cell></row><row><cell></cell><cell>SDGCN-G</cell><cell>82.95</cell><cell>75.79</cell><cell>75.55</cell><cell>71.35</cell></row><row><cell>BERT</cell><cell>AEN-BERT SDGCN-BERT</cell><cell>83.12 83.57</cell><cell>73.76 76.47</cell><cell>79.93 81.35</cell><cell>76.31 78.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The effect of GCN.</figDesc><table><row><cell>Models</cell><cell cols="2">Restaurant</cell><cell cols="2">Laptop</cell></row><row><cell></cell><cell cols="4">Acc Macro-F1 Acc Macro-F1</cell></row><row><cell>Att</cell><cell>81.43</cell><cell>72.40</cell><cell>72.12</cell><cell>68.67</cell></row><row><cell>Att+GCN</cell><cell>82.77</cell><cell>74.33</cell><cell>74.61</cell><cell>70.33</cell></row><row><cell>BiAtt</cell><cell>81.61</cell><cell>73.49</cell><cell>73.51</cell><cell>69.73</cell></row><row><cell cols="2">BiAtt+GCN (SDGCN) 82.95</cell><cell>75.79</cell><cell>75.55</cell><cell>71.35</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code is available at https://github.com/Pinlong-Zhao/ SDGCN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The detailed introduction of this task can be found at http://alt.qcri. org/semeval2014/task4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://nlp.stanford.edu/projects/glove/ 4 https://github.com/google-research/bert#pre-trained-models 5 https://www.tensorflow.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nrccanada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dcu: Aspect-based polarity classification for semeval task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international workshop on semantic evaluation</title>
		<meeting>the 8th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A deeper look into sarcastic tweets using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1601" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5876" to="5883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Attentional encoder network for targeted sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<idno>CoRR abs/1902.09314</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Featurebased compositing memory networks for aspect-based sentiment classification in social internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Sangaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Liaqat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Comp. Syst</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="879" to="888" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentiment analysis through recurrent variants latterly on convolutional neural network of twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Comp. Syst</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="292" to="308" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised polarity lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="675" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 international conference on web search and data mining</title>
		<meeting>the 2008 international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shirai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep memory networks for attitude identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="671" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transformation networks for targetoriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling multi-aspects within one opinionated sentence simultaneously for aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Comp. Syst</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="304" to="311" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
		<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Spectral networks and locally connected networks on graphs</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR abs/1506.05163</idno>
		<title level="m">Deep convolutional networks on graphstructured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Urtasun, 3d graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="5209" to="5218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Factorizable net: An efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="346" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno>CoRR abs/1904.03582</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sima&apos;an, Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing7</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing7</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Classifying relations in clinical narratives using segment graph convolutional and recurrent neural networks (seg-gcrns)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="262" to="268" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large-scale hierarchical text classification with recursively regularized deep graph-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1063" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sentence-state LSTM for text representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="317" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>SemEval@COLING; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Semeval-2014 task 4: Aspect based sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A position-aware bidirectional attention network for aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-20" />
			<biblScope unit="page" from="774" to="784" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
